

## Papers for 2024-12-03

| Title | Authors | Summary |
|-------|---------|---------|
| X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models (Read more on [arXiv](https://arxiv.org/abs/2412.01824) or [HuggingFace](https://huggingface.co/papers/2412.01824))| lindahua, TheYJ, yuhangzang, tongwu2020, Zery | X-Prompt enhances in-context image generation in auto-regressive vision-language models.  The research aimed to improve auto-regressive VLM performance across diverse seen and unseen image generation tasks within a unified in-context learning framework. The key methodology involved compressing in-context example features into fixed-length tokens, unifying image generation and description tasks, and using a retrieval-augmented image editing strategy.  On the GenEval benchmark, X-Prompt with text prediction improved overall text-to-image generation by 0.08 compared to the baseline Chameleon model. This research provides AI practitioners with a method for enhancing the generalizability and efficiency of auto-regressive VLMs in diverse image generation applications, by enabling effective in-context learning with shorter context lengths.  |
| GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation (Read more on [arXiv](https://arxiv.org/abs/2411.18499) or [HuggingFace](https://huggingface.co/papers/2411.18499))| LiruiZhao, yefly, xuzhaopan, xiaopengpeng, lyuukuu | OpenING is a new benchmark for evaluating open-ended interleaved image-text generation.  The research aimed to create a comprehensive benchmark and robust judge model for open-ended interleaved image-text generation.  The authors curated a dataset of 5,400 human-annotated instances across 56 real-world tasks and developed a judge model, IntJudge, trained with a novel reference-augmented generation approach.  IntJudge achieved an 82.42% agreement rate with human judgments, outperforming GPT-based evaluators by 11.34%. AI practitioners can use OpenING to evaluate and benchmark new interleaved generation models and IntJudge as a more robust automated evaluation tool compared to GPT-based judges.  |
| Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis (Read more on [arXiv](https://arxiv.org/abs/2412.01819) or [HuggingFace](https://huggingface.co/papers/2412.01819))| Dmitry Baranchuk, Valentin Khrulkov, Mikhail Khoroshikh, Anton Voronov, SpiridonSunRotator | SWITTI is a scale-wise transformer model for text-to-image synthesis designed for improved speed and quality.  The research aimed to develop a faster, higher-quality text-to-image generation model using a scale-wise transformer architecture while investigating the role of autoregression and text conditioning across scales.  The key methodology involved modifying a scale-wise autoregressive transformer architecture to improve training stability, removing the autoregressive component based on analysis of attention maps, and disabling classifier-free guidance at the highest resolution scales.  SWITTI achieves comparable performance to state-of-the-art diffusion models on automated metrics and human evaluations while being up to 7x faster, with a single-step generation time of 9.5 milliseconds for a batch of 8 512x512 images on an NVIDIA A100 80GB GPU.    The removal of the autoregressive component and disabling of classifier-free guidance at later stages significantly improved sampling speed while maintaining or slightly enhancing quality, offering practitioners a more efficient model for text-to-image generation.  |
| Open-Sora Plan: Open-Source Large Video Generation Model (Read more on [arXiv](https://arxiv.org/abs/2412.00131) or [HuggingFace](https://huggingface.co/papers/2412.00131))| Xinhua Cheng, Yunyang Ge, Lin-Chen, BestWishYsh, LanguageBind | Open-Sora Plan is an open-source project for generating high-resolution, long-duration videos.  The objective is to develop a large generation model capable of producing desired videos from various user inputs, including text, images, and structure control signals.  The project uses a Wavelet-Flow Variational Autoencoder (WF-VAE), a Joint Image-Video Skiparse Denoiser with 3D attention, and various condition controllers, along with training and inference optimization strategies like a min-max token strategy and adaptive gradient clipping.  WF-VAE-L achieves a throughput of 5.55 videos/second when encoding 33-frame 512x512 videos, 7.8 times faster than Allegro with 8 times less memory usage.  This project offers AI practitioners a comprehensive framework and efficient methods for developing and implementing high-quality video generation models.  |
| TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video (Read more on [arXiv](https://arxiv.org/abs/2411.18671) or [HuggingFace](https://huggingface.co/papers/2411.18671))| Zhaoyang Zeng, Tianhe Ren, Shilong Liu, Hongyang Li, Jinyuan Qu | TAPTRv3 enhances point tracking robustness in long videos using spatial and temporal context.  The research aimed to improve the long-video tracking performance of TAPTRv2, which struggles with feature querying due to increasing target variation and scene cuts.  The authors introduce Context-aware Cross-Attention (CCA) and Visibility-aware Long-Temporal Attention (VLTA) to enhance spatial and temporal feature querying, respectively, along with a global matching module for scene cut handling.  TAPTRv3 achieves state-of-the-art performance on multiple datasets, showing a 9.3 average Jaccard (AJ) improvement over TAPTRv2 on long video datasets (Kinetics, RGB-Stacking, and RoboTAP). This allows AI practitioners to implement more accurate and robust point tracking in long videos for applications such as video editing, SLAM, and robotic manipulation, even without large amounts of real training data.  |
| o1-Coder: an o1 Replication for Coding (Read more on [arXiv](https://arxiv.org/abs/2412.00154) or [HuggingFace](https://huggingface.co/papers/2412.00154))| Jinlin Xiao, Jiangming Shu, Yuqi Yang, Shangxi Wu, Yuxiang Zhang | O1-CODER replicates OpenAI's o1 model, focusing on coding tasks.  The objective is to enhance a language model's System-2 thinking (deliberate, analytical processing) for code generation using reinforcement learning (RL) and Monte Carlo Tree Search (MCTS).  The methodology involves training a Test Case Generator, using MCTS to generate reasoning-enhanced code data, and iteratively fine-tuning a policy model with a process reward model.  Pseudocode-based code generation with Qwen2.5-Coder-7B achieved an Average Sampling Pass Rate (ASPR) of 74.9% on the MBPP benchmark, significantly exceeding vanilla Qwen2.5-7B's 49.3% ASPR. This implies that generating accurate pseudocode is crucial for correct code generation, highlighting the importance of methods like RL and MCTS for refining the reasoning process in LLMs for coding tasks.  |
| TinyFusion: Diffusion Transformers Learned Shallow (Read more on [arXiv](https://arxiv.org/abs/2412.01199) or [HuggingFace](https://huggingface.co/papers/2412.01199))| Xinchao Wang, Xinyin Ma, Kunjun Li, Gongfan Fang | TinyFusion is a learnable depth pruning method for compressing diffusion transformers.  The objective is to create shallower diffusion transformer models with reduced inference costs while maintaining competitive post-fine-tuning performance.  The method utilizes a differentiable sampling technique for layer mask selection, co-optimized with a weight update (using LoRA or full fine-tuning) to estimate recoverability. Experiments on DiT-XL show TinyFusion achieves an FID score of 2.86 after pruning to 14 layers and fine-tuning with Masked Knowledge Distillation, using only 7% of the original training cost. This allows AI practitioners to significantly reduce the computational cost of deploying diffusion transformers for image generation without drastically sacrificing generative quality.  |
| VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models (Read more on [arXiv](https://arxiv.org/abs/2412.01822) or [HuggingFace](https://huggingface.co/papers/2412.01822))| Yueh-Hua Wu, Yong Man Ro, Yu-Chiang Frank Wang, Ryo Hachiuma, BK-Lee | VLsI is a new family of efficient vision-language models (VLMs) in 2B and 7B sizes.  The research aimed to develop smaller VLMs that perform comparably to larger models without architectural changes.  The key methodology involves layer-wise distillation using intermediate "verbalizers" that map each layer's output to natural language, aligning the smaller VLM's reasoning process with a larger one.  VLsI-7B achieved a 17.4% performance improvement over GPT-4V on ten vision-language benchmarks.  AI practitioners can utilize VLsI's layer-wise verbalization technique for efficient VLM distillation, enabling deployment on resource-constrained devices without significant performance degradation.  |
| WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model (Read more on [arXiv](https://arxiv.org/abs/2411.17459) or [HuggingFace](https://huggingface.co/papers/2411.17459))| Liuhan Chen, Yang Ye, Zongjian Li, BestWishYsh, LanguageBind | WF-VAE enhances video reconstruction quality and computational efficiency for latent video diffusion models. The research aimed to address the computational bottlenecks and latent space discontinuities in existing video VAEs, particularly for long, high-resolution videos. The authors introduce Wavelet Flow VAE (WF-VAE), leveraging multi-level wavelet transforms to prioritize low-frequency information and a Causal Cache mechanism for lossless block-wise inference.  WF-VAE-L achieves a PSNR of 35.87 and an LPIPS of 0.0175 on the Panda70M dataset with 16 latent channels, outperforming CogVideoX VAE in these metrics. This improvement enables AI practitioners to train and deploy more efficient and higher-quality video generation models, especially for resource-intensive, large-scale applications.  |
| SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters (Read more on [arXiv](https://arxiv.org/abs/2412.00174) or [HuggingFace](https://huggingface.co/papers/2412.00174))| Huaizhong Zhang, Zhengyu Lin, Weiye Xiao, Jianping Jiang, caizhongang | SOLAMI is a novel end-to-end social Vision-Language-Action (VLA) framework for immersive interaction with 3D autonomous characters.  The research aimed to create 3D autonomous characters capable of perceiving, understanding, and interacting with humans in immersive environments using multiple modalities.  The researchers developed a unified social VLA architecture trained on a synthesized multimodal social interaction dataset (SynMSI) and implemented in a VR interface. SOLAMI achieved a lower inference latency (2.639 seconds) than the LLM+Speech and DLP baseline methods.  This lower latency, coupled with improved performance in motion quality and context relevance, indicates that an end-to-end VLA model like SOLAMI can enable more natural and responsive real-time interactions with 3D characters in immersive applications.  |
| Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation (Read more on [arXiv](https://arxiv.org/abs/2412.01316) or [HuggingFace](https://huggingface.co/papers/2412.01316))| Yuan Zhou, Qiuyue Wang, Yuxuan Cai, hyang0511, Cakeyan | Presto generates 15-second videos with enhanced content richness and long-range coherence.  The research aimed to address the challenges of generating long videos with diverse scenarios and consistent storylines.  The core methodology involves Segmented Cross-Attention (SCA), dividing hidden states into segments that cross-attend to corresponding sub-captions, and a curated LongTake-HD dataset of long videos with progressive sub-captions. Presto achieved a 78.5% VBench Semantic Score, outperforming state-of-the-art models.  This provides AI practitioners with a novel architecture and dataset for generating longer, more coherent, and content-rich videos using diffusion models.  |
| Collaborative Instance Navigation: Leveraging Agent Self-Dialogue to Minimize User Input (Read more on [arXiv](https://arxiv.org/abs/2412.01250) or [HuggingFace](https://huggingface.co/papers/2412.01250))| Alessandro Farinelli, Alberto Castellini, Gianni Franchi, e-zorzi, ftaioli | AIUTA enables embodied agents to locate target objects in unknown environments through collaborative dialogue with users.  The research addresses the challenge of instance navigation with minimal initial user input. The proposed method, AIUTA (Agent-user Interaction with Uncertainty Awareness), utilizes a self-questioning module with a VLM and LLM to refine object descriptions and an interaction trigger to determine when to query the user.  On the CoIN-Bench with simulated users, AIUTA achieved a 14.47% success rate on the Train split, substantially outperforming a zero-shot baseline that lacked user interaction. This work provides a framework for building more practical and user-friendly instance navigation systems by reducing the burden of providing detailed upfront instructions.  |
| VLSBench: Unveiling Visual Leakage in Multimodal Safety (Read more on [arXiv](https://arxiv.org/abs/2411.19939) or [HuggingFace](https://huggingface.co/papers/2411.19939))| Jing Shao, Xuanjing Huang, LLLeo612, Max9803, Foreshhh | VLSBench, a new multimodal safety benchmark, is designed to address visual safety information leakage (VSIL) in existing multimodal datasets.  The research aimed to understand why textual alignment performs comparably to multimodal alignment on existing multimodal safety benchmarks, suspecting a VSIL problem. The authors constructed VLSBench with 2.4k image-text pairs, preventing leakage from image to text through an automated pipeline involving harmful query generation, detoxification, iterative image generation, and filtration.  Multimodal alignment methods outperformed textual alignment methods on VLSBench, with the best close-source model (Gemini-1.5-pro) achieving a 49.78% safety rate. This highlights the need for AI practitioners to prioritize multimodal alignment over textual alignment when addressing safety in multimodal models, especially in scenarios where sensitive visual content is not explicitly described in the text.  |
| INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge (Read more on [arXiv](https://arxiv.org/abs/2411.19799) or [HuggingFace](https://huggingface.co/papers/2411.19799))| atcbosselut, jjzha, jebish7, shayekh, angelika | INCLUDE benchmarks multilingual LLMs' understanding of regional knowledge.  The study investigates how large language models perform on questions requiring cultural and regional knowledge across diverse languages.  Researchers compiled a novel dataset of 197,243 multiple-choice questions from local exams in 44 languages and 15 scripts, avoiding translation artifacts by using original-language sources and annotating questions for regionality and academic domain.  GPT-4 achieved the highest overall accuracy of 77.1% on the INCLUDE-BASE subset.  AI practitioners should account for regional knowledge variance when developing and evaluating multilingual LLMs and consider that model performance varies considerably based on language and question type, even within a single model.  |
| Efficient Track Anything (Read more on [arXiv](https://arxiv.org/abs/2411.18933) or [HuggingFace](https://huggingface.co/papers/2411.18933))| Chenchen Zhu, Lemeng Wu, Xiaoyu Xiang, Chong Zhou, yunyangx | EfficientTAMs are lightweight models for video object segmentation and tracking with reduced computational complexity compared to SAM 2.  The research aimed to create more efficient track-anything models with low latency and small model size, suitable for mobile deployment. The methodology involves utilizing a vanilla Vision Transformer (ViT) as the image encoder and introducing an efficient memory module based on coarser representations of memory spatial tokens for cross-attention. On the SA-V test dataset for semi-supervised video object segmentation, EfficientTAM-S achieves 74.5 J&F, comparable to SAM 2, with ~2x speedup on A100 GPUs and ~2.4x parameter reduction.  This allows AI practitioners to deploy real-time video object segmentation models on resource-constrained devices, such as mobile phones, broadening the potential applications of this technology.  |
| VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information (Read more on [arXiv](https://arxiv.org/abs/2412.00947) or [HuggingFace](https://huggingface.co/papers/2412.00947))| Rui Zhang, Ranran Haoran Zhang, Sarkar Snigdha Sarathi Das, Yusen Zhang, ryokamoi | VisOnlyQA, a new dataset, reveals that Large Vision Language Models (LVLMs) struggle with visual perception of geometric information in scientific figures.  The research aimed to evaluate the visual perception capabilities of LVLMs independent of reasoning and knowledge.  The authors created VisOnlyQA, including real and synthetically generated scientific figures paired with multiple-choice questions about geometric and numerical information, and tested 20 different LVLMs.  State-of-the-art models like GPT-40 and Gemini 1.5 Pro achieved only 51.4% and 54.2% accuracy respectively on the real image split, compared to near-perfect human performance (93.5%). The principal implication for AI practitioners is that both training data and model architectures need improvement to enhance the visual perception capabilities of LVLMs, as this weakness significantly limits performance on visual tasks.  |
| VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation (Read more on [arXiv](https://arxiv.org/abs/2412.00927) or [HuggingFace](https://huggingface.co/papers/2412.00927))| Wenhu Chen, Cong Wei, Jie Min, hyang0511, wren93 | VISTA improves long and high-resolution video understanding in Large Multimodal Models (LMMs) through data augmentation.  The research aimed to address the scarcity of high-quality, long/high-resolution video instruction-following datasets.  The key methodology involved spatially and temporally combining videos from existing datasets to create synthetic long and high-resolution video samples, followed by generating corresponding question-answer pairs using a language model (Gemini).  Finetuning LMMs on VISTA-400K resulted in an average 3.3% improvement across four long-video understanding benchmarks and a 6.5% gain on the newly introduced HRVideoBench for high-resolution video understanding.  This provides AI practitioners with a cost-effective method to improve LMM performance on long and high-resolution video understanding tasks through data augmentation, eliminating the need for costly manual annotation.  |
| Steering Rectified Flow Models in the Vector Field for Controlled Image Generation (Read more on [arXiv](https://arxiv.org/abs/2412.00100) or [HuggingFace](https://huggingface.co/papers/2412.00100))| Yezhou Yang, Dimitris N. Metaxas, Song Wen, mpatel57 | FlowChef steers rectified flow models' denoising trajectories for controlled image generation.  The paper investigates how to efficiently guide rectified flow models (RFMs) for tasks like image editing, classifier guidance, and solving linear inverse problems without computationally expensive inversion or backpropagation.  The key methodology involves leveraging the smooth vector field dynamics of RFMs and a gradient skipping approach to directly adjust the trajectory during denoising. On linear inverse problems, FlowChef achieves 26.32 PSNR on box inpainting with a 20x20 mask, surpassing baselines on the pixel-space Rectified Flow++ model.  This offers AI practitioners a computationally efficient and inversion-free method for controlled image generation using RFMs, potentially improving performance and reducing resource demands for applications like image editing and guided synthesis.  |
| PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos (Read more on [arXiv](https://arxiv.org/abs/2412.01800) or [HuggingFace](https://huggingface.co/papers/2412.01800))| Hangyu Guo, Haoze Zhao, Haoran Tang, Meng Cao, zhangysk | PhysGame introduces a benchmark to evaluate the ability of video LLMs to understand physical commonsense violations in gameplay videos.  The research aimed to assess and improve video LLMs' ability to recognize glitches that defy real-world physics.  Researchers created PhysGame, a benchmark with 880 videos of glitches, PhysInstruct, an instruction tuning dataset with 140,057 question-answer pairs, and PhysDPO, a preference optimization dataset with 34,358 pairs using misleading video data. Their proposed PhysVLM model, trained on these datasets, achieved state-of-the-art performance on PhysGame and an overall accuracy of 61.1% on the Video-MME benchmark with subtitles. This work provides a benchmark and resources for training video LLMs capable of robust physical commonsense reasoning, crucial for developing more realistic and reliable AI agents in game development and broader applications.  |
| FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait (Read more on [arXiv](https://arxiv.org/abs/2412.01064) or [HuggingFace](https://huggingface.co/papers/2412.01064))| Gyoungsu Chae, Dongchan Min, Taekyung Ki | FLOAT generates talking portrait videos from a single source image and audio using a flow matching generative model.  The objective is to synthesize realistic talking motions from audio, including lip synchronization, head movements, and facial expressions, while addressing limitations of diffusion-based methods like slow sampling.  The key methodology involves modeling talking motion within a learned motion latent space using a transformer-based vector field predictor and decoding the sampled motion latents into video frames. On the HDTF dataset, FLOAT achieves a Fr√©chet Inception Distance (FID) of 21.100, outperforming compared baselines.  This efficient and high-quality approach offers AI practitioners a more effective method for generating realistic and temporally consistent talking portrait videos.  |
| A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2411.19477) or [HuggingFace](https://huggingface.co/papers/2411.19477))| Jingren Zhou, Bolin Ding, Yaliang Li, Xuchen Pan, yanxi-chen | This paper proposes a two-stage algorithm (generation and knockout) for improving the test-time compute of Large Language Models (LLMs).  The research aims to boost the success probability of LLMs by increasing test-time compute, specifically addressing the challenge of ensuring high reliability in high-stakes scenarios. The proposed algorithm involves generating multiple candidate solutions and selecting the best one through a knockout tournament with pairwise comparisons.  On a subset of the MMLU-Pro benchmark, the algorithm's accuracy improved from approximately 60% to over 65% for the "engineering" category when scaling the number of initial candidate solutions (N) from 1 to 32 with comparison parameter K=2 using Llama3.1.  AI practitioners can leverage this method to enhance LLM reliability for complex tasks by scaling test-time computation with provable performance guarantees, provided the underlying assumptions regarding solution generation and comparison probabilities hold.  |
| Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings with Few-Shot Learning (Read more on [arXiv](https://arxiv.org/abs/2412.01408) or [HuggingFace](https://huggingface.co/papers/2412.01408))| Noel Crespi, Reza Farahbaksh, callmesan | This paper explores cross-lingual few-shot learning for audio abuse detection in low-resource languages.  The research objective is to develop a model capable of detecting abusive language in multiple Indian languages using limited labeled data.  The methodology involves extracting audio features using pre-trained Wav2Vec and Whisper models, normalizing these features using Temporal Mean or L2-Norm, and classifying them with a Model-Agnostic Meta-Learning (MAML) based few-shot classifier.  Whisper with L2-Norm normalization achieved the highest accuracy, reaching 85.22% for Malayalam in the 100-shot setting.  AI practitioners can leverage pre-trained audio representations and meta-learning techniques to develop robust abuse detection systems for low-resource languages, even with limited labeled data, highlighting the potential for improved content moderation across diverse linguistic groups.  |
