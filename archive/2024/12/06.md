

## Papers for 2024-12-06

| Title | Authors | Summary |
|-------|---------|---------|
| Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection (Read more on [arXiv](https://arxiv.org/abs/2412.04455) or [HuggingFace](https://huggingface.co/papers/2412.04455))| Zhongyuan Wang, Zhizheng Zhang, Qi Su, chengchi, Zhoues | Code-as-Monitor (CaM) uses a vision-language model to generate code that monitors for and prevents robot failures in real time.  The research aims to create a unified system for both reactive (detecting failures after they occur) and proactive (preventing foreseeable failures) open-set failure detection in robotic tasks.  The key methodology involves formulating robotic failure detection as a constraint satisfaction problem, using visually-prompted code to monitor if these constraints are met during task execution.  In simulated "Stack in Order" tasks with severe disturbances, CaM achieved a 17.5% higher success rate than the DoReMi baseline.  This allows AI practitioners to build more robust and reliable closed-loop robotic systems capable of handling unexpected events and complex, long-horizon tasks.  |
| Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction (Read more on [arXiv](https://arxiv.org/abs/2412.04454) or [HuggingFace](https://huggingface.co/papers/2412.04454))| tianbaoxiexxx, ludunjie, ZeonLap, kugwzk, ranpox | AGUVIS is a unified, pure vision-based framework for building generalizable GUI agents.  The research aimed to develop a cross-platform autonomous GUI agent capable of performing complex tasks independently without relying on external closed-source models.  The key methodology involved a two-stage training pipeline using a Vision-Language Model (VLM): first for GUI grounding on a newly created template-augmented dataset, followed by planning and reasoning training on a VLM-augmented trajectory dataset.  AGUVIS-72B achieved a task success rate of 89.2% on ScreenSpot, outperforming previous state-of-the-art methods in both offline and real-world online scenarios.  This indicates a significant advancement towards creating fully autonomous, vision-based GUI agents, offering AI practitioners a potentially more efficient and adaptable solution for automating interactions with diverse digital environments compared to text-based or LLM-dependent approaches.  |
| A Noise is Worth Diffusion Guidance (Read more on [arXiv](https://arxiv.org/abs/2412.03895) or [HuggingFace](https://huggingface.co/papers/2412.03895))| Minjae Kim, Sanghyun Lee, Jiwon Kang, Donghoon Ahn, Min-Jaewon | NoiseRefine improves text-to-image diffusion model quality without guidance methods like classifier-free guidance (CFG).  The research explores whether guidance can be replaced by refining initial noise in the diffusion pipeline. The authors train a noise refining model using multistep score distillation (MSD) to map standard Gaussian noise to a learned "guidance-free" noise space, derived from inverting guided high-quality images.  Refined noise achieved FID scores comparable to, and in some cases better than, CFG guidance. This method offers AI practitioners a faster and potentially higher-quality alternative to computationally expensive guidance methods for text-to-image diffusion models.  |
| Evaluating Language Models as Synthetic Data Generators (Read more on [arXiv](https://arxiv.org/abs/2412.03679) or [HuggingFace](https://huggingface.co/papers/2412.03679))| Seongyun Lee, Vijay Viswanathan, Xiang Yue, Juyoung Suk, seungone | AGORABENCH benchmarks language models' (LMs) abilities to generate synthetic training data for other LMs.  The research aimed to evaluate different LMs as synthetic data generators and understand the characteristics of effective training data generated by LMs.  The study employed a controlled setting where various LMs generated 1.26 million training instances using existing data generation methods (instance generation, response generation, quality enhancement) across three domains (math, instruction-following, code), which were then used to fine-tune a student LM (Llama 3.1-8B).  GPT-40 achieved the highest average Performance Gap Recovered (PGR) score of 46.8% in instance generation.  AI practitioners can utilize AGORABENCH to select appropriate LMs for synthetic data generation based on the specific task and available resources, considering that problem-solving ability does not directly correlate with data generation effectiveness.  |
| MV-Adapter: Multi-view Consistent Image Generation Made Easy (Read more on [arXiv](https://arxiv.org/abs/2412.03632) or [HuggingFace](https://huggingface.co/papers/2412.03632))| Ran Yi, Haoran Wang, pookiefoof, bennyguo, huanngzh | MV-Adapter is a plug-and-play adapter enabling pre-trained text-to-image (T2I) diffusion models to generate multi-view consistent images.  The objective is to efficiently generate multi-view consistent images while preserving the quality and knowledge of pre-trained T2I models, without full fine-tuning. The key methodology involves duplicating and parallelizing the self-attention layers of the base T2I model to create separate multi-view and image cross-attention layers within the adapter.  On camera-guided image-to-multiview generation on the GSO dataset, MV-Adapter achieved 22.131 PSNR (Peak Signal-to-Noise Ratio) with SDXL. This allows AI practitioners to efficiently adapt existing high-quality T2I models for multi-view generation at high resolutions, reducing computational costs and mitigating overfitting risks associated with full model fine-tuning.  |
| Negative Token Merging: Image-based Adversarial Feature Guidance (Read more on [arXiv](https://arxiv.org/abs/2412.01339) or [HuggingFace](https://huggingface.co/papers/2412.01339))| Yejin Choi, Ranjay Krishna, Weijia Shi, Lindsey Li, Jaskirat Singh | NegToMe is a training-free method for adversarial guidance in text-to-image diffusion models using reference images.  The research aimed to improve adversarial guidance beyond text-based negative prompts by leveraging visual features.  The core methodology involves semantically matching and extrapolating source image tokens from their closest counterparts in a reference image during the reverse diffusion process.  NegToMe improved output diversity (lower DreamSim score and higher Entropy) while maintaining or improving image quality (FID and IS) across different classifier-free guidance scales. This provides AI practitioners with a simple, efficient technique to enhance control and diversity of generated images using directly image-based references, overcoming limitations of purely text-based negative prompts.  |
| Densing Law of LLMs (Read more on [arXiv](https://arxiv.org/abs/2412.04315) or [HuggingFace](https://huggingface.co/papers/2412.04315))| Xu Han, Guoyang Zeng, Weilin Zhao, Jie Cai, xcjthu | Here's a summary of the AI research paper "Densing Law of LLMs" following the provided guidelines:  i) **1-line summary:**  An empirical law, termed the "Densing Law," describes the exponential growth of Large Language Model (LLM) capacity density over time.  ii) **Main research question or objective:** To introduce the concept of "capacity density" as a metric for evaluating LLM training quality, considering both effectiveness and efficiency, and to analyze the trend of LLM capacity density.  iii) **Key methodology used:**  Capacity density was defined as the ratio of a model's effective parameter size (minimum parameters needed for equivalent performance) to its actual parameter size.  This was estimated using a two-step process:  first, fitting a Scaling Law to language modeling loss, and second, fitting a function to relate loss to downstream task performance.  Open-source base LLMs released since 2023 were evaluated against five benchmarks.  iv) **Primary results (include one specific quantitative finding):**  The maximum capacity density of LLMs doubles approximately every 3.3 months.  v) **Principal implication for AI practitioners:** The Densing Law suggests that achieving comparable performance to state-of-the-art LLMs using significantly fewer parameters is possible within a timeframe of approximately three months, thereby emphasizing the importance of optimizing LLM capacity density for improved efficiency and reduced computational costs in future LLM development.  |
| Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion (Read more on [arXiv](https://arxiv.org/abs/2412.04424) or [HuggingFace](https://huggingface.co/papers/2412.04424))| Dianqi Li, Haiping Wu, Jianwei Yang, Jiuhai Chen, zhoutianyi | Florence-VL enhances multimodal large language models (MLLMs) using the generative vision model Florence-2.  The research aimed to improve vision-language alignment and performance on diverse multimodal tasks by leveraging Florence-2's enriched visual representations.  The key methodology involved a novel "Depth-Breadth Fusion" (DBFusion) that combines visual features extracted from different layers and under multiple prompts of Florence-2, projecting these fused features into a pretrained LLM.  Florence-VL 8B achieved 89.9% on MMBench (EN) compared to 67.9% for LLaVA next 8B, demonstrating significant improvements across various benchmarks. This implies that AI practitioners can leverage generative vision models like Florence-2 and fusion techniques like DBFusion to build more robust and versatile MLLMs for tasks requiring detailed image understanding.  |
| Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis (Read more on [arXiv](https://arxiv.org/abs/2412.04431) or [HuggingFace](https://huggingface.co/papers/2412.04431))| Yuqi Zhang, Bin Yan, Yi Jiang, Jinlai Liu, Jian Han | Infinity introduces bitwise modeling for autoregressive high-resolution image synthesis.  The research aimed to improve the scaling and visual detail representation of discrete generative models for text-to-image synthesis.  The core methodology involved a bitwise multi-scale visual tokenizer, an infinite-vocabulary classifier, and a bitwise self-correction mechanism within a visual autoregressive model. On the GenEval benchmark, Infinity achieved an overall score of 0.73, surpassing the SD3-Medium score of 0.62. This work suggests that scaling tokenizer vocabulary and incorporating bitwise modeling can significantly enhance autoregressive models for image generation, providing AI practitioners with a faster, more detailed, and potentially superior alternative to diffusion-based models.  |
| Towards Universal Soccer Video Understanding (Read more on [arXiv](https://arxiv.org/abs/2412.01820) or [HuggingFace](https://huggingface.co/papers/2412.01820))| Yanfeng Wang, Ya Zhang, Hao Jiang, haoningwu, Homie0609 | This paper introduces a new framework for multi-modal soccer video understanding.  The objective is to develop a comprehensive model adaptable to various soccer video understanding tasks.  The researchers constructed SoccerReplay-1988, a dataset of 1,988 soccer matches with rich annotations, and trained MatchVision, a visual-language foundation model, using supervised classification and video-language contrastive learning. MatchVision achieved 80.1% top-1 accuracy on event classification on the SoccerReplay-test benchmark. This work provides AI practitioners with a new dataset and a foundation model for developing more versatile and robust soccer video understanding applications, potentially enabling advancements in automated sports analysis and content generation.  |
| HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing (Read more on [arXiv](https://arxiv.org/abs/2412.04280) or [HuggingFace](https://huggingface.co/papers/2412.04280))| Juncheng Li, Xiangtai Li, Ling Yang, WeiChow, BryanW | HumanEdit is a human-rewarded dataset for instruction-based image editing.  The objective was to create a high-quality dataset aligned with human preferences for training and evaluating instruction-guided image editing models, addressing limitations of existing datasets like noisy instructions and low-resolution images.  The dataset was created through a four-stage pipeline involving annotator training, image selection, instruction and edited image generation using DALL-E 2, and a two-tiered human quality review process.  On the HumanEdit-core subset, the mask-free InstructPix2Pix model achieved a CLIP-I score of 0.8946, while the mask-provided Meissonic model achieved a CLIP-I score of 0.9348.  The paper presents quantitative results for multiple baselines across different editing types (add, remove, replace, etc.) but doesn't explicitly compare them or declare a "best" overall.  AI practitioners can use HumanEdit to train and benchmark instruction-based image editing models, especially for high-resolution, photorealistic editing tasks that better align with human expectations than previous datasets.  The availability of masks, along with a subset allowing mask-free editing, allows for more flexible and diverse model training and evaluation.  |
| Personalized Multimodal Large Language Models: A Survey (Read more on [arXiv](https://arxiv.org/abs/2412.02142) or [HuggingFace](https://huggingface.co/papers/2412.02142))| Zhehao Zhang, Yu Xia, Hanjia Lyu, Junda Wu, Franck-Dernoncourt | This paper surveys techniques for personalizing multimodal large language models (MLLMs).  The objective is to categorize and analyze existing methods for adapting MLLMs to individual user preferences across various modalities (text, image, audio, etc.). The authors propose a taxonomy classifying personalization techniques based on instruction, alignment, generation, and fine-tuning across different MLLM applications like text/image generation, recommendation, and retrieval. While specific quantitative results are inconsistently reported across surveyed works, the paper notes ConCon-Chi dataset contains 4008 images and 20 concepts within 101 contexts for evaluating personalized vision-language tasks.  AI practitioners can use this taxonomy to understand the landscape of MLLM personalization techniques and identify suitable approaches for specific applications, though further research on standardized evaluation metrics and benchmark datasets is needed.  |
| ZipAR: Accelerating Autoregressive Image Generation through Spatial Locality (Read more on [arXiv](https://arxiv.org/abs/2412.04062) or [HuggingFace](https://huggingface.co/papers/2412.04062))| Hong Zhou, Shaoxuan He, Yuanyu He, Feng Chen, Yefei He | ZipAR is a training-free, plug-and-play parallel decoding framework for accelerating auto-regressive visual generation.  The research aims to reduce the latency of auto-regressive image generation models which typically decode visual tokens sequentially.  ZipAR leverages the spatial locality of images by decoding tokens from different rows in parallel, based on a defined local window size.  Experiments demonstrated up to a 91% reduction in forward steps on the Emu3-Gen model with minimal impact on image quality. This allows AI practitioners to significantly accelerate auto-regressive visual generation without retraining or architectural modifications.   |
| MRGen: Diffusion-based Controllable Data Engine for MRI Segmentation towards Unannotated Modalities (Read more on [arXiv](https://arxiv.org/abs/2412.04106) or [HuggingFace](https://huggingface.co/papers/2412.04106))| Yanfeng Wang, Weidi Xie, Ya Zhang, Ziheng Zhao, haoningwu | MRGen synthesizes training data for MRI segmentation models targeting modalities without existing mask annotations.  The research aims to improve MRI segmentation model performance on unannotated modalities due to the cost and scarcity of annotated data.  A two-stage training process involves text-guided pretraining on a large radiology image-text dataset (MedGen-1M) followed by mask-conditioned fine-tuning.  On average, MRGen improved Dice Similarity Coefficient (DSC) scores by 25% compared to models trained on source-domain data only. This provides AI practitioners with a method to extend existing segmentation models to new MRI modalities without needing manually annotated data, potentially accelerating development and deployment of robust medical image analysis tools.  |
| Discriminative Fine-tuning of LVLMs (Read more on [arXiv](https://arxiv.org/abs/2412.04378) or [HuggingFace](https://huggingface.co/papers/2412.04378))| Ioannis Maniadis Metaxas, Anestis Zaganidis, Alexandros Xenos, Adrian Bulat, Yassine Ouali | This paper introduces VladVA, a novel framework for adapting generative Large Vision-Language Models (LVLMs) for discriminative vision-language tasks.  The objective is to enhance LVLMs' discriminative capabilities while preserving their compositional strengths, addressing the limitations of contrastively-trained VLMs and autoregressive LVLMs.  The key methodology involves fine-tuning LVLMs with both contrastive and next-token prediction losses on image-text pairs of variable lengths, combined with parameter-efficient adaptation using soft prompting and LoRA.  On Flickr30k, VladVA achieves 85.0% recall@1 for image retrieval, a 5.5% absolute improvement over the baseline LLaVA 1.5-7B model. This work provides AI practitioners with a method to leverage the strengths of generative LVLMs for discriminative tasks like image-text retrieval, potentially leading to more robust and nuanced multimodal systems.  |
| Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation (Read more on [arXiv](https://arxiv.org/abs/2412.03304) or [HuggingFace](https://huggingface.co/papers/2412.03304))| Jian Gang Ngui, David I. Adelani, Cl√©mentine Fourrier, Angelika Romanou, Shivalika Singh | This paper investigates cultural and linguistic biases in the Massive Multitask Language Understanding (MMLU) benchmark and proposes an improved multilingual version.  The research aims to understand how cultural biases in translated datasets influence the performance of multilingual language models and to improve the quality of these datasets.  A large-scale evaluation of state-of-the-art language models was conducted using subsets of questions annotated as either culturally sensitive or culturally agnostic, alongside an improved, 42-language translated MMLU dataset called Global-MMLU.  Analysis found that 28% of the English MMLU questions require culturally sensitive knowledge, with 86.5% of culturally sensitive questions focused on Western culture.  AI practitioners should use Global-MMLU and report performance on culturally sensitive and agnostic subsets separately to better understand model capabilities across diverse cultures and languages, and to avoid inadvertently setting multilingual evaluation standards aligned with a single cultural paradigm.  |
| Monet: Mixture of Monosemantic Experts for Transformers (Read more on [arXiv](https://arxiv.org/abs/2412.04139) or [HuggingFace](https://huggingface.co/papers/2412.04139))| Jaewoo Kang, Kee-Eung Kim, Young Jin Ahn, affjljoo3581 | Here is a summary of the AI research paper "Monet: Mixture of Monosemantic Experts for Transformers," following the provided guidelines:   i) **One-line summary:**  The MONET architecture integrates sparse dictionary learning into Mixture-of-Experts (MoE) transformer training to achieve parameter-efficient scaling of monosemantic experts and enhance mechanistic interpretability.   ii) **Main research question/objective:** How can the internal computations of large language models (LLMs) be made more interpretable by disentangling polysemantic features and scaling the number of experts in a parameter-efficient manner?   iii) **Key methodology:**  The MONET architecture uses a novel expert decomposition method within a Mixture-of-Experts framework, employing product key composition of experts to achieve a square root scaling of total parameters with respect to the number of experts.  This is implemented via Horizontal and Vertical Decomposition approaches.   iv) **Primary results:** MONET achieves competitive performance with total parameter-matched dense LLMs on various benchmarks;  MONET-VD (Vertical Decomposition) consistently outperforms MONET-HD (Horizontal Decomposition) across benchmarks and model sizes.  Specific quantitative results from open-ended LLM benchmarks are provided in Table 2 of the paper.   v) **Principal implication for AI practitioners:** The parameter-efficient scaling of monosemantic experts in MONET enables the creation of highly interpretable LLMs with a significantly increased number of experts. This facilitates robust knowledge manipulation (e.g., domain, language, toxicity control) without sacrificing overall model performance.  The methodology offers a novel approach to scaling MoE architectures with enhanced interpretability and control.  |
| OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows (Read more on [arXiv](https://arxiv.org/abs/2412.01169) or [HuggingFace](https://huggingface.co/papers/2412.01169))| Yusuke Kato, Zichun Liao, Akash Gokul, Konstantinos Kallidromitis, Shufan Li | OmniFlow is a novel generative AI model for any-to-any multi-modal generation.  The research aimed to develop a unified model capable of generating various output modalities (text, image, audio) given any input modality combination.  The core methodology involves extending rectified flows (RF) to a multi-modal setting, integrating a multi-modal guidance mechanism within a modular architecture inspired by Stable Diffusion 3.  On the GenEval benchmark, OmniFlow achieves a score of 0.62 for text-to-image generation. This modular design, allowing for pretraining of individual components and subsequent merging, offers AI practitioners a more efficient and resource-conscious approach to developing and training unified multi-modal generative models, potentially reducing computational overhead compared to training large unified models from scratch.  |
| AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2412.04146) or [HuggingFace](https://huggingface.co/papers/2412.04146))| Zhichao Liao, Fulong Ye, Pengze Zhang, Qichao Sun, Crayon-Shinchan | AnyDressing generates customized images of characters wearing multiple garments based on user-provided garments and text prompts.  The research aims to address the limitations of existing virtual dressing methods that struggle with multi-garment combinations and text prompt fidelity. The proposed AnyDressing model uses two primary networks: GarmentsNet, with a Garment-Specific Feature Extractor for parallel encoding of garment textures, and DressingNet, with a Dressing-Attention mechanism and Instance-Level Garment Localization Learning for integrating features and preserving text-image consistency. On a multi-garment evaluation, AnyDressing achieves a CLIP-T score of 0.296, demonstrating improved text consistency. This provides AI practitioners with a more robust and controllable approach for generating virtual dressing images, enabling diverse combinations of attire and improved adherence to user-specified text prompts.  |
| KV Shifting Attention Enhances Language Modeling (Read more on [arXiv](https://arxiv.org/abs/2411.19574) or [HuggingFace](https://huggingface.co/papers/2411.19574))| Weipeng Chen, Bingning Wang, Wei Cheng, xumingyu16 | Here's a concise summary of the AI research paper following your strict guidelines:  i) **1-line summary:**  A novel KV shifting attention mechanism is proposed and empirically shown to improve language model training efficiency and performance, reducing the depth and width requirements of induction heads.  ii) **Main research question/objective:**  Can modifications to the transformer's attention mechanism improve the efficiency and effectiveness of learning induction heads, thus enhancing language modeling performance?  iii) **Key methodology:** A novel "KV shifting attention" mechanism was proposed, decoupling keys and values in the attention mechanism to reduce the structural requirements for depth and width needed for induction heads.  This was theoretically analyzed and empirically validated through experiments on both toy and large-scale language models.  iv) **Primary results:** The KV shifting attention demonstrated superior performance to conventional multi-layer transformers, with a 2.9B parameter model achieving an average benchmark score of 38.57 (compared to 36.45 for Vanilla) after 500B training tokens.  Specific details regarding the toy model experiments (Figure 1a and 1b) were provided but lacked complete numerical representation in the main text.  v) **Principal implication for AI practitioners:**  KV shifting attention offers a method to potentially improve the efficiency of training large language models by reducing computational resources required for induction heads, leading to better performance or faster convergence.  Further investigation is needed to assess the applicability and impact across a wider range of architectures and model sizes, and additional numerical results from the small-scale and large-scale experiments would improve the clarity and impact of the conclusions.  |
| Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement (Read more on [arXiv](https://arxiv.org/abs/2412.04003) or [HuggingFace](https://huggingface.co/papers/2412.04003))| Yu Zhao, Tianqi Shi, Chenyang Lyu, Bo Zeng, Lingfeng Ming | Here is a summary of the AI research paper following your guidelines:  i) Marco-LLM, a multilingual large language model (LLM), is developed using massive multilingual continual pre-training and post-training to bridge the performance gap between high- and low-resource languages.  ii) The main objective is to develop a multilingual LLM that performs exceptionally well in multilingual tasks, including low-resource languages, while maintaining strong performance in high-resource languages like English.  iii) The key methodology involves compiling a large-scale multilingual dataset, conducting two-stage continual pre-training using Qwen2 models, and performing extensive multilingual post-training including supervised fine-tuning and preference alignment.  iv) Marco-LLM achieved substantial improvements over state-of-the-art LLMs in various multilingual benchmarks, for example, Marco-72B achieved a 93.7% accuracy on CEVAL and 81.2% accuracy on X-MMLU.  v)  The significant improvement in multilingual understanding and reasoning tasks across various benchmarks, especially for low-resource languages, highlights the efficacy of massive multilingual training and demonstrates the potential to improve LLM capabilities for under-resourced languages.  Further investigation of continual learning parameters and data quality will be essential for future model iterations.  |
