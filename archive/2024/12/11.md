

## Papers for 2024-12-11

| Title | Authors | Summary |
|-------|---------|---------|
| Evaluating and Aligning CodeLLMs on Human Preference (Read more on [arXiv](https://arxiv.org/abs/2412.05210) or [HuggingFace](https://huggingface.co/papers/2412.05210))| JustinLin610, huybery, misakamage, instro, jx-yang | Here is a concise summary of the paper "Evaluating and Aligning CodeLLMs on Human Preference":  i) **Summary:** This paper introduces CodeArena, a new benchmark for evaluating code language models (codeLLMs) based on human preferences, and SynCode-Instruct, a large-scale synthetic instruction dataset for enhancing codeLLM alignment with human preferences. ii) **Main Research Question/Objective:** How to evaluate and improve the alignment of codeLLMs with human preferences in realistic code generation scenarios. iii) **Key Methodology:** Development of CodeArena with 397 human-curated samples across 40 categories and 44 programming languages, and creation of SynCode-Instruct, a 20 billion token synthetic instruction dataset derived from web data. iv) **Primary Results:** CodeArena reveals a significant performance gap between open-source and proprietary LLMs, with Qwen2.5-SynCoder achieving the best performance among open-source models evaluated (49.2/22.3 win rate/tie rate). v) **Principal Implication for AI Practitioners:** AI practitioners should consider human preference alignment in codeLLM evaluation and training, utilizing benchmarks like CodeArena and large-scale synthetic instruction datasets for improved performance.  |
| DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation (Read more on [arXiv](https://arxiv.org/abs/2412.07589) or [HuggingFace](https://huggingface.co/papers/2412.07589))| Chao Tang, LXT, zengyh1900, JingboWang, jianzongwu | Here's a summary of the research paper "DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation" following your specified guidelines:  i) **Summary:** DiffSensei is a novel framework for customized manga generation that integrates diffusion models with a multimodal large language model (MLLM) for dynamic, multi-character control based on text prompts and user inputs. ii) **Main research question/objective:** How to generate customized manga panels with multiple characters, precise layout control, and dynamic adaptation to textual prompts. iii) **Key methodology:** The approach employs an MLLM as a text-compatible identity adapter for diffusion-based image generation, using masked cross-attention to incorporate character features and a dialog embedding technique for precise dialog placement. iv) **Primary results:** DiffSensei outperforms existing models in experiments, achieving a 0.06 improvement in CLIP metrics compared to the multi-subject customization baseline, MS-Diffusion. v) **Principal implication for AI practitioners:** AI practitioners can leverage DiffSensei to create manga generation tools with enhanced character customization and layout control, enabling more dynamic and interactive storytelling capabilities.  |
| STIV: Scalable Text and Image Conditioned Video Generation (Read more on [arXiv](https://arxiv.org/abs/2412.07730) or [HuggingFace](https://huggingface.co/papers/2412.07730))| jefflai, JesseAllardice, tsujuifu, wenzehu, Jiasenlu | Here is a concise summary of the research paper "STIV: Scalable Text and Image Conditioned Video Generation" following your guidelines:  i) **Summary:** This paper introduces STIV, a scalable text-image-conditioned video generation model based on a Diffusion Transformer (DiT) architecture that can perform both text-to-video (T2V) and text-image-to-video (TI2V) tasks.  ii) **Main research question/objective:** How to develop a robust and scalable video generation model that effectively integrates text and image conditioning within a unified framework.  iii) **Key methodology:** The authors integrated image conditioning into a DiT through frame replacement and text conditioning via joint image-text conditional classifier-free guidance, and conducted a systematic study on model architectures, training recipes, and data curation strategies.  iv) **Primary results:** The 8.7B parameter STIV model achieved a state-of-the-art VBench T2V score of 83.1 and a VBench I2V score of 90.1 at 512x512 resolution, surpassing models like CogVideoX-5B, Pika, Kling, and Gen-3.  v) **Principal implication for AI practitioners:**  AI practitioners can leverage the STIV framework and the provided recipes for building and scaling video generation models, enabling the development of more versatile and reliable video generation solutions for various downstream applications.  |
| Hidden in the Noise: Two-Stage Robust Watermarking for Images (Read more on [arXiv](https://arxiv.org/abs/2412.04653) or [HuggingFace](https://huggingface.co/papers/2412.04653))| Niv Cohen, chegde, rtealwitter, penfever, kasraarabi | Here's a concise summary of the research paper "Hidden in the Noise: Two-Stage Robust Watermarking for Images" based on the provided guidelines:  i) **Summary:** The paper introduces WIND, a two-stage watermarking method for images generated by diffusion models, designed to be robust against removal and forgery attacks.  ii) **Main research question/objective:** How to develop a distortion-free watermarking technique for diffusion-generated images that is robust to common attacks while maintaining detection efficiency?  iii) **Key methodology:** WIND employs a two-stage approach, first embedding a group identifier in the Fourier space of the initial noise and then using a secret salt and hash function to generate a unique, reproducible initial noise for watermarking.  iv) **Primary results:** WIND achieved a 94.7% average detection accuracy across various image transformation attacks when using 128 groups of initial noises, and the proposed method demonstrates resilience against a regeneration attack.  v) **Principal implication for AI practitioners:** AI practitioners can utilize WIND to watermark images generated by their models, enabling them to verify image origins and protect against unauthorized use, with a negligible impact on image quality and a demonstrated detection accuracy of 94.7% under various attacks.  |
| UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics (Read more on [arXiv](https://arxiv.org/abs/2412.07774) or [HuggingFace](https://huggingface.co/papers/2412.07774))| Yuqian Zhou, He Zhang, Zhifei Zhang, jimmie33, xichenhku | Here is a concise summary of the research paper "UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics":  i) **Summary:** UniReal is a unified framework for diverse image generation and editing tasks, treating image tasks as discontinuous video generation and learning from large-scale videos. ii) **Main research question/objective:** To develop a unified framework that can address various image generation and editing tasks within a single model using a scalable training paradigm. iii) **Key methodology:** The paper proposes leveraging a video generation framework based on a diffusion transformer, treating input/output images as video frames, and employing hierarchical prompts and image index embeddings for task and image coordination. iv) **Primary results:** UniReal outperforms existing methods in instructive image editing, customized image generation, and object insertion; e.g. UniReal achieves a CLIP score of 0.851 and a DINO score of 0.790 on the EMU Edit test set. v) **Principal implication for AI practitioners:** AI practitioners can leverage UniReal as a versatile tool for various image generation and editing tasks, simplifying development by using a single model trained on readily available video data instead of task-specific datasets.  |
| OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations (Read more on [arXiv](https://arxiv.org/abs/2412.07626) or [HuggingFace](https://huggingface.co/papers/2412.07626))| conghui, friskit, Liam-Liu, wanderkid, ouyanglinke | Here's a concise summary of the research paper "OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations" based on your specified guidelines:  i)  **Summary:** This paper introduces OmniDocBench, a new benchmark for evaluating PDF document parsing methods, featuring a diverse dataset with comprehensive annotations. ii) **Main research question/objective:** To develop a robust, diverse, and fair evaluation standard for document content extraction methods. iii) **Key methodology:** Construction of a high-quality dataset with 981 PDF pages across nine types, with 19 layout category labels and 14 attribute labels for evaluating pipeline and end-to-end document parsing methods. iv) **Primary results:** Pipeline-based methods like MinerU and Mathpix achieved the best overall parsing performance (e.g., MinerU achieved 0.188 average edit distance across 9 PDF types); however, general VLMs showed stronger generalization on specialized data. v) **Principal implication for AI practitioners:** OmniDocBench provides a standardized benchmark to systematically evaluate and improve the accuracy, robustness, and generalization capabilities of document parsing models across diverse document types and layouts, which can directly improve the tools that AI practitioners work with.  |
| FiVA: Fine-grained Visual Attribute Dataset for Text-to-Image Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2412.07674) or [HuggingFace](https://huggingface.co/papers/2412.07674))| myownskyW7, guandao, Dubhe-zmc, justimyhxu, tongwu2020 | Here's a concise summary of the paper:  i) **Summary:** The paper introduces FiVA, a new dataset of 1 million images with fine-grained visual attribute annotations, and FiVA-Adapter, a framework for controlling image generation using these attributes. ii) **Main research question or objective:** To develop a method for decomposing the aesthetics of an image into specific visual attributes and enable users to control image generation based on these attributes. iii) **Key methodology:** Construction of a dataset (FiVA) using a pipeline involving attribute definition, prompt creation, LLM-based filtering, and human validation, followed by the development of an adaptation framework (FiVA-Adapter) that integrates a multimodal encoder into an image feature encoder for attribute extraction. iv) **Primary results:** The FiVA-Adapter achieved a subject accuracy of 0.817 in user studies, outperforming baseline methods. v) **Principal implication for AI practitioners:** AI practitioners can leverage the FiVA dataset and FiVA-Adapter to enhance the controllability of text-to-image diffusion models, enabling more precise manipulation of fine-grained visual attributes in generated images.  |
| Perception Tokens Enhance Visual Reasoning in Multimodal Language Models (Read more on [arXiv](https://arxiv.org/abs/2412.03548) or [HuggingFace](https://huggingface.co/papers/2412.03548))| Dongping Chen, Ethan Shen, Cheng-Yu Hsieh, Zelun Luo, Mahtab Bigverdi | Here is a concise summary of the research paper "Perception Tokens Enhance Visual Reasoning in Multimodal Language Models":  i) **Summary:** This paper introduces "Perception Tokens," a novel approach to enhance visual reasoning in multimodal language models (MLMs) by using intermediate image representations as auxiliary reasoning tokens. ii) **Main research question or objective:** The main objective is to develop a method for augmenting MLMs with the ability to reason over intrinsic image representations, such as depth maps and bounding boxes, to improve performance on visual reasoning tasks. iii) **Key methodology:** The authors propose AURORA, a multi-task training framework that uses a VQVAE to transform intermediate image representations into tokenized formats and bounding box tokens, which are then used to train MLMs to leverage these "Perception Tokens" as chain-of-thought prompts. iv) **Primary results:** AURORA significantly improves performance on counting benchmarks, achieving a +10.8% improvement on BLINK. v) **Principal implication for AI practitioners:** AI practitioners can leverage AURORA to expand the scope of MLMs beyond language-based reasoning, enabling more effective visual reasoning capabilities by incorporating intermediate visual representations directly into the model's reasoning process.  |
| 3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation (Read more on [arXiv](https://arxiv.org/abs/2412.07759) or [HuggingFace](https://huggingface.co/papers/2412.07759))| Menghan Xia, Sida Peng, Xintao Wang, Xian Liu, lemonaddie | Here is a summary of the provided AI research paper, strictly adhering to the specified guidelines:  i) 3DTrajMaster achieves state-of-the-art accuracy in controlling multi-entity 3D motions in video generation using 6DoF pose sequences as input.  ii) The research objective was to manipulate multi-entity 3D motions in video generation, overcoming the limitations of prior methods that primarily used 2D control signals.  iii) The core methodology involved a plug-and-play 3D-motion grounded object injector that fused multiple input entities with their 3D trajectories via a gated self-attention mechanism.  A 360Â°-Motion Dataset was created for training, incorporating a domain adaptor and annealed sampling strategy to improve video quality.  iv)  The primary results showed that 3DTrajMaster achieved a 0.398m translation error and a 0.277-degree rotation error on average in controlling multiple entity motions.  v)  For AI practitioners, the development of 3DTrajMaster provides a novel approach for controlling multi-entity 3D motions in video generation; the creation of a new dataset with synchronized multi-camera recordings of diverse 3D entities addresses the limited availability of training data for this task.  The paper does not explicitly detail the model architecture's specific components (e.g., layer sizes, activation functions, etc.), limiting direct application without further clarification.  |
| Frame Representation Hypothesis: Multi-Token LLM Interpretability and Concept-Guided Text Generation (Read more on [arXiv](https://arxiv.org/abs/2412.07334) or [HuggingFace](https://huggingface.co/papers/2412.07334))| Kazuhiro Fukui, Erica K. Shimomoto, Lincon S. Souza, Pedro H. V. Valois | Here is a concise summary of the research paper "Frame Representation Hypothesis: Multi-Token LLM Interpretability and Concept-Guided Text Generation":  i) **Summary:** This paper introduces the Frame Representation Hypothesis (FRH) to interpret and control Large Language Models (LLMs) by representing words as frames (ordered sequences of linearly independent token vectors) and concepts as the average of word frames. ii) **Main research question/objective:** How can multi-token words be effectively modeled to enhance LLM interpretability and control? iii) **Key methodology:** The authors propose representing words as frames and concepts as the average of word frames within a defined Semantic Frame Space and introduce Top-k Concept-Guided Decoding to steer text generation. iv) **Primary results:** The FRH is validated by showing that over 99% of words across multiple languages in the Open Multilingual WordNet (OMW) are composed of linearly independent token vectors, and concept-guided generation effectively steers output towards desired concepts. v) **Principal implication for AI practitioners:** The FRH offers a novel framework for AI researchers and engineers to enhance LLM interpretability and control by leveraging multi-token word representations, enabling more precise manipulation of model outputs.  |
| Video Motion Transfer with Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2412.07776) or [HuggingFace](https://huggingface.co/papers/2412.07776))| Sergey Tulyakov, fabvio, philiptorr, aliaksandr-siarohin, alexpondaven | Here is a concise summary of the paper "Video Motion Transfer with Diffusion Transformers":  i) **Summary:** The paper introduces DiTFlow, a novel method for transferring motion from a reference video to a newly synthesized video using Diffusion Transformers (DiTs). ii) **Main research question/objective:** How to transfer the motion of a reference video to a newly synthesized one, specifically for Diffusion Transformers (DiT). iii) **Key methodology:** DiTFlow extracts an Attention Motion Flow (AMF) from a reference video by analyzing cross-frame attention maps in a pre-trained DiT, then uses this AMF to guide the latent denoising process in an optimization-based, training-free manner. iv) **Primary results:** DiTFlow outperforms all baseline methods in motion transfer on multiple metrics; specifically, it achieves a Motion Fidelity (MF) score of 0.785 on the 5B parameter model, compared to 0.766 for the best-performing baseline. v) **Principal implication for AI practitioners:** AI practitioners can leverage DiTFlow for improved motion transfer in video synthesis using DiTs, enabling more precise control over the motion of generated video content without the need for model retraining.  |
| EMOv2: Pushing 5M Vision Model Frontier (Read more on [arXiv](https://arxiv.org/abs/2412.06674) or [HuggingFace](https://huggingface.co/papers/2412.06674))| Zhucun Xue, Teng Hu, Jiangning Zhang, LXT, hhy724 | Here is a concise summary of the research paper "EMOv2: Pushing 5M Vision Model Frontier" based on the provided guidelines:  i) This paper introduces EMOv2, a new family of efficient vision models designed for resource-constrained scenarios, focusing on optimizing the trade-off between parameters, FLOPs, and performance within the 5M parameter magnitude. ii) The main research objective is to establish a new performance frontier for 5M parameter magnitude lightweight models on various downstream visual tasks. iii) The key methodology involves abstracting a Meta Mobile Block (MMBlock) to unify the design of Inverted Residual Block (IRB) and attention-based modules, and deducing an improved Inverted Residual Mobile Block (i2RMB) with a novel spanning attention mechanism. iv) EMOv2-5M achieves 79.4 Top-1 accuracy on ImageNet-1K classification, outperforming prior state-of-the-art models of similar size. v) For AI practitioners, EMOv2 provides a highly efficient and versatile backbone that can be readily adapted to various vision tasks, including classification, detection, segmentation, and generation, offering a strong baseline for mobile and edge device applications with strict parameter constraints.  |
| Granite Guardian (Read more on [arXiv](https://arxiv.org/abs/2412.07724) or [HuggingFace](https://huggingface.co/papers/2412.07724))| Tejaswini Pedapati, Subhajit Chaudhury, Manish Nagireddy, Inkit Padhi, Giandomenico | Okay, here is a concise summary of the Granite Guardian AI research paper, following your specified guidelines:  1. **Summary:** The paper introduces Granite Guardian, a suite of open-source Large Language Model (LLM) safeguards designed for risk detection in prompts and responses across various dimensions, including harmful content and Retrieval-Augmented Generation (RAG) hallucination. 2. **Main research question/objective:** To develop and evaluate a unified risk detection model family capable of identifying a broad spectrum of risks in LLM inputs and outputs, including those typically overlooked by traditional risk detection models. 3. **Key methodology:** Supervised fine-tuning of Granite 3.0 language models on a dataset combining human annotations from diverse sources and synthetic data, with a specialized safety instruction template for risk categorization. 4. **Primary results:** Granite Guardian achieves state-of-the-art risk detection with an AUC score of 0.871 on harmful content benchmarks. 5. **Principal implication for AI practitioners:**  AI practitioners can use Granite Guardian as adaptable, plug-and-play components to enhance the safety and reliability of LLMs in various applications by enabling robust risk detection across multiple risk dimensions.  |
| ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance (Read more on [arXiv](https://arxiv.org/abs/2412.06673) or [HuggingFace](https://huggingface.co/papers/2412.06673))| Jianhua Han, Runhui Huang, Junwei Yang, Guansong Lu, Chunwei Wang | Here is a concise summary of the research paper "ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance":  i)  ILLUME is a unified multimodal large language model (MLLM) that integrates visual understanding and generation through a unified next-token prediction formulation. ii) **Main research question/objective:** Can a unified MLLM be developed more efficiently, and can the discriminative and generative capabilities of an MLLM enhance each other? iii) **Key methodology:** A semantic vision tokenizer incorporating semantic information and a progressive multi-stage training procedure are used to enhance data efficiency, alongside a novel self-enhancing multimodal alignment scheme. iv) **Primary results:** ILLUME requires only 15M data for image-text alignment during pretraining and achieves 7.76 FID score on the MJHQ30K benchmark. v) **Principal implication for AI practitioners:** AI practitioners can leverage ILLUME's efficient training approach and architecture for developing unified MLLMs with strong visual understanding and generation capabilities, potentially reducing the data and computational resources typically required.  |
| ObjCtrl-2.5D: Training-free Object Control with Camera Poses (Read more on [arXiv](https://arxiv.org/abs/2412.07721) or [HuggingFace](https://huggingface.co/papers/2412.07721))| Chen Change Loy, Shangchen Zhou, Yushi Lan, Zhouxia Wang | Here is a concise summary of the research paper "ObjCtrl-2.5D: Training-free Object Control with Camera Poses":  i) **Summary:** The paper introduces ObjCtrl-2.5D, a training-free method for controlling object motion in image-to-video generation by extending 2D trajectories to 3D and representing them as camera poses. ii) **Main research question or objective:** The main objective is to achieve more precise and versatile object control in image-to-video (I2V) generation compared to existing methods. iii) **Key methodology used:** ObjCtrl-2.5D extends 2D trajectories to 3D using depth information, models object movement as camera poses, and utilizes a Layer Control Module and Shared Warping Latent to adapt a camera motion control model for object motion control. iv) **Primary results:** ObjCtrl-2.5D achieved an Object Motion Control (ObjMC) score of 91.42 on the DAVIS dataset when combining a 2D trajectory with depth from the conditional image. v) **Principal implication for AI practitioners:** ObjCtrl-2.5D provides a training-free approach for precise object motion control in video generation, offering more diverse control capabilities than existing 2D trajectory-based methods without the need for model training.  |
| LoRA.rar: Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation (Read more on [arXiv](https://arxiv.org/abs/2412.05148) or [HuggingFace](https://huggingface.co/papers/2412.05148))| Umberto Michieli, Pietro Zanuttigh, Mete Ozay, obohdal, donaldssh | Okay, here is a concise summary of the research paper "LoRA.rar: Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation," strictly adhering to your guidelines:  **i) Summary:** LoRA.rar is a novel method that efficiently merges subject and style LoRAs using a pre-trained hypernetwork for fast, high-quality, personalized image generation.  **ii) Main research question or objective:** The main objective is to develop a method for merging content and style LoRAs that achieves superior image quality compared to state-of-the-art methods while enabling real-time performance on resource-constrained devices.  **iii) Key methodology used:** The key methodology involves pre-training a hypernetwork on a diverse dataset of content-style LoRA pairs to predict merging coefficients, enabling generalization to unseen pairs during deployment.  **iv) Primary results:** LoRA.rar outperforms existing methods, including ZipLoRA, in both content and style fidelity, achieving a merging speedup of over 4000x and a score of 0.71 in average case using the proposed Multimodal Assistant Rating Subject & Style (MARS2) metric, compared to 0.58 for the next best method.  **v) Principal implication for AI practitioners:** AI practitioners can leverage LoRA.rar for efficient, high-quality, subject-style conditioned image generation, particularly in applications requiring real-time performance on devices with limited computational resources.  |
| Fully Open Source Moxin-7B Technical Report (Read more on [arXiv](https://arxiv.org/abs/2412.06845) or [HuggingFace](https://huggingface.co/papers/2412.06845))| Sung-En Chang, Yixin Shen, Zhenglun Kong, Xuan Shen, Pu Zhao | Here is a summary of the research paper "Fully Open Source Moxin-LLM Technical Report" based on your specified format:  i)  **Summary:** This paper introduces Moxin-7B, a fully open-source large language model (LLM) developed in accordance with the Model Openness Framework (MOF), emphasizing complete transparency in training, datasets, and implementation. ii) **Main research question or objective:** The main objective is to develop a high-performing, fully open-source 7B parameter LLM that adheres to the principles of open science, open source, open data, and open access as defined by the MOF. iii) **Key methodology used:** The model architecture extends the Mistral model, utilizing grouped-query attention and sliding window attention, trained on a mix of SlimPajama and DCLM-BASELINE datasets, with capability enhancement using data from HuggingFace. iv) **Primary results:** Moxin-7B-finetuned achieves superior performance in zero-shot evaluation compared with popular 7B models, notably scoring 82.24% on the PIQA benchmark. v) **Principal implication for AI practitioners:** AI practitioners can leverage Moxin-7B's open-source nature, including its training code, datasets, and checkpoints, to further innovate, customize, and deploy LLMs across diverse applications, fostering a more transparent and collaborative AI ecosystem.  |
| Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation (Read more on [arXiv](https://arxiv.org/abs/2412.07338) or [HuggingFace](https://huggingface.co/papers/2412.07338))| Felice Dell'Orletta, Marco Avvenuti, Amaury Trujillo, Alessio Miaschi, Lorenzo Cima | Here's a concise summary of the paper based on your guidelines:  i)  This paper investigates strategies for generating tailored counterspeech using the LLaMA2-13B model, focusing on adaptation to conversation context and personalization to the user. ii)  The main research question is whether contextualized counterspeech, adapted to the community and conversation and personalized to the user, is more persuasive than generic counterspeech. iii)  The key methodology involved fine-tuning LLaMA2-13B with various configurations of contextual information (community, conversation, user history) and evaluating the generated counterspeech through quantitative indicators and a crowdsourced human evaluation. iv)  The primary results show that contextualized counterspeech can outperform generic counterspeech in adequacy and persuasiveness; for instance, the configuration [Ba Pr Hi] outperformed the baseline in user-persuasiveness with a statistically significant difference (p < 0.01). v)  The principal implication for AI practitioners is that incorporating contextual information like conversation history can significantly enhance the effectiveness of AI-generated counterspeech, though there exists a discrepancy between algorithmic and human evaluations of the output.  |
| Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment (Read more on [arXiv](https://arxiv.org/abs/2412.04835) or [HuggingFace](https://huggingface.co/papers/2412.04835))| Jitendra Malik, Masayoshi Tomizuka, Chenfeng Xu, Yilin Wu, Ran Tian | Here is a concise summary of the research paper:  i) **Summary:** The paper introduces Representation-Aligned Preference-based Learning (RAPL), an observation-only method for learning visual rewards from human preference feedback to align visuomotor robot policies. ii) **Main research question or objective:** How can visuomotor robot policies be aligned with end-user preferences using minimal human feedback? iii) **Key methodology:** RAPL focuses human feedback on fine-tuning pre-trained vision encoders to align with the end-user's visual representation, then constructs a dense visual reward via feature matching using optimal transport in this aligned representation space. iv) **Primary results:** RAPL can fine-tune visuomotor policies with 5x less real human preference data compared to traditional reinforcement learning from human feedback (RLHF) methods. v) **Principal implication for AI practitioners:** AI practitioners can leverage RAPL to align pre-trained visuomotor policies with significantly less human feedback, making it more feasible to deploy such policies in real-world scenarios where collecting extensive human feedback is impractical.  |
| Chimera: Improving Generalist Model with Domain-Specific Experts (Read more on [arXiv](https://arxiv.org/abs/2412.05983) or [HuggingFace](https://huggingface.co/papers/2412.05983))| Renrui Zhang, Renqiu Xia, Hongbin Zhou, Mingsheng Li, Tianshuo Peng | Here is a concise summary of the research paper "Chimera: Improving Generalist Model with Domain-Specific Experts":  i)  **Summary:** This paper introduces Chimera, a multi-modal pipeline that integrates domain-specific expert models into a generalist large multi-modal model (LMM) to enhance performance on specialized tasks. ii) **Main research question or objective:** How to effectively improve the performance of generalist LMMs on domain-specific tasks without sacrificing their general capabilities. iii) **Key methodology:** A progressive training strategy with a Generalist-Specialist Collaboration Masking (GSCM) mechanism was used to merge features from expert models into the input of a generalist LMM, along with a router to determine expert model invocation. iv) **Primary results:** Chimera achieved state-of-the-art performance on multi-modal reasoning benchmarks, with an overall accuracy of 64.9 on MathVista. v) **Principal implication for AI practitioners:** AI practitioners can leverage Chimera's pipeline to scale up existing LMMs with domain-specific experts, significantly enhancing performance on specialized tasks without extensive retraining or compromising generalist capabilities.  |
| A New Federated Learning Framework Against Gradient Inversion Attacks (Read more on [arXiv](https://arxiv.org/abs/2412.07187) or [HuggingFace](https://huggingface.co/papers/2412.07187))| Weihong Ren, Xiaodan Zhang, Wenhao Chen, Shuang Zeng, gpx333 | Okay, here is a concise summary of the paper, strictly following your guidelines:  i)  This paper introduces HyperFL, a new federated learning framework designed to protect against gradient inversion attacks. ii) The main research objective is to develop a federated learning framework that offers a favorable privacy-utility trade-off against gradient inversion attacks without relying on existing defense mechanisms like SMC, HE, and DP. iii) The key methodology involves using hypernetworks to generate the parameters of local models, sharing only hypernetwork parameters for server aggregation, and decomposing local models into shared feature extractors and private classifiers. iv) Primary results show that HyperFL achieves comparable performance to state-of-the-art methods while enhancing privacy; for instance, HyperFL achieved 76.29% accuracy on the EMNIST dataset with 20 clients, surpassing several existing methods. v) The principal implication for AI practitioners is that HyperFL can be used as a more privacy-preserving alternative to traditional federated learning frameworks, particularly in applications where data sensitivity is a critical concern.  |
