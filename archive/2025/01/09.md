

## Papers for 2025-01-09

| Title | Authors | Summary |
|-------|---------|---------|
| rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking (Read more on [arXiv](https://arxiv.org/abs/2501.04519) or [HuggingFace](https://huggingface.co/papers/2501.04519))| Youran Sun, Yifei Liu, Xinyu Guan, J-shang, lynazhang | rStar-Math demonstrates that small language models (SLMs) can achieve advanced math reasoning through self-evolved deep thinking. The main research question is whether SLMs can rival or surpass the mathematical reasoning capabilities of larger models like OpenAI's models without distillation from superior models. The key methodology involves a novel code-augmented Chain-of-Thought data synthesis method, Monte Carlo Tree Search (MCTS) for test-time search guided by an SLM-based process reward model, and a four-round self-evolution recipe to iteratively improve the policy SLM and process preference model (PPM). The primary result is that rStar-Math improves the accuracy of the Qwen2.5-Math-7B model on the MATH benchmark from 58.8% to 90.0% with 64 search trajectories. The principal implication for AI practitioners is that they can leverage rStar-Math's self-evolutionary framework to enhance the mathematical reasoning capabilities of SLMs without relying on larger, more resource-intensive models.  |
| URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics (Read more on [arXiv](https://arxiv.org/abs/2501.04686) or [HuggingFace](https://huggingface.co/papers/2501.04686))| Xinzhe Ni, Yiyao Yu, Yifan Wang, fun6668, AntimageTHU | URSA-7B is a new model for multimodal mathematical reasoning that uses chain-of-thought (CoT) supervision to improve performance. The main research question is how to enhance the CoT reasoning capabilities of Multimodal Large Language Models (MLLMs) in mathematical problem-solving using a new dataset and training method. The key methodology involves a three-module synthesis strategy that integrates CoT distillation, trajectory-format rewriting, and format unification to create a high-quality CoT reasoning instruction fine-tuning dataset, MMathCoT-1M, and a dual-view process supervision data synthesis to train a reward model, URSA-RM-7B. The primary results show that URSA-7B achieves state-of-the-art performance on multiple multimodal mathematical benchmarks, with a 97.1 pass@64 accuracy on the GPS task of MathVista. The principal implication for AI practitioners is that using high-quality CoT datasets and advanced process supervision can significantly enhance MLLMs' mathematical reasoning capabilities, offering a pathway to improve performance in tasks requiring complex, multi-step reasoning.  |
| Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Though (Read more on [arXiv](https://arxiv.org/abs/2501.04682) or [HuggingFace](https://huggingface.co/papers/2501.04682))| Kanishk Gandhi, Charlie Snell, Violet Xiang, nlile, Asap7772 | This paper introduces Meta Chain-of-Thought (Meta-CoT), a framework for enhancing reasoning in large language models (LLMs) by explicitly modeling the underlying thought processes involved in reaching a solution. The main research question is how to enable LLMs to perform complex reasoning analogous to System 2 cognitive processes by integrating search, verification, and iterative refinement into their operational framework. The key methodology involves process supervision, synthetic data generation via search algorithms (e.g. Monte Carlo Tree Search, A*), and reinforcement learning to train models on linearized search traces. Primary results indicate that models trained with Meta-CoT, specifically when using a backtracking strategy at a rate of 50% for incorrect steps, can achieve up to 94% accuracy on hard math problems, compared to 78% for standard Chain-of-Thought models. The principal implication for AI practitioners is that incorporating Meta-CoT into model training can significantly improve the ability of LLMs to solve complex reasoning tasks, suggesting that future model development should focus on integrating explicit search and verification mechanisms.  |
| Agent Laboratory: Using LLM Agents as Research Assistants (Read more on [arXiv](https://arxiv.org/abs/2501.04227) or [HuggingFace](https://huggingface.co/papers/2501.04227))| Jialian Wu, Ximeng Sun, Ze Wang, Yusheng Su, Samuel Schmidgall | Agent Laboratory is an autonomous LLM-based framework designed to conduct the entire research process, from literature review to experimentation and report writing, with optional human feedback. The main research question is whether this framework can accelerate scientific discovery, reduce research costs, and improve research quality. The key methodology involves a three-stage process: literature review using the arXiv API, experimentation using specialized agents and tools like `mle-solver` for code generation, and report writing with a module called `paper-solver` for iterative report generation and refinement. The primary results show that Agent Laboratory driven by o1-preview generates the best research outcomes, and human involvement at each stage improves the overall quality of research, with an 84% decrease in research expenses compared to previous autonomous research methods. The principal implication for AI practitioners is that Agent Laboratory can enable researchers to allocate more effort toward creative ideation rather than low-level coding and writing, potentially accelerating scientific discovery in machine learning.  |
| LLM4SR: A Survey on Large Language Models for Scientific Research (Read more on [arXiv](https://arxiv.org/abs/2501.04306) or [HuggingFace](https://huggingface.co/papers/2501.04306))| Xinya Du, Wei Yang, Ziming Luo, Ason-jay, ZonglinY | LLM4SR is a survey that systematically explores the application of large language models (LLMs) across the scientific research lifecycle. The main research question is how LLMs are being integrated into various stages of scientific research, including hypothesis discovery, experiment planning and implementation, scientific writing, and peer review. The key methodology used involves a comprehensive review and analysis of existing literature, focusing on task-specific methodologies, evaluation benchmarks, and the unique roles LLMs play in each research stage. The primary results indicate that LLMs have been used to generate novel hypotheses, with one study showing LLMs generating hypotheses in chemistry and material science published in high impact journals such as Nature or Science after the training cutoff date of the LLM; however, the paper does not explicitly state quantitative results across all stages. The principal implication for AI practitioners is that LLMs present significant opportunities for enhancing and automating various aspects of the scientific research process, but challenges remain in areas such as ensuring the validity of generated hypotheses and addressing ethical considerations.  |
| InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection (Read more on [arXiv](https://arxiv.org/abs/2501.04575) or [HuggingFace](https://huggingface.co/papers/2501.04575))| Xueyu Hu, Congkai Xie, Zishu Wei, Yuhang Liu, pengxiang | InfiGUIAgent is a multimodal GUI agent designed for task automation on computing devices, trained through a two-stage supervised fine-tuning pipeline. The main research objective is to develop a GUI agent with enhanced reasoning capabilities and reduced reliance on textual annotations. The key methodology involves two-stage supervised fine-tuning (SFT), with Stage 1 focusing on fundamental skills like GUI understanding and grounding using diverse datasets, and Stage 2 integrating hierarchical reasoning and expectation-reflection reasoning skills into synthesized data. Primary results show that InfiGUIAgent-2B achieves 76.3% accuracy on the ScreenSpot benchmark, surpassing several strong baselines. For AI practitioners, the principal implication is that a two-stage SFT approach incorporating hierarchical and expectation-reflection reasoning can significantly enhance GUI agents' performance on benchmarks without reliance on additional GUI metadata, suggesting a path towards more robust and autonomous GUI automation.  |
| GeAR: Generation Augmented Retrieval (Read more on [arXiv](https://arxiv.org/abs/2501.02772) or [HuggingFace](https://huggingface.co/papers/2501.02772))| Hao Sun, Yuefeng Zhan, Jianfeng Liu, Shaohan Huang, noobimp | GeAR: Generation Augmented Retrieval introduces a novel method to enhance document retrieval with fine-grained information localization. The main research question is whether integrating information localization capabilities into existing retrievers is possible without sacrificing their retrieval capabilities. The key methodology involves constructing (query-document-information) triples and employing a text decoder to generate relevant fine-grained information from fused query and document representations, optimized with contrastive learning. The primary results show that GeAR achieves competitive performance on retrieval tasks, with a recall rate of 0.963 at rank 5 on the PAQ dataset, and effectively localizes information within documents. The principal implication for AI practitioners is that GeAR provides a flexible framework capable of handling both document retrieval and fine-grained unit localization simultaneously, offering new insights into the interpretation of retrieval results.  |
| Chirpy3D: Continuous Part Latents for Creative 3D Bird Generation (Read more on [arXiv](https://arxiv.org/abs/2501.04144) or [HuggingFace](https://huggingface.co/papers/2501.04144))| Chee Seng Chan, Jiankang Deng, Jia Wei Sii, Jing Yang, Kam Woh Ng | This paper introduces Chirpy3D, a novel framework for fine-grained, creative 3D bird generation using continuous part latents. The main research objective is to enable the generation of detailed and creative 3D objects by lifting 2D fine-grained understanding into 3D space and enabling part-level control. The key methodology involves fine-tuning a multi-view diffusion model (MVDream) with 2D images, modeling part latents as continuous Gaussian distributions, and introducing a self-supervised feature consistency loss. Primary results show that Chirpy3D effectively reconstructs 3D subjects, with a cosine similarity score of 0.724 for part composition, and generates novel species with diverse parts. The principal implication for AI practitioners is that Chirpy3D offers a new approach for generating high-quality, creative 3D assets with fine-grained control, which is directly applicable to improve creative freedom and output detail in 3D content creation.  |
| SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single Images (Read more on [arXiv](https://arxiv.org/abs/2501.04689) or [HuggingFace](https://huggingface.co/papers/2501.04689))| Varun Jampani, James M. Rehg, Aaryaman Vasishta, Zixuan Huang, mboss | SPAR3D is a two-stage model for reconstructing 3D objects from single images. The main research question is how to combine the strengths of regression-based and diffusion-based methods for single-image 3D object reconstruction while avoiding their limitations. The key methodology involves a two-stage approach: first, a point diffusion model generates a sparse 3D point cloud, and second, a meshing stage uses the point cloud and the input image to create a detailed mesh. On the GSO dataset, SPAR3D achieves a Chamfer Distance (CD) of 0.120, outperforming prior methods. The principal implication for AI practitioners is that SPAR3D offers a computationally efficient approach to generate high-quality 3D meshes from single images, with an inference speed of 0.7 seconds per object, and enables interactive user edits.  |
| DPO Kernels: A Semantically-Aware, Kernel-Enhanced, and Divergence-Rich Paradigm for Direct Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2501.03271) or [HuggingFace](https://huggingface.co/papers/2501.03271))| Rajarshi Roy, Danush Khanna, Suranjana Trivedy, Amitava Das, amanchadha | Here is a concise summary of the AI research paper "DPO-Kernels: A Semantically-Aware, Kernel-Enhanced, and Divergence-Rich Paradigm for Direct Preference Optimization":  i) This paper introduces DPO-Kernels, an enhanced framework for direct preference optimization (DPO) that integrates kernel methods and alternative divergence measures to improve alignment of large language models with human preferences. ii) The main research objective is to address the limitations of standard DPO in aligning models with diverse human values and preferences by proposing a more expressive and adaptable framework. iii) The key methodology involves integrating kernelized representations (using polynomial, RBF, Mahalanobis, and spectral kernels), a hybrid loss function combining probability-based and embedding-based signals, and alternative divergence measures (Jensen-Shannon, Hellinger, RÃ©nyi, Bhattacharyya, Wasserstein, and f-divergences), along with data-driven selection of kernel-divergence pairs and a Hierarchical Mixture of Kernels (HMK). iv) Evaluations on 12 datasets show that DPO-Kernels, particularly HMK, achieve state-of-the-art generalization in factuality, safety, reasoning, and instruction-following tasks, with HMK demonstrating a performance improvement of up to 9.2% over baseline DPO. v) The principal implication for AI practitioners is that DPO-Kernels provide a more robust and flexible framework for preference alignment in large language models, but they must carefully consider the 3-4x higher computational costs associated with HMK.  |
| EpiCoder: Encompassing Diversity and Complexity in Code Generation (Read more on [arXiv](https://arxiv.org/abs/2501.04694) or [HuggingFace](https://huggingface.co/papers/2501.04694))| Xiao Liu, Jie Wu, Yaoxiang Wang, CharonBony, Ringo1110 | EpiCoder is a novel feature tree-based code synthesis framework designed to enhance the diversity and complexity of code generation. The main research question is how to generate more nuanced, diverse, and complex code instruction data that aligns with real-world programming scenarios. The key methodology involves a feature tree-based synthesis inspired by Abstract Syntax Trees (AST) that models semantic relationships between code elements, iteratively refined to enhance feature diversity. The primary results show that EpiCoder-Qwen-7B achieves state-of-the-art performance on function-level code generation benchmarks, with an 81.7% average pass rate on HumanEval and MBPP. The principal implication for AI practitioners is that using EpiCoder's feature tree-based framework can significantly improve the quality and diversity of synthesized code data, leading to more robust and adaptable code generation models.  |
