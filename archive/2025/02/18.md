

## Papers for 2025-02-18

| Title | Authors | Summary |
|-------|---------|---------|
| Learning Getting-Up Policies for Real-World Humanoid Robots (Read more on [arXiv](https://arxiv.org/abs/2502.12152) or [HuggingFace](https://huggingface.co/papers/2502.12152))| Saurabh Gupta, Zixuan Chen, Xialin He, RunpeiDong | The paper introduces HUMANUP, a learning framework for training humanoid robots to get up from various lying positions on diverse terrains. The main research objective is to develop a controller that enables humanoid robots to autonomously recover from falls in real-world settings. The key methodology is a two-stage reinforcement learning approach with a curriculum, where Stage I discovers a getting-up trajectory and Stage II refines it into a deployable, robust policy via imitation learning and control regularization. The primary results show that the learned policy enables a Unitree G1 robot to get up from supine poses with a 78.3% success rate on varied terrains, outperforming the robot's built-in controller. The principal implication is that this framework provides AI practitioners a method to train robust fall recovery policies for humanoid robots, enhancing their real-world deployability by making robots more resilient.  |
| Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention (Read more on [arXiv](https://arxiv.org/abs/2502.11089) or [HuggingFace](https://huggingface.co/papers/2502.11089))| Liang Zhao, Junyu Luo, Damai Dai, Huazuo Gao, Jingyang Yuan | The paper introduces NSA, a natively trainable sparse attention mechanism for efficient long-context modeling in large language models. The main research objective is to develop a sparse attention mechanism that improves computational efficiency during both training and inference while maintaining or exceeding the performance of full attention. The key methodology involves a dynamic hierarchical sparse strategy combining coarse-grained token compression with fine-grained token selection, alongside hardware-aligned optimizations for modern GPUs. Results show that NSA achieves up to 9.0x forward and 6.0x backward propagation speedup on 64k-length sequences compared to Full Attention, and outperforms Full Attention on average across general benchmarks (average score of 0.456 vs 0.443). For AI practitioners, NSA provides a method to train and deploy long-context language models with significantly reduced computational cost and improved performance, particularly on tasks requiring long-range dependencies.  |
| ReLearn: Unlearning via Learning for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.11190) or [HuggingFace](https://huggingface.co/papers/2502.11190))| Sendong Zhao, Liming Yang, Ningyuan Zhao, Haoming Xu, Ningyu | ReLearn is a new method for unlearning in large language models that uses data augmentation and positive optimization, addressing limitations of reverse optimization methods. The main research objective is to develop an unlearning method that effectively removes targeted knowledge while preserving model performance, linguistic coherence, and robustness against attacks. ReLearn employs data augmentation with diverse question variations and fine-tuning on synthesized non-sensitive data, along with a comprehensive evaluation framework including Knowledge Forgetting Rate (KFR), Knowledge Retention Rate (KRR), and Linguistic Score (LS). The primary result is that ReLearn achieved a KFR of 0.85 on both KnowUnDo and TOFU datasets while maintaining a high KRR (0.74 on KnowUnDo and 0.89 on TOFU) and preserving linguistic abilities. AI practitioners can utilize ReLearn as an alternative to reverse optimization-based unlearning, providing a method to balance knowledge removal with the preservation of model utility and robustness in applications requiring privacy or copyright compliance.  |
| SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering? (Read more on [arXiv](https://arxiv.org/abs/2502.12115) or [HuggingFace](https://huggingface.co/papers/2502.12115))| Johannes Heidecke, Tejal Patwardhan, Michele Wang, Samuel Miserendino | SWE-Lancer is a benchmark of over 1,400 real-world freelance software engineering tasks from Upwork, valued at $1 million USD, to evaluate large language models' (LLMs) coding and managerial capabilities.  The main research objective is to assess whether frontier LLMs can successfully complete real-world freelance software engineering tasks and earn substantial income.  The key methodology involves evaluating LLMs on two task types: Individual Contributor (IC) SWE tasks, graded via human-verified end-to-end tests, and SWE Manager tasks, assessed by comparing model choices to those of original engineering managers.  Primary results show that the best-performing model, Claude 3.5 Sonnet, achieves 26.2% success on IC SWE tasks and 44.9% on SWE Management tasks on the Diamond set, earning $208,050 out of a possible $500,800.  Principal implication for AI practitioners is that while frontier LLMs demonstrate some capability in real-world software engineering scenarios, significant improvement is needed for reliable, autonomous deployment in freelance work.  |
| HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and Generation (Read more on [arXiv](https://arxiv.org/abs/2502.12148) or [HuggingFace](https://huggingface.co/papers/2502.12148))| Minghao Xu, Chenming Shang, Ye Tian, Ling Yang, comin | HermesFlow is a framework designed to reduce the performance disparity between multimodal understanding and generation in Multimodal Large Language Models (MLLMs). The main research objective is to close the gap between the understanding and generative capabilities of MLLMs. The key methodology used is Pair-DPO, which leverages homologous preference data for both understanding and generation, combined with self-play iterative optimization. The primary results show that HermesFlow achieves an understanding score of 0.533 and a generation score of 0.497, reducing the gap to 0.036, compared to the baseline Show-o's gap of 0.087. For AI practitioners, HermesFlow provides a general alignment framework that demonstrably closes the gap between multimodal understanding and generation tasks within existing MLLM architectures.  |
| SURGE: On the Potential of Large Language Models as General-Purpose Surrogate Code Executors (Read more on [arXiv](https://arxiv.org/abs/2502.11167) or [HuggingFace](https://huggingface.co/papers/2502.11167))| Siqiao Huang, zcliang22, Bohan22 | This paper introduces SURGE, a benchmark for evaluating large language models (LLMs) as general-purpose surrogate code executors. The main research objective is to assess whether LLMs can predict the output and behavior of programs across diverse tasks without actually running the code. The methodology involves creating a benchmark (SURGE) with eight distinct code execution aspects, evaluating various open-source and proprietary LLMs, and conducting a scaling study. A key finding is that Claude-3.5-Sonnet achieves an average accuracy of 34.31% across all subsets in the zero-shot setting. The principal implication for AI practitioners is that while LLMs show some capability in predicting code execution, there are still limitations in their ability to serve as general-purpose surrogate code executors, especially for time-consuming computations.  |
| Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening (Read more on [arXiv](https://arxiv.org/abs/2502.12146) or [HuggingFace](https://huggingface.co/papers/2502.12146))| Mengdi Wang, Yunhai Tong, Ling Yang, Ye Tian, comin | Diffusion-Sharpening fine-tunes diffusion models by optimizing sampling trajectories using a path integral framework, enhancing downstream alignment. The main research objective is to improve diffusion model alignment with user preferences by optimizing the entire sampling trajectory, overcoming limitations of single-timestep optimization. The key methodology, Diffusion-Sharpening, uses a path integral framework to select optimal trajectories during training and leverages reward feedback, implementing this via SFT and RLHF approaches. Primary results show that RLHF Diffusion-Sharpening achieves a CLIP score of 0.338, outperforming baseline SDXL and other methods. The principal implication is that AI practitioners can achieve superior training and inference efficiency, along with better alignment to diverse metrics, by using trajectory-level optimization for diffusion model fine-tuning.  |
| I Think, Therefore I Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2502.10458) or [HuggingFace](https://huggingface.co/papers/2502.10458))| Runtao Liu, Hanrong Ye, Guocheng Qian, Kuan-Chieh Wang, Mifucius | Here's a concise summary of the research paper, adhering strictly to the guidelines provided:  ThinkDiff aligns vision-language models (VLMs) with diffusion models to enable multimodal in-context reasoning in image generation. The main research objective is to empower text-to-image diffusion models with multimodal in-context understanding and reasoning capabilities. The key methodology is aligning VLMs with the decoder of an encoder-decoder large language model (LLM) through a proxy task of vision-language training, leveraging the shared input feature space between the LLM decoder and diffusion decoders. The primary result is that ThinkDiff significantly improves accuracy on the CoBSAT benchmark for multimodal in-context reasoning generation, achieving 46.3% accuracy compared to the previous 19.2%, with only 5 hours of training on 4 A100 GPUs. Principal implication for AI practioners: transfer the multimodal capabilities of VLM without complex reasoning datasets for in-context reasoning tasks, enhancing image generation from diffusion models.  |
| SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL (Read more on [arXiv](https://arxiv.org/abs/2502.11438) or [HuggingFace](https://huggingface.co/papers/2502.11438))| Hwanhee Lee, Byeongjeong Kim, Ingeol Baek, Jimin Lee | SAFE-SQL is a framework that improves Text-to-SQL performance by using large language models (LLMs) to generate and filter synthetic examples for in-context learning. The main research objective is to enhance Text-to-SQL accuracy in an unsupervised manner, particularly in complex or unseen scenarios, without additional fine-tuning. The key methodology involves schema linking, LLM-based example generation, relevance scoring (embedding similarity, keyword/structural alignment, reasoning path validity), and threshold-based filtering. Primary results show SAFE-SQL achieved 87.9% execution accuracy on the Spider development set, outperforming zero-shot and few-shot methods, especially in hard and extra hard categories. The principal implication for AI practitioners is that using self-augmented, fine-grained example selection with LLMs can significantly improve the accuracy and robustness of Text-to-SQL systems without requiring additional model training or relying on predefined training sets.  |
| CRANE: Reasoning with constrained LLM generation (Read more on [arXiv](https://arxiv.org/abs/2502.09061) or [HuggingFace](https://huggingface.co/papers/2502.09061))| Gagandeep Singh, Sasa Misailovic, Shubham Ugare, Tarun Suresh, Debangshu Banerjee | Constrained LLM generation can reduce reasoning abilities, but augmenting output grammars with reasoning rules can preserve it. The main research questions are whether LLMs truly lose reasoning capabilities under constrained decoding and how to reduce syntax errors while preserving unconstrained reasoning. The key methodology is a reasoning-augmented constrained decoding algorithm (CRANE) that alternates between unconstrained generation for reasoning and constrained generation for structurally correct outputs, supported by theoretical analysis of LLM expressivity. CRANE significantly outperforms state-of-the-art constrained decoding strategies and unconstrained decoding, showing up to a 10% accuracy improvement on the GSM-symbolic and FOLIO benchmarks. AI practitioners can use CRANE to improve the accuracy and syntactic correctness of LLM outputs in tasks requiring formal constraints, such as code generation and symbolic reasoning.  |
| Intuitive physics understanding emerges from self-supervised pretraining on natural videos (Read more on [arXiv](https://arxiv.org/abs/2502.11831) or [HuggingFace](https://huggingface.co/papers/2502.11831))| Laurent Najman, Adrien Bardes, Mahmoud Assran, Nicolas Ballas, Quentin Garrido | V-JEPA, a video joint embedding predictive architecture, demonstrates an understanding of intuitive physics when pretrained on natural videos. The main research objective was to investigate the emergence of intuitive physics understanding in deep neural networks trained to predict masked regions in natural videos. Researchers leveraged the violation-of-expectation framework and compared video prediction models in a learned representation space with pixel-space prediction and multimodal large language models. A V-JEPA model trained on natural videos achieved 98% zero-shot accuracy on the IntPhys benchmark. AI practitioners can apply the principle of joint learning of abstract representation space with sensory input prediction, as a robust objective for acquiring intuitive physics understanding in AI models, challenging the reliance on core knowledge.  |
| Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest (Read more on [arXiv](https://arxiv.org/abs/2502.11275) or [HuggingFace](https://huggingface.co/papers/2502.11275))| Jingbo Shang, Feng Yao, Zilong Wang, Letian Peng | Cuckoo is a novel information extraction (IE) model that leverages large language model (LLM) resources for pre-training via a new paradigm called Next Tokens Extraction (NTE). The main research objective is to demonstrate that IE models can be effectively pre-trained using the same data and a similar paradigm as LLMs, overcoming data scarcity limitations in traditional IE pre-training. The key methodology is converting next token prediction in LLMs to next token extraction (NTE) using BIO tags, applied to 102.6M instances derived from the C4 and TuluV3 datasets. Cuckoo outperforms existing pre-trained IE models in few-shot settings, achieving a 70.63 average F1 score across six basic IE tasks, surpassing baselines significantly. AI practitioners can leverage the NTE paradigm to train versatile and efficient IE models using readily available LLM pre-training resources, avoiding expensive manual annotation and enabling adaptation to a variety of IE tasks.  |
| Dyve: Thinking Fast and Slow for Dynamic Process Verification (Read more on [arXiv](https://arxiv.org/abs/2502.11157) or [HuggingFace](https://huggingface.co/papers/2502.11157))| Qiang Xu, Xiangyu Wen, Zhijian Xu, Zeju Li, Jianyuan1 | Dyve is a dynamic process verifier that enhances reasoning error detection in large language models by integrating fast and slow thinking. The main research objective is to improve the accuracy and efficiency of process verification in large language models' reasoning. The key methodology is a dual-system approach, adaptively applying "System 1" (fast, token-level) and "System 2" (slow, comprehensive) verification, supported by step-wise consensus-filtered process supervision using Monte Carlo estimation, LLM-as-a-Judge, and specialized reasoning models. Dyve achieved an F1 score of 68.5 on the GSM8K subset of ProcessBench, outperforming existing process-based verifiers. AI practitioners can use Dyve's dual-system approach for more reliable and efficient process verification in LLM-based reasoning systems, as it offers superior error detection to traditional process-based methods.  |
| PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning (Read more on [arXiv](https://arxiv.org/abs/2502.12054) or [HuggingFace](https://huggingface.co/papers/2502.12054))| Jiaxing Huang, Yanrui Wu, Yuxuan Dong, Xinyu Zhang, ChengyouJia | PhysReason is a new benchmark for evaluating physics-based reasoning capabilities of large language models (LLMs). The main research objective is to create a comprehensive benchmark to assess LLMs' ability to solve physics problems requiring multi-step reasoning and application of physics theorems. The methodology involves compiling 1,200 physics problems categorized by difficulty and knowledge/reasoning type, and proposing the Physics Solution Auto Scoring Framework (PSAS) for evaluation. Primary results showed that even top-performing models like Deepseek-R1 achieved less than 60% on answer-level evaluation, with performance dropping from 75.11% on knowledge questions to 31.95% on hard problems. Principal implication for AI practitioners: the benchmark highlights limitations of current LLMs and can help to improve future models on tasks for physics-based reasoning and applications such as robotics.  |
| System Message Generation for User Preferences using Open-Source Models (Read more on [arXiv](https://arxiv.org/abs/2502.11330) or [HuggingFace](https://huggingface.co/papers/2502.11330))| Teakgyu Hong, Dawoon Jung, Minsoo Khang, Jungho Cho, Minbyul Jeong | SYSGEN, a data construction pipeline, generates system messages and aligned assistant responses for large language models using open-source models. The main research objective is to address the scarcity and license restrictions of existing datasets with system messages by automatically generating diverse, instruction-aligned system messages. The key methodology involves a four-phase pipeline: generating system messages with eight key functionalities, filtering mis-specified tags, verifying functionalities using an LLM-as-a-judge approach, and generating new, aligned assistant responses. Training on SYSGEN data improved model alignment, with LLaMA-3.1-8B-instruct and Phi-4 models achieving +0.9 and +0.13 absolute improvements, respectively, on the Multifacet benchmark. AI practitioners can leverage SYSGEN to enhance model alignment with user instructions and preferences while minimizing performance degradation on unseen benchmarks and avoiding licensing issues related to training data.  |
| video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model (Read more on [arXiv](https://arxiv.org/abs/2502.11775) or [HuggingFace](https://huggingface.co/papers/2502.11775))| Yixuan Li, Changli Tang, Jimin Zhuang, Yudong Yang, Guangzhi Sun | video-SALMONN-01 is an open-source audio-visual large language model designed for enhanced reasoning in general video understanding tasks. The main research objective is to improve the reasoning capabilities of audio-visual LLMs for general video understanding, beyond the existing focus on mathematical problems and visual graphical inputs. The key methodology involves developing a reasoning-intensive dataset with step-by-step solutions, proposing process direct preference optimization (pDPO) for step-level reward modeling, and introducing RivaBench, a new video understanding benchmark. Primary results show that video-SALMONN-01 achieves 3-8% accuracy improvements over the LLaVA-OneVision baseline across different video reasoning benchmarks, and pDPO achieves 6-8% improvements compared to the supervised fine-tuning model on RivaBench. AI practitioners can utilize video-SALMONN-01 and the pDPO method for building applications requiring advanced audio-visual reasoning, such as complex video comprehension and synthetic video detection.  |
| Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarsity (Read more on [arXiv](https://arxiv.org/abs/2502.11901) or [HuggingFace](https://huggingface.co/papers/2502.11901))| Tianran Sun, Justin Wang, Dylan Zhang | This paper introduces PoPilot, a fine-tuned language model designed to address data scarcity in proof-oriented programming with F*. The main research objective is to improve language models' performance on project-level proof generation and repair in F* under data-scarce conditions. The key methodology involves synthetic data augmentation, creating new proof-oriented programming problems, incorporating diverse coding data, and generating repair data within existing repositories. The primary result shows that the 14B parameter model, PoPilot, outperforms GPT-4o in project-level proof-oriented programming by a 64% relative margin. AI practitioners can leverage the proposed synthetic data generation strategies to create specialized verification assistants capable of both synthesizing and repairing proofs to reduce the cost of adaptation of language model.  |
| MagicArticulate: Make Your 3D Models Articulation-Ready (Read more on [arXiv](https://arxiv.org/abs/2502.12135) or [HuggingFace](https://huggingface.co/papers/2502.12135))| Yiwen Chen, Fan Yang, Xiu Li, Jianfeng Zhang, chaoyue7 | MagicArticulate is a framework that automatically converts static 3D models into animation-ready assets with skeletons and skinning weights. The main research objective is to develop a scalable method for automatically generating articulation-ready 3D models, addressing the limitations of manual annotation and existing template-based or template-free approaches. The key methodology involves a two-stage pipeline: an auto-regressive transformer for skeleton generation formulated as a sequence modeling problem, followed by a functional diffusion process for skinning weight prediction that incorporates volumetric geodesic distance priors. The method achieves a Chamfer Distance (CD-J2J) of 2.586 on the Articulation-XL dataset for skeleton generation, outperforming existing methods. For AI practitioners, MagicArticulate provides a scalable solution to automatically rig 3D models, significantly reducing the manual effort required for animation content creation and potentially accelerating the development of animation pipelines.  |
| Talk Structurally, Act Hierarchically: A Collaborative Framework for LLM Multi-Agent Systems (Read more on [arXiv](https://arxiv.org/abs/2502.11098) or [HuggingFace](https://huggingface.co/papers/2502.11098))| Shingo Takamatsu, Briti Gangopadhyay, Wei-Yao Wang, Sota Moriyama, Zhao Wang | i) The paper introduces TalkHier, a novel framework for LLM Multi-Agent (LLM-MA) systems designed to improve communication and refinement in complex collaborative tasks. ii) The research aims to address challenges in managing communication and refinement among agents in LLM-MA systems. iii) The methodology involves a structured communication protocol and a hierarchical refinement system. iv) TalkHier achieves 88.38% accuracy on the MMLU benchmark when built on GPT40, outperforming inference scaling models and open-source multi-agent models. v) The principal implication for AI practitioners is a new standard for LLM-MA systems, providing a more effective, adaptable, and collaborative framework.  |
| One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMs (Read more on [arXiv](https://arxiv.org/abs/2502.10454) or [HuggingFace](https://huggingface.co/papers/2502.10454))| Xinnian Liang, Zhikun Xu, Haojing Huang, Jiayi Kuang, Yinghui Li | This paper introduces COUNTERMATH, a new benchmark for evaluating counterexample-driven conceptual reasoning in mathematical Large Language Models (LLMs). The main research objective is to assess and enhance LLMs' ability to understand mathematical concepts through counterexample-driven proofs, moving beyond reliance on "drill-based" learning. The key methodology involves creating a dataset of 1,216 university-level mathematical statement-rationale pairs from textbooks and developing a data engineering framework for automatically acquiring training data. Primary results show that even advanced LLMs like OpenAI o1 achieve a relatively low F1 score (60.1) on COUNTERMATH, and a fine-tuned model with only 1,025 training samples significantly outperformed baseline models. The principal implication for AI practitioners is that strengthening LLMs' counterexample-driven reasoning is crucial for improving their overall mathematical capabilities, and this work provides a benchmark and methodology to pursue this.  |
| Better Embeddings with Coupled Adam (Read more on [arXiv](https://arxiv.org/abs/2502.08441) or [HuggingFace](https://huggingface.co/papers/2502.08441))| Tobias Stollenwerk, flxst | The paper introduces Coupled Adam, a modification of the Adam optimizer, to address the anisotropy problem in language model embeddings. The main research question is whether the second moment in the Adam optimizer contributes to anisotropic word embeddings in language models and how this can be mitigated. The key methodology involves analyzing the embedding update vectors under SGD and Adam, proposing a modified Adam optimizer ("Coupled Adam") that averages the second moment across vocabulary items, and empirically evaluating its impact on embedding quality and model performance. Primary results show Coupled Adam improves embedding isotropy significantly, achieving values above 0.90 in most small-scale experiments, and enhances upstream/downstream performance on sufficiently large datasets. For AI practitioners, using Coupled Adam instead of standard Adam can improve the quality of word embeddings and boost model performance, particularly for large language models.  |
| Towards Data-Efficient Pretraining for Atomic Property Prediction (Read more on [arXiv](https://arxiv.org/abs/2502.11085) or [HuggingFace](https://huggingface.co/papers/2502.11085))| Bernard Ghanem, Yasir Ghunaim, hammh0a | This paper investigates data-efficient pretraining for atomic property prediction, showing that strategic dataset selection can match or surpass large-scale pretraining with significantly reduced computational cost. The main research objective is to determine if pretraining on a smaller, task-relevant dataset can achieve comparable or superior performance to large-scale pretraining in atomic property prediction. The key methodology introduces the Chemical Similarity Index (CSI), a metric inspired by Fr√©chet Inception Distance, to quantify the alignment between upstream pretraining datasets and downstream tasks, and uses this to select pretraining data. A primary result is that models pretrained on the ANI-1x dataset (using the CSI for selection) achieved a Mean Absolute Error (MAE) of 5.4 on rMD17, outperforming JMP-S (MAE of 6.7) with 24 times less computational budget. Principal implication for AI practitioners is that strategic selection of pretraining data based on task relevance, assessed using metrics like CSI, can achieve competitive performance with significantly reduced computational resources in atomic property prediction, favoring quality over quantity.  |
| Large Language Models and Mathematical Reasoning Failures (Read more on [arXiv](https://arxiv.org/abs/2502.11574) or [HuggingFace](https://huggingface.co/papers/2502.11574))| birgermoell, jboye | This paper evaluates the mathematical reasoning capabilities of large language models (LLMs) using newly constructed word problems and identifies common failure modes. The main research question is: How good are LLMs at mathematical reasoning when evaluated on both answer correctness and solution steps? The key methodology involved creating a dataset of 50 high-school-level mathematical word problems and manually assessing the answers and solutions provided by eight LLMs, including Mixtral, Llama, Gemini, and GPT-4o. The primary result was that the o1 model achieved the highest accuracy, correctly solving 37 out of 50 problems, while all models exhibited errors in spatial reasoning, strategic planning, and arithmetic. The principal implication for AI practitioners is the need to evaluate LLMs' reasoning processes, not just their final answers, to avoid overestimating their problem-solving proficiency.  |
| Language Complexity Measurement as a Noisy Zero-Shot Proxy for Evaluating LLM Performance (Read more on [arXiv](https://arxiv.org/abs/2502.11578) or [HuggingFace](https://huggingface.co/papers/2502.11578))| jboye, birgermoell | This paper evaluates the capability of Large Language Models (LLMs) to measure language complexity as a proxy for general LLM performance. The main research objective is to examine the performance of state-of-the-art LLMs on computing the LIX readability metric and performing dependency parsing to calculate Average Dependency Distance (ADD). The methodology involves evaluating six LLMs using Swedish essays, comparing their LIX and ADD computations against ground truth values, and correlating these with MMLU benchmark scores. A primary result is a strong significant correlation of -0.875 (p=0.026) between the models' accuracy in computing LIX and their MMLU performance. For AI practitioners, language complexity measurement abilities, specifically LIX computation, can serve as a practical, noisy zero-shot proxy for assessing general LLM capabilities, without needing extensive benchmarking datasets.  |
