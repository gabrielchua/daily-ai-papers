

## Papers for 2025-02-20

| Title | Authors | Summary |
|-------|---------|---------|
| Qwen2.5-VL Technical Report (Read more on [arXiv](https://arxiv.org/abs/2502.13923) or [HuggingFace](https://huggingface.co/papers/2502.13923))| Keqin Chen, Shuai Bai, xhyandwyy, darkpromise, ayumiymk | i) Qwen2.5-VL is a new vision-language model in the Qwen series with advancements in visual recognition, object localization, document parsing, and long-video comprehension. ii) The research aims to improve the foundational and agentic capabilities of vision-language models, particularly in fine-grained visual perception and real-world applications. iii) The methodology involves training a native dynamic-resolution Vision Transformer (ViT) from scratch, incorporating Window Attention, dynamic FPS sampling, absolute time encoding with MROPE, and curating a large pre-training dataset of 4.1 trillion tokens. iv) The Qwen2.5-VL-72B model achieves 74.8 on MathVista and mIoU score of 50.9 on Charades-STA, and matches state-of-the-art performance, while smaller models offer strong capabilities in resource-constrained environments. v) AI practitioners can leverage Qwen2.5-VL's improved document understanding, precise object grounding, and long-video comprehension to develop more robust and versatile multimodal applications, particularly in domains requiring detailed visual analysis and interactive agent functionalities, with attention to the computational benefits conferred by Window Attention and dynamic resolution processing.  |
| RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2502.13144) or [HuggingFace](https://huggingface.co/papers/2502.13144))| Yiang Shi, Bencheng Liao, Bo Jiang, Shaoyu Chen, Hao605 | RAD establishes a 3DGS-based closed-loop Reinforcement Learning (RL) paradigm for training end-to-end autonomous driving policies. The main research objective is to address causal confusion and the open-loop gap in existing Imitation Learning (IL) methods for autonomous driving. The key methodology involves constructing photorealistic digital replicas of the real world using 3D Gaussian Splatting (3DGS) techniques, incorporating IL as a regularization term in RL training, and designing specialized safety-related rewards. The primary results show that, compared to IL-based methods, RAD achieves a 3x lower collision rate on a closed-loop evaluation benchmark consisting of unseen 3DGS environments. For AI practitioners, this suggests that 3DGS-based RL training, combined with IL, can improve the safety and robustness of end-to-end autonomous driving policies, by allowing large scale training in a realistic virtual world.  |
| SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation (Read more on [arXiv](https://arxiv.org/abs/2502.13128) or [HuggingFace](https://huggingface.co/papers/2502.13128))| Pan Zhang, Xiaoyi Dong, Zhixiong Zhang, Shuangrui Ding, Zihan Liu | SongGen is a single-stage auto-regressive transformer model for generating songs with vocals and accompaniment from text inputs. The main research objective is to investigate whether a single-stage model can achieve effective text-to-song generation, simplifying the often cumbersome multi-stage pipelines. The key methodology involves a transformer decoder that predicts audio tokens, incorporating user controls via cross-attention, and exploring mixed and dual-track output modes with diverse token patterns. Primary results show that the "Interleaving (A-V)" dual-track mode achieves a Frechet Audio Distance (FAD) of 1.87, competitive with mixed-mode generation. AI practitioners can use SongGen as an open-source, controllable baseline for text-to-song generation, and the provided annotated data and preprocessing pipeline simplify future research.  |
| MoM: Linear Sequence Modeling with Mixture-of-Memories (Read more on [arXiv](https://arxiv.org/abs/2502.13685) or [HuggingFace](https://huggingface.co/papers/2502.13685))| Yu Cheng, Jiaxi Hu, Disen Lan, Jusen Du, weigao266 | MoM introduces a linear sequence modeling architecture that uses multiple memory states to improve recall performance. The main research objective is to enhance the memory capacity and reduce memory interference in linear sequence models, addressing limitations of existing approaches that compress sequences into a single fixed-size state. The methodology involves a Mixture-of-Memories (MoM) architecture with multiple independent memory states and a router network that directs input tokens to specific memory states, using an RNN-like update mechanism. Primary results show that MoM significantly outperforms current linear sequence models on downstream language tasks, with the 1.3B parameter MoM achieving an average score of 36.04 on recall-intensive tasks, close to the Transformer model's 37.31. For AI practitioners, MoM offers a more efficient architecture to enhance the memory and recall of linear sequence modeling for applications, retaining linear-time training and constant-memory inference, presenting itself as an alternative to Transformers.  |
| Craw4LLM: Efficient Web Crawling for LLM Pretraining (Read more on [arXiv](https://arxiv.org/abs/2502.13347) or [HuggingFace](https://huggingface.co/papers/2502.13347))| Chenyan Xiong, Zhiyuan Liu, yushi | CRAW4LLM is an efficient web crawling method that prioritizes webpages based on their predicted influence on large language model (LLM) pretraining. The research objective is to improve the efficiency of web crawling for LLM pretraining data collection by aligning crawler priorities with LLM pretraining needs. The key methodology is to use a pretraining influence scorer, derived from data-filtering pipelines, to score newly discovered documents and prioritize them in the crawler's queue, replacing traditional graph-connectivity-based metrics. Primary results show that LLMs pretrained on data crawled by CRAW4LLM, using only 21% of the URLs, achieve the same downstream performance as previous crawls that used more data. Principal implication is that by using CRAW4LLM AI practitioners can get similar performing LLM, while significantly reducing the required web crawling and data processing, thus saving time and resources.  |
| LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2502.13922) or [HuggingFace](https://huggingface.co/papers/2502.13922))| Lidong Bing, Michael Qizhe Shieh, Xin Li, Guanzheng Chen | LongPO is a method that enables short-context LLMs to self-evolve to handle long-context tasks by internally transferring short-context capabilities through preference optimization. The main research objective is to address the challenges of long-context alignment in LLMs, specifically the scarcity of long-context annotated data and the difficulty in balancing short- and long-context performance. The key methodology involves generating short-to-long preference data using a short-context LLM and applying a DPO-style objective with a KL constraint to maintain short-context performance. The primary result is that LongPO applied to Mistral-7B-Instruct-v0.2 improved performance on InfiniteBench by 25.45 points and achieved comparable or superior results to larger LLMs like GPT-4-128K. The principal implication for AI practitioners is that LongPO offers an efficient way to extend the context length of LLMs without extensive long-context data annotation or significant degradation of short-context capabilities, providing a more balanced approach to developing long-context LLMs.  |
| Small Models Struggle to Learn from Strong Reasoners (Read more on [arXiv](https://arxiv.org/abs/2502.12143) or [HuggingFace](https://huggingface.co/papers/2502.12143))| Luyao Niu, Fengqing Jiang, Xiang Yue, Yuetai Li, flydust | Small language models (≤3B parameters) do not consistently benefit from complex reasoning data or distillation from larger models, instead performing better with simpler reasoning. The main research question is whether small language models can effectively learn from the reasoning capabilities of larger, more powerful language models. The key methodology involves fine-tuning student models of varying sizes on different types of Chain-of-Thought (CoT) data (long, short, large teacher, small teacher) generated from the MATH dataset and evaluating their performance on multiple math benchmarks. A key result is that Qwen2.5-3B-Instruct improves by more than 8 points on MATH and AMC using Mix-Long, compared to direct training on long CoT data. The principal implication is that AI practitioners should adapt reasoning complexity during distillation, using techniques like Mix Distillation, to effectively transfer reasoning capabilities to smaller models, instead of directly using complex reasoning data from large models.  |
| Autellix: An Efficient Serving Engine for LLM Agents as General Programs (Read more on [arXiv](https://arxiv.org/abs/2502.13965) or [HuggingFace](https://huggingface.co/papers/2502.13965))| Tianjun Zhang, Colin Cai, Xiaoxiang Shi, Michael Luo, Chrisyichuan | Autellix is an LLM inference system designed to efficiently serve agentic programs, treating them as first-class citizens to minimize end-to-end latency. The main research objective is to reduce the end-to-end latencies of agentic programs composed of dynamic, non-deterministic DAGs of LLM calls and interrupts. The key methodology used is program-aware scheduling, prioritizing LLM calls based on program-level statistics (cumulative service time) and employing a data locality-aware load balancer across multiple engines. Primary results show that Autellix improves program throughput by 4-15x compared to state-of-the-art systems like vLLM, across diverse LLMs and agentic workloads. The principal implication is that AI practitioners can significantly improve the performance of LLM agent applications by using a serving system that prioritizes the scheduling of LLM calls based on full program execution, and data-locality, rather than treating each call independently.  |
| Presumed Cultural Identity: How Names Shape LLM Responses (Read more on [arXiv](https://arxiv.org/abs/2502.11995) or [HuggingFace](https://huggingface.co/papers/2502.11995))| Lucie-Aimée Kaffee, Arnav Arora, Siddhesh Pawar, IAugenstein | LLMs exhibit cultural biases in responses based on user names, influencing personalization.  The main research objective is to investigate cultural presumptions in LLM responses when presented with common suggestion-seeking queries including user names.  The key methodology involves prompting LLMs with names from 30 cultures and analyzing generated responses for cultural bias using an LLM-as-a-judge approach and assertion-based evaluation.  The primary result showed that LLM responses exhibit varying degrees of cultural bias, with clothing-related queries showing a roughly 70% increase in bias when names were included.  Principal implication is that AI practitioners need to consider the impact of names on LLM outputs and design personalisation systems that avoid reinforcing stereotypes while utilizing names.  |
| Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region (Read more on [arXiv](https://arxiv.org/abs/2502.13946) or [HuggingFace](https://huggingface.co/papers/2502.13946))| Wenjie Li, Jian Wang, Qingyu Yin, Chak Tou Leong | Aligned large language models (LLMs) exhibit a vulnerability where their safety mechanisms overly rely on information within a specific "template region" inserted between user input and model output. The research investigates the phenomenon of "template-anchored safety alignment" (TASA) in aligned LLMs. The methodology involves analyzing attention weight distributions, performing activation patching interventions, and probing harmfulness features across different layers and positions, and propose a detaching safety mechanism. Results show that intervening in intermediate states in template region significantly increases the likelihood of harmful initial compliance decisions, with a normalized indirect effect (NIE) showing considerable gains by patching small number of heads. The findings suggest AI practitioners should develop more robust safety alignment techniques that are less reliant on the template region for safety-related decision-making to reduce the risk of adversarial attacks.  |
| SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question Answering? (Read more on [arXiv](https://arxiv.org/abs/2502.13233) or [HuggingFace](https://huggingface.co/papers/2502.13233))| Tianming Liu, Quanzheng Li, Canyu Chen, Tianze Yang, YuchengShi | SearchRAG is a novel retrieval-augmented generation framework that leverages search engines to enhance large language models' (LLMs) performance in medical question answering. The main research objective is to determine how to effectively integrate search engines with LLMs for improved retrieval of medical knowledge. The key methodology involves synthetic query generation using LLMs to create search-engine-friendly queries and uncertainty-based knowledge selection to filter retrieved information. Primary results show that SearchRAG improved the LLaMA 8B model's accuracy by an average of 12.61% compared to baseline methods on medical QA tasks. Principal implication for AI practitioners is that SearchRAG's method is capable of adressing limitations of conventional Retrieval-Augmented Generation (RAG) systems, showing that real-time search integration improves response accuracy.  |
| Thinking Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2502.13173) or [HuggingFace](https://huggingface.co/papers/2502.13173))| Xiaotian Han, Vipin Chaudhary, Jingfeng Yang, Hongye Jin, Wang Yang | Thinking Preference Optimization (ThinkPO) enhances reasoning in fine-tuned language models without requiring new long chain-of-thought (CoT) responses. The main research objective is to improve the reasoning performance of supervised fine-tuned (SFT) language models without collecting new long CoT data or repeatedly training on existing SFT datasets. The key methodology is to use readily available short CoT reasoning responses as rejected answers and existing long CoT responses as chosen answers, applying direct preference optimization (DPO) to encourage longer reasoning outputs. The primary result is that ThinkPO increases the math reasoning accuracy of SFT-ed models by 8.6% and output length by 25.9%, for example it increased performance on MATH500 of a tested model from 87.4% to 91.2%. AI practitioners can use ThinkPO as a post-SFT method to further improve the reasoning performance of their models, especially when acquiring new long CoT data is costly or repeated training leads to a performance plateau.  |
| Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering (Read more on [arXiv](https://arxiv.org/abs/2502.13962) or [HuggingFace](https://huggingface.co/papers/2502.13962))| Benjamin Van Durme, Jeffrey Cheng, wjurayj | Test-time scaling of compute improves the performance of large language models on selective question answering by increasing confidence in correct answers. The research investigates how increasing computational budget at inference time impacts model confidence and accuracy in question answering. The methodology involves evaluating models at varying compute budgets and confidence thresholds, using a selection function that rejects answers below a confidence threshold. The results show that increasing the compute budget improves the average confidence of correct answers, and selective answering at a threshold of 0.95 dramatically improves performance in a Jeopardy setting where incorrect answers are penalized. AI practitioners should report test-time scaling performance under conditions that penalize incorrect answers ("Jeopardy Odds") in addition to traditional settings, to accurately reflect selective question answering capabilities.  |
| AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence (Read more on [arXiv](https://arxiv.org/abs/2502.13943) or [HuggingFace](https://huggingface.co/papers/2502.13943))| Jason Klein Liu, Chaofeng Qu, Zhaoling Chen, Junjie Lu, Yuliang Liu | AdaptiveStep, a novel method, automatically divides reasoning steps in large language models (LLMs) based on model confidence to enhance process reward model (PRM) training and performance. The main research objective is to develop an automated, informative, and general method for dividing reasoning steps that improves upon existing rule-based approaches. The key methodology, AdaptiveStep, utilizes the LLM's prediction confidence for the next word to identify critical breaking points, creating more informative step divisions without manual annotation. Results show that the AdaptiveStep-trained PRM (ASPRM) achieves state-of-the-art Best-of-N performance, outperforming greedy search with token-level value-guided decoding (TVD) by 3.15% on GSM8k. For AI practitioners, AdaptiveStep provides a more efficient and precise method for training PRMs, reducing construction costs and enhancing downstream task performance, specifically in mathematical reasoning and code generation.  |
| NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation (Read more on [arXiv](https://arxiv.org/abs/2502.12638) or [HuggingFace](https://huggingface.co/papers/2502.12638))| Enzhi Zhang, Han Huang, Yanchen Luo, Zhiyuan Liu, xiangwang1223 | NExT-Mol is a foundation model for 3D molecule generation that combines 3D diffusion with 1D language modeling. The main research objective is to improve 3D molecule generation by integrating the strengths of 1D SELFIES-based language models (LMs) and 3D diffusion models. The methodology involves pretraining a 960M parameter 1D molecule LM (MoLlama) on 1.8B SELFIES, then predicting 3D conformers with a novel diffusion model (Diffusion Molecule Transformer, DMT) and using cross-model transfer learning to enhance DMT. NExT-Mol achieves a 26% relative improvement in 3D FCD for *de novo* 3D generation on GEOM-DRUGS compared to previous methods. AI practitioners can leverage this approach to generate 3D molecules with improved validity and distributional similarity, facilitating drug discovery and material design by combining large-scale 1D pretraining with 3D diffusion.  |
| ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation (Read more on [arXiv](https://arxiv.org/abs/2502.13581) or [HuggingFace](https://huggingface.co/papers/2502.13581))| Wang-Cheng Kang, Noveen Sachdeva, Zhankui He, Jianmo Ni, hyp1231 | ActionPiece is a novel tokenization method for generative recommendation that incorporates contextual information to improve performance. The main research objective is to develop a context-aware action sequence tokenizer for generative recommendation models, addressing the limitation of existing models that tokenize each action independently. The key methodology, ActionPiece, represents each action as a set of item features, constructs a vocabulary by merging frequent feature patterns, and uses set permutation regularization to produce multiple segmentations. The primary result is that ActionPiece outperforms existing action tokenization methods, improving NDCG@10 by 6.00% to 12.82% on public datasets. The principal implication is that AI practitioners can use ActionPiece to improve the accuracy and efficiency of generative recommendation systems by considering contextual relationships among user actions.  |
| Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.13533) or [HuggingFace](https://huggingface.co/papers/2502.13533))| Ke Chen, Lidan Shou, Huan Li, Jue Wang, junzhang98 | LORAM is introduced as a memory-efficient LoRA training scheme for LLMs. This research aims to reduce the memory footprint of LoRA training by training on a pruned model and recovering weights for inference on the original model. LORAM employs pruning during training followed by a recovery and alignment phase utilizing continual pre-training on a small dataset. QLORAM, combining structured pruning and 4-bit quantization, achieved a 15.81× parameter storage reduction for LLaMA-3.1-70B while maintaining or improving performance. LORAM enables training on resource-constrained hardware and suggests an alternative to full fine-tuning.  |
| GIMMICK -- Globally Inclusive Multimodal Multitask Cultural Knowledge Benchmarking (Read more on [arXiv](https://arxiv.org/abs/2502.13766) or [HuggingFace](https://huggingface.co/papers/2502.13766))| Anne Lauscher, Chris Biemann, Carolin Holtermann, floschne | i) GIMMICK introduces a multimodal benchmark for evaluating cultural knowledge in large vision-language models (LVLMs). ii) The research aims to identify regional biases in LLMs' and LVLMs' cultural understanding and assess the impact of model size, input modalities, and external cues on cultural knowledge. iii) The methodology employs six tasks built on three newly created datasets spanning 728 cultural events across 144 countries, evaluating 31 models using multimodal and unimodal inputs. iv) Results reveal significant regional biases, with models exhibiting up to 14.72pp performance difference between Western and Sub-Saharan African cultural contexts, and multimodal input consistently improving performance. v) AI practitioners should be aware of biases in cultural understanding and leverage multimodal inputs to create more globally inclusive AI systems.  |
| InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning (Read more on [arXiv](https://arxiv.org/abs/2502.11573) or [HuggingFace](https://huggingface.co/papers/2502.11573))| Zhijie Sang, Pengxiang Li, Wenjun Wang, Shuo Cai, Congkai Xie | InfiR introduces efficient Small Language Models (SLMs) and Multimodal SLMs with enhanced reasoning capabilities, deployable on edge devices. The main research objective is to develop SLMs and MSLMs that retain competitive reasoning abilities while reducing model size and computational demands. The key methodology involves a novel pre- and post-training pipeline that includes heuristic filtering, reasoning-oriented text recall, data annealing, and supervised fine-tuning with synthetic data. The InfiR-1B-Instruct model achieved a 2.26x reasoning-related average score improvement over Llama3.2-1B-Base. AI practitioners can leverage InfiR's training pipeline and models to build efficient and privacy-preserving AI systems with strong reasoning capabilities, particularly for edge deployment.  |
| Noise May Contain Transferable Knowledge: Understanding Semi-supervised Heterogeneous Domain Adaptation from an Empirical Perspective (Read more on [arXiv](https://arxiv.org/abs/2502.13573) or [HuggingFace](https://huggingface.co/papers/2502.13573))| Qiang Yang, Jian Jin, Yu Zhang, Xiaopu Zhang, yyyaoyuan | This paper empirically investigates transferable knowledge in semi-supervised heterogeneous domain adaptation (SHDA) tasks. The main research question is: "What is the transferable knowledge in SHDA?" The authors develop a unified Knowledge Transfer Framework (KTF) for SHDA and conduct extensive experiments, including manipulating source sample categories, features, and introducing synthesized noise distributions. A primary result across nearly 330 SHDA tasks is that varying source sample category orders has almost no change in the performance, i.e. average accuracy remains nearly constant. For AI practitioners, the results imply that the discriminability and transferability of the source domain, rather than the category or feature information, are the main factors for effective transfer in SHDA, meaning the choice of origin for source domains is less critical than ensuring those two qualities.  |
