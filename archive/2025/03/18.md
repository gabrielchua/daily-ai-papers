

## Papers for 2025-03-18

| Title | Authors | Summary |
|-------|---------|---------|
| DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal
  Consistent Video Generation (Read more on [arXiv](https://arxiv.org/abs/2503.06053) or [HuggingFace](https://huggingface.co/papers/2503.06053))| Runze Zhang, NeilXu, EllenAP, lixiaochuan, georgedu | DropletVideo introduces a new dataset and model for generating videos with integral spatio-temporal consistency, addressing plot coherence and visual consistency across viewpoints. The main research question is how to ensure integral spatio-temporal consistency in video generation, considering the interplay between plot progression, camera techniques, and prior content impact. The key methodology involves constructing a large-scale dataset (DropletVideo-10M) with detailed captions and developing a diffusion model (DropletVideo) with motion-adaptive generation. Primary results show DropletVideo achieves 37.93% in Camera Motion and 98.94% in Motion Smoothness on VBench++-ISTP benchmarks, indicating a strong ability of DropletVideo to generate videos with integral spatiotemporal consistency. AI practitioners can utilize the open-sourced DropletVideo dataset and model to advance video generation research and applications requiring robust spatio-temporal coherence, particularly multi-plot narratives.  |
| Being-0: A Humanoid Robotic Agent with Vision-Language Models and
  Modular Skills (Read more on [arXiv](https://arxiv.org/abs/2503.12533) or [HuggingFace](https://huggingface.co/papers/2503.12533))| tellarin, SherryXu, takenpeanut, fuyh, Yaya041 | i) Being-0, a hierarchical framework, effectively controls a full-sized humanoid robot for complex embodied tasks by integrating a Foundation Model (FM) with a modular skill library. ii) The research aims to develop a humanoid robotic agent that can perform complex, long-horizon tasks efficiently and robustly in real-world environments. iii) The methodology involves using an FM for high-level planning, a VLM-based Connector module for bridging the gap between the FM and low-level skills, and a modular skill library for locomotion and manipulation. iv) Experiments demonstrate Being-0 achieves an 84.4% average completion rate on long-horizon tasks and 4.2x efficiency in navigation compared to fully FM-based agents when modules (except the FM) are deployed on onboard computation devices. v) The principal implication for AI practitioners is the demonstration of a hierarchical architecture using a lightweight VLM Connector which significantly enhances the embodied decision-making capabilities of humanoid robots and efficiently coordinates locomotion and manipulation.  |
| DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale
  Text-to-Image Models (Read more on [arXiv](https://arxiv.org/abs/2503.12885) or [HuggingFace](https://huggingface.co/papers/2503.12885))| Yi Yang, z-x-yang, aiJojosh, limuloo1999 | DreamRenderer is a training-free approach for controlling attributes of multiple instances in image-conditioned text-to-image generation. The research aims to enable precise control over the content of individual instances or regions within images generated from textual descriptions and conditioning inputs like depth or canny maps. The key methodology involves "Bridge Image Tokens" for Hard Text Attribute Binding to correctly associate text embeddings with visual attributes, and selective application of "Hard Image Attribute Binding" in vital layers of the FLUX model. DreamRenderer improves the Image Success Ratio by 17.7% over FLUX on the COCO-POS benchmark and enhances performance of layout-to-image models like GLIGEN by up to 26.8%. AI practitioners can leverage DreamRenderer as a plug-and-play controller for fine-grained control over multi-instance image generation without additional training, enhancing controllability in applications like animation and game development.  |
| Edit Transfer: Learning Image Editing via Vision In-Context Relations (Read more on [arXiv](https://arxiv.org/abs/2503.13327) or [HuggingFace](https://huggingface.co/papers/2503.13327))| Qi Mao, AnalMom, guyuchao, Orannue | Edit Transfer introduces a new image editing paradigm that learns transformations from single source-target examples and applies them to new images. The main research question is whether an image editing transformation can be learned from a single source-target example and applied to a new query image. The key methodology is visual relation in-context learning, adapting a DiT-based text-to-image model with a four-panel composite input and lightweight LoRA fine-tuning. The primary result is that Edit Transfer outperforms state-of-the-art TIE and RIE methods in non-rigid editing scenarios, achieving a user preference rate exceeding 80% across all aspects in user studies. The principal implication is that AI practitioners can achieve sophisticated non-rigid image editing using minimal data (42 training images total) and a visual relation in-context learning approach, reducing the need for large-scale datasets and extensive training.  |
| Personalize Anything for Free with Diffusion Transformer (Read more on [arXiv](https://arxiv.org/abs/2503.12590) or [HuggingFace](https://huggingface.co/papers/2503.12590))| Lu Sheng, Lin Li, Haoran Feng, lvhairong, huanngzh | Personalize Anything is a training-free framework for personalized image generation in Diffusion Transformers (DiTs) that achieves high-fidelity subject reconstruction and flexible editing. The research aims to develop a training-free method for personalized image generation in DiTs that preserves identity and supports diverse editing scenarios. The key methodology involves timestep-adaptive token replacement with patch perturbation, injecting reference subject tokens in early denoising steps and transitioning to multi-modal attention in later steps. Evaluations on DreamBench demonstrate state-of-the-art performance, with the method achieving a CLIP-I score of 0.876 and a DreamSim score of 0.179 in single-subject personalization, surpassing existing approaches. AI practitioners can leverage this framework for efficient, high-fidelity personalized image generation and editing in DiTs without the need for training or fine-tuning, achieving superior identity preservation.  |
| WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range
  Movements and Scenes (Read more on [arXiv](https://arxiv.org/abs/2503.13435) or [HuggingFace](https://huggingface.co/papers/2503.13435))| mingbao, zbhpku, Juanxi, czkk566, Lingaaaaaaa | WideRange4D enables high-quality 4D scene reconstruction, including wide-range spatial movements of objects, by introducing a new benchmark and a two-stage reconstruction method. The main research objective is to address the limitations of existing 4D reconstruction methods and datasets in handling scenes with significant object spatial variations. The key methodology involves curating a new benchmark, WideRange4D, and proposing a two-stage 4D reconstruction method, Progress4D, which first initializes a high-quality 3D scene and then progressively fits 4D dynamics. Primary results show that Progress4D achieves a PSNR of 28.86 on the WideRange4D benchmark, outperforming existing state-of-the-art methods. The principal implication for AI practitioners is that WideRange4D provides a more challenging and comprehensive benchmark for evaluating 4D generation methods, while Progress4D offers a more stable and higher-quality approach for reconstructing complex 4D scenes with wide-range object movement.  |
| BlobCtrl: A Unified and Flexible Framework for Element-level Image
  Generation and Editing (Read more on [arXiv](https://arxiv.org/abs/2503.13434) or [HuggingFace](https://huggingface.co/papers/2503.13434))| HongxiangLi, daoyuan98, ZyZcuhk, l-li, Yw22 | BlobCtrl is a unified framework for element-level image generation and editing using a probabilistic blob-based representation. The main research objective is to develop a method for precise and flexible manipulation of visual elements in images, overcoming limitations of current diffusion-based methods. The key methodology involves a dual-branch diffusion model with a blob-based representation, self-supervised training with data augmentation, and controllable dropout strategies. BlobCtrl achieves a significantly higher average CLIP-I score of 87.48 for identity preservation tasks, relative to the next best result. AI practitioners can use BlobCtrl for element-level image generation and editing, benefiting from its precise control over visual appearance and spatial layout that improves fidelity.  |
| reWordBench: Benchmarking and Improving the Robustness of Reward Models
  with Transformed Inputs (Read more on [arXiv](https://arxiv.org/abs/2503.11751) or [HuggingFace](https://huggingface.co/papers/2503.11751))| Yoon Kim, Andrew Cohen, mghazvininejad, michiyasunaga, ZhaofengWu | Reward models (RMs) are brittle and their performance degrades substantially when inputs are transformed in meaning- or ranking-preserving ways.  The main research objective is to evaluate and improve the robustness of state-of-the-art reward models against input transformations.  Key methodology used involves creating reWordBench, a benchmark of transformed RewardBench instances, and regularizing RM training by encouraging similar scores for paraphrased inputs.  Primary results show that RM ranking accuracy on RewardBench can drop by 15.3% on the Chat subset when transformed with reWordBench, and regularization reduces the drop to 7.9%.  Principal implication for AI practitioners is that RMs need to be explicitly trained for robustness, such as through paraphrase regularization, to ensure reliable performance and avoid potential reward hacking in downstream alignment tasks.  |
| MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based
  Scientific Research (Read more on [arXiv](https://arxiv.org/abs/2503.13399) or [HuggingFace](https://huggingface.co/papers/2503.13399))| lundbergemma, chadliu, shcohen, suyc21, jmhb | MicroVQA is a new benchmark for evaluating multimodal reasoning in AI, specifically for microscopy-based biological research. The main research objective is to assess AI models' ability to perform expert visual understanding, hypothesis generation, and experiment proposal using microscopy images and associated questions. The key methodology involves curating a dataset of 1,042 multiple-choice questions (MCQs) created by biology experts, with a two-stage MCQ generation pipeline involving optimized LLM prompting and an agent-based "RefineBot" to remove language shortcuts. The primary result is that state-of-the-art multimodal large language models (MLLMs) achieve a peak performance of only 53% accuracy on the benchmark. For AI practitioners, this benchmark highlights the need for improved multimodal reasoning capabilities beyond language understanding, specifically in integrating visual information, prior scientific knowledge, and complex reasoning, suggesting that current models are far from expert-level scientific reasoning in this domain.  |
| Free-form language-based robotic reasoning and grasping (Read more on [arXiv](https://arxiv.org/abs/2503.13082) or [HuggingFace](https://huggingface.co/papers/2503.13082))| Matteo Bortolon, Alice Fasoli, Runyu Jiao, SPovoli, FGiuliari | FreeGrasp enables robots to perform grasping tasks based on free-form language instructions by leveraging Vision-Language Models (VLMs) for spatial reasoning. The research explores how pre-trained VLMs can interpret human instructions and understand spatial relationships for robotic grasping in a zero-shot setting. The proposed method, FreeGrasp, uses mark-based visual prompting and object keypoints to facilitate GPT-4o's spatial reasoning about object arrangements and obstructions. Experiments on the new FreeGraspData dataset show FreeGrasp achieves a Reasoning Success Rate (RSR) of 0.83 without object ambiguity, outperforming the ThinkGrasp baseline. AI practitioners can use FreeGrasp's approach, combining VLMs with visual prompting, to enhance robotic manipulation tasks requiring complex language understanding and spatial reasoning without the need for more training data.  |
| R1-VL: Learning to Reason with Multimodal Large Language Models via
  Step-wise Group Relative Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2503.12937) or [HuggingFace](https://huggingface.co/papers/2503.12937))| Jingyi Zhang, Xikun, liushunyu, HuanjinYao, huangjiaxing | R1-VL introduces Step-wise Group Relative Policy Optimization (StepGRPO) to enhance reasoning in Multimodal Large Language Models (MLLMs). The research aims to improve MLLMs' reasoning abilities beyond simply imitating successful reasoning paths, addressing the sparse reward issue in online reinforcement learning. StepGRPO uses online reinforcement learning with two novel rule-based rewards: Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity Reward (StepRVR), evaluating intermediate reasoning steps and logical structure. R1-VL, developed with StepGRPO, achieved a 63.5% accuracy on the MathVista benchmark, outperforming the baseline Qwen2-VL-7B by 3.8%. AI practitioners can use StepGRPO to train MLLMs with improved reasoning capabilities, achieving more reliable and structured outputs through a process that mitigates sparse reward issues without needing process reward models.  |
| V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning (Read more on [arXiv](https://arxiv.org/abs/2503.11495) or [HuggingFace](https://huggingface.co/papers/2503.11495))| Wei Li, Ziquan Liu, ChenyangSi, lwpyh, Cade921 | This paper introduces V-STaR, a new benchmark for evaluating Video-LLMs' spatio-temporal reasoning abilities, including a dataset and evaluation metrics. The main research objective is to assess how well Video-LLMs can integrate spatial, temporal, and causal relationships in video understanding, moving beyond simple object recognition. The key methodology is a Reverse Spatio-Temporal Reasoning (RSTR) task that decomposes video understanding into "what", "when", and "where" questions, evaluated with coarse-to-fine Chain-of-Thought (CoT) questions generated by a semi-automated GPT-4-powered pipeline. Primary results show that while some models like GPT-4o perform well on "what" questions (60.78% accuracy), their performance on integrated spatio-temporal reasoning is significantly lower, with the best LGM score of 39.51 on the "what-when-where" chain. The principal implication is that current Video-LLMs have significant limitations in consistent spatio-temporal reasoning, requiring AI practitioners to develop methods that enhance causal and relational understanding in video processing models.  |
| VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning (Read more on [arXiv](https://arxiv.org/abs/2503.13444) or [HuggingFace](https://huggingface.co/papers/2503.13444))| Chang Wen Chen, Ye Liu, AnalMom, KevinQHLin | Here's a concise summary of the research paper:  i) VideoMind is a video-language agent that uses a Chain-of-LoRA strategy for temporal-grounded video understanding. ii) The main research objective is to develop an agent that can effectively reason about long videos by identifying and integrating essential capabilities for temporal reasoning. iii) Key methodology involves a role-based agentic workflow (Planner, Grounder, Verifier, Answerer) and a Chain-of-LoRA strategy for efficient role-switching using lightweight LoRA adaptors on a single base model (Qwen2-VL). iv) Primary results: On the CG-Bench long video benchmark, the 2B VideoMind model achieved a 5.94 mIoU, surpassing GPT-40-mini (3.75) and approaching GPT-40 (5.62). v) Principal implication for AI practitioners: The Chain-of-LoRA approach enables the creation of efficient and flexible video reasoning agents, reducing the computational overhead of using multiple models while demonstrating strong performance on grounded video question-answering.  |
| Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation (Read more on [arXiv](https://arxiv.org/abs/2503.13070) or [HuggingFace](https://huggingface.co/papers/2503.13070))| Jing Tang, Kenji Kawaguchi, Weijian Luo, whatlegequ, Luo-Yihong | This paper introduces R0, a novel approach for fast text-to-image generation that relies solely on reward maximization, challenging the necessity of diffusion distillation. The main research question is whether reward signals alone, without diffusion losses, are sufficient for high-quality, few-step text-to-image generation. The key methodology is R0, a conditional generation approach via regularized reward maximization, that treats image generation as an optimization problem in data space. The results show that R0 outperforms previous methods such as RG-LCM and DI++, achieving a HPS of 34.37 and Image Reward of 1.27 using SD-v1.5 in 4 steps. AI practitioners can develop fast and high-quality text-to-image models by focusing on proper reward functions and regularization, without relying on computationally expensive diffusion distillation, and may adapt the framework to other conditional image generation tasks.  |
| MTV-Inpaint: Multi-Task Long Video Inpainting (Read more on [arXiv](https://arxiv.org/abs/2503.11412) or [HuggingFace](https://huggingface.co/papers/2503.11412))| CeciliaJL, XiaodongChen, magicwpf, lianghou, GuZheng | MTV-Inpaint is a unified video inpainting framework that supports multiple tasks, including text/image-guided object insertion and scene completion, and handles long videos. The main research objective is to develop a video inpainting model capable of handling both scene completion and controllable object insertion tasks in long videos, unifying these tasks and with enhanced input controllability. The key methodology involves a dual-branch spatial attention mechanism in a T2V diffusion U-Net, integration of image inpainting models via an I2V mode, and a two-stage pipeline (keyframe plus in-between frame propagation) for long videos. In object insertion, the method achieved a mIOU of 85.00%, surpassing existing baselines. For AI practitioners, MTV-Inpaint offers a single framework capable of various video inpainting tasks and their derivates like multi-modal inpainting, editing and object removal with state-of-art performance, avoiding the needs of training specialized models.  |
| Error Analyses of Auto-Regressive Video Diffusion Models: A Unified
  Framework (Read more on [arXiv](https://arxiv.org/abs/2503.10704) or [HuggingFace](https://huggingface.co/papers/2503.10704))| duchao, TIanyupang, xiaolili, Fengzhuo, k-nick | This paper develops a theoretical framework for analyzing errors in auto-regressive video diffusion models (ARVDMs) and uses the analysis to propose architectural improvements. The main research question is what types of errors are shared by most ARVDMs, why do those errors appear, and how can they be mitigated. The key methodology involves developing a unified framework, Meta-ARVDM, analyzing the KL-divergence between generated and true videos to identify error sources, and deriving an information-theoretic impossibility result related to the error. A primary result is the identification of "error accumulation" and "memory bottleneck", with the KL-divergence bound including terms for noise initialization, score estimation, discretization errors, and a memory bottleneck term specifically I(Output; Past | Input). The principal implication is that AI practitioners can mitigate the memory bottleneck by modifying the network structure, such as using prepending and channel concatenation, leading to improved trade-offs between error and computational cost.  |
| Sightation Counts: Leveraging Sighted User Feedback in Building a
  BLV-aligned Dataset of Diagram Descriptions (Read more on [arXiv](https://arxiv.org/abs/2503.13369) or [HuggingFace](https://huggingface.co/papers/2503.13369))| Jaime-Choi, sangryul, namin0202, eunkey, soarhigh | SIGHTATION, a novel dataset, enhances diagram descriptions for blind and low-vision (BLV) users by incorporating sighted user feedback on Vision Language Model (VLM) outputs. The main research objective is to create a BLV-aligned dataset of diagram descriptions that addresses the misalignment between sighted annotators and BLV user preferences. The key methodology involves a two-pass VLM inference with latent supervision using a guide generated, followed by sighted-user assessments of the VLM-generated descriptions in terms of preference, completion, retrieval, and question answering. Primary results reveal that preference-tuning a 2B model on the dataset increased usefulness ratings by BLV educators by an average of 1.670 standard deviations. Principal implication for AI practitioners is that leveraging sighted user assessments of VLM-generated content, guided by a multi-pass inference, provides a scalable and effective method to develop datasets that meet the needs of BLV users.  |
| Long-Video Audio Synthesis with Multi-Agent Collaboration (Read more on [arXiv](https://arxiv.org/abs/2503.10719) or [HuggingFace](https://huggingface.co/papers/2503.10719))| Li Liu, Xiaojie Xu, yingcongchen, Xxlbigbrother, Buzz-lightyear | i) The paper introduces LVAS-Agent, a novel multi-agent framework for end-to-end long-video audio synthesis. ii) The primary research objective is to address the challenges of long-video dubbing, including semantic shifts and temporal misalignment, by mimicking professional dubbing workflows. iii) The methodology decomposes the synthesis process into scene segmentation, script generation, sound design, and audio synthesis, utilizing VLM and LLM-based agents with discussion-correction and generation-retrieval-optimization mechanisms. iv) The study demonstrates superior audio-visual alignment over baseline methods using LVAS-Bench, a new benchmark dataset with 207 professionally curated long videos, and achieves state-of-the-art performance across distribution matching, audio quality, semantic alignment, and temporal alignment metrics. v) The principal implication for AI practitioners is the provision of a structured, collaborative framework and corresponding dataset that enables higher-quality, contextually aware audio synthesis in long-form video content creation, potentially enhancing viewer immersion and narrative coherence.  |
| Basic Category Usage in Vision Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.12530) or [HuggingFace](https://huggingface.co/papers/2503.12530))| KyleMoore, JesseTNRoberts, HTSawyer | Vision Language Models (VLMs) exhibit human-like basic-level categorization preferences, distinctions between biological/non-biological objects, and expert-level shifts. The main research question is whether basic-level categorization behaviors observed in humans transfer to large language models. The key methodology involved prompting two VLMs (Llama 3.2 Vision Instruct and Molmo 7B-D) with images and comparing model-generated descriptions to a dataset of basic-level image labels, using two-proportion Z-tests for statistical analysis. Primary results showed that Llama 3.2 produced basic-level categorizations in 60.2% of outputs, and both models used basic-level terms significantly less (p<0.01) for non-biological items. The principal implication is that understanding how LLMs represent object categories, mirroring human cognition, is essential for developing models that align more closely with human behavior and interpretability.  |
| Investigating Human-Aligned Large Language Model Uncertainty (Read more on [arXiv](https://arxiv.org/abs/2503.12528) or [HuggingFace](https://huggingface.co/papers/2503.12528))| Pamela Wisniewski, Daryl Watson, Kyle Moore, JesseTNRoberts | This work investigates how well various large language model (LLM) uncertainty measures correlate with human uncertainty. The main research question is what LLM uncertainty measures best align with human group-level uncertainty on non-factual questions. The methodology involves comparing LLM uncertainty on a curated dataset of survey questions against human response distributions, using measures like self-reporting, entropy, and ensemble methods. The primary result is that top-k entropy correlates negatively with human uncertainty and decreases in human-similarity with increased model size (r > 0.3 for many models), but combining multiple measures produces a generalizable model (r â‰ˆ 0.5 for cross-validation and r>0.6 on full data) . AI practitioners can use mixtures of uncertainty quantification methods, and potentially combining methods such as nucleus size and top-k entropy, to create LLMs that better reflect human-like uncertainty, especially for applications requiring calibrated trust and human-AI collaboration.  |
