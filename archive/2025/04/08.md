

## Papers for 2025-04-08

| Title | Authors | Summary |
|-------|---------|---------|
| One-Minute Video Generation with Test-Time Training (Read more on [arXiv](https://arxiv.org/abs/2504.05298) or [HuggingFace](https://huggingface.co/papers/2504.05298))| guestrin, zhaoyue-zephyrus, GashonHussein, koceja, karansdalal | This paper introduces Test-Time Training (TTT) layers integrated into a Diffusion Transformer to generate coherent one-minute videos from text storyboards. The main objective is to address the inefficiency of self-attention and the limited expressiveness of standard RNN hidden states for generating long videos with complex narratives. The key methodology involves adding TTT layers, whose hidden states are neural networks (specifically two-layer MLPs) updated via test-time gradient descent on a self-supervised reconstruction task, to a pre-trained CogVideo-X 5B model and fine-tuning on a curated Tom and Jerry dataset. The primary result shows that TTT layers significantly improve video coherence and storytelling for one-minute videos compared to baselines like Mamba 2 and Gated DeltaNet, leading by 34 Elo points in human evaluations, although some artifacts persist and efficiency needs improvement. For AI practitioners, this demonstrates TTT layers as a viable approach to enhance temporal consistency in long video generation, offering a mechanism to handle extended contexts beyond typical attention or RNN limitations, but requiring consideration of current efficiency trade-offs. |
| SmolVLM: Redefining small and efficient multimodal models (Read more on [arXiv](https://arxiv.org/abs/2504.05299) or [HuggingFace](https://huggingface.co/papers/2504.05299))| eliebak, mervenoyan, mfarre, orrzohar, andito | SmolVLM introduces a family of compact, efficient Vision-Language Models (VLMs) designed for resource-constrained inference on edge devices. The primary objective was to engineer small VLMs by systematically exploring architectural configurations, tokenization strategies, and data curation optimized for low computational overhead and minimal memory footprints. Key methodologies included investigating encoder-LM parameter balance, optimizing context length and pixel shuffling for token reduction, evaluating learned versus string positional tokens, using image splitting, and carefully curating training data mixes (including CoT and video duration). Results show the smallest model (SmolVLM-256M) achieves a 44.0% average score across benchmarks using less than 1GB GPU RAM, outperforming significantly larger models, while the 2.2B variant rivals state-of-the-art models requiring double the GPU memory. For AI practitioners, the principal implication is that strategic architectural optimizations, aggressive tokenization, and curated data enable high-performance multimodal capabilities at much smaller scales, facilitating practical deployment on edge devices. |
| T1: Tool-integrated Self-verification for Test-time Compute Scaling in
  Small Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.04718) or [HuggingFace](https://huggingface.co/papers/2504.04718))| Jaewoong Cho, Jongwon Jeong, Nardien | This paper introduces Tool-integrated Self-verification (T1) to enhance small language model (sLM) self-verification during test-time compute scaling by using external tools. The main research objective is to investigate if sLMs can reliably perform self-verification for test-time scaling, particularly for memorization-heavy tasks, and to improve this capability without resorting to larger models. The key methodology involves T1, a two-stage process combining a tool-based verifier (ToolV) leveraging external tools (e.g., code interpreter) for filtering, and a reward model (RM)-based verifier for scoring, with both components enhanced via knowledge distillation from larger teacher models. Primary results demonstrate that T1 significantly boosts sLM performance; specifically, a Llama-3.2 1B model using T1 under test-time scaling outperformed a significantly larger Llama-3.1 8B model on the MATH benchmark. The principal implication for AI practitioners is that integrating external tools via methods like T1 can substantially improve the reasoning and verification capabilities of computationally cheaper sLMs, enabling them to tackle complex tasks more effectively and potentially match larger model performance in specific domains. |
| URECA: Unique Region Caption Anything (Read more on [arXiv](https://arxiv.org/abs/2504.05305) or [HuggingFace](https://huggingface.co/papers/2504.05305))| Heeji Yoon, seungryong, crepejung00, junwann, SammyLim | URECA introduces a large-scale dataset and novel model for generating unique captions for image regions at multiple granularities. The primary objective is to address the limitation of existing methods that struggle to produce distinctive descriptions for regions across varying levels of detail, especially distinguishing visually similar regions. The methodology involves a four-stage automated data curation pipeline utilizing mask trees and MLLMs to generate unique captions, and a captioning model featuring a dynamic mask encoder that preserves spatial properties for multi-granularity inputs. The proposed URECA model achieves state-of-the-art performance on the new dataset, attaining a BERTScore of 75.11, and demonstrates strong zero-shot generalization on benchmarks like Visual Genome with a METEOR score of 18.4. For AI practitioners, this work provides a robust dataset and model architecture enabling the generation of precise, context-aware natural language descriptions for arbitrarily selected image regions, enhancing detailed visual understanding applications. |
| Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning
  Models (Read more on [arXiv](https://arxiv.org/abs/2504.04823) or [HuggingFace](https://huggingface.co/papers/2504.04823))| Yuxuan Sun, Tiezheng, baihaoli, manyi2024, ruikangliu | This paper empirically investigates the impact of quantization on the reasoning abilities of large language models. The primary objective is to systematically evaluate how weight-only, KV cache, and weight-activation quantization affect reasoning performance across various model families, sizes, and tasks. The study quantizes DeepSeek-R1-Distilled Qwen/LLaMA families (1.5B-70B) and QwQ-32B using state-of-the-art algorithms (e.g., AWQ, QuaRot, FlatQuant) and evaluates them on mathematical, scientific, and programming reasoning benchmarks. Key findings reveal that W8A8 weight-activation or W4A16 weight-only/KV cache quantization can achieve near-lossless performance (≤1% accuracy drop), whereas lower bit-widths introduce significant risks, influenced by model size, origin (distilled vs. RL), and task difficulty. For AI practitioners, this implies that while 8-bit or selective 4-bit quantization can preserve reasoning with minimal loss, aggressive low-bit quantization requires careful consideration of the specific model and task, with FlatQuant and AWQ/QuaRot being preferred algorithms for weight-activation and weight-only/KV cache respectively. |
| Concept Lancet: Image Editing with Compositional Representation
  Transplant (Read more on [arXiv](https://arxiv.org/abs/2504.02828) or [HuggingFace](https://huggingface.co/papers/2504.02828))| Hancheng Min, Tianjiao Ding, CCB, ryanckh, peterljq | Concept Lancet (CoLan) introduces a zero-shot, plug-and-play framework for diffusion-based image editing using sparse concept decomposition and transplant in latent space. The research aims to solve the challenge of accurately determining the required edit strength for concept manipulation in images, avoiding over/under-editing without costly trial-and-error. CoLan employs a large curated concept dictionary (CoLan-150K), VLM-based parsing for task-specific concepts, and sparse coding to decompose the source latent vector (text embedding or diffusion score), allowing targeted replacement (transplant) of concept vectors. Equipping editing backbones like P2P-Zero with CoLan significantly improved consistency preservation, reducing LPIPS by nearly 50% (from 273.8/142.4 to 120.3/68.43 x10^-3 on whole image/background) while enhancing edit effectiveness on the PIE-Bench dataset. AI practitioners can integrate CoLan into diffusion editing pipelines to achieve more precise and consistent edits automatically by estimating and applying appropriate concept-specific magnitudes, eliminating the need for manual edit strength tuning per image. |
| LiveVQA: Live Visual Knowledge Seeking (Read more on [arXiv](https://arxiv.org/abs/2504.05288) or [HuggingFace](https://huggingface.co/papers/2504.05288))| Yao Wan, Mingyang Fu, shuaishuaicdp, Tim666, Ayiirep | This paper introduces LIVEVQA, a benchmark dataset automatically collected from recent news to evaluate Multimodal Large Language Models (MLLMs) on live visual knowledge seeking. The research objective is to assess the capability of current MLLMs to answer questions demanding understanding of up-to-date visual knowledge synthesized from internet news content. Methodology involved creating the LIVEVQA dataset (3,602 single- and multi-hop visual questions from 1,233 news instances across 14 categories) and evaluating 15 MLLMs (e.g., GPT-4o, Gemma-3, Qwen-2.5-VL) with and without search tool integration. Primary results demonstrate that while stronger models perform better overall, significant performance gaps persist, particularly for complex multi-hop questions requiring recent visual knowledge; Gemini-2.0-Flash achieved the highest accuracy at 24.93% without search integration. The principal implication for AI practitioners is that current MLLMs, even sophisticated ones, struggle significantly with visual questions requiring timely, real-world knowledge and complex reasoning, highlighting a critical need for improved visual grounding and knowledge integration mechanisms. |
| Are You Getting What You Pay For? Auditing Model Substitution in LLM
  APIs (Read more on [arXiv](https://arxiv.org/abs/2504.04715) or [HuggingFace](https://huggingface.co/papers/2504.04715))| Tianneng Shi, Will Cai, dawnsong, Xuandong | This paper evaluates methods for detecting undisclosed model substitution in black-box Large Language Model (LLM) APIs. The objective is to formalize the API auditing problem and assess the robustness of software-based verification techniques (text classification, MMD, benchmarks, log probability analysis) and hardware solutions (TEEs) against adversarial attacks like quantization and randomized substitution. Methodology involves empirical evaluation of these techniques using various LLMs (Llama, Gemma, Mistral, Qwen2) under different attack scenarios, including comparing outputs, benchmark scores, and log probabilities. Primary results indicate that text-output-based methods are ineffective against subtle changes like quantization (e.g., text classifiers achieve only ~50% accuracy distinguishing original vs. quantized models) and randomized substitution (MMD test power drops significantly), while log probability analysis is more sensitive but relies on often unavailable API features; TEEs show promise with low performance overhead (<3% throughput impact under load). The principal implication for AI practitioners is that relying solely on current software-based verification for API model identity is unreliable, highlighting the need for enhanced provider transparency or hardware-attested environments like TEEs to ensure model integrity in critical applications and benchmarking. |
| Gaussian Mixture Flow Matching Models (Read more on [arXiv](https://arxiv.org/abs/2504.05304) or [HuggingFace](https://huggingface.co/papers/2504.05304))| saibi, wetzste1, luanfujun, zexiangxu, Lakonik | GMFlow introduces a novel flow matching model predicting Gaussian mixture (GM) parameters instead of just the mean velocity to enhance generative modeling. The primary objective is to overcome the limitations of discretization errors in few-step sampling and color over-saturation issues associated with classifier-free guidance (CFG) in existing diffusion and flow matching models. Key methodology involves parameterizing the flow velocity as a GM, training with a KL divergence loss, deriving novel GM-SDE/ODE solvers that leverage analytic distributions, and introducing a probabilistic guidance mechanism for CFG reweighting rather than extrapolation. GMFlow demonstrates superior performance, achieving a Precision of 0.942 with only 6 sampling steps and a state-of-the-art Precision of 0.950 with 32 steps on ImageNet 256x256, significantly outperforming baselines, especially in few-step scenarios. For AI practitioners, this provides a framework for developing generative models capable of faster, higher-fidelity sampling with reduced CFG-induced saturation artifacts. |
| DiaTool-DPO: Multi-Turn Direct Preference Optimization for
  Tool-Augmented Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.02882) or [HuggingFace](https://huggingface.co/papers/2504.02882))| Donghun Lee, dsindex, junrae, gaeunseo, hash2430 | This paper introduces DiaTool-DPO, a Direct Preference Optimization method enhancing Tool-Augmented LLMs' multi-turn dialogue control for information gathering and tool rejection. The primary objective was to improve TA-LLM handling of incomplete or out-of-scope user queries by adapting DPO without requiring new expert demonstrations. Key methodology involves modeling interactions as a Markov Decision Process, automatically constructing paired chosen/rejected dialogue trajectory datasets based on defined query types, and applying a specialized DiaTool-DPO objective loss with turn-length normalization and reward gap margins. Experiments showed DiaTool-DPO significantly improved LLaMA3-8B-Instruct's performance over SFT-only baselines, achieving 91.7% slot-filling accuracy (a 44% improvement) and 91.3% relevance accuracy (a 9.6% improvement), nearing GPT-4o performance. For AI practitioners, this method offers a way to train more robust TA-LLMs capable of managing ambiguous requests and unavailable tools using automatically generated preference data, reducing problematic tool calls without manual labeling. |
| VAPO: Efficient and Reliable Reinforcement Learning for Advanced
  Reasoning Tasks (Read more on [arXiv](https://arxiv.org/abs/2504.05118) or [HuggingFace](https://huggingface.co/papers/2504.05118))| Ruofei Zhu, Xiaochen Zuo, Qiying Yu, Yufeng Yuan, YuYue | This paper introduces VAPO, a value-based reinforcement learning framework designed to enhance the performance and efficiency of large language models on advanced reasoning tasks requiring long chain-of-thought. The primary objective is to overcome limitations inherent in value-based RL for long-CoT, specifically value model bias, handling heterogeneous sequence lengths, and sparse reward signals, aiming to surpass existing value-free methods. VAPO employs a modified Proximal Policy Optimization (PPO) approach incorporating seven key techniques, including Value-Pretraining, Decoupled and Length-Adaptive Generalized Advantage Estimation (GAE), Token-Level Loss, Clip-Higher clipping, Positive Example LM Loss, and Group-Sampling. Benchmarked on AIME 2024 using a Qwen-32B model, VAPO achieved a state-of-the-art score of 60.4 within 5,000 training steps, significantly outperforming the prior SOTA value-free method DAPO by over 10 points while demonstrating greater training stability and efficiency. For AI practitioners, VAPO presents a robust and efficient value-based RL alternative for training high-performance reasoning models, offering improved stability and potentially higher accuracy ceilings compared to value-free methods on complex, long-CoT tasks. |
| Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language
  Models for Domain-Generalized Semantic Segmentation (Read more on [arXiv](https://arxiv.org/abs/2504.03193) or [HuggingFace](https://huggingface.co/papers/2504.03193))| robbytan, XinNUS | This paper introduces MFuser, a Mamba-based framework to efficiently fuse Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) for Domain-Generalized Semantic Segmentation (DGSS). The primary objective is to combine the complementary strengths of VFMs (fine-grained features) and VLMs (robust text alignment) while overcoming the challenges of long-sequence modeling and computational cost associated with integrating large models. The key methodology involves two components: MVFuser, a Mamba-based co-adapter for joint parameter-efficient fine-tuning of VFM and VLM visual features, and MTEnhancer, a hybrid attention-Mamba module to refine VLM text embeddings using visual priors. MFuser significantly outperforms existing DGSS methods, achieving a state-of-the-art 68.20 mIoU on the synthetic-to-real benchmark (G→{C, B, M} average) using DINOv2 and EVA02-CLIP. For AI practitioners, this work presents a computationally efficient Mamba-based adapter approach (MVFuser) to synergistically combine diverse foundation models, enhancing generalization for semantic segmentation tasks without requiring full fine-tuning of the base models. |
| BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose
  Estimation (Read more on [arXiv](https://arxiv.org/abs/2504.02812) or [HuggingFace](https://huggingface.co/papers/2504.02812))| taeyeop, anas-gouda, mfourmy, swtyree, nv-nguyen | The BOP Challenge 2024 advanced the state-of-the-art in 6D object pose estimation by introducing model-free tasks, new high-resolution datasets (BOP-H3), and a practical 6D detection task. The main objective was to shift evaluation from lab-like setups towards real-world applicability, notably by requiring methods to onboard unseen objects from reference videos without CAD models in model-free tracks. Key methodology involved evaluating methods across seven tracks defined by task (6D localization, 6D detection, 2D detection), onboarding setup (model-based, model-free), and dataset group (BOP-Classic-Core, BOP-H3) using established metrics like Average Recall (AR) and Average Precision (AP). Primary results showed significant progress: the best model-based 6D localization method for unseen objects (FreeZeV2.1) achieved 82.1 AR on BOP-Classic-Core, 22% higher than the 2023 best, though 2D detection for unseen objects still lags significantly (-53% behind seen objects), indicating it's the main pipeline bottleneck. For AI practitioners, this highlights substantial improvements in unseen object pose estimation accuracy but underscores the critical need to advance 2D detection capabilities for robust real-world system deployment. |
| Clinical ModernBERT: An efficient and long context encoder for
  biomedical text (Read more on [arXiv](https://arxiv.org/abs/2504.03964) or [HuggingFace](https://huggingface.co/papers/2504.03964))| Jeffrey N. Chiang, Anthony Wu, Simonlee711 | This paper introduces Clinical ModernBERT, an efficient transformer encoder adapted for long-context biomedical and clinical text processing. The main objective is to leverage ModernBERT's architectural improvements (RoPE, Flash Attention, GeGLU, 8192 token context) and adapt them via domain-specific pretraining for enhanced clinical language understanding. Methodology involved continued pretraining of a ModernBERT-base model on a 13-billion-token corpus comprising PubMed abstracts, MIMIC-IV clinical notes, and structured medical ontologies using masked language modeling with token-aware masking. Primary results demonstrate strong performance on clinical NLP benchmarks, achieving a state-of-the-art 0.9769 AUROC on EHR classification and superior runtime efficiency compared to BioClinicalBERT, processing data ~1.6x faster at higher volumes. The principal implication for AI practitioners is the availability of a performant, efficient, and publicly released encoder backbone specifically optimized for long clinical sequences and medical code semantics, suitable for replacing older BERT variants in clinical applications. |
| JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language
  Model (Read more on [arXiv](https://arxiv.org/abs/2504.03770) or [HuggingFace](https://huggingface.co/papers/2504.03770))| Li Li, Yi Nian, yuehanqi, yuehanqi, Chouoftears | i) JailDAM is a novel framework for detecting and mitigating jailbreak attacks on Vision-Language Models (VLMs) using an adaptive memory mechanism. ii) The research aims to develop a robust and efficient jailbreak detection method for VLMs, addressing the limitations of existing approaches, such as reliance on model internals or expensive computations. iii) The methodology involves a memory-based approach, using policy-driven unsafe knowledge representations, test-time adaptation to refine the memory with emerging unsafe variations, and an autoencoder-based detection pipeline. iv) Experiments on VLM jailbreak benchmarks demonstrate that JailDAM delivers state-of-the-art performance in harmful content detection, improving both accuracy and speed by an average of 0.10 AUROC compared to the second-best method. v) JAILDAM offers AI practitioners a black-box compatible and computationally efficient solution for detecting jailbreak attempts in VLMs, adaptable to new attack strategies without requiring extensive harmful data or model retraining, enhancing the safety and robustness of VLM deployments.  |
| GlotEval: A Test Suite for Massively Multilingual Evaluation of Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.04155) or [HuggingFace](https://huggingface.co/papers/2504.04155))| Ona de Gibert, Sawal Devkota, Joseph Attieh, Zihao Li, zuenmin | i) GlotEval is introduced as a lightweight massively multilingual evaluation framework for Large Language Models (LLMs). ii) The research aims to address the challenge of evaluating LLMs in diverse linguistic environments, especially low-resource languages, by providing a consistent and flexible evaluation framework. iii) The methodology involves integrating 20+ existing multilingual benchmarks across seven key tasks including machine translation, text classification, and summarization, standardizing language codes, and incorporating language-specific prompt templates with optional Microsoft Translator integration. iv) Experiments with Qwen2-1.5B model show throughput variances across languages and hardware setups, with Nvidia A100 generally achieving higher throughput than AMD MI250X; for example, French translation achieved 969.55 tokens/s on Nvidia A100. v) GlotEval offers AI practitioners a tool for fine-grained diagnostics of model strengths and weaknesses across a wide array of languages, facilitating the development of more inclusive and robust multilingual language technologies.  |
| Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting
  LLMs Across Languages and Resources (Read more on [arXiv](https://arxiv.org/abs/2504.04152) or [HuggingFace](https://huggingface.co/papers/2504.04152))| Jörg Tiedemann, Hengyu Luo, Shaoxiong Ji, Zihao Li | i) This paper investigates data mixing strategies in multilingual continual pretraining (CPT) for adapting large language models (LLMs) across languages and resources. ii) The main objective is to evaluate the relative effectiveness of monolingual, bilingual, and code-augmented data strategies in multilingual CPT. iii) The study systematically evaluates 36 CPT configurations involving three multilingual base models across 30+ languages categorized as altruistic, selfish, and stagnant. iv) The findings reveal that bilingual CPT improves multilingual classification but often causes language mixing, while code data inclusion enhances classification but introduces generation trade-offs, with Llama-3.1-8B achieving only 7.47 BLEU with bilingual CPT versus 25.52 with monolingual CPT for high-resource languages. v) The principal implication for AI practitioners is the need for adaptive CPT methods that balance classification improvements and generation quality due to the complex interactions between language characteristics and data mixing strategies.  |
