

## Papers for 2025-04-11

| Title | Authors | Summary |
|-------|---------|---------|
| Kimi-VL Technical Report (Read more on [arXiv](https://arxiv.org/abs/2504.07491) or [HuggingFace](https://huggingface.co/papers/2504.07491))| dongliangwang, congcongwang, DuChenZhuang, tzzcl, xingbowei | Kimi-VL is presented as an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM). The objective is to develop a VLM offering advanced multimodal reasoning, long-context understanding (128K), and strong agent capabilities while activating only 2.8B parameters in its language decoder. Methodology involves pairing a native-resolution MoonViT vision encoder with an MoE language model (Moonlight), trained through multi-stage pre-training, joint supervised fine-tuning (SFT), and enhanced with long-CoT SFT and reinforcement learning (RL) for the Kimi-VL-Thinking variant. Primary results show Kimi-VL competes effectively with larger VLMs across various benchmarks, while the Kimi-VL-Thinking variant achieves 61.7 on MMMU and 36.8 on MathVision, demonstrating strong long-horizon reasoning with its compact 2.8B activated LLM parameters. For AI practitioners, this research indicates the viability of using MoE architectures and native-resolution vision encoders to create parameter-efficient VLMs capable of complex multimodal reasoning, long-context processing, and agentic behavior. |
| VCR-Bench: A Comprehensive Evaluation Framework for Video
  Chain-of-Thought Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.07956) or [HuggingFace](https://huggingface.co/papers/2504.07956))| lovesnowbest, Lin-Chen, Osilly, ChthollyTree, yukunqi | VCR-Bench introduces a novel benchmark for comprehensively evaluating video Chain-of-Thought (CoT) reasoning capabilities in Large Vision-Language Models (LVLMs). The primary objective is to rigorously assess the entire reasoning process, differentiating failures originating from perception versus reasoning deficits, which current benchmarks inadequately address. Methodology involves a new dataset (VCR-Bench) with 859 videos and 1,034 QA pairs, featuring manually annotated, stepwise CoT rationales tagged for perception/reasoning, and a CoT score derived from recall/precision evaluation of these steps. Experiments reveal significant limitations in existing LVLMs, with the top-performing model achieving only a 62.8% CoT score and 56.7% accuracy, and most models exhibiting lower performance on perception steps compared to reasoning steps. For AI practitioners, VCR-Bench offers a standardized framework to identify specific weaknesses, particularly in temporal-spatial perception, providing actionable insights for improving LVLMs on complex video reasoning tasks. |
| MM-IFEngine: Towards Multimodal Instruction Following (Read more on [arXiv](https://arxiv.org/abs/2504.07957) or [HuggingFace](https://huggingface.co/papers/2504.07957))| yhcao, sweetFruit, KennyUTC, yuhangzang, ChrisDing1105 | MM-IFEngine introduces a pipeline for generating multimodal instruction-following data and the MM-IFEval benchmark for evaluation. The research objective is to address the scarcity of high-quality training data and the limitations of existing benchmarks for evaluating multimodal instruction following (IF) in MLLMs. Key methodology involves the MM-IFEngine pipeline using LLMs (GPT-4o) for image filtering, task generation, and integrating 32 constraint categories to create the MM-IFInstruct-23k (SFT) and MM-IFDPO-23k (DPO) datasets, alongside the MM-IFEval benchmark featuring hybrid evaluation. Primary results show fine-tuning Qwen2-VL-7B on MM-IFDPO-23k significantly improves IF performance, achieving gains of +10.2% on MM-IFEval and +7.6% on MIA-Bench, while maintaining comparable VQA capabilities. For AI practitioners, this work provides datasets (MM-IFInstruct-23k, MM-IFDPO-23k) and a benchmark (MM-IFEval) to train and rigorously evaluate MLLMs for enhanced instruction adherence, crucial for applications needing precise, constrained multimodal outputs. |
| VisualCloze: A Universal Image Generation Framework via Visual
  In-Context Learning (Read more on [arXiv](https://arxiv.org/abs/2504.07960) or [HuggingFace](https://huggingface.co/papers/2504.07960))| mingming8688, cosumosu25, JonsonYan, RuoyiDu, lzyhha | VisualCloze presents a universal image generation framework leveraging visual in-context learning (ICL) to perform diverse tasks using a unified infilling model approach. Its primary objective is to overcome limitations of language-based instructions and task sparsity by enabling a model to understand and generalize visual tasks from examples. The key methodology involves formulating generation tasks as infilling problems on a grid of concatenated visual prompts and targets, fine-tuning the FLUX.1-Fill-dev model with LoRA on the proposed dense Graph200K dataset. Results demonstrate strong performance on in-domain tasks, generalization to unseen tasks, and task unification, with ICL quantitatively improving results (e.g., reducing Depth-to-Image RMSE from 10.31 to 9.68 using two in-context examples). For AI practitioners, this work implies that visual ICL combined with pre-trained infilling models offers a promising, unified paradigm for building versatile image generation systems that can learn complex visual relationships and adapt to new tasks with fewer explicit instructions compared to purely language-guided or task-specific models. |
| DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.07128) or [HuggingFace](https://huggingface.co/papers/2504.07128))| parishadbehnam, miladink, vaibhavad, arkilpatel, spaidartaigar | This paper introduces "Thoughtology," a systematic analysis of the internal reasoning chains ("thoughts") produced by the Large Reasoning Model (LRM) DeepSeek-R1. The main objective is to characterize DeepSeek-R1's reasoning patterns, evaluate the impact of thought length and context on performance, and assess its safety and cognitive parallels. Key methodologies include developing a taxonomy for reasoning steps, quantitative evaluation on math (AIME-24, GSM8k, multiplication), long-context (Needle-in-a-Haystack, CHASE-QA/Code), safety (HarmBench), and cognitive/cultural benchmarks. Primary results indicate a consistent reasoning structure but reveal an optimal thought length 'sweet spot' beyond which performance declines; notably, DeepSeek-R1 also exhibits significant safety vulnerabilities, responding harmfully to 30.0% of direct HarmBench requests. For AI practitioners, this implies that controlling LRM thought length is crucial for performance and efficiency, yet DeepSeek-R1 lacks inherent mechanisms for this, and its reasoning capabilities introduce new safety risks requiring specific mitigation strategies beyond standard LLM alignment. |
| HoloPart: Generative 3D Part Amodal Segmentation (Read more on [arXiv](https://arxiv.org/abs/2504.07943) or [HuggingFace](https://huggingface.co/papers/2504.07943))| Lp256, zouzx, KevinHuang, bennyguo, yhyang-myron | HoloPart introduces a generative approach for 3D part amodal segmentation, decomposing shapes into complete semantic parts, including occluded geometry. The primary objective is to address the limitations of standard 3D part segmentation by inferring and completing hidden part geometry while maintaining global shape consistency. The key methodology employs a two-stage approach: leveraging existing segmentation for initial surface patches, followed by HoloPart, a novel diffusion-based model using specialized local and context-aware attention mechanisms, to complete these patches into full parts. HoloPart significantly outperforms existing shape completion methods, achieving a mean instance IoU of 0.764 on the ABO benchmark compared to 0.565 for the next best baseline (Finetune-VAE). For AI practitioners, this work offers a tool to generate complete, semantically meaningful 3D parts from potentially incomplete data, enabling more robust downstream applications in 3D content creation, editing, and analysis. |
| C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization
  for Test-Time Expert Re-Mixing (Read more on [arXiv](https://arxiv.org/abs/2504.07964) or [HuggingFace](https://huggingface.co/papers/2504.07964))| Ziyue Li, zhoutianyi, Lzy01241010 | C3PO dynamically optimizes sub-optimal expert pathways in MoE LLMs at test-time to boost performance without retraining. The objective is to improve individual test sample predictions by re-mixing expert routing weights based on pathways from successful reference samples. C3PO employs collaborative optimization using neighbors in an embedding space to define a surrogate objective, focusing optimization on core experts within critical layers using methods like Neighborhood Gradient Descent (NGD). Results show C3PO improves base MoE accuracy by 7-15%; NGD on OLMoE-1B-7B achieved a 9.3% average accuracy increase (69.9% to 79.2%) across six benchmarks, enabling it to outperform 7-9B parameter dense models. AI practitioners can apply C3PO to enhance deployed MoE LLM performance on specific tasks or samples, potentially achieving higher accuracy with smaller models and reduced computational cost during inference. |
| MOSAIC: Modeling Social AI for Content Dissemination and Regulation in
  Multi-Agent Simulations (Read more on [arXiv](https://arxiv.org/abs/2504.07830) or [HuggingFace](https://huggingface.co/papers/2504.07830))| Marzyeh Ghassemi, saadia, elisakreiss, salmannyu, genglinliu | MOSAIC is an open-source multi-agent simulation framework using LLM agents to model social network content diffusion, user engagement, and moderation effects. The primary objective is to analyze LLM agent interactions, model misinformation propagation, and evaluate the efficacy of different content moderation strategies within a simulated social environment. The methodology employs LLM-driven agents (GPT-4o) assigned diverse personas who interact on a directed social graph, with their engagement patterns compared against human participants and tested under no-fact-checking, community-based, third-party, and hybrid moderation conditions. Key results indicate that simulated misinformation does not spread faster than factual content (unlike observed human behavior), and a hybrid fact-checking approach yielded the best balance of precision and recall (F1 score = 0.612) while enhancing factual content engagement. For AI practitioners, this suggests agent-based simulations can test moderation systems, but results must be critically evaluated as agent behavior, potentially influenced by safety training or simulation design, may deviate significantly from human patterns, impacting the direct applicability of findings to real-world platform governance. |
| Scaling Laws for Native Multimodal Models Scaling Laws for Native
  Multimodal Models (Read more on [arXiv](https://arxiv.org/abs/2504.07951) or [HuggingFace](https://huggingface.co/papers/2504.07951))| Joshua Susskind, Matthieu Cord, Victor Guilherme Turrisi da Costa, Enrico Fini, Mustafa Shukor | This paper investigates the scaling laws of native multimodal models (NMMs) trained from scratch, comparing early-fusion, late-fusion, and sparse architectures. The primary objective is to determine if commonly used late-fusion architectures hold an inherent advantage over early-fusion for NMMs and to characterize their scaling properties. The methodology involves training and evaluating 457 NMMs with varying architectures and training mixtures, deriving scaling laws by fitting power-law relationships between validation loss, compute (FLOPs), model parameters (N), and training tokens (D). Results indicate no inherent advantage for late-fusion; early-fusion performs comparably (loss L ‚àù C^-0.049 for both) while being more parameter-efficient for compute-optimal models, and sparse Mixture-of-Experts (MoE) significantly improve early-fusion performance. For AI practitioners, this suggests early-fusion NMMs, trained natively and potentially enhanced with MoEs, offer a viable and efficient alternative to late-fusion approaches that rely on separate pre-trained vision encoders, especially at lower parameter counts. |
| SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual
  Reasoning Self-Improvement (Read more on [arXiv](https://arxiv.org/abs/2504.07934) or [HuggingFace](https://huggingface.co/papers/2504.07934))| furongh-lab, kevinlin311tw, linjieli222, zyang39, russwang | This paper presents an MCTS-guided data selection method for efficient visual reasoning self-improvement in VLMs using less data and no knowledge distillation. The main objective is to enhance VLM reasoning capabilities through reinforcement fine-tuning (RFT) using a minimal set of appropriately challenging training samples identified based on difficulty. The key methodology involves repurposing Monte Carlo Tree Search (MCTS) to quantify sample difficulty by measuring the iterations required for the base VLM (Qwen2.5-VL-7B-Instruct) to solve each problem, filtering 70k samples down to 11k. The resulting model, ThinkLite-VL-7B, trained on only 11k samples, achieves 75.1 accuracy on MathVista, surpassing larger models and improving the average benchmark performance of the base VLM by 7% (from 59.69 to 63.89). For AI practitioners, this demonstrates that strategically selecting challenging training data using MCTS for RFT can yield state-of-the-art reasoning performance in VLMs with significantly reduced data requirements, optimizing resource utilization. |
| Towards Visual Text Grounding of Multimodal Large Language Model (Read more on [arXiv](https://arxiv.org/abs/2504.04974) or [HuggingFace](https://huggingface.co/papers/2504.04974))| Franck-Dernoncourt, YfZ, JoshuaGu, zhangry868, MingLiiii | This paper introduces TRIG, a novel task, benchmark (TRIG-Bench), and dataset to evaluate and improve the visual text grounding capabilities of Multimodal Large Language Models (MLLMs) on text-rich document images. The main research objective is to address the poor performance of existing MLLMs in localizing specific text regions within documents that support their generated answers for question-answering tasks. Methodology involved creating the TRIG-Bench benchmark (800 manually verified QA pairs) and a 90k synthetic instruction dataset using an OCR-LLM-human interaction pipeline, and proposing instruction-tuning and embedding-based grounding methods. Evaluation revealed significant limitations in current models on TRIG-Bench (e.g., GPT-4o achieved only 5.28% average pixel-level IoU in the OCR-free setting), while the proposed instruction-tuning method improved performance considerably to 29.98% average IoU after fine-tuning. For AI practitioners, this research provides a standardized benchmark and effective fine-tuning methods to assess and enhance MLLMs' ability to ground answers in documents, crucial for building more trustworthy and verifiable document understanding systems. |
| MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular
  Detection (Read more on [arXiv](https://arxiv.org/abs/2504.06801) or [HuggingFace](https://huggingface.co/papers/2504.06801))| R. Venkatesh Babu, Jogendra Kundu, Sarthak Vora, Srinjay Sarkar, RishubhPar | MonoPlace3D learns realistic, scene-aware 3D object placement to generate effective data augmentations for improving monocular 3D object detection. The main objective is to automatically determine plausible 3D bounding box parameters (position, dimensions, orientation) for inserting synthetic objects into real scenes, addressing a key limitation of prior augmentation methods focused mainly on appearance. The methodology involves training a Scene-Aware Placement Network (SA-PlaceNet) on inpainted scenes to predict a distribution over plausible 3D boxes, then sampling from this distribution and rendering realistic objects using synthetic assets refined by ControlNet. MonoPlace3D significantly improves detection accuracy across multiple detectors and datasets; for example, on KITTI (easy, AP40@IOU=0.7) with MonoDLE, it boosted AP from 17.45% to 22.49% and achieved performance comparable to using the full dataset with only 50% of the data. For AI practitioners, this work demonstrates that focusing on learning physically plausible object placement is crucial for creating highly effective 3D data augmentations, leading to substantial gains in detector performance and data efficiency. |
| Compass Control: Multi Object Orientation Control for Text-to-Image
  Generation (Read more on [arXiv](https://arxiv.org/abs/2504.06752) or [HuggingFace](https://huggingface.co/papers/2504.06752))| R. Venkatesh Babu, Vaibhav Agrawal, sachi1, RishubhPar | Compass Control introduces a method for precise, explicit 3D orientation control of individual objects within text-to-image diffusion models. The primary objective is to enable users to specify the desired 3D orientation for multiple objects in a scene alongside a text prompt, overcoming the limitations and imprecision of text-only control. Key methodology involves predicting orientation-aware 'compass tokens' via a lightweight encoder, prepending them to object tokens in the text prompt, and using 'Coupled Attention Localization (CALL)' to constrain the cross-attention maps of compass and object tokens to corresponding 2D bounding box regions. The approach achieves superior orientation control, yielding a significantly lower angular error (0.198 radians for single objects, 0.215 for multiple) compared to baselines like LooseControl (0.385 and 0.372 respectively), and generalizes effectively to unseen objects and scenes with more than two objects. For AI practitioners, this provides a user-friendly interface for granular 3D orientation control in generative models using only orientation angles and coarse 2D boxes, enhancing predictability and streamlining creative workflows without requiring dense 3D data. |
| TAPNext: Tracking Any Point (TAP) as Next Token Prediction (Read more on [arXiv](https://arxiv.org/abs/2504.05579) or [HuggingFace](https://huggingface.co/papers/2504.05579))| rgoroshin, apsarath, msajjadi, skoppula, artemZholus | TAPNext reformulates Tracking Any Point (TAP) in video as a sequential masked token decoding problem for online, low-latency tracking. The primary objective is to develop a simpler, more scalable TAP model by removing complex tracking-specific inductive biases and heuristics found in prior work. It employs a causal architecture combining ViT and SSM layers (TRecViT) to jointly process image patch tokens and masked point coordinate tokens, predicting trajectories via token imputation using a classification-based coordinate head. The method achieves state-of-the-art online tracking performance, with BootsTAPNext-B reaching 78.5 Average Jaccard (AJ) on DAVIS First at 256x256 resolution, outperforming previous frame-latency methods while operating purely online. For AI practitioners, TAPNext demonstrates that general-purpose sequence models with minimal task-specific components can achieve SOTA performance in complex correspondence tasks like point tracking, offering a potentially more scalable and easily adaptable approach for applications requiring online video understanding. |
