

## Papers for 2025-04-18

| Title | Authors | Summary |
|-------|---------|---------|
| CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for
  Language Model Pre-training (Read more on [arXiv](https://arxiv.org/abs/2504.13161) or [HuggingFace](https://huggingface.co/papers/2504.13161))| Dan Su, Xin Dong, Yonggan Fu, Yu Yang, shizhediao | CLIMB introduces an automated framework using clustering and iterative bootstrapping to optimize language model pre-training data mixtures. The main objective is to automatically discover, evaluate, and refine optimal data mixtures from large-scale corpora without manual curation or predefined domain labels to improve pre-training performance. The methodology involves embedding and clustering documents, followed by an iterative process that samples mixture configurations, trains proxy models, fits a performance predictor, and prunes the search space to find optimal weights. Primary results show that a 1B model trained continuously on 400B tokens using the CLIMB-optimized mixture (ClimbMix) surpassed the Llama-3.2-1B model by 2.0% on average across 12 reasoning benchmarks. The principal implication for AI practitioners is that CLIMB provides a data-driven, automated approach to curate high-quality pre-training datasets from unlabeled web-scale data, demonstrably improving model performance under fixed token budgets compared to baseline mixtures or random sampling, as evidenced by the released ClimbMix dataset. |
| Antidistillation Sampling (Read more on [arXiv](https://arxiv.org/abs/2504.13146) or [HuggingFace](https://huggingface.co/papers/2504.13146))| Avi Schwarzschild, Zhili Feng, Asher Trockman, arobey1, yashsavani | This paper introduces antidistillation sampling, a technique to generate reasoning traces from large language models (LLMs) that hinder model distillation while maintaining the original model's performance. The primary objective is to develop a sampling strategy that poisons generated data for distillation purposes, thereby protecting proprietary model capabilities, without sacrificing the utility of the model's outputs for downstream tasks. The key methodology involves modifying the teacher model's next-token sampling distribution by adding a penalty term proportional to an approximation of how a sampled token would increase a proxy student model's downstream loss, calculated efficiently via a finite difference approximation of a directional derivative. Results show that for comparable teacher model accuracy on GSM8K (around 68-69%), antidistillation sampling reduced the distilled student model's accuracy to 24.73%, significantly lower than the 51.86% achieved by a student distilled from traces generated via standard temperature sampling. For AI practitioners, this method offers a way to protect intellectual property embedded in frontier models by degrading the effectiveness of distillation when sharing model outputs, such as extended reasoning traces, while largely preserving the original model's task performance. |
| A Strategic Coordination Framework of Small LLMs Matches Large LLMs in
  Data Synthesis (Read more on [arXiv](https://arxiv.org/abs/2504.12322) or [HuggingFace](https://huggingface.co/papers/2504.12322))| Honglin Lin, Yu Li, Zinan Tang, Qizhi Pei, GX-XinGao | A coordination framework (GRA) using multiple small LLMs achieves data synthesis quality comparable to single large LLMs. The research objective is to design a resource-efficient framework enabling small LLMs to collectively match the data synthesis capabilities of monolithic LLMs without their associated high costs and limitations. GRA employs a peer-review-inspired methodology assigning distinct Generator, Reviewer, and Adjudicator roles to multiple small LLMs for iterative data generation, evaluation, and quality control. Primary results show GRA-produced data matches or surpasses large LLM quality; data synthesized using GRA with a Qwen-2.5-7B base model outperformed Qwen-2.5-72B-Instruct distilled data by 8.83% on average across tested benchmarks. The principal implication for AI practitioners is that strategically coordinating smaller models offers a computationally efficient alternative for generating high-quality synthetic training data, reducing reliance on large models for data synthesis and distillation. |
| Packing Input Frame Context in Next-Frame Prediction Models for Video
  Generation (Read more on [arXiv](https://arxiv.org/abs/2504.12626) or [HuggingFace](https://huggingface.co/papers/2504.12626))| Maneesh Agrawala, Lvmin Zhang | This paper presents FramePack, a structure for next-frame video prediction that compresses input frames to maintain a fixed transformer context length. The primary objective is to mitigate the "forgetting" (fading memory) and "drifting" (error accumulation) problems in generating long videos. FramePack employs progressive compression using varying transformer patchify kernel sizes based on frame importance and introduces anti-drifting sampling methods like inverted temporal ordering for bi-directional context. Results show that finetuning existing models with FramePack, especially using the inverted anti-drifting sampling (e.g., f1k1\_x\_g9\_f1k1f2k2f16k4\_td configuration), achieves superior performance across multiple metrics, including the highest human assessment ELO score of 1239 in ablation studies. For AI practitioners, FramePack offers a method to train video generation models capable of handling longer sequences with significantly higher batch sizes and reduced error accumulation, potentially improving visual quality and training efficiency. |
| Generate, but Verify: Reducing Hallucination in Vision-Language Models
  with Retrospective Resampling (Read more on [arXiv](https://arxiv.org/abs/2504.13169) or [HuggingFace](https://huggingface.co/papers/2504.13169))| Trevor Darrell, Joseph E. Gonzalez, Jiaxin Ge, Heekyung Lee, tsunghanwu | This paper introduces REVERSE, a unified framework reducing visual hallucinations in Vision-Language Models (VLMs) via hallucination-aware training and retrospective resampling. The objective is to enable a single VLM to both detect and dynamically correct its own hallucinations during text generation, unifying generation adjustment and post-hoc verification. Key methodology involves fine-tuning VLMs on a new 1.3M semi-synthetic dataset annotated with confidence tokens (</CN>, </UN>) and employing inference-time retrospective resampling triggered by token uncertainty to backtrack and regenerate content. Primary results demonstrate state-of-the-art performance, achieving up to a 12% reduction in CHAIR scores on CHAIR-MSCOCO compared to previous best methods. For AI practitioners, REVERSE offers a novel technique to enhance VLM reliability by embedding self-verification and correction capabilities directly into the model, reducing reliance on external verifiers or complex multi-stage pipelines. |
| WORLDMEM: Long-term Consistent World Simulation with Memory (Read more on [arXiv](https://arxiv.org/abs/2504.12369) or [HuggingFace](https://huggingface.co/papers/2504.12369))| Shuai Yang, Wenqi Ouyang, Yifan Zhou, Yushi Lan, Zeqi Xiao | WORLDMEM introduces a memory-augmented framework for long-term consistent world simulation, addressing temporal limitations in existing video diffusion models. The primary research objective is to mitigate the lack of long-term 3D spatial consistency in generative world simulators caused by limited temporal context windows. The methodology integrates an external memory bank (storing past frames with pose and timestamp states) into a Conditional Diffusion Transformer, using memory attention with relative state embeddings (Plücker for pose) and Diffusion Forcing to condition generation on retrieved memories. Quantitative results demonstrate improved consistency; for instance, on a Minecraft benchmark beyond the context window, WORLDMEM achieved a PSNR of 25.32 and LPIPS of 0.1429, significantly outperforming a Diffusion Forcing baseline (PSNR 18.04, LPIPS 0.4376). For AI practitioners, this approach offers a method to build more persistent and spatially coherent interactive simulations or virtual environments where maintaining state over extended periods is critical. |
| VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference
  Optimization for Large Video Models (Read more on [arXiv](https://arxiv.org/abs/2504.13122) or [HuggingFace](https://huggingface.co/papers/2504.13122))| Meng Luo, Haojian Huang, scofield7419, ChocoWu, Harold328 | VistaDPO introduces a hierarchical spatial-temporal direct preference optimization framework to enhance large video models (LVMs). The primary objective is to address LVM misalignment with human intuition and video hallucination by optimizing text-video preference alignment across instance, temporal, and perceptive hierarchical levels. The key methodology involves applying this hierarchical DPO framework, termed VistaDPO, using a newly constructed VistaDPO-7k dataset (7.2K QA pairs) annotated with chosen/rejected responses and spatial-temporal grounding information. Experimental results show VistaDPO significantly improves baseline LVMs, achieving average performance gains of 26.42% over PLLaVA and 53.92% over Video-LLaVA across hallucination, QA, and captioning benchmarks. For AI practitioners, this work demonstrates that incorporating hierarchical spatial-temporal preference optimization, beyond simple instance-level DPO, is crucial for improving the reliability and reducing hallucinations in LVMs for complex video understanding tasks. |
| NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation (Read more on [arXiv](https://arxiv.org/abs/2504.13055) or [HuggingFace](https://huggingface.co/papers/2504.13055))| Chao Du, Zijian Wu, Jinjie Ni, Xiangyan Liu, dreamerdeo | NoisyRollout introduces an RL fine-tuning approach for VLMs that enhances visual reasoning by incorporating trajectories from distorted images during rollout collection. The objective is to improve policy exploration diversity and mitigate issues arising from imperfect visual perception in VLMs without additional training costs. The key methodology involves a hybrid rollout strategy within GRPO, using both clean and noise-distorted images (with noise annealing) to generate trajectories for reward calculation, while policy updates use only clean images. Using just 2.1K samples, NoisyRollout achieved state-of-the-art average accuracy of 59.2% across five out-of-domain benchmarks compared to similar open-source RL-tuned models. For AI practitioners, this work demonstrates that targeted data augmentation during RL rollouts can effectively boost VLM generalization and robustness, particularly for visual reasoning, offering a cost-effective method to enhance exploration. |
| ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question
  Answering (Read more on [arXiv](https://arxiv.org/abs/2504.05506) or [HuggingFace](https://huggingface.co/papers/2504.05506))| Firoz Kabir, Aayush Bajaj, Mahir Ahmed, 38saidul, ahmed-masry | This paper introduces ChartQAPro, a diverse and challenging benchmark for Chart Question Answering (CQA). The primary objective was to address the limitations of existing CQA benchmarks, such as lack of diversity and performance saturation, and provide a more realistic evaluation of Large Vision-Language Models (LVLMs). The authors constructed ChartQAPro by collecting 1,341 charts from 157 diverse sources, including infographics and dashboards, paired with 1,948 human-verified questions covering multiple complex types like conversational and hypothetical queries. Evaluations on 21 LVLMs revealed a substantial performance decrease on ChartQAPro compared to prior benchmarks; for instance, Claude Sonnet 3.5's accuracy dropped from 90.5% on ChartQA to 55.81% on ChartQAPro. For AI practitioners, this implies that current LVLMs struggle significantly with complex, real-world chart reasoning, and ChartQAPro serves as a more robust tool for identifying these limitations and guiding future model development. |
| Exploring Expert Failures Improves LLM Agent Tuning (Read more on [arXiv](https://arxiv.org/abs/2504.13145) or [HuggingFace](https://huggingface.co/papers/2504.13145))| Ruochen Wang, Minhao Cheng, Andrew Bai, Li-Cheng Lan, zhoutianyi | This paper introduces Exploring Expert Failures (EEF), a fine-tuning method that improves LLM agent performance by utilizing information from failed expert trajectories. The objective is to address the limitation of Rejection Sampling Fine-Tuning (RFT), which discards failed expert trajectories, causing agents to struggle with complex, out-of-distribution subtasks where experts often fail. EEF simulates intermediate states from failed expert trajectories using the current agent policy, identifies beneficial action sequences leading to success via simulation, and selectively incorporates only these validated segments into the training data for supervised fine-tuning. The primary result shows EEF achieved a 62% win rate on the WebShop benchmark, significantly outperforming RFT (53.6%) and setting a new state-of-the-art score above 0.81. For AI practitioners, this implies that analyzing and selectively leveraging segments from *failed* expert demonstrations, rather than discarding them entirely, provides valuable training signals that enhance agent capabilities on complex tasks and improve overall tuning efficiency. |
| InstantCharacter: Personalize Any Characters with a Scalable Diffusion
  Transformer Framework (Read more on [arXiv](https://arxiv.org/abs/2504.12395) or [HuggingFace](https://huggingface.co/papers/2504.12395))| Yiji Cheng, Qixun Wang, Yanbing Zhang, Jiale Tao, wanghaofan | InstantCharacter presents a scalable diffusion transformer framework designed for high-fidelity, open-domain character personalization in image generation. The primary objective is to address the limited generalization, compromised image quality, and reduced textual controllability inherent in previous U-Net based or optimization-based character customization approaches, especially when applied to large Diffusion Transformers (DiTs). Methodologically, it introduces a scalable adapter with stacked transformer encoders, integrating features from SigLIP and DINOv2 via dual-stream fusion and a timestep-aware Q-former, trained progressively in three stages on a 10-million sample dataset containing paired and unpaired character images. Qualitative results demonstrate superior performance in maintaining character identity, fidelity, and text controllability compared to prior art like OminiControl, EasyControl, ACE++, and UNO, achieving comparable results to GPT4o, though specific quantitative metrics are not detailed in the provided text. For AI practitioners, this research offers a robust architecture and training strategy for adapting large foundation DiT models to specialized, controllable generation tasks like character personalization, enhancing flexibility and output quality without requiring test-time fine-tuning. |
| CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera
  Color Constancy (Read more on [arXiv](https://arxiv.org/abs/2504.07959) or [HuggingFace](https://huggingface.co/papers/2504.07959))| Seon Joo Kim, Michael S. Brown, Dongyun Kim, Mahmoud Afifi, dongyong2 | CCMNet introduces a lightweight framework utilizing pre-calibrated Color Correction Matrices (CCMs) for zero-shot cross-camera color constancy. The objective is to enable accurate illuminant estimation on unseen cameras without retraining or needing additional test images. The methodology involves using CCMs to map standard illuminants to the camera's raw space, encoding this trajectory into a Camera Fingerprint Embedding (CFE) via a CNN, and using this CFE to guide a hypernetwork (based on CCC/C5) for predicting illumination from uv-histograms; imaginary camera augmentation further improves robustness. CCMNet achieves state-of-the-art results, such as a 1.68° mean angular error on Cube+, outperforming previous methods while being computationally efficient. For AI practitioners, this provides a method to achieve consistent color rendering across diverse camera hardware by leveraging readily available ISP metadata (CCMs), eliminating the need for per-camera calibration data or model fine-tuning. |
| FocusedAD: Character-centric Movie Audio Description (Read more on [arXiv](https://arxiv.org/abs/2504.12157) or [HuggingFace](https://huggingface.co/papers/2504.12157))| Liangcheng Li, Sheng Zhou, Yiren Song, Chun Wang, Xiaojun Ye | FocusedAD introduces a novel framework for generating character-centric movie audio descriptions (AD) emphasizing narrative relevance. The main objective is to automatically produce AD for movies that explicitly identifies characters by name and focuses on plot-significant visual details, unlike generic video captioning. The methodology integrates a Character Perception Module (CPM) using an automated clustering-based query bank for character identification/tracking, a Dynamic Prior Module (DPM) injecting context via soft prompts, and a Focused Caption Module (FCM) generating descriptions from scene, character, and text tokens. FocusedAD achieves state-of-the-art performance on multiple benchmarks, including a BertScore of 57.7 on MAD-eval-Named and 64.5 on the introduced Cinepile-AD dataset, significantly outperforming prior AD methods and general MLLMs. For AI practitioners, this work provides a method for enhancing MLLM-based video understanding by incorporating specialized modules for character focus and contextual integration, leading to more narratively coherent and targeted outputs relevant for accessibility tools. |
| Retrieval-Augmented Generation with Conflicting Evidence (Read more on [arXiv](https://arxiv.org/abs/2504.13079) or [HuggingFace](https://huggingface.co/papers/2504.13079))| Mohit Bansal, Elias Stengel-Eskin, Archiki Prasad, HanNight | This paper introduces RAMDocs, a dataset for evaluating RAG systems against simultaneous ambiguity, misinformation, and noise, and proposes MADAM-RAG, a multi-agent debate framework to handle such conflicts. The main objective is to develop and evaluate a RAG approach capable of managing diverse, concurrent sources of conflict in retrieved documents, a common challenge in real-world scenarios. The key methodology involves assigning individual documents to LLM agents who debate their validity over multiple rounds, followed by an aggregator agent synthesizing a final response based on the discussion. MADAM-RAG significantly outperforms strong RAG baselines, improving accuracy by up to 11.40% on AmbigDocs and 15.80% on FaithEval using Llama3.3-70B-Instruct, while the new RAMDocs dataset proves challenging for existing methods. For AI practitioners, this indicates that standard RAG pipelines are insufficient for handling complex, realistic conflicts, and multi-agent debate frameworks like MADAM-RAG are needed to improve the reliability and factuality of RAG outputs when facing ambiguity, misinformation, and noise simultaneously. |
| Sleep-time Compute: Beyond Inference Scaling at Test-time (Read more on [arXiv](https://arxiv.org/abs/2504.13171) or [HuggingFace](https://huggingface.co/papers/2504.13171))| Sarah Wooders, Charles Packer, Yu Wang, Charlie Snell, Kevin Lin | This paper introduces sleep-time compute, a technique allowing LLMs to pre-process context offline to reduce test-time compute requirements. The research aims to evaluate the efficacy of sleep-time compute in improving the accuracy vs. test-time compute trade-off for stateful reasoning tasks. The methodology involves modifying reasoning datasets (GSM-Symbolic, AIME) into stateful versions where context is processed during "sleep-time" before a query arrives, comparing this to standard test-time scaling. Key results show sleep-time compute reduces the test-time compute needed for equivalent accuracy by approximately 5x on Stateful GSM-Symbolic and Stateful AIME, and scaling sleep-time compute can further improve accuracy by up to 18% on Stateful AIME. For AI practitioners, this implies that in stateful applications with available context (e.g., coding agents, document QA), implementing sleep-time compute can significantly cut test-time latency and cost while maintaining or improving accuracy, particularly when future queries are predictable. |
| Set You Straight: Auto-Steering Denoising Trajectories to Sidestep
  Unwanted Concepts (Read more on [arXiv](https://arxiv.org/abs/2504.12782) or [HuggingFace](https://huggingface.co/papers/2504.12782))| Adams Wai-Kin Kong, Yan Ren, Leyang Li, Shilin-LU | This paper introduces ANT, a finetuning framework for concept erasure in text-to-image diffusion models that automatically guides denoising trajectories away from unwanted concepts. The primary objective is to overcome limitations of prior methods by enabling precise content modification during mid-to-late denoising stages without disrupting early-stage structural integrity or relying on heuristic anchor concepts. ANT utilizes a trajectory-aware loss function that reverses the classifier-free guidance condition direction only after a specific timestep (t') and employs an augmentation-enhanced weight saliency map to identify and finetune only the most relevant parameters for erasure. ANT achieves state-of-the-art results, reducing inappropriate image detections (e.g., NSFW content) on the I2P benchmark to 23, significantly lower than prior methods, while maintaining competitive FID and CLIP scores on MS-COCO. For AI practitioners, ANT provides a more effective and robust finetuning method to build safer generative models by removing unwanted concepts with less impact on overall generative quality and without needing manual anchor selection. |
| Perception Encoder: The best visual embeddings are not at the output of
  the network (Read more on [arXiv](https://arxiv.org/abs/2504.13181) or [HuggingFace](https://huggingface.co/papers/2504.13181))| Andrea Madotto, Jang Hyun Cho, Peize Sun, Po-Yao Huang, Daniel Bolya | Perception Encoder (PE) introduces a state-of-the-art vision encoder family achieving top performance across diverse tasks using only scaled contrastive vision-language pretraining, finding optimal embeddings within intermediate network layers. The main objective was to investigate if a single, scalable contrastive pretraining approach could generate strong, general visual embeddings suitable for classification, retrieval, language modeling, and spatial tasks without complex multi-objective training. The key methodology involved developing a robust image pretraining recipe, creating a video data engine using synthetically generated captions for video finetuning, and introducing language and spatial alignment tuning methods to extract and adapt features from specific intermediate layers. Primary results show PE models achieve state-of-the-art performance; for instance, PEcoreG obtains 86.6% average zero-shot image classification accuracy, outperforming previous models, and its intermediate features rival specialized models like AIMv2 (language) and DINOv2 (spatial) before alignment tuning. The principal implication for AI practitioners is that powerful, general-purpose visual embeddings can be learned via scaled contrastive learning alone, but optimal performance on diverse downstream tasks necessitates extracting and aligning features from intermediate layers rather than solely relying on the final network output. |
