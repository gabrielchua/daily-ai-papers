

## Papers for 2025-05-01

| Title | Authors | Summary |
|-------|---------|---------|
| Sadeed: Advancing Arabic Diacritization Through Small Language Model (Read more on [arXiv](https://arxiv.org/abs/2504.21635) or [HuggingFace](https://huggingface.co/papers/2504.21635))| Sara Chrouf, hr99, Moatasem444, Hennara, ZeinaD | This paper introduces Sadeed, a compact decoder-only language model fine-tuned for Arabic diacritization, and SadeedDiac-25, a new benchmark for this task. The objective is to advance Arabic diacritization by developing an efficient model trained on high-quality data and establishing a more reliable evaluation framework. Methodology involved fine-tuning the Kuwain 1.5B model on rigorously cleaned Tashkeela and ATB data using a Question-Answering format, alongside creating the SadeedDiac-25 benchmark covering diverse Arabic styles with expert validation. Sadeed achieves competitive results, outperforming open-source models on SadeedDiac-25 (9.92 WER without case ending) and achieving state-of-the-art WER (2.9375 excluding non-diacritized chars) on a corrected version of the Fadel benchmark, though it requires post-processing via Needleman-Wunsch to correct hallucinations. For AI practitioners, this demonstrates the viability of small, specialized models for complex NLP tasks like diacritization and underscores the critical need for high-quality, curated training data and robust, uncontaminated benchmarks, revealing significant flaws in prior commonly used datasets. |
| WebThinker: Empowering Large Reasoning Models with Deep Research
  Capability (Read more on [arXiv](https://arxiv.org/abs/2504.21776) or [HuggingFace](https://huggingface.co/papers/2504.21776))| Yutao Zhu, Hongjin Qian, Guanting Dong, Jiajie Jin, Xiaoxi Li | WebThinker is a deep research agent empowering Large Reasoning Models (LRMs) with autonomous web exploration and report generation capabilities. The objective is to overcome LRM limitations stemming from reliance on static internal knowledge for complex, knowledge-intensive tasks requiring dynamic web information synthesis. The methodology integrates a Deep Web Explorer module for web search/navigation/extraction and an Autonomous Think-Search-and-Draft strategy, trained using RL-based iterative online Direct Preference Optimization (DPO). Results demonstrate significant improvements over baselines; for instance, WebThinker-32B-RL achieved 46.5% accuracy on the WebWalkerQA benchmark, substantially outperforming the Search-o1-32B baseline's 34.1%. AI practitioners can utilize this framework to develop LRMs that perform real-time, in-depth web research and synthesis concurrently with multi-step reasoning, enhancing performance on complex information-seeking tasks. |
| Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language
  Models in Math (Read more on [arXiv](https://arxiv.org/abs/2504.21233) or [HuggingFace](https://huggingface.co/papers/2504.21233))| Yen-Chun Chen, Dongdong Chen, Hany Awadalla, Baolin Peng, Haoran Xu | This paper introduces a systematic multi-stage training recipe to significantly enhance the mathematical reasoning abilities of small language models (SLMs). The primary objective is to overcome the limitations of SLM capacity and develop robust reasoning capabilities competitive with larger models. The methodology involves four sequential steps: large-scale distilled CoT mid-training, high-quality CoT supervised fine-tuning, Rollout DPO using curated preference pairs, and Reinforcement Learning with a verifiable reward signal incorporating specific stability enhancements. Applying this recipe to the 3.8B Phi-4-Mini model resulted in Phi-4-Mini-Reasoning, achieving 94.6% Pass@1 on Math-500, outperforming larger models like DeepSeek-R1-Distill-Qwen-7B. For AI practitioners, this demonstrates that a carefully orchestrated training strategy with high-quality synthetic data can enable resource-constrained SLMs to achieve strong reasoning performance, offering a blueprint for developing efficient and capable models. |
| Softpick: No Attention Sink, No Massive Activations with Rectified
  Softmax (Read more on [arXiv](https://arxiv.org/abs/2504.20966) or [HuggingFace](https://huggingface.co/papers/2504.20966))| Alham Fikri Aji, Erland Hilman Fuadi, Zayd M. K. Zuhri | This paper introduces Softpick, a rectified, non-sum-to-one drop-in replacement for softmax in transformer attention mechanisms designed to eliminate attention sinks and massive activations. The main objective is to evaluate if Softpick can maintain performance parity with softmax while mitigating these issues and improving model characteristics like quantization robustness. The methodology involves defining the Softpick function, training 340M parameter Llama-style models from scratch using both Softpick and standard softmax, and comparing their performance on benchmarks, quantization tasks, attention sink rates, activation distributions, and attention map sparsity. Key results show Softpick achieves comparable benchmark performance to softmax, eliminates attention sinks (0% sink rate vs 63.41%), drastically reduces hidden state kurtosis (340 vs 33,510), produces sparser attention maps (46.97% sparsity), and significantly outperforms softmax under quantization, especially at low bit-precisions. For AI practitioners, Softpick presents a viable attention mechanism that intrinsically avoids problematic massive activations, thereby simplifying quantization efforts and potentially enhancing low-precision training and model sparsity. |
| Phi-4-reasoning Technical Report (Read more on [arXiv](https://arxiv.org/abs/2504.21318) or [HuggingFace](https://huggingface.co/papers/2504.21318))| Harkirat Behl, Vidhisha Balachandran, Ahmed Awadallah, Sahaj Agarwal, Marah Abdin | This report introduces Phi-4-reasoning and Phi-4-reasoning-plus, 14-billion parameter models optimized for complex reasoning tasks via specialized training. The objective is to detail the models' creation through data curation, supervised fine-tuning (SFT), and reinforcement learning (RL), and evaluate their reasoning performance. Key methodologies include SFT of Phi-4 on curated prompts with o3-mini-generated reasoning traces for Phi-4-reasoning, followed by outcome-based RL on math problems for Phi-4-reasoning-plus. Primary results show both models significantly outperform larger open-weight models like DeepSeek-R1-Distill-Llama-70B (e.g., Phi-4-reasoning-plus achieves 78.0% on AIME 25 vs. 51.5%) and approach state-of-the-art performance, demonstrating substantial gains over the base Phi-4. The principal implication for AI practitioners is that meticulous data curation for SFT combined with targeted RL can yield highly capable reasoning models at smaller scales, rivaling significantly larger architectures. |
| Beyond the Last Answer: Your Reasoning Trace Uncovers More than You
  Think (Read more on [arXiv](https://arxiv.org/abs/2504.20708) or [HuggingFace](https://huggingface.co/papers/2504.20708))| Bernard Ghanem, Hani Itani, Hasan Abed Al Kader Hammoud | This research demonstrates that analyzing intermediate reasoning steps ("subthoughts") in LLMs yields more reliable answers than relying solely on the final output. The study investigates whether an LLM's final answer is its optimal conclusion and if alternative reasoning paths from intermediate points yield different results. Methodologically, it segments an initial reasoning trace, prompts the model to generate completions from each intermediate subthought endpoint, extracts the final numerical answer from each completion, and analyzes the resulting distribution of answers. The primary result shows that selecting the most frequent answer (mode) from these completions significantly boosts accuracy compared to the original final answer, with gains up to 13% on AIME2024, and that lower answer distribution entropy correlates with correctness. For AI practitioners, this implies that evaluating the distribution of answers derived from subthoughts, particularly using the mode, offers a more robust method for assessing LLM reasoning reliability than standard final-answer evaluation. |
| Taming the Titans: A Survey of Efficient LLM Inference Serving (Read more on [arXiv](https://arxiv.org/abs/2504.19720) or [HuggingFace](https://huggingface.co/papers/2504.19720))| Tong Liu, Zhenlin Yang, Yixin Ji, Juntao Li, zenRRan | This paper surveys methods for optimizing Large Language Model (LLM) inference serving efficiency. The objective is to provide a comprehensive, hierarchical overview of techniques addressing the memory and computational challenges in LLM deployment. The methodology involves systematically reviewing and categorizing research into instance-level (e.g., model placement, KV cache management, request scheduling, PD disaggregation), cluster-level (e.g., deployment, load balancing, cloud), emerging scenarios (e.g., long context, RAG, MoE), and miscellaneous areas. The survey organizes numerous optimization strategies, highlighting advancements like PagedAttention for KV cache memory fragmentation and Prefill-Decoding (PD) disaggregation for optimizing distinct inference phases, though specific quantitative performance improvements across all methods aren't aggregated due to the survey nature. For AI practitioners, this survey offers a structured map of optimization techniques, aiding in the selection and implementation of appropriate strategies to meet specific latency, throughput, and cost requirements for LLM serving. |
| COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning (Read more on [arXiv](https://arxiv.org/abs/2504.21850) or [HuggingFace](https://huggingface.co/papers/2504.21850))| Olga Russakovsky, Polina Kirichenko, Hee Seung Hwang, Xindi Wu | This paper introduces COMPACT, a data-efficient visual instruction tuning recipe to improve compositional reasoning in Multimodal Large Language Models (MLLMs) by explicitly combining atomic visual skills. The main objective is to enhance MLLM performance on complex visual tasks requiring multiple capabilities without massive data scaling, addressing the lack compositional complexity in existing VIT datasets. COMPACT's methodology involves defining 10 atomic capabilities, generating structured training data by prompting a generator model (Gemini-2.0-Flash) to create questions integrating exactly k={1, 2, 3} capabilities for sampled images, verifying quality, and mixing this data (32K samples) with 5% of LLaVA-665K VIT data. Primary results show COMPACT achieves performance comparable to the full LLaVA-665K baseline using less than 10% of the data, and significantly improves performance on complex tasks, achieving an 83.3% improvement on MMStar benchmark questions requiring four or more atomic capabilities. For AI practitioners, this implies that focusing on structured, compositionally complex training data generation offers a more data-efficient path to enhancing MLLM reasoning on complex multi-capability tasks compared to solely scaling undifferentiated instruction tuning data. |
| RoboVerse: Towards a Unified Platform, Dataset and Benchmark for
  Scalable and Generalizable Robot Learning (Read more on [arXiv](https://arxiv.org/abs/2504.18904) or [HuggingFace](https://huggingface.co/papers/2504.18904))| Bangjun Wang, Yuyang Li, Songlin Wei, Feishi Wang, Haoran Geng | RoboVerse introduces a unified simulation platform, large-scale synthetic dataset (>10M transitions, >1k tasks), and benchmark aimed at scalable and generalizable robot learning. The primary objective is to overcome robotics' challenges in data scaling and standardized evaluation by unifying diverse simulation environments and datasets. Key methodology involves METASIM, an infrastructure that abstracts simulators (like Isaac Sim, MuJoCo) via a universal configuration system and API, enabling cross-simulator integration, hybrid simulation, large-scale data migration, and augmentation. Experiments demonstrate that RoboVerse improves imitation learning (e.g., ACT achieved 50.0% average success on representative tasks), reinforcement learning, and world models, facilitating direct sim-to-real transfer (e.g., OpenVLA achieved 7.0/10.0 on a real-world wash soap task after sim training). For AI practitioners, RoboVerse provides a standardized framework and a large, diverse synthetic dataset to train and evaluate robot learning models more efficiently, potentially accelerating development and improving sim-to-real generalization. |
| ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D
  Physics Modeling for Complex Motion and Interaction (Read more on [arXiv](https://arxiv.org/abs/2504.21855) or [HuggingFace](https://huggingface.co/papers/2504.21855))| Alan Yuille, Liang-Chieh Chen, Qihang Yu, Ju He, Qihao Liu | ReVision enhances pre-trained video diffusion models by explicitly integrating parameterized 3D physical knowledge for high-quality, controllable generation of complex motion and interaction. The objective is to improve video generation quality, motion consistency, and control, particularly for complex actions and interactions, by leveraging 3D physical priors without needing massive models. The methodology involves a three-stage "Extract-Optimize-Reinforce" pipeline: generating a coarse video, extracting/optimizing 3D features using a Parameterized Physical Prior Model (PPPM), and regenerating the video conditioned on the refined 3D motion. Applied to Stable Video Diffusion (1.5B parameters), ReVision significantly improves motion coherence, outperforming a 13B parameter model in user preference studies and achieving an FVD of 130.14 on dance generation, adding only 5 seconds to inference time. For AI practitioners, this demonstrates that incorporating explicit physical knowledge can enhance smaller models to generate complex, controllable, physically plausible motions, offering an alternative to massive parameter scaling. |
| Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report (Read more on [arXiv](https://arxiv.org/abs/2504.21039) or [HuggingFace](https://huggingface.co/papers/2504.21039))| Anu Vellore, Blaine Nelson, Alexander Chen, Baturay Saglam, paulkass | This paper introduces Foundation-Sec-8B, a Llama 3.1-8B model further pretrained on a curated 5.1 billion token cybersecurity corpus. The main objective was to enhance LLM performance on specialized cybersecurity tasks by addressing domain-specific data scarcity and knowledge representation challenges. Key methodology involved collecting and filtering web data for cybersecurity relevance, performing continued pretraining, and evaluating the model on benchmarks like CTIBench, CyberMetric, and SecBench using few-shot or zero-shot prompting. Foundation-Sec-8B demonstrated significant improvement over the base Llama 3.1-8B, notably achieving 14.29% higher accuracy (0.720Â±0.017) on the CTIBench-RCM task, matching or exceeding larger models like Llama 3.1-70B and GPT-4o-mini in specific CTI evaluations while showing minimal general knowledge degradation. The principal implication for AI practitioners is the availability of a publicly released, cybersecurity-specialized 8B parameter model that serves as a strong foundation for developing more capable AI-driven security tools. |
| UniBiomed: A Universal Foundation Model for Grounded Biomedical Image
  Interpretation (Read more on [arXiv](https://arxiv.org/abs/2504.21336) or [HuggingFace](https://huggingface.co/papers/2504.21336))| Hao Chen, Jiaxin Zhuang, Sunan He, Yuxiang Nie, Linshan Wu | UniBiomed is presented as the first universal foundation model integrating segmentation and text generation for grounded biomedical image interpretation across diverse modalities. The main objective is to address the inflexibility and lack of holistic information usage in conventional AI approaches by unifying the generation of clinical texts and the segmentation of corresponding biomedical objects. UniBiomed integrates a Multi-modal Large Language Model (MLLM, InternVL2.5) with a Segment Anything Model (SAM, SAM2), trained end-to-end on a novel dataset of 27 million image-annotation-text triplets spanning 10 biomedical imaging modalities. Extensive validation on 84 datasets shows UniBiomed achieves state-of-the-art performance, surpassing the previous best segmentation model (BiomedParse) by an average of 10.25% in Dice score across 60 segmentation datasets, and demonstrates strong capabilities in grounded disease recognition, VQA, and report generation. For AI practitioners, UniBiomed offers a single, versatile model capable of end-to-end grounded interpretation, potentially streamlining biomedical image analysis workflows by eliminating the need for separate segmentation/text models and manual prompt engineering. |
| Generative AI for Character Animation: A Comprehensive Survey of
  Techniques, Applications, and Future Directions (Read more on [arXiv](https://arxiv.org/abs/2504.19056) or [HuggingFace](https://huggingface.co/papers/2504.19056))| Alireza Mirrokni, Hossein Behzadasl, Pardis Sadat Zahraei, Omid Ghahroodi, Mohammad Mahdi Abootorabi | This survey comprehensively reviews generative AI techniques, applications, datasets, evaluation metrics, and future directions for character animation, integrating traditionally fragmented subfields. The main objective is to provide a unified perspective on state-of-the-art generative AI applications, including facial animation, expression rendering, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. Key methodologies surveyed include GANs, VAEs, Transformers, and Diffusion Models, alongside foundational frameworks like SMPL and evaluation metrics like FID and CLIP Score across numerous datasets (e.g., LAION, AMASS, FFHQ). The paper finds that foundation and diffusion models have significantly advanced realism and reduced production costs but highlights challenges in cross-domain generalization, real-time performance, controllability, and standardized evaluation. For AI practitioners, this survey offers a structured roadmap to the field, detailing key models, essential resources (datasets, benchmarks), evaluation techniques, and identifying open research problems crucial for developing advanced AI-driven animation systems. |
