

## Papers for 2025-06-06

| Title | Authors | Summary |
|-------|---------|---------|
| RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language
  Models for Robotics (Read more on [arXiv](https://arxiv.org/abs/2506.04308) or [HuggingFace](https://huggingface.co/papers/2506.04308))| Shanyu Rong, Yi Han, Cheng Chi, Jingkun An, Zhoues | This paper introduces RoboRefer, a 3D-aware VLM for spatial referring with reasoning for embodied AI. The research aims to improve robots' understanding of 3D scenes and their ability to follow spatially constrained instructions through both precise spatial understanding and multi-step reasoning. The methodology involves supervised fine-tuning (SFT) with a disentangled depth encoder and reinforcement fine-tuning (RFT) using metric-sensitive process reward functions. Experiments show SFT-trained RoboRefer achieves state-of-the-art spatial understanding on existing benchmarks, and RFT-trained RoboRefer surpasses Gemini-2.5-Pro by 17.4% in average accuracy on a newly introduced RefSpatial-Bench. RoboRefer facilitates controlling robots for complex tasks in real-world environments, enabling effective manipulation and navigation, and can be used by AI practitioners working on robotics applications.  |
| SeedVR2: One-Step Video Restoration via Diffusion Adversarial
  Post-Training (Read more on [arXiv](https://arxiv.org/abs/2506.05301) or [HuggingFace](https://huggingface.co/papers/2506.05301))| Meng Wei, Yuxi Ren, Zhijie Lin, Shanchuan Lin, Jianyi Wang | i) SeedVR2, a one-step diffusion-based video restoration (VR) model, is introduced employing adversarial post-training. ii) The research aims to achieve high-resolution VR in a single step while enhancing visual quality and computational efficiency. iii) The methodology includes an adaptive window attention mechanism and adversarial post-training with feature matching loss. iv) Experiments demonstrate SeedVR2 achieves over 4x faster processing compared to existing diffusion-based VR methods, while maintaining comparable performance. v) The adaptive window attention mechanism improves robustness and reduces boundary artifacts for high-resolution video, offering AI practitioners an efficient means for VR tasks.  |
| Video World Models with Long-term Spatial Memory (Read more on [arXiv](https://arxiv.org/abs/2506.05284) or [HuggingFace](https://huggingface.co/papers/2506.05284))| Ziwei Liu, Yinghao Xu, Ryan Po, Shuai Yang, Tong Wu | i) This paper introduces a novel framework for enhancing long-term consistency in video world models using geometry-grounded spatial memory. ii) The research aims to address the issue of scene inconsistency in autoregressive video generation caused by limited temporal context windows. iii) The methodology involves integrating short-term working memory with long-term spatial memory (point cloud representation of static scenes) and episodic memory (historical keyframes), trained on a custom dataset generated from MiraData. iv) Evaluations show improved quality and 3D consistency compared to baselines, with user studies indicating superior performance in camera accuracy, static consistency, and dynamic plausibility; view recall consistency (PSNR) improves significantly from 12.16 to 19.10. v) The framework provides AI practitioners with an approach to improve the long-term coherence and realism of generated video environments by incorporating explicit 3D spatial reasoning and memory mechanisms.  |
| ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow
  Development (Read more on [arXiv](https://arxiv.org/abs/2506.05010) or [HuggingFace](https://huggingface.co/papers/2506.05010))| Zijiao Wu, Qingli Hu, Yiyu Wang, Xue Yang, imryanxu | ComfyUI-Copilot, an LLM-powered plugin, assists users in AI workflow development within ComfyUI. The research addresses usability challenges in ComfyUI, aiming to automate workflow construction and provide intelligent recommendations. A hierarchical multi-agent framework with a central assistant agent and specialized worker agents was implemented, supported by curated ComfyUI knowledge bases. ComfyUI-Copilot achieved high recall rates (over 88.5%) for both node and workflow recommendations using GPT-40 and DeepSeek-V3. This tool lowers the entry barrier for ComfyUI and enhances workflow efficiency, providing AI practitioners with a means to automate workflow development and improve node/model selection, though the paper lacks information on the type of queries that failed and the reasons why.  |
| Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights (Read more on [arXiv](https://arxiv.org/abs/2506.02865) or [HuggingFace](https://huggingface.co/papers/2506.02865))| Emilien Biré, Breno Baldas Skuk, Mathieu Andreux, tonywu71, hamza-hcompany | Surfer-H, a cost-efficient web agent, is introduced alongside Holol, a collection of open-weight Vision-Language Models (VLMs) specialized for web navigation and information extraction. The research aims to develop and evaluate a cost-effective web agent leveraging specialized VLMs. Surfer-H integrates a policy, localizer, and validator, powered by Holol models trained on web content, synthetic examples, and agentic data. Surfer-H achieves 92.2% state-of-the-art performance on WebVoyager when powered by Holol. The open-sourcing of both the WebClick dataset and Holol model weights enables AI practitioners to build more efficient and accurate web agents, but it is unclear what datasets were used in the performance evaluation.  |
| Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers
  for Long Contexts (Read more on [arXiv](https://arxiv.org/abs/2506.05229) or [HuggingFace](https://huggingface.co/papers/2506.05229))| Ivan Oseledets, Yuri Kuratov, Gleb Kuzmin, Ivan Rodkin, Danil Sivtsov | i) Diagonal Batching is introduced to unlock parallelism in Recurrent Memory Transformers (RMTs) for long contexts. ii) The research aims to mitigate the sequential execution bottleneck inherent in RMTs while preserving recurrence. iii) A scheduling scheme is developed that reorders computations into independent diagonals, enabling concurrent GPU execution without retraining. iv) Applying Diagonal Batching to a LLaMA-1B ARMT model achieves a 3.3x speedup compared to standard LLaMA-1B and a 1.8x speedup over sequential RMT on 131,072-token sequences. v) The technique's ability to enhance the performance of RMTs offers AI practitioners a more efficient method for processing long-context inputs in real-world applications.  |
| VideoREPA: Learning Physics for Video Generation through Relational
  Alignment with Foundation Models (Read more on [arXiv](https://arxiv.org/abs/2505.23656) or [HuggingFace](https://huggingface.co/papers/2505.23656))| Xiangpeng Wan, Fanqing Meng, Shaofeng Zhang, Jiaqi Liao, aHapBean | VideoREPA distills physics understanding from Video Foundation Models (VFMs) into text-to-video (T2V) diffusion models by aligning token-level relations. The research aims to improve the physical plausibility of generated videos by transferring physics knowledge from VFMs to VDMs. The methodology involves a Token Relation Distillation (TRD) loss for spatio-temporal alignment between VFM representations and diffusion transformer blocks. VideoREPA achieves a state-of-the-art Physical Commonsense (PC) score of 40.1 on VideoPhy, a 24.1% improvement over the CogVideoX baseline. For AI practitioners, VideoREPA provides a feature alignment framework that enhances the physical realism of generated videos, enabling more intuitive and physically plausible content generation, with potential applications in creating virtual environments and simulations.  |
| Qwen3 Embedding: Advancing Text Embedding and Reranking Through
  Foundation Models (Read more on [arXiv](https://arxiv.org/abs/2506.05176) or [HuggingFace](https://huggingface.co/papers/2506.05176))| Huan Lin, Mingxin Li, Yanzhao Zhang, izhx, thenlper | Qwen3 Embedding presents a new text embedding and reranking series based on the Qwen3 foundation models. The research aims to improve text embedding and reranking capabilities by leveraging Qwen3 LLMs and a multi-stage training pipeline. This pipeline combines unsupervised pre-training with supervised fine-tuning, aided by data synthesized using Qwen3 models. The Qwen3-8B-Embedding model achieves a score of 70.58 on the MTEB Multilingual benchmark. AI practitioners can utilize the open-sourced Qwen3 Embedding models to achieve state-of-the-art performance in multilingual text understanding and retrieval tasks.  |
| Aligning Latent Spaces with Flow Priors (Read more on [arXiv](https://arxiv.org/abs/2506.05240) or [HuggingFace](https://huggingface.co/papers/2506.05240))| Ping Luo, Ying Shan, Yixiao Ge, Yuying Ge, liyz | i) This paper introduces a novel framework for aligning latent spaces to arbitrary target distributions using flow-based generative models as priors. ii) The main research question is whether a learnable latent space can be efficiently aligned to an arbitrary target distribution using a pre-trained flow model as a prior. iii) The methodology involves pretraining a flow model on the target features and using it to regularize the latent space through an alignment loss based on the flow matching objective. iv) Experiments show that minimizing the alignment loss approximates maximizing the log-likelihood of the latents under the target distribution and large-scale image generation on ImageNet achieves a FID score of 6.56 with textual embeddings. v) The principal implication for AI practitioners is a computationally efficient method for incorporating complex distributional priors into latent models, enhancing structured representation learning.  |
| Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual
  Simulations (Read more on [arXiv](https://arxiv.org/abs/2506.04633) or [HuggingFace](https://huggingface.co/papers/2506.04633))| Yinuo Yang, Zixian Ma, Mahtab Bigverdi, Linjie Li, kuvvi | i) The paper introduces STARE, a new benchmark for evaluating multimodal models on spatial reasoning tasks requiring visual simulation. ii) The research objective is to assess the ability of multimodal large language models to perform complex visual reasoning through multi-step simulations. iii) The methodology involves curating a dataset of ~4K tasks spanning geometric transformations, integrated spatial reasoning, and real-world spatial reasoning, with variations in difficulty and evaluation setups. iv) Evaluations revealed that models excel at simpler 2D transformations but perform close to random chance on tasks requiring multi-step visual simulations; humans achieve near-perfect accuracy, speeding up on complex tasks with intermediate visual simulations. v) The inconsistent performance gains of models from visual simulations, with improvements on some tasks and declines in others, indicates that AI/ML practitioners should be aware that current models may not effectively leverage intermediate visual information for spatial cognition.  |
| SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs (Read more on [arXiv](https://arxiv.org/abs/2506.05344) or [HuggingFace](https://huggingface.co/papers/2506.05344))| Jiwen Lu, Yongming Rao, Jiahui Wang, Zuyan | i) The paper introduces SparseMM, a KV-Cache optimization strategy for accelerating Multimodal Large Language Models (MLLMs) by exploiting the discovered sparsity of visual-relevant attention heads. ii) The research aims to investigate how MLLMs process visual inputs by analyzing attention mechanisms and to develop a method for efficient MLLM inference. iii) The methodology involves analyzing attention mechanisms in MLLMs, identifying visual heads through targeted response analysis using OCR as an anchor task, and designing an asymmetric KV-Cache allocation strategy. iv) The primary results indicate that less than 5% of attention heads actively contribute to visual understanding, and SparseMM achieves 1.38x real-time acceleration and 52% memory reduction during generation while maintaining performance parity. v) The principal implication for AI practitioners is a computationally efficient method to accelerate MLLM inference by strategically allocating resources to visual-relevant attention heads, enabling better accuracy-efficiency trade-offs under limited computational budgets.  |
| AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual
  Counting for MLLMs (Read more on [arXiv](https://arxiv.org/abs/2506.05328) or [HuggingFace](https://huggingface.co/papers/2506.05328))| Tong Lu, Yicheng Liu, Zhiqi Li, cg1177, lulidong | i) The paper introduces CG-AV-Counting, a new clue-grounded audio-visual counting benchmark, and AV-Reasoner, a model trained to improve counting abilities in MLLMs. ii) The main objective is to address limitations in existing counting benchmarks and improve the counting capability of MLLMs. iii) The methodology involves manual annotation of a new benchmark with 1,027 multimodal questions and training AV-Reasoner using GRPO and curriculum learning, transferring counting ability from related tasks. iv) AV-Reasoner achieves a 44.00 accuracy on DVD-Counting, surpassing Video-R1 by 9.50 points, and demonstrates state-of-the-art results across multiple audio-visual understanding tasks. v) The findings suggest that reinforcement learning and clue-grounded benchmarks can improve multimodal reasoning for tasks requiring spatial-temporal grounding, implying improved MLLM performance in complex audio-visual environments, but a requirement for reasoning-answer consistency during training is necessary. |
| StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence
  Training of LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.03077) or [HuggingFace](https://huggingface.co/papers/2506.03077))| Xiao Li, Lei Zhao, Qijun Luo, Kullpar | i) StreamBP is introduced as a memory-efficient exact backpropagation algorithm for long sequence training of LLMs. ii) The research aims to reduce the memory cost associated with storing activation values during backpropagation in LLMs, particularly for long sequence data. iii) The methodology involves a linear decomposition of the chain rule along the sequence dimension performed layer-wise. iv) StreamBP scales up the maximum sequence length of BP by 2.8 – 5.5x larger while using comparable or even less BP time compared to gradient checkpointing. v) Practitioners can use StreamBP to train LLMs on significantly longer sequences with similar or reduced computational cost, facilitating improved performance on complex tasks like long-chain reasoning, and the technique can be directly transferred to batch size scaling for accelerating training. |
| MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical
  Chain-of-Thought Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.05331) or [HuggingFace](https://huggingface.co/papers/2506.05331))| Shilin Yan, Aojun Zhou, Renrui Zhang, CaraJ, xy06 | i) The paper introduces MINT-CoT, a method enabling interleaved visual tokens in mathematical chain-of-thought (CoT) reasoning for Large Language Models (LLMs). ii) The research objective is to enhance multimodal mathematical reasoning in LLMs by adaptively interleaving relevant visual tokens within textual reasoning steps. iii) The methodology involves an Interleave Token mechanism, a new MINT-CoT dataset with 54K mathematical problems, and a three-stage training strategy (Text-only CoT SFT, interleaved CoT SFT and interleaved CoT RL). iv) The MINT-CoT-7B model achieves +34.08% performance improvement on MathVista, +28.78% on GeoQA and +23.2% on MMStar compared to the baseline. v) The work provides AI practitioners with a method for improving visual-mathematical reasoning in multimodal LLMs, demonstrating significant performance gains over text-only and box-shaped visual CoT approaches.  |
| VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal
  Understanding in Videos (Read more on [arXiv](https://arxiv.org/abs/2506.05349) or [HuggingFace](https://huggingface.co/papers/2506.05349))| Ming-Hsuan Yang, Muhammad Maaz, Anqi Tang, Abdelrahman Shaker, Hanoona Rasheed | i) VideoMathQA is introduced as a new benchmark for evaluating mathematical reasoning in videos. ii) The primary objective is to assess temporally extended cross-modal reasoning capabilities of AI models on videos involving mathematical problems. iii) The methodology involves creating a dataset of 420 annotated video-question pairs spanning 10 mathematical domains with expert-provided step-by-step reasoning trails. iv) Evaluation of 30 models reveals that GPT-04-mini achieves the highest step evaluation score of 6.9, while Qwen2.5-VL-72B leads among open-source models with a score of 5.0. v) The key implication is that AI practitioners need to focus on improving models' ability to integrate fine-grained audio cues with visual information over extended time to solve complex mathematical problems in video settings.  |
| Inference-Time Hyper-Scaling with KV Cache Compression (Read more on [arXiv](https://arxiv.org/abs/2506.05345) or [HuggingFace](https://huggingface.co/papers/2506.05345))| Edoardo M. Ponti, Piotr Nawrot, Konrad Staniszewski, Adrian Łańcucki | i) This paper introduces Dynamic Memory Sparsification (DMS), a novel method for compressing the key-value (KV) cache in Transformer LLMs. ii) The main objective is to enhance inference-time scaling by improving reasoning accuracy within a fixed compute budget via KV cache compression. iii) The methodology involves retrofitting LLMs with DMS, which uses a learned eviction policy trained with a Gumbel-sigmoid to sparsify the KV cache. iv) The primary result is an average improvement of 9.1 points on AIME 24, 7.6 on GPQA, and 9.6 on LiveCodeBench for Qwen-R1 32B due to DMS. v) DMS provides AI practitioners with a method to improve the performance of LLMs in resource-constrained environments, enabling better reasoning capabilities within a given inference budget.  |
| Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting (Read more on [arXiv](https://arxiv.org/abs/2506.05327) or [HuggingFace](https://huggingface.co/papers/2506.05327))| Jia-Wang Bian, Zeyu Zhang, Donny Y. Chen, lhmd, dc-walker | i) This paper introduces PM-Loss, a novel regularization loss to improve geometry in feed-forward 3D Gaussian Splatting (3DGS) by leveraging pointmap priors. ii) The main objective is to mitigate depth discontinuities at object boundaries, a known limitation in depth map-based 3DGS pipelines. iii) The methodology involves using a pre-trained transformer to predict a pointmap which is then used as a pseudo-ground truth to regularize the unprojected depth maps via a single-directional Chamfer loss. iv) Experiments show that models trained with PM-Loss achieve a consistent PSNR gain of at least 2 dB on DL3DV and RealEstate10K datasets compared to baselines. v) PM-Loss provides AI practitioners with a plug-and-play, efficient, and effective method for improving the geometric quality and rendering results of feed-forward 3DGS models.  |
| EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an
  Egocentric World? (Read more on [arXiv](https://arxiv.org/abs/2506.05287) or [HuggingFace](https://huggingface.co/papers/2506.05287))| Dian Jiao, Wentong Li, Long Li, Ronghao Dang, CircleRadon | i) EOC-Bench, a new benchmark, evaluates object-centric embodied cognition in multimodal large language models (MLLMs) within dynamic egocentric scenarios. ii) The paper investigates the capabilities of MLLMs to identify, recall, and forecast object states, locations, and relationships in dynamic egocentric videos. iii) The methodology involves a mixed-format human-in-the-loop annotation framework generating 3,277 QA pairs, categorized temporally into Past, Present, and Future and including visual object referencing prompts. iv) Results show the GPT-4o model achieves an overall accuracy of 61.83% on the EOC-Bench, with lower performance on tasks requiring absolute time perception. v) The benchmark highlights limitations in temporal reasoning and object-level spatiotemporal understanding in current MLLMs, which require robust designs for embodied object cognitive tasks.  |
| Language-Image Alignment with Fixed Text Encoders (Read more on [arXiv](https://arxiv.org/abs/2506.04209) or [HuggingFace](https://huggingface.co/papers/2506.04209))| Yi Ma, Yue Zhao, robinwuzy, JingfengY | Language-Image alignment is achievable by solely training the image encoder with a fixed, pre-trained large language model (LLM) as the text encoder. The research investigates if costly joint training of text and image encoders is necessary for language-image alignment, proposing instead to learn Language-Image alignment with a Fixed Text encoder (LIFT). The methodology involves using a pre-trained text encoder fine-tuned on an LLM to embed texts offline and solely training the image encoder to align visual representations with the text embeddings using CLIP's contrastive loss. The results show that LIFT outperforms CLIP in most scenarios involving compositional understanding, achieving an average accuracy gain of 7.4% across seven compositional understanding tasks and demonstrates FLOPs reduction up to 35.7% in long caption training. LIFT provides AI practitioners with an alternative design choice for learning language-aligned visual representations, offering gains in computational efficiency and improved performance in compositional tasks by leveraging LLMs.  |
| FlexPainter: Flexible and Multi-View Consistent Texture Generation (Read more on [arXiv](https://arxiv.org/abs/2506.02620) or [HuggingFace](https://huggingface.co/papers/2506.02620))| Luozhou Wang, Jiantao Lin, Leyi Wu, yingcongchen, StarYDY | FlexPainter is a novel texture generation pipeline facilitating flexible multi-modal conditional guidance and consistent multi-view texture synthesis. The research aims to improve texture generation quality and control by enabling flexible prompt integration and mitigating inconsistencies in multi-view images. It employs a shared conditional embedding space for multi-modal aggregation, an image-based classifier-free guidance method for stylization, and a multi-view grid representation with view synchronization for consistency. Experiments demonstrate the framework achieves significantly better FID score compared to existing methods in text-to-texture generation. FlexPainter offers AI practitioners an improved approach to 3D texture generation with enhanced control and consistency, beneficial for applications in 3D modeling and computer graphics.  |
| The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly
  Licensed Text (Read more on [arXiv](https://arxiv.org/abs/2506.05209) or [HuggingFace](https://huggingface.co/papers/2506.05209))| Stella Biderman, Colin Raffel, Brian Lester, Nikhil Kandpal, storytracer | i) The paper introduces the Common Pile v0.1, an 8TB dataset of public domain and openly licensed text, for large language model (LLM) pretraining. ii) The research aims to demonstrate the feasibility of training performant LLMs on openly licensed data as an alternative to unlicensed text sources. iii) The methodology involves collecting and curating text from 30 diverse sources, and training two 7B parameter LLMs, Comma v0.1-1T and Comma v0.1-2T, on 1 and 2 trillion tokens, respectively. iv) Results show Comma v0.1-1T and Comma v0.1-2T attain competitive performance to LLMs trained on unlicensed text with similar computational budgets, such as Llama 1 and 2 7B. v) AI practitioners can leverage the Common Pile v0.1 dataset and Comma v0.1 models to develop ethically-sourced LLMs, with code, data and models released.  |
| Autoregressive Images Watermarking through Lexical Biasing: An Approach
  Resistant to Regeneration Attack (Read more on [arXiv](https://arxiv.org/abs/2506.01011) or [HuggingFace](https://huggingface.co/papers/2506.01011))| Wenli Huang, Ye Deng, Sanping Zhou, Yiren Song, Siqi Hui | Autoregressive Images Watermarking through Lexical Biasing (LBW) is proposed as a novel watermarking framework for autoregressive image generation models, resistant to regeneration attacks. The paper addresses the challenge of robust watermarking in AR models by introducing a lexical bias during token selection, using a multi-green-list strategy for enhanced security. LBW embeds watermarks by biasing token selection toward a predefined green list during image generation or substituting red tokens with green tokens post-hoc. Experiments demonstrate that LBW achieves superior robustness, with LBW-Post on RAR attaining an AUC of 0.995 and TPR@1FPR of 0.937 against regeneration attacks, outperforming WatermarkDM. AI practitioners can leverage LBW to ensure traceability and prevent misuse of images generated by AR models.  |
| MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at
  Scale (Read more on [arXiv](https://arxiv.org/abs/2506.04405) or [HuggingFace](https://huggingface.co/papers/2506.04405))| Yue Yu, Yishan Zhong, Yuchen Zhuang, Ran Xu, wshi83 | i) MedAgentGym, a publicly available training environment, facilitates training large language model (LLM) agents for code-based medical reasoning. ii) The research aims to enhance coding-based medical reasoning capabilities in LLM agents using a specialized training environment. iii) The methodology involves creating 72,413 task instances across 129 categories from 12 real-world biomedical scenarios, encapsulated within executable coding environments, and benchmarking over 25 LLMs. iv) Med-Copilot-7B, leveraging MedAgentGym, achieves substantial performance gains through supervised fine-tuning (+36.44%) and reinforcement learning (+42.47%). v) This integrated platform can be used by AI practitioners to develop LLM-based coding assistants for advanced biomedical research and practice, offering an affordable and privacy-preserving alternative to commercial models for complex code-based medical reasoning.  |
| Geometry-Editable and Appearance-Preserving Object Compositon (Read more on [arXiv](https://arxiv.org/abs/2505.20914) or [HuggingFace](https://huggingface.co/papers/2505.20914))| Liang Lin, Zhijing Yang, Chunmei Qing, Haojie Li, Jianman Lin | i) The paper introduces DGAD, a diffusion model for geometry-editable and appearance-preserving object composition. ii) The main objective is to achieve both precise geometric editing and faithful appearance preservation when integrating objects into scenes. iii) The methodology involves disentangling geometry editing via CLIP/DINO-derived embeddings and appearance preservation via a dense cross-attention retrieval mechanism, integrating these into a pre-trained diffusion model. iv) Experiments show DGAD achieves a 61.14 IR score, indicating improved editability over existing methods. v) The principal implication is providing AI practitioners with an improved method for generating geometrically consistent and visually faithful composite images.  |
| FreeTimeGS: Free Gaussians at Anytime and Anywhere for Dynamic Scene
  Reconstruction (Read more on [arXiv](https://arxiv.org/abs/2506.05348) or [HuggingFace](https://huggingface.co/papers/2506.05348))| Zhanhua Zhang, Jiaming Sun, Zhen Xu, Peishan Yang, Yifan Wang | i) The paper introduces FreeTimeGS, a novel 4D Gaussian representation for dynamic scene reconstruction enabling Gaussian primitives at arbitrary times and locations. ii) The main objective is to improve dynamic 3D scene reconstruction, particularly for scenes with complex motions. iii) The method endows each Gaussian primitive with an explicit motion function and incorporates a temporal opacity function and 4D regularization to enhance representational capacity and optimize rendering quality. iv) Experimental results on the SelfCap dataset demonstrate a PSNR improvement of 2.4dB (entire image) and 4.1dB (dynamic regions) over 4DGS and achieves real-time rendering speeds of 450 FPS at 1080p resolution. v) FreeTimeGS offers AI practitioners a more flexible and efficient method for dynamic scene reconstruction, potentially improving performance in applications requiring high-quality, real-time rendering of complex dynamic environments.  |
| Rectified Point Flow: Generic Point Cloud Pose Estimation (Read more on [arXiv](https://arxiv.org/abs/2506.05282) or [HuggingFace](https://huggingface.co/papers/2506.05282))| Iro Armeni, Shuran Song, Shengyu Huang, Liyuan Zhu, Tao Sun | i) Rectified Point Flow is introduced as a unified parameterization for point cloud registration and shape assembly. ii) The research aims to develop a generic point cloud pose estimation method that addresses both pairwise registration and multi-part shape assembly in a single conditional generative framework. iii) The methodology involves learning a continuous point-wise velocity field to transport noisy points toward target positions, conditioned on unposed part point clouds, along with a self-supervised encoder pretrained on point-wise overlap. iv) The proposed method achieves state-of-the-art performance on six benchmarks, and notably improves performance by jointly training on diverse datasets. v) This unified approach facilitates learning shared geometric priors for AI practitioners, leading to improved accuracy and generalizability across various 3D reasoning tasks.  |
| Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning
  Capabilities Through Evaluation Design (Read more on [arXiv](https://arxiv.org/abs/2506.04734) or [HuggingFace](https://huggingface.co/papers/2506.04734))| Xiaoqi Jian, Yongfu Zhu, Jinzhu Wu, Weihong Lin, lincharliesun | i) This paper investigates the sensitivity of LLM reasoning benchmark evaluations to subtle configuration variations. ii) The main objective is to assess how minor changes in evaluation conditions impact the reliability of reported LLM performance. iii) The study employs controlled experiments on Deepseek-R1-Distill series models, varying parameters like seed initialization, dataset version, instruction position, option bias, and tensor parallelism. iv) Results indicate that fluctuations caused by varying seed can be greater than baseline, with changes in option order and answer position causing performance fluctuations above 5 percentage points on GPQA Diamond; 67% of experimental groups exhibited TP fluctuation exceeding baseline reference. v) AI practitioners need to standardize evaluation methodologies, including disclosing evaluation settings and statistically supported stable performance, to ensure the reliability and fairness of LLM comparisons.  |
| Scaling Laws for Robust Comparison of Open Foundation Language-Vision
  Models and Datasets (Read more on [arXiv](https://arxiv.org/abs/2506.04598) or [HuggingFace](https://huggingface.co/papers/2506.04598))| Romain Beaumont, Tommie Kerssies, Giovanni Pucceti, Tomer Porian, Marianna Nezhurina | i) The paper derives scaling laws for CLIP and MaMMUT language-vision models to enable robust model and dataset comparison across varying scales. ii) The objective is to determine the dependence of model performance on pre-training compute for model and dataset comparison in language-vision learning. iii) The study derives full scaling laws based on dense measurements across model and samples seen scales for CLIP and MaMMUT architectures trained on DataComp-1.4B, DFN-1.4B and Re-LAION-1.4B datasets, evaluating downstream tasks such as zero-shot classification, retrieval, and segmentation. iv) The results indicate that MaMMUT shows stronger improvement with scale and better sample efficiency than standard CLIP, and openMaMMUT-L/14 achieves 80.3% zero-shot ImageNet-1k accuracy trained on 12.8B samples from DataComp-1.4B. v) Practitioners can utilize the derived scaling laws to systematically compare and improve open foundation models and datasets, avoiding misleading conclusions based on single reference scales, allowing for informed selection of pre-training procedures.  |
| SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video
  Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2506.00830) or [HuggingFace](https://huggingface.co/papers/2506.00830))| Youqiang Zhang, Baoxuan Gu, Hao Jiang, Zhengcong Fei, diqiu7 | SkyReels-Audio introduces a unified framework for generating and editing audio-conditioned talking portrait videos using diffusion transformers. The research aims to synthesize high-fidelity, temporally coherent talking portrait videos from multimodal inputs including audio, text, images, and videos. A hybrid curriculum learning strategy progressively aligns audio with facial motion, enhanced by a facial mask loss and audio-guided classifier-free guidance with a sliding-window denoising approach for visual fidelity. Evaluations demonstrate SkyReels-Audio achieves superior performance in lip-sync accuracy with a reported FID of 38.32, identity consistency, and realistic facial dynamics on the HDTF dataset. For AI practitioners, this work offers a scalable architecture and training methodology for generating controllable and coherent talking head videos, enabling diverse applications in digital media and interactive AI.  |
| Contextual Integrity in LLMs via Reasoning and Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2506.04245) or [HuggingFace](https://huggingface.co/papers/2506.04245))| Janardhan Kulkarni, Huseyin A. Inan, wulu, sahar-abdelnabi, Eric-Lan | i) The paper introduces a reinforcement learning (RL) framework to improve contextual integrity (CI) in Large Language Models (LLMs). ii) The research aims to reduce inappropriate information disclosure by LLMs while maintaining task performance by improving reasoning capabilities around CI. iii) The methodology involves prompting LLMs for explicit reasoning about CI, followed by RL-based post-training using a synthetic dataset and the GRPO algorithm. iv) The results demonstrate up to a 40% reduction in privacy leakage rate on the PrivacyLens benchmark, showing effective transfer of CI reasoning capabilities. v) The research implies that supporting CI reasoning should be a core part of the alignment process for real-world LLM-based agents, improving their safety and context-awareness.  |
| Micro-Act: Mitigate Knowledge Conflict in Question Answering via
  Actionable Self-Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.05278) or [HuggingFace](https://huggingface.co/papers/2506.05278))| Xiaolong Li, Ge Qu, Bowen Qin, Jinyang Li, NanHUO | i) The paper introduces MICRO-ACT, a framework for mitigating knowledge conflicts in retrieval-augmented question answering (QA) systems. ii) The research objective is to improve QA accuracy by addressing inconsistencies between retrieved external knowledge and the internal, parametric knowledge of large language models (LLMs). iii) MICRO-ACT employs a hierarchical action space and adaptive granularity through decomposition, enabling fine-grained comparisons between knowledge sources. iv) Experiments on five benchmark datasets showed that MICRO-ACT improved QA accuracy over state-of-the-art baselines by up to 9.40% on ConflictBank and 6.65% on KRE datasets for GPT-40-mini. v) AI practitioners can leverage MICRO-ACT's dynamic decomposition to build more reliable RAG systems that are more resilient to knowledge conflicts, especially in temporal and semantic contexts.  |
| RobustSplat: Decoupling Densification and Dynamics for Transient-Free
  3DGS (Read more on [arXiv](https://arxiv.org/abs/2506.02751) or [HuggingFace](https://huggingface.co/papers/2506.02751))| Yuan Xiong, Guanying Chen, Kunbin Yao, Yuqi Zhang, fcy99 | RobustSplat addresses artifact generation in 3D Gaussian Splatting (3DGS) due to transient objects in dynamic scenes. The paper aims to improve 3DGS optimization in in-the-wild scenarios by mitigating the influence of transient objects. RobustSplat employs a delayed Gaussian growth strategy and scale-cascaded mask bootstrapping approach, prioritizing static scene reconstruction before densification. Experiments on the NeRF On-the-go dataset demonstrate that RobustSplat achieves state-of-the-art performance, improving PSNR across six scenes. This improved method can be directly incorporated into existing 3DGS pipelines by AI practitioners for enhanced robustness in dynamic environments.  |
| Diffusion-Based Generative Models for 3D Occupancy Prediction in
  Autonomous Driving (Read more on [arXiv](https://arxiv.org/abs/2505.23115) or [HuggingFace](https://huggingface.co/papers/2505.23115))| Yingshi Liang, Yucheng Mao, Tianyuan Yuan, Yicheng Liu, Yunshen Wang | i) This paper introduces a diffusion-based generative model for 3D occupancy prediction in autonomous driving. ii) The research aims to improve 3D occupancy prediction by reframing it as a generative modeling task that incorporates 3D scene priors and handles noisy data. iii) The methodology involves adapting diffusion models for occupancy prediction, incorporating conditional sampling with a U-Net variant denoiser network and a BEV visual encoder, and exploring different occupancy representations including spatial latent, triplane, and discrete categorical variables. iv) Experiments show that the diffusion-based generative model outperforms state-of-the-art discriminative approaches, achieving a 7.05 mIoU improvement over BEVFormer, especially in occluded or low-visibility regions. v) The research implies that AI practitioners can leverage diffusion models to enhance the realism, accuracy, and consistency of 3D occupancy predictions, particularly in autonomous driving applications with noisy or incomplete sensor data.  |
| Images are Worth Variable Length of Representations (Read more on [arXiv](https://arxiv.org/abs/2506.03643) or [HuggingFace](https://huggingface.co/papers/2506.03643))| Zineng Tang, Wenhao Yan, Xin Liang, Rodolfo Corona, Lingjun Mao | i) The paper introduces DOVE, a dynamic vision encoder that generates variable-length token sequences for image reconstruction. ii) The research aims to improve the efficiency and expressiveness of visual representations by adaptively adjusting token sequence length based on image complexity. iii) The methodology involves extending the standard autoencoder framework with a transformer-based dynamic token generator and jointly optimizing image reconstruction quality and EOS token prediction. iv) Results demonstrate that DOVE significantly reduces the average number of tokens while maintaining high reconstruction quality and outperforms autoencoder-based tokenization methods in downstream tasks, achieving a token compression rate averaging 68% in a query-conditioned setup. v) DOVE's dynamic token generation and query-conditioned approach provide AI practitioners with a more efficient and semantically richer vision encoder for various tasks including visual question answering.  |
| Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric
  Approach (Read more on [arXiv](https://arxiv.org/abs/2506.03238) or [HuggingFace](https://huggingface.co/papers/2506.03238))| Weidi Xie, Yanfeng Wang, Ya Zhang, Lisong Dai, zzh99 | i) This paper presents OminiAbnorm-CT, a system and dataset for abnormality-centric whole-body CT image interpretation. ii) The research aims to develop an AI system capable of automatically detecting, localizing, and describing abnormal findings across multi-plane, whole-body CT scans based on text or visual prompts. iii) The methodology involves creating a hierarchical taxonomy of 404 abnormal findings, curating a dataset of 14.5K CT images with 19K abnormality annotations, and developing a multi-modal language model integrated with a segmentation module, trained jointly with a text generation loss and a segmentation loss. iv) The OminiAbnorm-CT system significantly outperforms existing methods in grounded report generation, text-guided grounded report generation, and visual-prompted report generation, achieving a RaTEScore of 86.35 on the axial visual prompted report generation task, indicating superior performance in generating clinically relevant reports. v) The principal implication for AI practitioners is the demonstration of an abnormality-centric approach for improving the explainability and clinical relevance of automated CT image interpretation systems, which can be used to inform the design of more effective diagnostic tools.  |
| BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View
  Representations (Read more on [arXiv](https://arxiv.org/abs/2506.02587) or [HuggingFace](https://huggingface.co/papers/2506.02587))| Konstantinos Karydis, Divyank Shah, Justin Yue, Jerry Li, Yewandou | BEVCALIB is a novel LiDAR-camera calibration model using bird's-eye view (BEV) representations. The research aims to perform LiDAR-camera calibration from raw data by leveraging BEV features. It extracts and fuses camera and LiDAR BEV features into a shared space and employs a geometry-guided feature selector for efficient training. Evaluations show BEVCALIB outperforms baselines, achieving an average improvement of (47.08%, 82.32%) on the KITTI dataset and (78.17%, 68.29%) on the NuScenes dataset, in terms of (translation, rotation) respectively, under various noise conditions. BEVCALIB provides AI practitioners with an open-source, high-performing tool for LiDAR-camera calibration, improving accuracy and robustness compared to existing methods.  |
| PATS: Proficiency-Aware Temporal Sampling for Multi-View Sports Skill
  Assessment (Read more on [arXiv](https://arxiv.org/abs/2506.04996) or [HuggingFace](https://huggingface.co/papers/2506.04996))| Antonio Liotta, EdBianchi | i) The paper introduces Proficiency-Aware Temporal Sampling (PATS), a novel video sampling strategy designed for multi-view sports skill assessment. ii) The main objective is to improve the accuracy of automated skill assessment by preserving temporal continuity within continuous video segments. iii) The methodology involves adaptively segmenting videos to ensure each analyzed portion contains a complete fundamental movement, maximizing information coverage while maintaining temporal coherence. iv) Evaluated on the EgoExo4D benchmark, PATS surpasses the state-of-the-art accuracy across all viewing configurations, achieving up to a +3.05% improvement, including a +26.22% gain in bouldering accuracy. v) For AI practitioners, PATS offers an architecture-agnostic pre-processing step that can be integrated with existing temporal modeling frameworks to enhance model accuracy in sports skill assessment without adding computational overhead.  |
| What do self-supervised speech models know about Dutch? Analyzing
  advantages of language-specific pre-training (Read more on [arXiv](https://arxiv.org/abs/2506.00981) or [HuggingFace](https://huggingface.co/papers/2506.00981))| Willem Zuidema, Gaofei Shen, Charlotte Pouw, Hosein Mohebbi, Marianne de Heer Kloots | i) This paper analyzes the encoding of Dutch phonetic and lexical features in self-supervised Wav2Vec2 models pre-trained with varying amounts of Dutch, English, and multilingual data. ii) The research investigates whether language-specific pre-training improves the representation of Dutch linguistic features in SSL models compared to English or multilingual pre-training. iii) The methodology includes pre-training Wav2Vec2 models with different language configurations, extracting internal representations, and evaluating them using phone identity probing, ABX tasks, phone/word clustering, representational similarity analysis, and downstream ASR fine-tuning. iv) Results indicate that pre-training exclusively on Dutch improves the representation of Dutch linguistic features, with the Dutch-trained model achieving lower word error rates (WER) of 10.4 on the CGN-o test set in downstream ASR compared to English (21.5) and multilingual (12.7) models. v) The principal implication is that language-specific pre-training can substantially enhance the encoding of language-specific features in SSL models, improving downstream ASR performance, particularly for languages with unique phonetic characteristics; indicating that careful selection of pre-training data is crucial for optimizing SSL models for specific languages.  |
