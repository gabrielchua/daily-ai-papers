

## Papers for 2025-07-01

| Title | Authors | Summary |
|-------|---------|---------|
| Ovis-U1 Technical Report (Read more on [arXiv](https://arxiv.org/abs/2506.23044) or [HuggingFace](https://huggingface.co/papers/2506.23044))| Pengxin Zhan, Liangfu Cao, Xinjie Zhang, Shanshan Zhao, Flourish | This report introduces Ovis-U1, a 3-billion-parameter unified model integrating multimodal understanding, text-to-image generation, and image editing. The research objective is to develop an effective architecture and unified training procedure for a model that performs both understanding and generation, starting from a base language model rather than a pre-trained MLLM. The methodology involves augmenting a Qwen3-1.7B LLM with a diffusion-based visual decoder and a bidirectional token refiner, trained via a six-stage process on a diverse mix of understanding, generation, and editing data. Ovis-U1 achieves a score of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing other models in its parameter class, and scores 4.00 on the ImgEdit-Bench. For AI practitioners, the principal implication is that a unified training approach, which integrates generation and understanding tasks from an early stage, yields superior performance in both domains compared to training on siloed tasks, demonstrating a synergistic benefit from co-training. |
| VMoBA: Mixture-of-Block Attention for Video Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2506.23858) or [HuggingFace](https://huggingface.co/papers/2506.23858))| Ye Tian, Xin Tao, Haotian Yang, Jianzong Wu, lianghou | This paper introduces VMoBA, a sparse attention mechanism that adapts Mixture-of-Block Attention to efficiently train and run Video Diffusion Models on long video sequences. The primary objective is to mitigate the quadratic complexity of full attention in Video Diffusion Models (VDMs) to enable efficient training and inference on long-duration, high-resolution videos. The method enhances the MoBA framework by introducing a layer-wise recurrent 1D-2D-3D block partitioning scheme for spatio-temporal data, a global block selection algorithm to prioritize important query-key interactions, and a threshold-based mechanism to dynamically determine the number of attended blocks. Experiments show VMoBA accelerates VDM training on longer sequences, achieving up to a 1.48x training speedup while attaining comparable or superior generation quality (VBench score 68.34 vs. 68.25) compared to full attention. For AI practitioners, VMoBA offers a method to significantly reduce the computational cost and time for training high-fidelity VDMs on longer video sequences, making the development of such models more feasible without sacrificing output quality. |
| Calligrapher: Freestyle Text Image Customization (Read more on [arXiv](https://arxiv.org/abs/2506.24123) or [HuggingFace](https://huggingface.co/papers/2506.24123))| Ka Leong Cheng, Hao Ouyang, Qingyan Bai, Yue Ma, JingyeChen22 | Calligrapher is a diffusion-based framework for freestyle text image customization, enabling the transfer of artistic styles from arbitrary reference images onto target text. The primary objective is to automate typography generation by precisely emulating a reference style while ensuring accurate character rendering and seamless integration into the source image. The core methodology involves a self-distillation pipeline to auto-construct a style-centric typography benchmark and a localized style injection mechanism, which uses a trainable style encoder (with a Qformer) to inject style features into the cross-attention layers of a frozen diffusion model. Calligrapher significantly outperforms state-of-the-art baselines, achieving a Fréchet Inception Distance (FID) of 38.09, compared to the next-best score of 66.68 from TextDiffuser-2, and was preferred by users 72% of the time. The principal implication for AI practitioners is the provision of a highly effective and efficient method for style adaptation in generative models, which automates complex typographic design and introduces a scalable self-distillation strategy for creating specialized training data without manual annotation. |
| Listener-Rewarded Thinking in VLMs for Image Preferences (Read more on [arXiv](https://arxiv.org/abs/2506.22832) or [HuggingFace](https://huggingface.co/papers/2506.22832))| Anton Gusarov, Andrey Galichin, Li Pengyi, barracuda049, alexgambashidze | This paper introduces a listener-augmented reinforcement learning framework to improve vision-language model (VLM) reasoning for human visual preferences. The primary objective is to address a failure mode in reinforcement learning where a model's reasoning trace contradicts its final decision, thereby improving generalization and alignment with human intent. The key methodology is a listener-augmented Group Relative Policy Optimization (GRPO) framework where a frozen, independent VLM (the "listener") re-evaluates the "reasoner" model's chain-of-thought to generate a dense, calibrated confidence score that shapes the RL reward signal, penalizing unpersuasive explanations. The proposed listener-shaped reward scheme achieves state-of-the-art accuracy of 67.4% on the ImageReward benchmark, improves out-of-distribution performance by up to +6% over a naive reasoner, and reduces reasoning contradictions from 10.1% to 8.3%. The principal implication for AI practitioners is that this listener-based reward provides a scalable and data-efficient method for aligning generative models, enabling the development of more robust VLMs that produce not only correct but also persuasive and consistent explanations without requiring new, expensive human annotation. |
| SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via
  Multi-Agent Multi-Turn Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2506.24119) or [HuggingFace](https://huggingface.co/papers/2506.24119))| Penghui Qi, Leon Guertler, lkevinzc, simonycl, Benjamin-eecs | This paper introduces SPIRAL, a framework where language models autonomously improve their reasoning by playing multi-turn, zero-sum games against themselves. The research objective is to determine if competitive self-play, without human-curated data, can cultivate transferable reasoning skills that generalize to academic benchmarks. The core methodology is a fully online, multi-agent reinforcement learning system using a novel Role-conditioned Advantage Estimation (RAE) technique to stabilize training. The primary result shows that training a Qwen3-4B model on Kuhn Poker alone achieves an 8.6% improvement on math and 8.4% on general reasoning benchmarks, outperforming supervised fine-tuning on 25,000 expert trajectories. For AI practitioners, this implies that complex reasoning can be developed through autonomous, competitive interaction, suggesting that game environments can serve as a scalable alternative to expensive, human-curated datasets for enhancing core cognitive abilities. |
| Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective (Read more on [arXiv](https://arxiv.org/abs/2506.17930) or [HuggingFace](https://huggingface.co/papers/2506.17930))| LidongBing, Zhiqiang007, Jianyu | This research demonstrates that pruning in-context learning (ICL) demonstrations into syntactically incoherent "gibberish" significantly improves LLM performance and introduces PROMPTQUINE, an evolutionary framework to automate this discovery. The primary objective is to investigate if pruning ICL demonstrations into seemingly nonsensical forms can outperform conventionally well-crafted, natural language prompts. The paper proposes PROMPTQUINE, a self-replicating framework based on a Genetic Algorithm, which evolves a population of pruned prompts by applying mutations (token removal) and using task performance as a fitness function for selection. This method consistently outperforms baselines; for instance, on Llama-3-8B-Instruct, 4-shot PROMPTQUINE achieved an average classification accuracy of 81.3%, a significant improvement over the original 4-shot ICL's 72.0%. The principal implication for AI practitioners is that optimal prompts may not be human-intuitive, and employing open-ended, evolutionary search algorithms is a highly effective strategy for prompt optimization, suggesting a shift from natural language design to exploring the model's preferred input structures. |
| Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric
  Attention (Read more on [arXiv](https://arxiv.org/abs/2506.23542) or [HuggingFace](https://huggingface.co/papers/2506.23542))| Di Qiu, Jin Zeng, Changyong He, weidawang | The paper introduces GIGA-ToF, a denoising network that enhances temporal consistency and spatial sharpness in Time-of-Flight (ToF) depth videos by fusing motion-invariant graph structures across frames. The objective is to develop a ToF depth video denoising method that resolves the trade-off between spatial sharpness and temporal consistency by leveraging the temporal self-similarity of geometric graph structures. The method formulates denoising as a Maximum a Posteriori (MAP) problem with a graph Laplacian smoothness prior defined on a fused graph and a data fidelity term based on ToF noise statistics; this MAP problem is then unrolled into an iterative deep network where a Graph-Informed Geometric Attention (GIGA) module learns to fuse intra-frame graphs from consecutive frames. On the synthetic DVToF dataset, the proposed GIGA-ToF model achieves state-of-the-art performance, outperforming competing methods by at least 37.9% in Mean Absolute Error (MAE) and 13.2% in Temporal End-Point Error (TEPE), while also demonstrating strong generalization to real, unseen Kinectv2 data. AI practitioners can use this method to obtain significantly cleaner and more temporally stable depth streams from commodity ToF sensors, providing a higher-quality input for downstream tasks like 3D reconstruction and robotic navigation, with the added benefit of an interpretable model architecture derived from algorithm unrolling. |
| Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in
  Inference-time Scaling? (Read more on [arXiv](https://arxiv.org/abs/2506.17417) or [HuggingFace](https://huggingface.co/papers/2506.17417))| Kaizhuo Yan, Jize Jiang, Jingcheng Yang, Meitang Li, Mingyuan1997 | This paper investigates the effectiveness of inference-time self-verification techniques in reinforcement learning (RL)-trained Vision-Language Models (VLMs), revealing a significant gap between their generation and verification abilities. The research aims to determine whether RL-trained VLMs genuinely benefit from self-correction and "aha moments" or if these behaviors are surface-level artifacts that do not improve reasoning performance. The study compares the performance of generation-reliant strategies (e.g., majority voting) against verification-reliant strategies (e.g., self-verified Best-of-N) on RL-tuned VLMs using the GeoQA and MathVista benchmarks. The primary result shows that generation-heavy methods consistently outperform verification-based methods; for instance, on GeoQA, a VLM's accuracy decreased by as much as 16.7% with self-verification compared to greedy decoding, and models often performed verification better when the visual input was omitted. The principal implication for practitioners is that self-verification mechanisms from LLMs do not directly translate to VLMs, which currently lack robust multimodal verification capabilities, indicating that simply applying RL-tuning is insufficient and new methods are needed to bridge this generation-verification gap. |
| MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame
  Optical Flow Estimation (Read more on [arXiv](https://arxiv.org/abs/2506.23151) or [HuggingFace](https://huggingface.co/papers/2506.23151))| Dmitriy Vatolin, Egor Chistov, Vladislav Bargatin, a-yakovenko | The paper introduces MEMFOF, a multi-frame optical flow model that achieves state-of-the-art accuracy at high resolutions while being significantly more memory-efficient than existing methods. The objective is to enable the training and inference of optical flow models on high-resolution (1080p) video without prohibitive GPU memory costs, thus avoiding performance degradation from input downsampling. The methodology extends a two-frame RAFT-like architecture to a three-frame, bidirectional paradigm and drastically reduces memory by lowering the correlation volume's working resolution from the standard 1/8 to 1/16 of the input size, coupled with a high-resolution training strategy on upscaled datasets. MEMFOF ranks first on the Spring benchmark with a 1-pixel outlier rate of 3.289 while requiring only 2.09 GB of GPU memory for 1080p inference, a nearly 4x reduction compared to the baseline RAFT. This allows AI practitioners to deploy high-accuracy, multi-frame optical flow models on high-resolution video using consumer-grade GPUs, enabling applications previously limited by memory constraints without sacrificing fine motion details. |
| SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity (Read more on [arXiv](https://arxiv.org/abs/2506.16500) or [HuggingFace](https://huggingface.co/papers/2506.16500))| Ligeng Zhu, Junxian Guo, Xiuyu Li, zhijianliu, Skhaki | SparseLoRA accelerates LLM fine-tuning by applying structured, contextual sparsity to reduce computational load while preserving the efficiency of LoRA. The primary objective is to reduce the computational cost of parameter-efficient fine-tuning, which existing methods like LoRA and QLoRA do not address. The core methodology involves a lightweight, training-free SVD sparsity estimator that dynamically selects a sparse subset of weights for computation, combined with a sensitivity analysis that applies sparsity non-uniformly across layers, tokens, and training steps. On the Math10K benchmark with a LLaMA3-8B model, SparseLoRA achieved a 1.6x fine-tuning speedup over standard LoRA by reducing FLOPs by 54% while maintaining comparable accuracy. For AI practitioners, this method offers a way to significantly decrease fine-tuning time and cost in compute-bound scenarios without sacrificing model performance or requiring complex modifications to existing training pipelines. |
| MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning (Read more on [arXiv](https://arxiv.org/abs/2506.22992) or [HuggingFace](https://huggingface.co/papers/2506.22992))| Maria Brbić, Yekun Chai, mdmoor, yljblues | This paper introduces MARBLE, a challenging benchmark to evaluate the step-by-step multimodal spatial reasoning and planning capabilities of Multimodal Language Models (MLLMs). The primary objective is to scrutinize the ability of current MLLMs to solve complex problems that require crafting and understanding multi-step plans under spatial, visual, and physical constraints, moving beyond simple information retrieval. The methodology involves two new tasks: M-PORTAL, inspired by the game Portal 2, which tests spatial planning, and M-CUBE, a 3D assembly puzzle, which tests spatial reasoning and perception. The primary result is that current MLLMs perform extremely poorly, with all 12 evaluated models achieving near-random performance on M-PORTAL and a 0% accuracy on the full M-CUBE task. The principal implication for AI practitioners is that state-of-the-art MLLMs are not yet capable of robust, deep spatial reasoning or planning, revealing significant limitations in both their perceptual abilities and their capacity for structured, sequential thought that must be considered before deployment in embodied or physically-grounded applications. |
| Teaching a Language Model to Speak the Language of Tools (Read more on [arXiv](https://arxiv.org/abs/2506.23394) or [HuggingFace](https://huggingface.co/papers/2506.23394))| s-emanuilov | This paper presents TUCAN, a series of Bulgarian language models adapted from the BgGPT family to provide robust function-calling capabilities in a non-English context. The primary objective is to develop and validate a methodology for enabling reliable tool use in language models for non-English languages, addressing the capability gap in multilingual tool integration. The methodology involved parameter-efficient fine-tuning (LoRA) of the BgGPT model series (2.6B, 9B, 27B) on a newly created bilingual dataset of 10,035 function-calling examples. The resulting TUCAN models demonstrated significant improvements in tool-use accuracy, with the TUCAN-2.6B model achieving a 28.75% absolute increase over its base model, while preserving core linguistic competence on established Bulgarian benchmarks. The principal implication for AI practitioners is the provision of a practical, open-source blueprint—including models, dataset, and an evaluation framework (Tucan-Eval)—for extending tool-augmented capabilities to other languages, enabling the development of production-ready AI agents beyond English-centric systems. |
| UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence
  with Spatial Reasoning and Understanding (Read more on [arXiv](https://arxiv.org/abs/2506.23219) or [HuggingFace](https://huggingface.co/papers/2506.23219))| Yong Li, Yanxin Xi, Tianhui Liu, Shengyuan Wang, JJ-TMT | The paper introduces UrbanLLaVA, a multi-modal large language model fine-tuned on a new urban instruction dataset to perform diverse intelligence tasks involving geospatial, trajectory, street view, and satellite data. The primary objective is to develop a unified MLLM capable of simultaneously processing four major types of urban data (visual, geo-text, structured geospatial, and spatiotemporal series) to achieve comprehensive spatial reasoning and understanding across a range of urban tasks. The methodology involves creating UData, a diverse urban instruction dataset; proposing UTrain, a three-stage training framework that decouples task alignment from knowledge learning; and developing UBench, an extended benchmark for evaluation. On the UBench benchmark for Beijing, UrbanLLaVA significantly outperforms its VILA1.5-8B base model, achieving a performance increase of 375.38% on the Geo+Traj task, and demonstrates gains ranging from 3.48% to 132.23% over the best-performing baselines across all tasks. For AI practitioners, this research provides a framework for adapting general MLLMs to specialized, multi-modal domains like urban intelligence by demonstrating that targeted data curation and a staged fine-tuning process can yield substantial performance improvements for complex reasoning tasks. |
| VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.22694) or [HuggingFace](https://huggingface.co/papers/2506.22694))| Yifan Zao, Junyoung Park, Mukul Gagrani, Sudhanshu Agrawal, Raghavv Goel | The paper introduces VOCABTRIM, a training-free method that prunes the vocabulary of the drafter model to accelerate speculative decoding in LLMs. The research objective is to mitigate the significant inference overhead caused by the drafter model's large language modeling (LM) head, particularly in memory-bound deployment scenarios. The methodology involves reconstructing the drafter's LM head to only contain a limited set of high-frequency tokens, which are identified by analyzing a calibration dataset of target model generations, thus reducing the drafter's parameter count and latency. When applied to a Llama-3.2-3B-Instruct model with an Eagle drafter, VOCABTRIM increased the memory-bound speed-up (MBSU) by up to 19% on the Dolly task (from 1.52 to 1.809) with a minimal drop in block efficiency. For AI practitioners, this provides a simple, plug-and-play technique to improve the throughput of LLMs on resource-constrained hardware by reducing the memory footprint and computational cost of the speculative decoding drafter, without requiring any model retraining. |
| RoboScape: Physics-informed Embodied World Model (Read more on [arXiv](https://arxiv.org/abs/2506.23135) or [HuggingFace](https://huggingface.co/papers/2506.23135))| Chen Gao, Lei Jin, Yinzhou Tang, Xin Zhang, Yu Shang | RoboScape is a physics-informed embodied world model that improves the physical plausibility of generated robotic videos by integrating depth and motion dynamics. The primary objective is to create a unified world model that generates realistic videos for contact-rich robotic scenarios by overcoming the physical unawareness of existing models. The methodology involves a dual-branch auto-regressive Transformer that jointly learns to predict RGB video and temporal depth maps, combined with an adaptive keypoint dynamics learning task that enforces temporal consistency on high-motion object regions to implicitly model physical properties. In policy evaluation experiments, RoboScape demonstrated a Pearson correlation of 0.953 between policy success rates predicted by the model and those in the ground-truth simulator, significantly outperforming baselines. The principal implication for AI practitioners is the ability to use RoboScape to generate large-scale, physically consistent synthetic data, which can be used to effectively train and evaluate robotic policies, reducing the need for real-world data and improving simulation-to-reality transfer. |
| Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography (Read more on [arXiv](https://arxiv.org/abs/2506.22753) or [HuggingFace](https://huggingface.co/papers/2506.22753))| Xiaokang Yang, Feiyu Ji, Jiayi Zhu, Jianing Zhang, XiaoyunYuan | The paper presents Degradation-Modeled Multipath Diffusion (DMDiff), a framework for restoring images from an ultra-compact, millimeter-scale metalens camera. The objective is to reconstruct high-fidelity images from inputs with complex, spatially varying optical and sensor-induced degradations, without relying on large paired datasets, and providing tunable control over the restoration process. The methodology leverages a pre-trained diffusion model fine-tuned with LoRA, guided by a novel Spatially Varying Degradation-Aware (SVDA) attention module that quantifies degradation using both simulated Point Spread Functions and a no-reference image quality metric. The system uses a multipath training strategy with positive, neutral, and negative prompts to balance detail enhancement, structural fidelity, and suppression of metalens-specific artifacts. The proposed method outperforms state-of-the-art baselines, achieving a MUSIQ score of 51.85, significantly higher than SwinIR (36.86) and OSEDiff (34.52), while enabling an instantly tunable trade-off between fidelity and perceptual quality. The principal implication for AI practitioners is that it provides a concrete methodology for adapting large generative models to specialized hardware with complex, non-ideal degradation characteristics by integrating physics-based simulations and data-driven quality metrics into the fine-tuning process, reducing the need for extensive, precisely aligned training data. |
| ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language
  Models for Audio Generation and Editing (Read more on [arXiv](https://arxiv.org/abs/2506.21448) or [HuggingFace](https://huggingface.co/papers/2506.21448))| Qian Chen, Wen Wang, Kaicheng Luo, Jialei Wang, Huadai Liu | ThinkSound is a framework that leverages Chain-of-Thought (CoT) reasoning within a Multimodal Large Language Model (MLLM) to enable a three-stage, interactive process for high-fidelity video-to-audio generation and editing. The primary objective is to overcome the limitations of end-to-end video-to-audio (V2A) models by decomposing the complex synthesis task into explicit, stepwise reasoning stages, thereby improving the contextual fidelity, temporal precision, and user controllability of the generated audio. The methodology uses an MLLM (fine-tuned VideoLLaMA2) on a newly introduced AudioCoT dataset to generate CoT instructions that guide a unified, flow-matching-based audio foundation model through initial foley generation, interactive object-centric refinement, and targeted editing from natural language commands. Experiments demonstrate that ThinkSound achieves state-of-the-art performance on the VGGSound test set, improving the CoT-based alignment score (CLAP_CoT) to 0.46 compared to the 0.40 of the strong baseline MMAudio, with performance degrading to 0.41 when CoT reasoning is removed. The principal implication for AI practitioners is that decomposing complex generative tasks using MLLM-driven CoT reasoning can significantly enhance output quality and control; this paradigm of separating high-level reasoning from low-level synthesis is applicable to other multimodal domains to create more precise and interactive systems beyond monolithic end-to-end architectures. |
| Tower+: Bridging Generality and Translation Specialization in
  Multilingual LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.17080) or [HuggingFace](https://huggingface.co/papers/2506.17080))| Pedro Teixeirinha, João Alves, José Pombal, Nuno M. Guerreiro, RicardoRei | This paper introduces TOWER+, a suite of multilingual LLMs that achieve strong performance in both machine translation and general-purpose capabilities, addressing the common trade-off between specialization and generality. The primary objective is to develop state-of-the-art translation models without compromising their core instruction-following and conversational skills. The authors employ a novel four-stage post-training pipeline consisting of continued pre-training (CPT), supervised fine-tuning (SFT), preference optimization (WPO/GRPO), and reinforcement learning with verifiable rewards (RLVR). The resulting 72B model substantially improves general-purpose performance over its predecessor (TOWER-V2) from a 4.01 to a 54.52 win rate on M-ArenaHard, while maintaining state-of-the-art translation quality (83.74 XCOMET-XXL on WMT24++). For AI practitioners, this work provides a blueprint for adapting base LLMs to specialized business domains, such as localization, without sacrificing the general capabilities essential for complex, real-world applications. |
