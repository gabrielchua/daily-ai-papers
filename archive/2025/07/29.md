

## Papers for 2025-07-29

| Title | Authors | Summary |
|-------|---------|---------|
| Agentic Reinforced Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2507.19849) or [HuggingFace](https://huggingface.co/papers/2507.19849))| Yifei Chen, Licheng Bao, Kai Ma, Hangyu Mao, Guanting Dong | This paper presents Agentic Reinforced Policy Optimization (ARPO), a novel reinforcement learning algorithm for training multi-turn, tool-using LLM agents. The research aims to address the high uncertainty (token entropy) that LLMs exhibit after interacting with external tools, which current trajectory-level RL methods handle inadequately. ARPO's methodology incorporates an entropy-based adaptive rollout mechanism that dynamically balances global trajectory sampling with partial, branched sampling at high-entropy steps, along with an advantage attribution estimation to learn from these stepwise interactions. Across 13 benchmarks, ARPO demonstrates superior performance, notably achieving better results while using only half the tool-use budget of existing methods. For AI practitioners, ARPO offers a scalable and cost-efficient solution to align LLM agents for complex, real-time tasks, improving performance and significantly reducing the computational expense of tool calls during training. |
| ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World
  Shorts (Read more on [arXiv](https://arxiv.org/abs/2507.20939) or [HuggingFace](https://huggingface.co/papers/2507.20939))| Junfu Pu, Teng Wang, Chen Li, Yixiao Ge, Yuying Ge | ARC-Hunyuan-Video-7B is a 7B-parameter multimodal model for the structured, end-to-end comprehension of real-world short videos. The paper's objective is to develop a model capable of deep, temporally-aware understanding of complex user-generated shorts by jointly processing visual, audio, and textual signals, a task for which current models are inadequate. The methodology involves augmenting a vision-language model with a dedicated audio encoder for fine-grained audio-visual synchronization and an explicit timestamp overlay on video frames for temporal awareness, trained via a multi-stage regimen that includes pre-training, reinforcement learning (GRPO) on verifiable tasks, and instruction fine-tuning. The model achieves a state-of-the-art accuracy of 74.3% on the authors' custom ShortVid-Bench, significantly outperforming baselines, and shows an inference time of just 10 seconds for a one-minute video on an NVIDIA H20 GPU. For AI practitioners, this work demonstrates that combining an audio-visual architecture with a multi-stage training strategy, especially using RL to ground the model in objective tasks, is a highly effective, production-ready approach for building systems that can perform nuanced analysis of short-form video content. |
| Rep-MTL: Unleashing the Power of Representation-level Task Saliency for
  Multi-Task Learning (Read more on [arXiv](https://arxiv.org/abs/2507.21049) or [HuggingFace](https://huggingface.co/papers/2507.21049))| Dan Xu, Lupin1998, ZedongWangAI | Rep-MTL is a regularization method that leverages representation-level task saliency to enhance multi-task learning by preserving task-specific patterns while promoting inter-task complementarity. The research objective is to develop a multi-task optimization strategy that operates directly on the shared representation space to explicitly facilitate positive knowledge transfer, as opposed to solely focusing on optimizer-centric conflict resolution. The methodology introduces two regularization components: Task-specific Saliency Regulation (TSR), which uses entropy-based penalization to maintain distinct task patterns, and Cross-task Saliency Alignment (CSA), which employs a contrastive paradigm to align sample-wise saliencies for information sharing. On the challenging NYUv2 benchmark, Rep-MTL achieved a task-level performance gain (ΔP_task) of +1.70 over the single-task baseline, outperforming prior state-of-the-art methods. For AI practitioners, Rep-MTL provides an efficient, optimizer-agnostic module that can be added to standard multi-task architectures to mitigate negative transfer and achieve performance gains without complex gradient manipulation or loss scaling strategies. |
| Reconstructing 4D Spatial Intelligence: A Survey (Read more on [arXiv](https://arxiv.org/abs/2507.21045) or [HuggingFace](https://huggingface.co/papers/2507.21045))| Chengfeng Zhao, Zhuowei Shen, Zhisheng Huang, Jiahao Lu, Yukang Cao | This survey organizes the field of 4D spatial intelligence by proposing a new five-level hierarchical framework to provide a structured overview of reconstructing dynamic 3D scenes from visual data. The paper's objective is to address the lack of a comprehensive, hierarchical analysis in prior works by categorizing existing methods into a progressive taxonomy. The core methodology is this novel five-level classification system: (1) low-level 3D cues, (2) 3D scene components, (3) 4D dynamic scenes, (4) interaction modeling, and (5) incorporation of physical laws. The primary result is the structured synthesis of the field, which highlights key advancements such as end-to-end frameworks like VGGT that can estimate fundamental 3D cues within seconds. For AI practitioners, this survey offers a systematic map to understand the state-of-the-art, pinpoint challenges at each level of abstraction, and guide the development of more physically grounded and interactive models for embodied AI and AR/VR applications. |
| SmallThinker: A Family of Efficient Large Language Models Natively
  Trained for Local Deployment (Read more on [arXiv](https://arxiv.org/abs/2507.20984) or [HuggingFace](https://huggingface.co/papers/2507.20984))| Dongliang Wei, Zhenliang Xue, qsstcl, Sorrymaker2024, yixinsong | The paper introduces SmallThinker, a family of large language models architected from the ground up for efficient local deployment on resource-constrained devices. The main research objective is to design an LLM natively for local hardware constraints (weak compute, limited memory, slow storage) instead of adapting cloud-based models. The key methodology is a deployment-aware co-design featuring a two-level sparse structure with Mixture-of-Experts (MoE), a pre-attention router to prefetch expert parameters and hide I/O latency, and a NoPE-RoPE hybrid sparse attention mechanism to reduce KV cache. The primary result is that the SmallThinker-21B-A3B model achieves a state-of-the-art MMLU score of 84.4 and, with Q4_0 quantization on a consumer PC with an 8GB memory limit, attains an inference speed of 20.30 tokens/s. The principal implication for AI practitioners is that co-designing model architecture and the inference engine for specific hardware enables high-performance LLM execution on local, non-GPU devices, demonstrating a viable alternative to simple model compression or cloud-only deployment. |
| A Survey of Self-Evolving Agents: On Path to Artificial Super
  Intelligence (Read more on [arXiv](https://arxiv.org/abs/2507.21046) or [HuggingFace](https://huggingface.co/papers/2507.21046))| Jiayi Geng, Huan-ang Gao, XiangJinYu, didiforhugface, Alphamasterliu | This survey provides a systematic framework for self-evolving agents by categorizing them along the dimensions of what, when, and how they evolve as a path toward Artificial Super Intelligence. The paper's objective is to establish the first comprehensive, systematic review of self-evolving agents by organizing the field around three foundational dimensions: what to evolve (e.g., models, context, tools, architecture), when to evolve (intra-test-time vs. inter-test-time), and how to evolve (e.g., reward-based, imitation, population-based methods). As a survey, its methodology is a taxonomic decomposition of existing research, analyzing and structuring prior work into a unified framework that also covers evaluation paradigms and applications. The primary result is a synthesis of findings demonstrating that self-evolution mechanisms significantly improve agent capabilities; for example, the paper cites that the WebVoyager agent improved its end-to-end success rate on unseen websites from 30% to 59% via successive self-fine-tuning. The principal implication for AI practitioners is that this survey provides a structured design framework (Figure 3) for developing adaptive agentic systems, enabling engineers to systematically analyze, compare, and select appropriate evolutionary components and learning strategies for specific applications, thereby creating more robust and versatile real-world agents. |
| Geometric-Mean Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2507.20673) or [HuggingFace](https://huggingface.co/papers/2507.20673))| Xun Wu, Jingye Chen, Yue Liu, Yuzhong Zhao, jeepliu | This paper introduces Geometric-Mean Policy Optimization (GMPO), a method that stabilizes Group Relative Policy Optimization (GRPO) by maximizing the geometric mean, rather than the arithmetic mean, of token-level rewards. The primary objective is to mitigate the unstable policy updates in GRPO caused by extreme importance sampling ratios. The core methodology replaces the arithmetic mean in the GRPO objective with a geometric mean and applies token-level clipping, which is inherently less sensitive to outlier rewards and allows for a wider clipping range to enhance exploration. GMPO-7B outperforms GRPO by an average of 4.1% on mathematical benchmarks and by 1.4% on the Geometry3K multimodal benchmark, while maintaining a more stable importance sampling ratio, lower KL divergence, and higher token entropy during training. For AI practitioners, GMPO provides a more stable and effective algorithm for reinforcement learning post-training of LLMs on reasoning tasks, improving final performance by reducing update instability. |
| Region-based Cluster Discrimination for Visual Representation Learning (Read more on [arXiv](https://arxiv.org/abs/2507.20025) or [HuggingFace](https://huggingface.co/papers/2507.20025))| Yongle Zhao, Yin Xie, Athinklo, xiangan, Kaichengalex | The paper introduces RICE, a region-aware visual representation learning method that uses cluster discrimination to improve performance on dense prediction tasks like segmentation and OCR. The main objective is to overcome the limitations of global representations in vision-language models by developing a framework that learns effective region-level visual features without relying on per-region textual annotations. The methodology involves creating a billion-scale region dataset with pseudo-labels derived from k-means clustering (for objects) and text tokenization (for OCR), and then training a model that incorporates a novel Region Transformer layer using a unified region-cluster discrimination loss. Extensive experiments show RICE outperforms prior methods, with its ViT-B/16 model achieving a 38.9% detection AP on COCO, surpassing a strong SigLIP baseline by +3.9%. For AI practitioners, the pre-trained RICE models provide a superior vision encoder backbone for MLLMs and other downstream applications requiring robust, localized object and text recognition. |
| GPT-IMAGE-EDIT-1.5M: A Million-Scale, GPT-Generated Image Dataset (Read more on [arXiv](https://arxiv.org/abs/2507.21033) or [HuggingFace](https://huggingface.co/papers/2507.21033))| Qing Liu, Letian Zhang, Siwei Yang, Yuhan Wang, tennant | The paper introduces GPT-IMAGE-EDIT-1.5M, a large-scale, publicly available dataset of over 1.5 million image editing triplets, created by systematically refining existing datasets using GPT-4o. The research objective is to bridge the performance gap between proprietary and open-source instruction-guided image editing models by creating and releasing a high-quality, large-scale training dataset. The key methodology involves unifying and refining three popular datasets (OmniEdit, HQ-Edit, UltraEdit) by leveraging GPT-4o to 1) regenerate output images for enhanced visual quality and instruction alignment, and 2) selectively rewrite prompts for improved semantic clarity. The primary result is that an open-source model fine-tuned on the new dataset achieves state-of-the-art performance among open-source methods, scoring 7.24 on the GEdit-EN-full benchmark, markedly exceeding previously published models. The principal implication for AI practitioners is the provision of a direct, high-quality data resource for training superior open-source image editing models, along with a validated methodology for using frontier models to systematically enhance the quality and alignment of existing datasets. |
| UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing
  Large Language Models' Reasoning Abilities (Read more on [arXiv](https://arxiv.org/abs/2507.19766) or [HuggingFace](https://huggingface.co/papers/2507.19766))| Yang Li, Shaohua Chen, Tao Yang, forestliutc, dongdongdongdu | This paper introduces UloRL, a reinforcement learning approach that improves Large Language Model reasoning by efficiently training on ultra-long output sequences.  The primary objective is to overcome the inefficiencies of traditional reinforcement learning, specifically long-tail distribution delays and entropy collapse, when training LLMs with outputs up to 128k tokens.  The key methodology involves two main techniques: 1) Segment Rollout, which divides the decoding of ultra-long outputs into shorter segments to accelerate training, and 2) Dynamic Masking of well-Mastered Positive Tokens (DMMPTs), which prevents entropy collapse by adaptively excluding high-confidence positive tokens from training updates when model entropy falls below a target threshold.  The proposed UloRL approach, when applied to the Qwen3-30B-A3B model with 128k-token outputs, improved performance on the AIME2025 benchmark from 70.9% to 85.1% and on the BeyondAIME benchmark from 50.7% to 61.9%.  For AI practitioners, the principal implication is that employing segment rollouts and dynamic token masking provides a scalable and efficient method to conduct reinforcement learning on ultra-long sequences, overcoming critical training bottlenecks to significantly enhance the complex reasoning capabilities of LLMs. |
| ForCenNet: Foreground-Centric Network for Document Image Rectification (Read more on [arXiv](https://arxiv.org/abs/2507.19804) or [HuggingFace](https://huggingface.co/papers/2507.19804))| Jia Li, Dong Guo, Qiang Li, Peng Cai, Kaichengalex | ForCenNet is a deep learning framework for document image rectification that leverages foreground-centric information generated from undistorted images to guide the unwarping process. The primary objective is to enhance rectification accuracy and preserve document readability by focusing the model's attention on critical foreground elements like text and table lines, which existing methods often treat uniformly with the background. Its methodology combines a novel label generation process to create foreground masks and line elements from clean images, a mask-guided Transformer decoder that directs attention to these foreground regions, and a curvature consistency loss to maintain the geometric structure of linear elements. The network achieves state-of-the-art performance, attaining an MS-SSIM of 0.713 on the DIR300 benchmark, surpassing prior models. For AI practitioners, the principal implication is that explicitly modeling and preserving the geometric properties of semantically meaningful foreground content, even with synthetically generated labels, is a highly effective strategy for improving performance on document image restoration and subsequent OCR tasks. |
| Met^2Net: A Decoupled Two-Stage Spatio-Temporal Forecasting Model for
  Complex Meteorological Systems (Read more on [arXiv](https://arxiv.org/abs/2507.17189) or [HuggingFace](https://huggingface.co/papers/2507.17189))| Xiaolin Qin, Min Chen, Hao Yang, Shaohan Li | Met²Net is a decoupled, two-stage spatio-temporal forecasting model that improves multivariate meteorological prediction by addressing representation and task inconsistencies. The primary objective is to develop a forecasting framework that effectively integrates highly divergent meteorological variables by resolving the performance degradation caused by representation inconsistency and the sub-optimal training resulting from task inconformity between reconstruction and prediction stages. The methodology involves an implicit two-stage training paradigm where in stage one, variable-specific encoders and decoders are trained for reconstruction while a translator is frozen, and in stage two, the encoders/decoders are frozen while the translator, using a self-attention mechanism, is trained on a latent space prediction task, with momentum updates applied to frozen components to align objectives. The proposed model achieves state-of-the-art performance, reducing the Mean Squared Error (MSE) for near-surface air temperature and relative humidity predictions by 28.82% and 23.39%, respectively, compared to the TAU baseline. For AI practitioners, this research provides a powerful framework for multivariate time-series forecasting, demonstrating that treating input variables as independent modalities with dedicated encoders combined with an implicit two-stage training strategy effectively fuses heterogeneous data and improves prediction accuracy in complex systems. |
| ScenePainter: Semantically Consistent Perpetual 3D Scene Generation with
  Concept Relation Alignment (Read more on [arXiv](https://arxiv.org/abs/2507.19058) or [HuggingFace](https://huggingface.co/papers/2507.19058))| Khodchaphun Hirunyaratsameewong, Chang Liu, Fangfu Liu, Shengjun Zhang, xiac24 | ScenePainter is a framework for generating long-range, semantically consistent 3D view sequences from a single image by aligning concept relations. The primary objective is to address the semantic drift problem in perpetual 3D scene generation, where iteratively generated views deviate from the original scene's semantic context due to accumulated outpainting errors. The key methodology uses a hierarchical graph structure, SceneConceptGraph, to model relations among multi-level scene concepts, which then directs a customized outpainting diffusion model to generate consistent novel views. The framework significantly improves scene fidelity, achieving a state-of-the-art DINO score of 0.931 and was preferred for consistency over the WonderJourney baseline in 92.6% of user study comparisons. For AI practitioners, the main implication is a novel technique for maintaining long-term semantic control in iterative generative models, which can mitigate cumulative error in applications like long-form video synthesis or 3D world building. |
| Music Arena: Live Evaluation for Text-to-Music (Read more on [arXiv](https://arxiv.org/abs/2507.20900) or [HuggingFace](https://huggingface.co/papers/2507.20900))| Wei-Lin Chiang, Anastasios N. Angelopoulos, Wayne Chi, Yonghyun Kim, chrisdonahue | The paper presents Music Arena, an open platform for scalable, live human preference evaluation of text-to-music (TTM) models. The primary objective is to establish a rigorous, renewable evaluation protocol and an open dataset to address the lack of standardized, human-centric evaluation for TTM systems. The methodology involves users engaging in pairwise "battles" between two TTM models, with a backend powered by an LLM (GPT-4o) that moderates prompts and routes them to heterogeneous model endpoints, while collecting detailed preference and fine-grained listening data. While the paper states that aggregate user preference results are not yet available, it provides a specific system-level quantitative finding from a battle log where a Riffusion FUZZ 1.0 model generated audio at 8.0x real-time speed. The principal implication for AI practitioners is the provision of a unified, open-source, Docker-based framework and a recurring, transparently-released dataset of human preferences, enabling more rigorous model evaluation and the development of TTM systems better aligned with human intent. |
| JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability
  and Aesthetic Alignment (Read more on [arXiv](https://arxiv.org/abs/2507.20880) or [HuggingFace](https://huggingface.co/papers/2507.20880))| Amir Ali Bagherzadeh, Taylor Gautreaux, Navonil Majumder, Renhang Liu, hungchiayu | The paper presents JAM, a 530M-parameter flow-matching model for lyrics-to-song generation that offers fine-grained word-level timing control and aesthetic alignment. The primary objective is to create a compact and efficient song generation model that overcomes the limitations of prior work by enabling precise control over word timing, overall song duration, and improving lyrical fidelity. The methodology utilizes a rectified-flow model with a Diffusion Transformer (DiT) backbone, conditioned on word-level temporal annotations, and employs iterative Direct Preference Optimization (DPO) with synthetic preference labels from the SongEval toolkit to enhance aesthetic quality without manual annotation. On the custom JAME benchmark, JAM achieves a Word Error Rate (WER) of 0.151, which is less than half that of the next-best system, demonstrating significantly improved lyrical alignment and vocal clarity. For AI practitioners, this research provides a framework for building highly controllable audio generation systems by showing that explicit, fine-grained temporal conditioning is a critical mechanism for improving both user control and objective metrics like WER, making AI tools more viable for professional creative workflows. |
| Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty (Read more on [arXiv](https://arxiv.org/abs/2507.16806) or [HuggingFace](https://huggingface.co/papers/2507.16806))| Leshem Choshen, Idan Shenfeld, Stewart Slocum, Isha Puri, Mehul Damani | This paper introduces RLCR (Reinforcement Learning with Calibration Rewards), a method that trains language models to improve both accuracy and calibrated confidence estimation. The research objective is to determine if models can be optimized for both correctness and calibration by having the model's own reasoning chain inform its confidence. The key methodology is to train a model via reinforcement learning using a composite reward function that augments a standard binary correctness score with a Brier score, a proper scoring rule that penalizes poorly calibrated confidence estimates. On the HotpotQA dataset, RLCR reduced the expected calibration error to 0.03 from 0.37 in standard RL training, while maintaining competitive accuracy and improving out-of-domain performance. The principal implication for AI practitioners is that explicitly training for calibration using this method can produce more reliable reasoning models that better communicate their own uncertainty, a critical feature for trustworthy AI systems. |
| GenoMAS: A Multi-Agent Framework for Scientific Discovery via
  Code-Driven Gene Expression Analysis (Read more on [arXiv](https://arxiv.org/abs/2507.21035) or [HuggingFace](https://huggingface.co/papers/2507.21035))| Haohan Wang, Yijiang Li, Liu-Hy | GenoMAS is a code-driven, multi-agent framework using six specialized, heterogeneously-backed LLM agents to automate complex gene expression analysis workflows. The main objective is to develop an automated system that bridges the gap between general-purpose agentic reasoning and the precise, code-driven, domain-specific requirements of scientific computation, specifically for end-to-end gene expression analysis from raw data. The methodology involves orchestrating six specialized agents (PI, Data Engineer, Statistician, Code Reviewer, Domain Expert) with distinct LLM backbones (Claude Sonnet 4, OpenAI o3, Gemini 2.5 Pro). The system uses a guided planning framework where tasks are decomposed into editable "Action Units," an iterative code generation-review-revision loop, and a dynamic code memory for reusing validated snippets, all managed via a typed message-passing protocol. On the GenoTEX benchmark, GenoMAS achieves a 60.48% F1 score in gene identification, a 16.85% absolute improvement over the previous state-of-the-art, GenoAgent. The principal implication for AI practitioners is that for complex, domain-specific tasks requiring scientific rigor, an architecture treating agents as collaborative programmers with specialized roles, heterogeneous LLM backbones, and structured mechanisms for code generation and review is more effective than general-purpose autonomous agents or rigid, tool-based workflow orchestrators. |
