

## Papers for 2025-08-25

| Title | Authors | Summary |
|-------|---------|---------|
| AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs (Read more on [arXiv](https://arxiv.org/abs/2508.16153) or [HuggingFace](https://huggingface.co/papers/2508.16153))| Xue Yan, Siyuan Guo, Yihang Chen, linyiyang2023, Zhouhc | AgentFly introduces a memory-based learning paradigm for LLM agents that enables continuous adaptation without fine-tuning the base LLM parameters. The primary objective is to develop LLM agents that can continually learn from changing environments without the high computational cost of fine-tuning the underlying models. This is achieved by formalizing the agent's decision-making as a Memory-augmented Markov Decision Process (M-MDP), where a neural case-selection policy guides action decisions by retrieving past experiences from an episodic memory (either differentiable or non-parametric) and using soft Q-learning for policy optimization. AgentFly achieves top-1 performance on GAIA validation with 87.88% Pass@3 and on the test set with 79.40%, and significantly outperforms state-of-the-art training-based methods on the DeepResearcher dataset, reaching 66.6% F1 and 80.4% PM. This approach offers a scalable and efficient pathway for developing generalist LLM agents capable of continuous, real-time learning without gradient updates, making it highly relevant for open-ended skill acquisition and deep research applications. |
| ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for
  Long-Horizon Tasks (Read more on [arXiv](https://arxiv.org/abs/2508.08240) or [HuggingFace](https://huggingface.co/papers/2508.08240))| Zeju Li, Jianuo Jiang, Mingyu Liu, Liqin Lu, Ka12un | ODYSSEY presents a unified framework for open-world mobile manipulation, aiming to seamlessly integrate high-level task planning with low-level whole-body control for agile quadruped robots in unstructured environments. The methodology combines a hierarchical vision-language planner for task decomposition and action execution with a reinforcement learning-based whole-body control policy, trained via a two-stage curriculum, terrain-invariant end-effector sampling, and extensive domain randomization. Quantitatively, ODYSSEY achieved a 51.32% success rate on REORIENTOBJECT (Seen) for ARNOLD short-horizon tasks, significantly outperforming PerAct's 19.48%, and demonstrated robust generalization on novel splits and superior base velocity tracking (0.08 ex error vs 0.32 for baseline in static conditions). This work's principal implication for AI practitioners is the demonstrated feasibility and practicality of deploying generalized legged mobile manipulators in real-world unstructured environments, advancing the development of capable robotic assistants. |
| Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains
  RLVR (Read more on [arXiv](https://arxiv.org/abs/2508.14029) or [HuggingFace](https://huggingface.co/papers/2508.14029))| Ying Nian Wu, Yelong Shen, Yeyun Gong, Zhongzhi Li, MasterVito | This paper proposes Self-play with Variational problem Synthesis (SvS) to address policy entropy collapse and limited Pass@k performance in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Models. The primary objective is to develop an online, self-improving problem augmentation method that sustains data diversity and ensures accurate reference answers. SvS leverages the policy's correct solutions to under-performing problems to synthesize new variational problems, which retain the original golden answers and are then solved by the same policy. This strategy achieved absolute gains of 18.3% and 22.8% in Pass@32 performance on AIME24 and AIME25 benchmarks, respectively, compared to standard RLVR, while maintaining stable policy entropy. Consequently, AI practitioners can utilize SvS to robustly enhance LLM reasoning capabilities and sustain exploration in RLVR without relying on external data or complex labeling. |
| CRISP: Persistent Concept Unlearning via Sparse Autoencoders (Read more on [arXiv](https://arxiv.org/abs/2508.13650) or [HuggingFace](https://huggingface.co/papers/2508.13650))| Yonatan Belinkov, Martin Tutek, Aaron Mueller, Dana Arad, Tomertech | CRISP introduces a parameter-efficient method for persistent concept unlearning in Large Language Models (LLMs) using Sparse Autoencoders (SAEs). Its primary objective is to selectively remove unwanted, safety-critical knowledge while preserving general model utility and generating coherent text. The methodology involves automatically identifying salient SAE features via contrastive activation analysis between target and benign corpora, then suppressing their activations on the target corpus through LoRA-based fine-tuning using a multi-component loss. CRISP significantly outperforms prior approaches on safety-critical unlearning tasks, achieving an overall score of 60.10 for Llama-3.1-8B on WMDP-Bio, demonstrating improved trade-offs between unlearning efficacy and benign knowledge retention. This enables AI practitioners to achieve precise and robust knowledge removal with minimal impact on benign capabilities and fluency, leveraging the interpretability of SAE features for targeted interventions. |
| Selective Contrastive Learning for Weakly Supervised Affordance
  Grounding (Read more on [arXiv](https://arxiv.org/abs/2508.07877) or [HuggingFace](https://huggingface.co/papers/2508.07877))| Jae-Pil Heo, hynnsk, WJ0830 | This paper introduces a selective contrastive learning framework for weakly supervised affordance grounding that adaptively learns from both object and part-level cues to precisely localize action-relevant regions. The primary objective is to overcome the model's tendency to focus on class-specific but affordance-irrelevant patterns by forcing it to distinguish relevant cues from the background. The key methodology leverages CLIP-generated object affinity maps to discover object and part clues from egocentric and exocentric views, then applies selective prototypical and pixel-level contrastive objectives that adapt the learning granularity based on cue reliability. The model achieves state-of-the-art performance, attaining a Kullback-Leibler Divergence of 1.124 and a Similarity score of 0.433 on the AGD20K-Seen dataset. For AI practitioners, this provides a robust strategy for weakly supervised localization, demonstrating that adaptively applying contrastive objectives at multiple granularities significantly improves model focus and generalization when fine-grained supervision is unreliable. |
| AetherCode: Evaluating LLMs' Ability to Win In Premier Programming
  Competitions (Read more on [arXiv](https://arxiv.org/abs/2508.16402) or [HuggingFace](https://huggingface.co/papers/2508.16402))| Yidi Du, Markus Mak, Zhicheng Liu, Jiaze Chen, zhwang01 | AetherCode introduces a new benchmark for robustly evaluating large language models' (LLMs) code reasoning abilities on challenging problems from premier programming competitions. The paper addresses limitations in existing code reasoning benchmarks, specifically insufficient problem difficulty/scope and low-quality test cases, to provide a more accurate assessment of LLM capabilities. The benchmark curates problems from top-tier Olympiad in Informatics (OI) and International Collegiate Programming Contest (ICPC) series, and constructs high-quality test cases using a hybrid approach of automated generation and expert annotation, validated to achieve 100% True Positive Rate and 100% True Negative Rate against human solutions. Evaluation revealed a significant performance gap between reasoning and non-reasoning LLMs, with top-performing models like 04-mini-high and Gemini-2.5-Pro achieving Pass@1 accuracies of 35.5% and 32.7%, respectively, across all problem difficulties. AI practitioners should note that current LLMs still have substantial room for improvement in complex code reasoning and algorithmic knowledge, particularly in areas like computational geometry and dynamic programming, underscoring the need for advanced research using more rigorous benchmarks like AetherCode. |
| EgoTwin: Dreaming Body and View in First Person (Read more on [arXiv](https://arxiv.org/abs/2508.13013) or [HuggingFace](https://huggingface.co/papers/2508.13013))| Wentao Wang, Mengze Li, Yicong Li, Fangzhou Hong, Jingqiao Xiu | EgoTwin is a diffusion-based framework that jointly generates egocentric video and human motion in a viewpoint consistent and causally coherent manner. Its primary objective is to advance egocentric video generation by synchronously synthesizing first-person video and corresponding human motion, addressing Viewpoint Alignment and Causal Interplay challenges. EgoTwin employs a triple-branch diffusion transformer with asynchronous diffusion, featuring a head-centric motion representation and a cybernetics-inspired interaction mechanism that models causal interplay via a structured joint attention mask. Quantitatively, EgoTwin demonstrates strong performance, achieving an I-FID of 98.17 and a HandScore of 0.81, significantly outperforming the VidMLD baseline across video quality, motion quality, and video-motion consistency metrics. This framework provides a robust foundation for AI practitioners to develop advanced embodied AI systems and applications requiring realistic, causally coherent first-person visual and motion synthesis. |
| Do What? Teaching Vision-Language-Action Models to Reject the Impossible (Read more on [arXiv](https://arxiv.org/abs/2508.16292) or [HuggingFace](https://huggingface.co/papers/2508.16292))| Roei Herzig, Trevor Darrell, Dantong Niu, Elvis Hsieh, Wen-Han Hsieh | The paper presents Instruct-Verify-and-Act (IVA), a framework that enables Vision-Language-Action (VLA) models to detect, clarify, and correct false-premise instructions in robotic environments. The core objective is to investigate how VLAs can effectively recognize, interpret, and respond to natural language commands referencing objects or conditions absent from the environment. IVA is a unified framework that detects false premises, engages in language-based clarification, and grounds plausible alternatives in perception and action, achieved by training a VLA model, based on LLARVA, end-to-end using a large-scale instruction tuning setup with contextually augmented, semi-synthetic datasets. IVA improved false premise detection accuracy by 97.56% over baselines and increased successful responses in false-premise scenarios by 50.78%, achieving 100% detection accuracy on In-Domain false-premise instructions. AI practitioners developing VLA models can leverage explicit false-premise reasoning to enhance robot robustness and safety, allowing systems to reason about user intent, clarify ambiguities, and interact naturally even when presented with unfulfillable commands. |
| End-to-End Agentic RAG System Training for Traceable Diagnostic
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2508.15746) or [HuggingFace](https://huggingface.co/papers/2508.15746))| Pengcheng Qiu, Chaoyi Wu, Yuze Sun, Qiaoyu Zheng, Angelakeke | The paper introduces Deep-DxSearch, an end-to-end trained agentic RAG system utilizing reinforcement learning for traceable medical diagnosis. The primary objective is to overcome knowledge limitations, hallucinations, suboptimal external knowledge utilization, and decoupled feedback-reasoning traceability in LLM-based diagnostic systems for medical diagnosis. Deep-DxSearch frames an LLM as the core agent and a large-scale medical retrieval corpus as its environment, employing an end-to-end reinforcement learning framework with tailored rewards on format, retrieval, reasoning structure, and diagnostic accuracy to train its agentic RAG policy. Evaluations show Deep-DxSearch significantly improves top-1 accuracy over medical foundation models by up to 19%/17% (in-distribution/out-of-distribution) for common diseases and 24%/17% for rare diseases. This demonstrates that end-to-end RL training, co-optimizing retrieval and reasoning with tailored rewards, is critical for developing robust, generalizable, and accurate agentic RAG systems for complex and high-stakes AI applications like medical diagnosis. |
| TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated
  Prefill \& Decode Inference (Read more on [arXiv](https://arxiv.org/abs/2508.15881) or [HuggingFace](https://huggingface.co/papers/2508.15881))| Di Yin, Yuxuan Wang, Pingzhi Tang, Fanxu Meng, xiaojuan0920 | TPLA proposes a tensor-parallel latent attention mechanism that partitions KV cache across devices, preserving MLA's benefits for efficient LLM inference under tensor parallelism. The primary objective is to address the memory inefficiency of Multi-Head Latent Attention (MLA) in tensor-parallel settings, where each device must load the full compressed Key-Value (KV) cache, eroding MLA's memory-saving advantage over Grouped Query Attention (GQA). TPLA partitions both the latent representation and each attention head's input dimension across devices, performing shard-independent attention and combining results via all-reduce, utilizing orthogonal transformations (Hadamard or PCA) for reparameterizing RMSNorm and softmax, and a prefill/decode separation strategy. TPLA reduces per-device KV cache size and achieves significant decoding speedups, specifically 1.79x for DeepSeek-V3 and 1.93x for Kimi-K2 at a 32K-token context length compared to MLA, while maintaining performance on commonsense and LongBench tasks. AI practitioners can leverage TPLA to deploy MLA-based LLMs more efficiently in tensor-parallel environments, reducing memory footprint and improving decoding throughput without requiring model retraining, thereby enabling longer context lengths and faster inference on existing hardware. |
| AgentScope 1.0: A Developer-Centric Framework for Building Agentic
  Applications (Read more on [arXiv](https://arxiv.org/abs/2508.16279) or [HuggingFace](https://huggingface.co/papers/2508.16279))| Liuyi Yao, Weirui Kuang, Yuexiang Xie, Zitao Li, Dawei Gao | AgentScope 1.0 is a developer-centric framework for building scalable and efficient LLM-based agentic applications. Its objective is to comprehensively support flexible and efficient tool-based agent-environment interactions. The framework grounds agent behaviors in the ReAct paradigm, leveraging abstract foundational components and advanced asynchronous agent-level infrastructure. The paper describes AgentScope's architecture and capabilities, but does not present specific quantitative performance results or empirical evaluations of the framework itself. This implies AI practitioners can utilize its modular design, unified interfaces, and robust engineering support to accelerate the development and deployment of adaptive agentic solutions. |
| InMind: Evaluating LLMs in Capturing and Applying Individual Human
  Reasoning Styles (Read more on [arXiv](https://arxiv.org/abs/2508.16072) or [HuggingFace](https://huggingface.co/papers/2508.16072))| Diping Song, Qi Chen, Yibin Wang, Chuanhao Li, Zizhen Li | InMind is a cognitively grounded framework for evaluating LLMs' ability to capture and apply individualized human reasoning styles in social deduction games. The paper's objective is to assess whether LLMs can internalize and adapt to personalized reasoning styles in dynamic, interactive social contexts. The methodology involves dual-layer cognitive annotations (strategy traces and reflective summaries) on human gameplay data in Avalon, used to define four evaluation tasks: Player Identification, Reflection Alignment, Trace Attribution, and Role Inference. Evaluation on 11 state-of-the-art LLMs revealed that most models, including GPT-4o, struggled with deeper strategic intent and temporal alignment; for example, in Player Identification, most models achieved Top-1 accuracy well below 0.20, though DeepSeek-R1 scored the highest at 0.240. This highlights a critical need for AI practitioners to develop LLMs that can integrate temporally structured and context-aware reasoning to achieve more personalized and socially aware AI systems. |
| CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated
  Chain-of-Thought-based Reinforced Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2508.15868) or [HuggingFace](https://huggingface.co/papers/2508.15868))| Yulun Zhang, Haipang Wu, Rongjuncheng Zhang, Ji Liu, Wenqiao Zhu | CARFT proposes a novel contrastive learning-based reinforced fine-tuning framework to enhance LLM reasoning by explicitly leveraging annotated Chain-of-Thought (CoT) data. The research addresses limitations in existing RL-based and SFT fine-tuning methods, which often ignore valuable annotated Chain-of-Thought (CoT) and suffer from unstable training, aiming to enhance LLM reasoning performance and stability. CARFT learns a unified representation for both annotated and on-policy sampled CoTs, then designs contrastive signals using a masked InfoNCE loss to guide fine-tuning, further incorporating an embedding-enhanced partial reward mechanism. Extensive experiments demonstrate CARFT's significant advantages, outperforming baseline SFT and ReFT methods in accuracy by up to 10.15% on average, and improving efficiency by up to 30.62%. This method provides AI practitioners with a more robust and effective fine-tuning strategy for LLMs performing complex reasoning tasks by systematically integrating high-quality annotated CoT data and stabilizing the reinforcement learning process. |
| Learnable SMPLify: A Neural Solution for Optimization-Free Human Pose
  Inverse Kinematics (Read more on [arXiv](https://arxiv.org/abs/2508.13562) or [HuggingFace](https://huggingface.co/papers/2508.13562))| Xiao Sun, Zhihang Zhong, Wei Wang, Linfeng Dong, Charlie019 | Learnable SMPLify replaces the iterative optimization in SMPLify with a single-pass neural regression model for fast and accurate human pose inverse kinematics. The main objective is to develop a neural framework that solves the SMPL inverse kinematics (IK) problem directly from joint data, eliminating the high computational cost of iterative optimization without sacrificing accuracy. The key methodology involves a neural solver that learns to regress residual pose parameters from an initial pose. This is enabled by a temporal sampling strategy that creates initialization-target training pairs from adjacent video frames and a human-centric normalization scheme on joint coordinates to improve generalization. The primary result is a nearly 200Ã— faster runtime compared to SMPLify, while also improving accuracy; for example, on the AMASS dataset (s=1), the proposed method achieves a 3.23 mm Per-Vertex Error (PVE) compared to 18.85 mm for SMPLify. The principal implication for AI practitioners is the availability of a fast, simple, and model-agnostic baseline that can directly replace the computationally expensive SMPLify process or serve as a plug-in post-processing tool to refine results from other image-based pose estimators in real-time applications. |
| Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts (Read more on [arXiv](https://arxiv.org/abs/2508.10390) or [HuggingFace](https://huggingface.co/papers/2508.10390))| Liming Fang, Jiafei Wu, Xiaogang Xu, Lu Zhou, AlienZhang1996 | This paper proposes a Malicious Detection-Human Hybrid (MDH) framework for red-teaming dataset cleaning and novel developer-message-based jailbreak attacks, D-Attack and DH-CoT, targeting black-box LLMs. The primary objective is to enhance the quality of red-teaming datasets by filtering non-explicitly harmful prompts and to develop more effective jailbreak techniques, particularly for advanced reasoning LLMs. The methodology includes MDH's three-stage process for prompt detection, achieving over 95% NHP detection with low manual effort, and two attack methods: D-Attack, which leverages crafted developer messages, and DH-CoT, which integrates these messages with deceptive chains of thought. MDH effectively cleans datasets, creating the RTA series, and for jailbreaking, DH-CoT achieved a significant Attack Success Rate of 0.96 on o3 and 0.66 on o4-Mini against the RTA-MaliciousEducator dataset, vastly outperforming prior SOTA methods on reasoning models. For AI practitioners, these findings underscore the necessity of robust, efficient content moderation tools like MDH and reveal critical vulnerabilities in commercial LLMs, highlighting the need for advanced safety mechanisms beyond simple prompt filtering to counteract sophisticated developer-message-based attacks. |
