

## Papers for 2025-08-26

| Title | Authors | Summary |
|-------|---------|---------|
| InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,
  Reasoning, and Efficiency (Read more on [arXiv](https://arxiv.org/abs/2508.18265) or [HuggingFace](https://huggingface.co/papers/2508.18265))| jinglinglin, WesKwong, MIASANMIA, gulixin0922, Weiyun1025 | InternVL3.5 introduces a new family of open-source multimodal models with enhanced versatility, reasoning, and efficiency via novel training and deployment strategies. The primary objective is to significantly advance open-source multimodal models by improving their capabilities and inference efficiency, aiming to close the performance gap with leading commercial models. The key methodologies include a novel Cascade Reinforcement Learning (RL) framework, which combines offline and online RL for robust reasoning, and efficiency optimizations such as a Visual Resolution Router (ViR) for dynamic visual token resolution and Decoupled Vision-Language Deployment (DvD) for parallel vision-language processing across GPUs. As a primary result, the largest model, InternVL3.5-241B-A28B, achieves up to a +16.0% gain in overall reasoning performance and a 4.05x inference speedup compared to its predecessor, InternVL3, while narrowing the performance gap with GPT-5 to 3.9%. AI practitioners can leverage InternVL3.5's publicly released models and code to develop more efficient and versatile multimodal AI applications, directly benefiting from its advanced reasoning capabilities, enhanced inference speed, and support for novel functionalities like GUI interaction and embodied agency. |
| Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance
  for Text-to-Image Generation (Read more on [arXiv](https://arxiv.org/abs/2508.18032) or [HuggingFace](https://huggingface.co/papers/2508.18032))| Haoxiang Shi, Bu Pi, Mingyang Han, Peng Chen, Yaqi Li | Visual-CoG is a novel reinforcement learning framework that enhances text-to-image generation through stage-aware rewards and a chain of guidance. It aims to address the limitations of existing autoregressive models in handling multi-attribute and ambiguous prompts by providing immediate feedback throughout the image generation pipeline. The methodology decomposes image generation into three distinct stages—semantic reasoning, process refining, and outcome evaluation—with specific reward signals (Rr, Rp, Ro) guiding each stage. Visual-CoG demonstrates superior performance, achieving a 15.57% average enhancement on GenEval and significant improvements on T2I-CompBench and VisCog-Bench, particularly for complex and reasoning-demanding prompts. This approach offers AI practitioners a more effective policy learning mechanism and improved semantic alignment, leading to higher-fidelity and more semantically consistent image outputs for challenging generative tasks. |
| MV-RAG: Retrieval Augmented Multiview Diffusion (Read more on [arXiv](https://arxiv.org/abs/2508.16577) or [HuggingFace](https://huggingface.co/papers/2508.16577))| sagiebenaim, omerbenishu, yosepyossi | MV-RAG introduces a retrieval-augmented multiview diffusion model for text-to-3D generation. The primary objective is to enhance the generation of geometrically consistent and accurate multiview outputs for out-of-domain (OOD) or rare concepts. The key methodology involves a hybrid training strategy that combines structured multiview 3D data with diverse 2D image collections, conditioning a multiview diffusion model on retrieved 2D images, and employing an adaptive fusion mechanism. Quantitatively, MV-RAG achieved a CLIP score of 71.77 (4-views) on OOD/rare concepts, outperforming MVDream (66.47), and a user study MOS of 4.44 for 3D consistency, significantly higher than MVDream's 3.24. This approach implies that AI practitioners can effectively generate high-fidelity 3D assets for novel and rare concepts by leveraging retrieval from vast 2D image databases, reducing the dependency on extensive 3D datasets for such specialized content. |
| T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image
  Generation (Read more on [arXiv](https://arxiv.org/abs/2508.17472) or [HuggingFace](https://huggingface.co/papers/2508.17472))| Xihui Liu, Xian Liu, Chengqi Duan, Rongyao Fang, Kaiyue | T2I-ReasonBench is a novel benchmark for evaluating reasoning-informed text-to-image (T2I) generation models. The paper's objective is to assess T2I models' ability to reason over prompts, infer implicit meaning, and resolve contextual nuances, moving beyond literal prompt following. The key methodology involves a benchmark of 800 prompts across four dimensions (Idiom Interpretation, Textual Image Design, Entity-Reasoning, Scientific-Reasoning) and a two-stage evaluation using an LLM to generate prompt-specific question-criterion pairs, followed by an MLLM to score generated images. Primary results indicate that open-source models have critical limitations in reasoning, while proprietary models, particularly GPT-Image-1, exhibit stronger reasoning and achieve the highest overall accuracy of 78.7%. The principal implication for AI practitioners is the necessity to improve reasoning capabilities in next-generation T2I systems by integrating structured knowledge bases and advanced reasoning mechanisms. |
| Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory
  and Test-Time Compute Scaling (Read more on [arXiv](https://arxiv.org/abs/2508.16745) or [HuggingFace](https://huggingface.co/papers/2508.16745))| Daniil Orel, mbur, yurakuratov, b1l4lx1, irodkin | This paper investigates how recurrence, memory, and test-time compute scaling enhance multi-step reasoning capabilities in neural models using a cellular automata benchmark. The study aims to understand how different neural architectures and training methods affect multi-step reasoning, disentangling genuine generalization from memorization, and how reasoning depth scales with task complexity. The authors trained Transformers, LSTMs, Mamba, and Associative Recurrent Memory Transformers (ARMT) on a 1D Cellular Automata (1dCA) benchmark with disjoint train/test rule sets and various prediction horizons. Fixed-depth (4-layer) autoregressive models achieved 95% accuracy for single-step prediction (k=1) but dropped below 25% for k >= 3, whereas token-level Chain-of-Thought training enabled GPTNeox to achieve >99% accuracy up to k=4. Adaptive Computation Time (ACT) consistently yielded approximately one additional effective reasoning step, and RL with GRPO attained k=3 accuracy. For AI practitioners, objectives enforcing multi-step prediction and mechanisms adaptively allocating computational depth are crucial, with explicit intermediate representations like Chain-of-Thought offering the most reliable route to deeper generalization. |
| Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement
  Learning for General LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2508.16949) or [HuggingFace](https://huggingface.co/papers/2508.16949))| Jiale Zhao, Wenkai Fang, Shunyu Liu, Sunzhu Li, BAOLONGZHANSHEN | Rubric-Scaffolded Reinforcement Learning (RuscaRL) is a novel instructional scaffolding framework designed to break the exploration bottleneck for general LLM reasoning in Reinforcement Learning with Verifiable Rewards (RLVR). The research aims to overcome the dilemma where RL improvement in LLMs is bounded by limited exploration for high-quality samples. RuscaRL employs checklist-style rubrics as (1) explicit scaffolding during rollout generation, incorporating intra-group differentiation and inter-step decay to steer diverse high-quality responses, and (2) verifiable rewards during model training, utilizing LLM-as-a-Judge for binary evaluation and weighted aggregation. Notably, RuscaRL boosts Qwen-2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500, surpassing GPT-4.1, and a fine-tuned Qwen3-30B-A3B-Instruct variant achieves 61.1 on HealthBench-500, outperforming OpenAI-03. This framework implies that AI practitioners can effectively expand LLM reasoning boundaries and enhance performance, particularly for general open-ended tasks lacking objective ground-truth answers, by integrating external guidance and fine-grained, verifiable rewards. |
| PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent
  LLMs (Read more on [arXiv](https://arxiv.org/abs/2508.17188) or [HuggingFace](https://huggingface.co/papers/2508.17188))| Chenyu You, Yiwei Xu, Xiang Zhang, VitaCoco, HadlayZ | PosterGen introduces an aesthetic-aware multi-agent LLM framework for generating academic posters from research papers. This work addresses the challenge of automating high-quality academic poster creation by embedding core design and aesthetic principles often neglected by existing methods. Its methodology employs a multi-agent system comprising Parser & Curator, Layout, Styling (Color & Font), and Renderer agents, which mimic professional design workflows and integrate principles like ABT narrative, a three-column layout, and a CSS-like box model. Quantitative evaluations using VLM-as-Judge metrics reveal PosterGen achieves content fidelity comparable to human designs and significantly outperforms state-of-the-art multi-agent methods in visual designs, with an average design score of 4.44 versus 4.26 for PosterAgent (GPT-40 evaluation) and a peak of 4.90 in 'Font Legibility'. This framework provides AI practitioners with a robust, design-centric solution to automate complex document-to-visual generation, thereby streamlining academic communication and reducing manual design efforts. |
| UQ: Assessing Language Models on Unsolved Questions (Read more on [arXiv](https://arxiv.org/abs/2508.17580) or [HuggingFace](https://huggingface.co/papers/2508.17580))| Wei Liu, Rui Sun, Zihao Wang, Fan Nie, kzliu | UQ introduces a novel paradigm and platform for evaluating Large Language Models (LLMs) on difficult, realistic, and unsolved questions. The main objective is to establish a benchmark that challenges frontier models and reflects natural, real-world information needs, pushing the boundaries of AI capabilities. The methodology involves a three-stage pipeline to curate 500 unsolved questions (UQ-Dataset), LLM-based validation strategies (UQ-Validators) to pre-screen candidate solutions by leveraging the generator-validator gap, and an open platform (UQ-Platform) for community-driven human verification. Primary results show the UQ-Dataset filtering process significantly increases question difficulty, with LLM-judged answer correctness dropping from 51.2% to 14.1%, and the strongest LLM-based validator achieving 85.4% accuracy and 40.0% precision on a challenging surrogate dataset. For AI practitioners, UQ offers a unique testbed to evaluate frontier models on open-ended, real-world problems where ground-truth is absent, fostering progress in advanced AI development through continuous, community-driven evaluation. |
| MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for
  N-level Assessment (Read more on [arXiv](https://arxiv.org/abs/2508.17290) or [HuggingFace](https://huggingface.co/papers/2508.17290))| Doratossadat Dastgheib, Seyed Mohammad Hadi Hosseini, Marzia Nouri, Arshia Hemmat, omidgh | MEENA (PersianMMMU) introduces the first multimodal-multilingual benchmark for evaluating Persian Vision-Language Models (VLMs) across N-level educational exams. The objective is to address the gap in assessing Persian VLMs across scientific, reasoning, and human-level understanding tasks, moving beyond English-centric evaluations. The methodology involved compiling a dataset of approximately 7,500 Persian and 3,000 English questions with rich metadata, covering diverse educational subjects, and evaluating models like GPT-4o, GPT-4-Turbo, Gemini-2.0-flash, and InstructBLIP-T5 across various experimental settings including Zero-Shot and Wrong Image. Primary results show knowledge-based tasks outperformed reasoning tasks by +10–19% in absolute accuracy, and Gemini 2.0-flash demonstrated higher hallucination detection rates for image mismatches in Persian contexts, with over 400 more detections than GPT-4 Mini on the MEENA dataset. For AI practitioners, these findings underscore the critical need to enhance complex multimodal reasoning and robust hallucination detection capabilities in VLMs, particularly when extending to diverse languages and culturally nuanced content. |
| Explain Before You Answer: A Survey on Compositional Visual Reasoning (Read more on [arXiv](https://arxiv.org/abs/2508.17298) or [HuggingFace](https://huggingface.co/papers/2508.17298))| Xin Zheng, Zixian Ma, Joy Hsu, Fucai Ke, ControlNet | This survey provides a comprehensive review of Compositional Visual Reasoning (CVR) research published between 2023 and 2025. It aims to provide a unified taxonomy, historical roadmap, and critical outlook on CVR by addressing its necessity, architectural paradigms, benchmarks, and limitations. The methodology involves systematically reviewing over 260 papers from top AI venues and distilling the field into a five-stage developmental roadmap. Key findings include identifying a paradigm shift from prompt-enhanced language-centric methods to unified agentic VLMs, and cataloging over 60 benchmarks and evaluation metrics. This work offers a foundational reference for AI practitioners, guiding the development of more interpretable, generalizable, and robust visual reasoning systems. |
| ST-Raptor: LLM-Powered Semi-Structured Table Question Answering (Read more on [arXiv](https://arxiv.org/abs/2508.18190) or [HuggingFace](https://huggingface.co/papers/2508.18190))| Wei Zhou, Boxiu Li, Xuanhe Zhou, Boyu Niu, Zirui Tang | ST-Raptor is an LLM-powered, tree-based framework designed for accurate question answering over semi-structured tables with complex layouts. The main objective is to overcome limitations of existing methods in understanding and accurately answering natural language questions over semi-structured tables, which often feature hierarchical headers and merged cells. The key methodology involves introducing a Hierarchical Orthogonal Tree (HO-Tree) structural model to represent table layouts, defining a set of basic tree operations to guide LLM execution, decomposing questions into sub-questions with operation pipeline generation and table alignment, and employing a two-stage verification mechanism for answer reliability. Experiments on the SSTQA dataset show that ST-Raptor achieves the highest overall accuracy, exceeding the second-best method by 10.23% on the SSTQA benchmark. This framework implies that AI practitioners can significantly improve the accuracy and reliability of LLM-powered data extraction and question answering from complex semi-structured documents by adopting structured table representations and pipeline-based verification mechanisms. |
| SpotEdit: Evaluating Visually-Guided Image Editing Methods (Read more on [arXiv](https://arxiv.org/abs/2508.18159) or [HuggingFace](https://huggingface.co/papers/2508.18159))| Ersin Yumer, Haitong Tian, Wei-An Lin, Sara Ghazanfari | SpotEdit introduces a comprehensive benchmark for evaluating visually-guided image editing methods, including a novel hallucination subset. The main objective is to systematically assess the performance and robustness of generative models in complex editing scenarios, particularly when visual cues are incomplete. The methodology involves a three-stage data generation pipeline leveraging Llama-3.1-8B-Instruct, InternVL3-8B, and GPT-40 to create diverse benchmark instances from video keyframes, evaluated by Global Score, Object Fidelity, Background Fidelity, and a Failure Rate metric for hallucination. Primary results indicate that visually-guided editing remains challenging, with the strongest open-source model achieving only a 0.685 Global Score, and GPT-40 exhibiting high hallucination failure rates, such as 91.7% for Inp. Robustness on real data. This highlights to AI practitioners the critical need for developing more robust and reliable visually-guided image editing systems capable of handling incomplete visual cues and avoiding spurious content generation. |
| German4All - A Dataset and Model for Readability-Controlled Paraphrasing
  in German (Read more on [arXiv](https://arxiv.org/abs/2508.17973) or [HuggingFace](https://huggingface.co/papers/2508.17973))| Cristian-George Craciun, Maximilian Müller, Eslam Nasrallah, Thanh Mai Pham, Miriam Anschütz | The paper introduces German4All, the first large-scale German dataset and an associated model for readability-controlled, paragraph-level paraphrasing across five complexity levels. The objective is to provide suitable resources for fine-grained text adaptation in German, addressing the limitation of single-output simplification systems. The methodology involved synthesizing over 25,000 Wikipedia paragraphs into five distinct readability levels using GPT-4, followed by rigorous evaluation via human annotators and an LLM-as-a-judge, and subsequent fine-tuning of an open-source Flan-T5-xl model. A primary result is that the fine-tuned model achieved state-of-the-art SARI scores on German text simplification benchmarks, for instance, German4All-level1 obtained 53.9 SARI on the German4All-Corrected test set, surpassing other compared models. This implies AI practitioners can utilize German4All to develop and evaluate multi-level paraphrasing models, enabling more nuanced and reader-specific text adaptations for improved accessibility and diverse applications in German. |
| Limitations of Normalization in Attention Mechanism (Read more on [arXiv](https://arxiv.org/abs/2508.17821) or [HuggingFace](https://huggingface.co/papers/2508.17821))| Radu State, Tatiana Petrova, mbur, opensapce | This paper theoretically and empirically investigates the limitations of normalization, particularly softmax, in attention mechanisms. The main objective is to quantify the selective ability, geometric separation, and gradient sensitivity of attention mechanisms under various normalization schemes. The methodology involves deriving non-asymptotic bounds for representation distance, geometric separability, and Jacobian norm, validated by experiments on a pre-trained GPT-2 model. Key findings include that no more than ≈ 80% of top-N tokens can be geometrically distinguished, and the Jacobian norm of softmax normalization scales as 1/(4T), indicating high gradient sensitivity at low temperatures. For AI practitioners, this implies limiting the active token set to a sub-linear function of context length and avoiding aggressive temperature scaling (T < 0.1) to ensure training stability and effective token differentiation. |
| TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language
  Modeling (Read more on [arXiv](https://arxiv.org/abs/2508.16790) or [HuggingFace](https://huggingface.co/papers/2508.16790))| Jiaqi Li, Junan Zhang, Xueyao Zhang, Dekun Chen, Yuancheng Wang | TaDiCodec is a novel text-aware diffusion speech tokenizer that leverages a single-stage, end-to-end training paradigm for efficient speech language modeling. The primary objective is to address limitations of existing speech tokenizers, such as dependence on multi-layer RVQ, high frame rates, reliance on auxiliary pre-trained models, and complex two-stage training processes. It employs a Transformer-based diffusion autoencoder with Binary Spherical Quantization (BSQ) and a flow matching-based decoder, integrating text and prompt guidance during de-tokenization for enhanced reconstruction and compression. TaDiCodec achieves an ultra-low frame rate of 6.25 Hz and a bitrate of 0.0875 kbps for 24 kHz speech using a single-layer codebook, while achieving a Word Error Rate (WER) of 2.28 on SeedTTS test-en in zero-shot TTS. This approach offers AI practitioners a highly compressed, efficient, and direct solution for integrating speech into LLM-based systems, reducing architectural complexity and improving scalability for speech generation tasks. |
| Neither Valid nor Reliable? Investigating the Use of LLMs as Judges (Read more on [arXiv](https://arxiv.org/abs/2508.18076) or [HuggingFace](https://huggingface.co/papers/2508.18076))| Golnoosh Farnadi, Jackie Chi Kit Cheung, Mohammed Haddou, Khaoula Chehbouni | This position paper critically examines the reliability and validity of Large Language Models (LLMs) when employed as judges (LLJs) for Natural Language Generation (NLG) evaluation. Its primary objective is to rigorously scrutinize four core assumptions underpinning LLJ adoption: their capacity to proxy human judgment, their capabilities as evaluators, their scalability, and their cost-effectiveness. The methodology involves applying a social science measurement theory framework and conducting a qualitative review of LLJ literature across use cases such as text summarization, data annotation, and safety alignment. Key findings indicate LLJs are susceptible to cognitive biases and adversarial attacks; for example, LLM safety judges have been shown to misclassify up to 100% of harmful generations as harmless due to simple prompt variations. Consequently, AI practitioners must adopt more rigorous, context-aware, and transparent evaluation practices for LLJs, addressing their inherent limitations and biases to ensure responsible integration into NLG development. |
