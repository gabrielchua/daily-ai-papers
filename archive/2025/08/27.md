

## Papers for 2025-08-27

| Title | Authors | Summary |
|-------|---------|---------|
| CMPhysBench: A Benchmark for Evaluating Large Language Models in
  Condensed Matter Physics (Read more on [arXiv](https://arxiv.org/abs/2508.18124) or [HuggingFace](https://huggingface.co/papers/2508.18124))| Dongchen Huang, komusama0930, BoringMarsh, di-zhang-fdu, weidawang | CMPhysBench is a novel benchmark evaluating Large Language Models' (LLMs) proficiency in Condensed Matter Physics. Its main objective is to assess LLM problem-solving abilities through 520 graduate-level, meticulously curated calculation problems across core CMP subfields. The benchmark introduces the Scalable Expression Edit Distance (SEED) metric, which uses tree-based representations of expressions and physics-aware normalizations for fine-grained, non-binary partial credit. Evaluation of 18 LLMs revealed a significant capability gap, with the top-performing model, Grok-4, achieving only a 36 average SEED score and 28% accuracy. This underscores the critical need for AI practitioners to focus on physics-aware training, improved scientific alignment, symbolic precision, and embedding physics-aware verification into LLM decoding for domain-specific scientific applications. |
| TreePO: Bridging the Gap of Policy Optimization and Efficacy and
  Inference Efficiency with Heuristic Tree-based Modeling (Read more on [arXiv](https://arxiv.org/abs/2508.17445) or [HuggingFace](https://huggingface.co/papers/2508.17445))| Zhoufutu Wen, Qingshui Gu, zhangysk, aaabiao, yizhilll | TreePO is a reinforcement learning framework designed to improve the efficacy and inference efficiency of large language models for complex reasoning through heuristic tree-based modeling. It aims to enable LLMs to explore diverse reasoning paths efficiently, reducing computational costs, and to accurately attribute sparse outcome rewards to specific tokens. TreePO reformulates on-policy rollouts as a segment-based tree search, leveraging KV-caching for shared prefixes and introducing a hierarchical advantage estimator that utilizes sub-groups for granular credit assignment, coupled with heuristic sampling. Empirically, TreePO reduces GPU hours by 12% to 43% during training and achieves a 40% reduction in trajectory-level inference time, while improving overall accuracy for GRPO from 46.63% to 54.61%. This provides AI practitioners with a more efficient and scalable method for RL-based post-training of LLMs, reducing sample and compute requirements for complex reasoning tasks. |
| VibeVoice Technical Report (Read more on [arXiv](https://arxiv.org/abs/2508.19205) or [HuggingFace](https://huggingface.co/papers/2508.19205))| Yaoyao Chang, Wenhui Wang, Jianwei Yu, Zhiliang Peng, unilm | VIBEVOICE is a novel model for scalable long-form, multi-speaker conversational audio synthesis, aiming to generate up to 90 minutes of speech. Its methodology combines next-token diffusion with a continuous speech tokenizer, achieving a 3200x compression rate at 7.5 Hz, and integrates a pre-trained Large Language Model with a token-level Diffusion Head. The VIBEVOICE-7B model outperforms top-tier systems, demonstrating a Word Error Rate (WER) of 1.29 and Speaker Similarity (SIM) of 0.692 on long conversational speech. Furthermore, its efficient tokenizer yields a leading PESQ score of 3.068 on the LibriTTS test-clean dataset, significantly boosting computational efficiency while preserving audio fidelity. For AI practitioners, this framework offers a powerful solution for high-fidelity, long-duration, multi-speaker speech synthesis, advancing capabilities for complex applications like podcasts and audiobooks. |
| VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D
  Space (Read more on [arXiv](https://arxiv.org/abs/2508.19247) or [HuggingFace](https://huggingface.co/papers/2508.19247))| Rui Chen, Gengxiong Zhuang, Zehuan Huang, fenghora, Nelipot | VoxHammer is a training-free framework for precise and coherent 3D local editing in native 3D space. The paper aims to enable precise and coherent 3D local editing of existing or AI-generated assets by leveraging pretrained 3D generative models, eliminating the need for additional training. Its methodology involves a two-stage process on a pretrained structured 3D latent diffusion model: precise 3D inversion to cache inverted latents and key-value tokens, followed by denoising-based editing with replacement of preserved regions' features by cached inverted latents and key-value tokens. Quantitative experiments on the Edit3D-Bench benchmark show VoxHammer achieves superior unedited region preservation, with a Chamfer Distance of 0.012 and a masked PSNR of 41.68, outperforming baselines in overall 3D quality and condition alignment. This training-free approach provides AI practitioners with a robust, high-fidelity method for 3D asset manipulation and enables the synthesis of paired data, laying a foundation for future in-context 3D generation. |
| Spacer: Towards Engineered Scientific Inspiration (Read more on [arXiv](https://arxiv.org/abs/2508.17661) or [HuggingFace](https://huggingface.co/papers/2508.17661))| zerojun48, kohandy, rallyduck1005, MoonRainy21, mhlee1022 | Spacer is a scientific discovery system that employs deliberate decontextualization and a multi-stage LLM pipeline to generate novel, high-impact scientific concepts. The system aims to overcome current LLM limitations in scientific creativity by generating original, factually grounded scientific concepts without external intervention, adhering to academic standards. Spacer utilizes NURI, a graph-based inspiration engine that extracts high-potential keyword sets from 180,000 biological publications, and a Manifesting Pipeline comprising Revealing, Scaffolding, and Assessment Frameworks, which use multi-agent LLMs to refine these sets into structured scientific statements. NURI's evaluation metric achieves an AUROC score of 0.737 for classifying high-impact publications, and the Manifesting Pipeline reconstructs core concepts from top-journal articles with over 85% accuracy. This work demonstrates a crucial architectural paradigm shift for AI development, proving that hybrid AI systems leveraging non-LLM components for creative ideation can overcome inherent limitations of pure generative LLMs, advancing the potential for automated scientific discovery. |
| OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive
  Simulation (Read more on [arXiv](https://arxiv.org/abs/2508.19209) or [HuggingFace](https://huggingface.co/papers/2508.19209))| Jiaqi Yang, Zerong Zheng, Weihong Zeng, Jianwen Jiang, chao0412 | OmniHuman-1.5 introduces a dual-system cognitive framework for generating semantically coherent and expressive avatar animations. The primary objective is to generate character animations that are not only physically plausible but also semantically coherent and expressive, moving beyond low-level audio synchronization to capture authentic essence and a deeper semantic understanding of emotion, intent, and context. The methodology integrates Multimodal Large Language Models (MLLMs) for deliberative (System 2) semantic guidance and a specialized Multimodal Diffusion Transformer (MMDiT) architecture with a novel Pseudo Last Frame design for reactive (System 1) rendering, synergistically fusing multimodal inputs while mitigating inter-modality conflicts. OmniHuman-1.5 demonstrates leading performance, significantly outperforming OmniHuman-1 [40] with an HKV score of 72.113 compared to 47.561 in full-body scenarios, and achieving a 33% top-1 selection rate in user preference studies against academic baselines. This framework provides AI practitioners with a robust and generalizable approach for creating more intelligent, context-aware, and emotionally resonant digital avatars, opening new avenues for applications in interactive agents and AI-driven content generation beyond simple motion synchronization. |
| UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior
  Long-Context Learning (Read more on [arXiv](https://arxiv.org/abs/2508.18756) or [HuggingFace](https://huggingface.co/papers/2508.18756))| Ran Guo, Siyan Chen, Qiyang Min, Yu Bao, FetchFortune | UltraMemV2 is a novel memory-layer architecture designed to achieve performance parity with 8-expert Mixture of Experts (MoE) models, offering an efficient alternative for sparse computation. The primary objective was to bridge the performance gap between prior memory-layer designs and state-of-the-art MoE configurations while retaining low memory access. Key methodological innovations include integrating memory layers into every transformer block, simplified value expansion, FFN-based value processing from PEER, principled parameter initialization, and rebalancing memory-to-FFN computation ratios. UltraMemV2 demonstrated performance parity with 8-expert MoE models, and achieved superior results on memory-intensive tasks, notably a +7.9 point improvement in in-context learning. This research indicates that memory-layer architectures, specifically UltraMemV2, are a compelling, efficient alternative for scaling AI models, suggesting that increasing activation density is more impactful than total sparse parameter count. |
| Pixie: Fast and Generalizable Supervised Learning of 3D Physics from
  Pixels (Read more on [arXiv](https://arxiv.org/abs/2508.17437) or [HuggingFace](https://huggingface.co/papers/2508.17437))| Dinesh Jayaraman, Chuhao Chen, Chen Wang, Ryan Lucas, vlongle | Pixie is a novel framework for fast and generalizable supervised learning of 3D physics from pixels, designed to predict object material properties for realistic simulations. Its objective is to overcome slow, per-scene optimization and lack of generalizability in inferring 3D scene physical properties from visual data. The key methodology involves training a 3D U-Net on a curated PIXIEVERSE dataset to map distilled CLIP 3D visual feature grids to voxelized material fields, predicting discrete material types and continuous parameters (Young's modulus, Poisson's ratio, density) via supervised losses, and integrating with Gaussian splatting and MPM solvers. Pixie achieves a VLM realism score of 4.35 ± 0.08, demonstrating 1.46-4.39x improvement in realism and orders of magnitude faster inference (2 seconds) compared to test-time optimization methods, alongside zero-shot generalization to real-world scenes. For AI practitioners, this provides an efficient and generalizable feed-forward approach for integrating physics into virtual environments, accelerating the development of dynamic and interactive AI systems. |
| Autoregressive Universal Video Segmentation Model (Read more on [arXiv](https://arxiv.org/abs/2508.19242) or [HuggingFace](https://huggingface.co/papers/2508.19242))| Albert Gu, Yu-Chiang Frank Wang, Sukjun Hwang, Miran Heo, cmhungsteve | The Autoregressive Universal Segmentation Model (AUSM) unifies prompted and unprompted video segmentation using an LLM-inspired autoregressive framework, enabling scalable processing of long video streams with efficient parallel training. The primary objective is to develop a single, scalable architecture for streaming video segmentation that unifies diverse tasks, preserves fine-grained spatio-temporal details, supports long video inference with constant memory, and allows efficient sequence-length-scalable training. AUSM recasts video segmentation as sequential mask prediction, leveraging "History Marker" for fine-grained detail and a Mamba-based "History Compressor" to maintain a fixed-size spatial state for past frames, while employing a parallel training strategy that avoids recurrent frame processing. AUSM demonstrates strong performance across seven benchmarks for both prompted and unprompted tasks, outperforming prior universal streaming methods, and achieves up to 2.5× faster training on 16-frame sequences compared to iterative baselines. This autoregressive formulation provides a practical, memory-efficient, and scalable solution for deploying universal video segmentation, reducing the need for task-specific models and offering substantial training speedups, particularly beneficial for long video sequences. |
| Wan-S2V: Audio-Driven Cinematic Video Generation (Read more on [arXiv](https://arxiv.org/abs/2508.18621) or [HuggingFace](https://huggingface.co/papers/2508.18621))| Chaonan Ji, Mingyang Huang, Siqi Hu, Li Hu, Xin Gao | Wan-S2V is an audio-driven cinematic video generation model designed to enhance expressiveness and fidelity in complex human video scenarios. The primary objective is to achieve film-level audio-driven character animation in complex film and television contexts, improving expressiveness and fidelity compared to existing methods. The model leverages the Wan text-to-video foundation model, integrating Wav2Vec-encoded audio features and detailed textual captions generated by Qwen-VL for character motion, with training employing a three-stage process and a hybrid parallel scheme combining FSDP and Context Parallelism. Wan-S2V significantly outperforms state-of-the-art models in quantitative metrics, achieving a Fréchet Inception Distance (FID) of 15.66, which is lower than Hunyuan-Avatar's FID of 18.07, demonstrating improved frame quality and consistency. This approach offers AI practitioners a more robust and accessible solution for generating high-quality, expressive human video content synchronized with audio, particularly beneficial for cinematic character animation, long-form video generation, and precise video lip-sync editing. |
| CineScale: Free Lunch in High-Resolution Cinematic Visual Generation (Read more on [arXiv](https://arxiv.org/abs/2508.15774) or [HuggingFace](https://huggingface.co/papers/2508.15774))| Ziwei Liu, Paul Debevec, Ziqi Huang, Ning Yu, Haonan Qiu | CineScale is a novel inference paradigm enabling high-resolution visual generation for image and video diffusion models. Its main objective is to overcome issues like repetitive patterns and quality degradation in high-resolution synthesis, extending capabilities across T2I, T2V, I2V, and V2V tasks for both UNet and DiT architectures. The methodology integrates tailored self-cascade upscaling, restrained dilated convolution, and multi-scale frequency component fusion into self-attention layers, further adapting DiT models with NTK-RoPE and Attentional Scaling. CineScale achieves 8k image generation without fine-tuning and 4k video generation with minimal LoRA fine-tuning, demonstrating an FID of 49.796 and KID of 0.004 for 4096x4096 image generation, and an FVD of 484.711 for video. This allows AI practitioners to deploy state-of-the-art high-resolution generation with existing pre-trained models, significantly reducing training effort for various visual synthesis applications. |
| FastMesh:Efficient Artistic Mesh Generation via Component Decoupling (Read more on [arXiv](https://arxiv.org/abs/2508.19188) or [HuggingFace](https://huggingface.co/papers/2508.19188))| Xingang Pan, Yongwei Chen, Armando Fortes, Yushi Lan, Jeonghwan Kim | FASTMESH is an efficient artistic mesh generation framework that decouples vertex and face creation to significantly reduce token redundancy and accelerate the generation process. The main objective is to overcome the inefficiency of traditional autoregressive approaches, which generate excessively long and redundant token sequences, by developing a framework that treats vertices and faces as separate components. The methodology is a two-stage process: first, an autoregressive model generates a compressed vertex sequence using block-wise indexing, which is refined by a "fidelity enhancer"; second, a bidirectional transformer constructs an adjacency matrix in a single step to define mesh faces. Experimental results on the Toys4K dataset show the method achieves more than 8× faster mesh generation speed compared to state-of-the-art approaches, while also improving mesh quality, evidenced by a superior Chamfer Distance of 4.05%. For AI practitioners, the principal implication is that decoupling the generation of structured data components (e.g., vertices and faces) can dramatically reduce input sequence length for transformers, leading to substantial gains in inference speed and efficiency without sacrificing output quality. |
| ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks (Read more on [arXiv](https://arxiv.org/abs/2508.15804) or [HuggingFace](https://huggingface.co/papers/2508.15804))| Kai Jia, Cong Ma, Zhihao Cheng, Ying Zeng, Minghao Li | ReportBench is a systematic benchmark designed to evaluate the content quality of research reports produced by Deep Research agents. Its primary objective is to assess the quality and relevance of cited literature alongside the faithfulness and veracity of statements within generated reports. The methodology involves using expert-authored arXiv survey papers as gold-standard references, employing reverse prompt engineering to create diverse evaluation prompts, and an agent-based framework for automated citation and statement verification. Empirical evaluations indicate that commercial Deep Research agents, such as OpenAI Deep Research, achieve higher performance (e.g., 78.87% citation match rate) compared to standalone LLMs. This benchmark provides AI practitioners with a robust framework to evaluate and enhance the factual accuracy and reliability of LLM-based knowledge synthesis. |
| ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.18773) or [HuggingFace](https://huggingface.co/papers/2508.18773))| Jiangjie Chen, Mingxuan Wang, Xuefeng Li, Siyu Yuan, Qianyu He | ThinkDial is the first open-recipe, end-to-end framework enabling gpt-oss-style controllable reasoning in LLMs through discrete operational modes. Its objective is to provide open-source large language models with fine-grained control over computational effort, mimicking proprietary systems' capabilities for diverse deployment scenarios. The methodology involves a novel end-to-end training paradigm combining Budget-Mode Supervised Fine-tuning with a two-phase Budget-Aware Reinforcement Learning strategy that employs adaptive reward shaping and a critical Leak Penalty mechanism. ThinkDial achieves target compression-performance trade-offs, providing Medium mode with 50% token reduction and <10% performance degradation, and Low mode with 75% token reduction and <15% performance degradation; the Leak Penalty was crucial to prevent reasoning leakage into answer sections, ensuring genuine token reduction. This framework offers AI practitioners a vital open-source solution for managing LLM reasoning depth and computational costs, facilitating optimized deployment for applications with varying accuracy-efficiency requirements. |
| MovieCORE: COgnitive REasoning in Movies (Read more on [arXiv](https://arxiv.org/abs/2508.19026) or [HuggingFace](https://huggingface.co/papers/2508.19026))| Hung-Ting Su, Ying Cheng, Jia-Fong Yeh, Gueter Josmy Faure, cmhungsteve | This paper introduces MovieCORE, a novel video question answering (VQA) dataset and an agentic enhancement module designed to advance System-2 cognitive reasoning in video understanding. The objective is to challenge Vision-Language Models (VLMs) with deeper cognitive understanding of movie content, moving beyond surface-level comprehension to infer emotions, character dynamics, causality, and psychological complexity. The authors developed an agentic brainstorming workflow using multiple LLMs as specialized thought agents to generate and refine high-quality question-answer pairs, and proposed Agentic Choice Enhancement (ACE) as a post-training VLM refinement plugin. MovieCORE demonstrates significantly higher cognitive demand, achieving a 99.2% rate for higher-order questions and answers based on Bloom's Taxonomy, and ACE improves existing VLMs' performance by up to 25% on this dataset. This work contributes to advancing AI systems' movie understanding, highlighting current VLM limitations in complex reasoning and offering a computationally efficient, training-free method for VLM output refinement. |
| Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning
  Tasks (Read more on [arXiv](https://arxiv.org/abs/2508.18672) or [HuggingFace](https://huggingface.co/papers/2508.18672))| Daisuke Nohara, Takumi Okamoto, Masaki Kawamura, Satoki Ishikawa, Taishi-N324 | This paper investigates optimal sparsity configurations for Mixture-of-Experts (MoE) Large Language Models for reasoning tasks. The objective was to identify how optimal MoE sparsity changes between memorization (TriviaQA, HellaSwag) and reasoning (GSM8K, GSM-Plus) tasks, and its interaction with total/active parameters, pre-training loss, and compute. Researchers trained families of Mixtral-style MoEs, sweeping architectural hyperparameters like model width, number of experts per layer (E), and top-k experts per token (k), and evaluated their performance on pre-training loss, task loss, and accuracy under pre-training, GRPO post-training, and test-time compute (Self-Consistency). While increasing total parameters consistently reduced pre-training loss, reasoning task performance (e.g., GSM8K) showed a U-shaped trend, with task loss worsening beyond a certain parameter count and accuracy peaking near a Tokens-per-Parameter (TPP) ratio of approximately 20. Neither GRPO post-training nor test-time compute (Self-Consistency) removed this inverted U-shaped relationship. For AI practitioners, this implies that under a fixed computational budget, optimizing MoE sparsity for reasoning tasks requires careful consideration of active parameter growth or a shift towards denser MoE layers, rather than simply increasing total parameters, to avoid performance degradation. |
| Training Language Model Agents to Find Vulnerabilities with CTF-Dojo (Read more on [arXiv](https://arxiv.org/abs/2508.18370) or [HuggingFace](https://huggingface.co/papers/2508.18370))| Zijian Wang, Varun Kumar, Hantian Ding, Dingmin Wang, terryyz | CTF-DOJO introduces a large-scale execution environment for training Language Model agents to identify software vulnerabilities in Capture-The-Flag challenges. The main objective is to overcome the scarcity of scalable, generalizable execution-grounded environments for training capable ML agents in offensive cybersecurity. The key methodology involves CTF-DOJO, which features 658 Dockerized CTF challenges, and CTF-FORGE, an automated LLM-powered pipeline that creates these runtime environments with over 98% success. LLM agents trained on just 486 execution-verified trajectories from CTF-DOJO achieved up to 11.6% absolute Pass@1 gains over baselines, with the 32B model reaching 31.9% Pass@1. This demonstrates that execution-grounded training signals are effective and pivotal for developing high-performance cybersecurity ML agents without dependence on costly proprietary systems. |
| ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion
  Models (Read more on [arXiv](https://arxiv.org/abs/2508.18271) or [HuggingFace](https://huggingface.co/papers/2508.18271))| Beiqi Chen, Gangshan Wu, Jie Tang, Jie Liu, Haitang Feng | ObjFiller-3D is a novel framework for consistent multi-view 3D object inpainting that leverages video diffusion models. The primary objective is to complete and edit high-quality, consistent 3D objects from partial inputs and 3D mask regions, addressing inconsistencies inherent in traditional 2D inpainting for 3D tasks. Its key methodology involves adapting a state-of-the-art video editing diffusion model, VACE, using Low-Rank Adaptation (LoRA) to fill masked 3D regions by processing multi-view renders as a looped video sequence, combined with 3D Gaussian Splatting for reconstruction and reference-based inpainting. ObjFiller-3D achieves superior performance, producing reconstructions with a PSNR of 26.6 (compared to NeRFiller's 15.9) and an LPIPS of 0.19 (compared to Instant3dit's 0.25). This method offers more faithful and fine-grained 3D reconstruction, demonstrating strong potential for practical deployment in real-world 3D editing applications and content creation by efficiently leveraging pre-trained video editing models. |
| QueryBandits for Hallucination Mitigation: Exploiting Semantic Features
  for No-Regret Rewriting (Read more on [arXiv](https://arxiv.org/abs/2508.16697) or [HuggingFace](https://huggingface.co/papers/2508.16697))| Manuela Veloso, Sumitra Ganesh, Alec Koppel, William Watson, Nicole Cho | QueryBandits introduces a contextual bandit framework for mitigating large language model (LLM) hallucinations via semantic query rewriting. The research objective is to proactively steer LLMs away from generating hallucinations by designing feature-aware rewrite strategies. The methodology employs a multi-armed bandit system with five rewrite strategies as arms and 17 linguistic features as contextual attributes, driven by a reward model combining LLM-judge, fuzzy-match, and BLEU scores. The top contextual QueryBandit (Thompson Sampling) achieved an 87.5% win rate over a no-rewrite baseline and outperformed zero-shot static prompting by 42.6% (paraphrase) and 60.3% (expand). This implies AI practitioners can leverage guided, feature-aware query rewrites as an efficient, forward-pass mechanism to reduce hallucination and interpret LLM sensitivity to query context, bypassing the need for retraining or gradient-based adaptation. |
| Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2508.19202) or [HuggingFace](https://huggingface.co/papers/2508.19202))| Arman Cohan, Doug Downey, Arpan Sarkar, Yixin Liu, Alan Li | This paper introduces benchmarks and a probing framework to demystify the contributions of knowledge and reasoning to LLM performance in scientific problem-solving. The primary objective was to systematically disentangle the distinct roles of knowledge recall and utilization from reasoning in LLMs, specifically addressing how external knowledge and CoT fine-tuning impact performance. The authors introduced SCIREAS (a unified suite of ten scientific benchmarks) and SCIREAS-PRO (a reasoning-intensive subset), alongside KRUX, a novel probing framework that supplies models with atomic "knowledge ingredients" (KIs) extracted from reasoning traces to study knowledge recall and usage. Key findings include that retrieving task-relevant knowledge is a critical bottleneck, with vanilla instruct models outperforming reasoning counterparts by ≥10% when provided with in-context KIs, and that reasoning fine-tuning enhances models' ability to surface helpful knowledge, even for known facts. For AI practitioners, these results highlight the importance of external knowledge injection via mechanisms like RAG and CoT fine-tuning for improving scientific reasoning LLMs, and suggest the need for task-specific evaluations to optimize cost-performance balance. |
| Unraveling the cognitive patterns of Large Language Models through
  module communities (Read more on [arXiv](https://arxiv.org/abs/2508.18192) or [HuggingFace](https://huggingface.co/papers/2508.18192))| Jianxi Gao, Pin-Yu Chen, KBhandari11 | This paper investigates the cognitive patterns of Large Language Models (LLMs) by developing a network-based framework linking cognitive skills, LLM architectures, and datasets. The main objective was to understand the underlying mechanisms of LLMs and how cognitive skills are encoded and localized within these models, drawing an analogy to human brain organization. The methodology involved constructing a multipartite network of skills, datasets, and LLM modules, applying Louvain community detection and spectral analysis, and evaluating fine-tuning strategies under block-based and channel-based pruning. Primary results show that LLMs do not exhibit precise alignment between predefined cognitive functions and detected skill communities (Adjusted Rand Score clustered around 0 for all models and sparsity ratios in Figure 11), and although community-based fine-tuning induced the most substantial weight changes, all-module fine-tuning achieved the highest overall accuracy (Figure 5f). The principal implication for AI practitioners is that effective fine-tuning strategies for LLMs should leverage distributed learning dynamics and network-wide dependencies, recognizing that rigidly localized modular interventions do not confer a performance advantage. |
