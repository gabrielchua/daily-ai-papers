

## Papers for 2025-08-28

| Title | Authors | Summary |
|-------|---------|---------|
| Self-Rewarding Vision-Language Model via Reasoning Decomposition (Read more on [arXiv](https://arxiv.org/abs/2508.19652) or [HuggingFace](https://huggingface.co/papers/2508.19652))| Zhenwen Liang, Rui Liu, Wenhao Yu, Zongxia Li, ChengsongHuang | Vision-SR1 introduces a self-rewarding reinforcement learning framework to enhance VLM visual reasoning and mitigate hallucinations and language shortcuts. The primary objective is to improve VLM performance by enforcing visual grounding without external visual supervision. This is achieved by decomposing VLM reasoning into visual perception and language reasoning stages, where the model self-evaluates its generated visual perception for sufficiency to answer the question, assigning a self-visual reward that is combined with traditional answer and format rewards. Using the Qwen2.5-VL-7B backbone, Vision-SR1 attained an average accuracy of 58.8 across diverse benchmarks, outperforming Vision-R1 (57.4) and supervised fine-tuning (55.1). AI practitioners can leverage this self-rewarding mechanism to develop more robust and visually grounded VLMs by integrating internal consistency checks, thereby reducing dependence on costly external annotations. |
| Beyond Transcription: Mechanistic Interpretability in ASR (Read more on [arXiv](https://arxiv.org/abs/2508.15882) or [HuggingFace](https://huggingface.co/papers/2508.15882))| Aviv Shamsian, Hilit Segev, Yael Segal-Feldman, AvivNavon, netag | This paper systematically adapts and applies mechanistic interpretability techniques to Automatic Speech Recognition (ASR) models, particularly Whisper and Qwen2-Audio, to reveal internal dynamics. The research aims to understand the internal behavior and dynamics of ASR systems, particularly the mechanisms behind error phenomena like hallucinations, repetition loops, and contextually biased outputs. Key methodologies include logit lens, linear probing, and intervention-based methods (component patching and ablation), alongside an adapted Encoder Lens technique, for analyzing hidden states and causal roles of components. Quantitative results show 94.6% linear decodability of speaker gender from Whisper's encoder layer 25, and 76% resolution of repetition hallucinations via cross-attention patching at decoder layer 23 (plus 13% at layer 18). These findings enable building internal monitors for hallucination, fine-grained debugging, and informing architectural choices for more robust and transparent ASR systems. |
| Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding
  in Vision-Language-Action Policies (Read more on [arXiv](https://arxiv.org/abs/2508.20072) or [HuggingFace](https://huggingface.co/papers/2508.20072))| Sitong Mao, Chengyue Wu, Tianshuo Yang, Yizhuo Li, Zhixuan Liang | Discrete Diffusion VLA is a novel Vision-Language-Action (VLA) policy that integrates discrete diffusion with a unified transformer for action decoding. The paper addresses the limitations of existing VLA decoders, which either use sequential autoregressive generation or separate continuous diffusion/flow matching heads, hindering unified, scalable architectures. The methodology involves a single-transformer architecture that applies discrete diffusion to discretized action chunks, trained via cross-entropy, and employs an adaptive re-masking policy for iterative refinement and error correction. Discrete Diffusion VLA achieved 96.3% average success rate on LIBERO, 71.2% visual matching on SimplerEnv-Fractal, and 49.3% overall on SimplerEnv-Bridge, consistently outperforming AR and continuous diffusion baselines. This unified discrete diffusion approach enables parallel, adaptive action decoding with robust error correction, providing a pathway for scalable VLA models that leverage pretrained VLM priors effectively. |
| CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer
  Use Agent with Decoupled Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2508.20096) or [HuggingFace](https://huggingface.co/papers/2508.20096))| Jianze Liang, Yuhang Cao, yuhangzang, rookiexiong, Zery | CODA introduces a novel trainable dual-brain agent architecture for GUI automation, addressing the trade-off between planning and execution in specialized domains by synergizing a generalist planner (Cerebrum) and a specialist executor (Cerebellum). The main objective is to bridge the gap between robust planning but poor execution in generalist GUI agents and precise execution but limited planning in specialist agents, especially in data-scarce scientific environments. Its methodology employs a two-stage pipeline: Stage 1 uses decoupled Group Relative Policy Optimization (GRPO) for planner specialization, and Stage 2 aggregates trajectories from these specialists for supervised fine-tuning (SFT) of a generalist planner. Evaluated on the ScienceBoard benchmark, CODA (Stage-2) achieved an overall Pass@8 success rate of 39.96%, significantly outperforming the Qwen2.5-VL-32B baseline at 19.49% and UI-TARS-1.5-7B at 15.36%. This framework allows AI practitioners to develop more robust and adaptable GUI automation agents for complex, data-scarce specialized domains by efficiently combining generalist planning with precise specialist execution through experience-driven learning. |
| MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time
  Autoregressive Video Generation (Read more on [arXiv](https://arxiv.org/abs/2508.19320) or [HuggingFace](https://huggingface.co/papers/2508.19320))| Yan Zhou, Haoxian Zhang, Wenyuan Zhang, Liyuan Cui, ChenMing-thu14 | MIDAS presents a multimodal interactive digital-human synthesis framework, aiming to achieve real-time, low-latency, and consistent video generation from diverse inputs over long horizons. The core methodology employs an autoregressive Large Language Model (LLM) backbone, conditioned by a multimodal projector encoding audio, pose, and text, and guided by a diffusion head for high-quality rendering. It introduces a Deep Compression Autoencoder (DC-AE) achieving a 64x spatial reduction ratio to reduce inference burden and is trained on a large-scale 20,000-hour dialogue dataset. Experiments validate the approach's low latency and high efficiency, enabling stable, drift-free video generation up to 4 minutes with only 4 denoising steps, and fine-grained multimodal controllability across duplex conversations and multi-lingual synthesis. This framework offers AI practitioners a robust solution for building interactive digital human systems and scalable real-time world models by addressing challenges in control, latency, and temporal consistency. |
| Predicting the Order of Upcoming Tokens Improves Language Modeling (Read more on [arXiv](https://arxiv.org/abs/2508.19228) or [HuggingFace](https://huggingface.co/papers/2508.19228))| Alham Fikri Aji, Erland, zaydzuhri | Token Order Prediction (TOP) is proposed as a novel auxiliary training objective to improve language modeling performance. The main objective is to enhance next-token prediction (NTP) by learning better internal representations, addressing Multi-Token Prediction's (MTP) difficulty in exact future token prediction. The key methodology involves training models to rank upcoming tokens by proximity using a ListNet-based learning-to-rank loss, requiring only a single additional unembedding layer parallel to the NTP head. Primary results show TOP models (340M, 1.8B, 7B parameters) generally outperform NTP and MTP on eight standard NLP benchmarks; for example, the 7B TOP model achieved a TriviaQA accuracy of 30.90 compared to 24.28 for NTP. This implies AI practitioners can leverage TOP as a more scalable and parameter-efficient auxiliary objective for pretraining LLMs, potentially yielding improved general language modeling performance. |
| Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health
  Biomarkers Estimation (Read more on [arXiv](https://arxiv.org/abs/2508.17924) or [HuggingFace](https://huggingface.co/papers/2508.17924))| Anton Ivaschenko, Galina Zubkova, Stepan Botman, Konstantin Egorov, blinoff | The paper introduces MCD-rPPG, a novel, comprehensive, large-scale multi-view video dataset for remote photoplethysmography (rPPG) and health biomarker estimation. The objective is to overcome limitations of existing rPPG datasets by providing 3600 synchronized multi-view video recordings from 600 subjects across varied conditions, paired with 100 Hz PPG signals and 13 extended health metrics. The methodology involved optical character recognition (OCR) of a tablet clock for video synchronization and a POS algorithm for video-PPG alignment, followed by the development of an efficient multi-task neural network baseline model. This model achieved an MAE of 0.68 for PPG and 4.86 for HR on the MCD-rPPG dataset, demonstrating up to 13% speed improvement on CPU compared to leading models. The public release of this diverse dataset and fast baseline model provides a crucial resource for AI/ML practitioners to train more robust models for rPPG and extended health biomarker estimation, thereby accelerating the development of AI medical assistants. |
| Diffusion Language Models Know the Answer Before Decoding (Read more on [arXiv](https://arxiv.org/abs/2508.19982) or [HuggingFace](https://huggingface.co/papers/2508.19982))| Shilin Yan, Lu Yin, Dilxat Muhtar, Yefan Zhou, Pengxiang Li | Diffusion Language Models (DLMs) exhibit early answer convergence, enabling accelerated inference. The paper aims to accelerate DLM inference by identifying and leveraging this early answer convergence phenomenon. Prophet, a training-free fast decoding paradigm, was introduced; it dynamically decides whether to continue refinement or commit early by decoding all remaining tokens based on a confidence gap metric derived from the top-2 prediction candidates. Empirical evaluations on LLaDA-8B and Dream-7B showed Prophet reduces decoding steps by up to 3.4x, for example achieving 3.40x speedup on Sudoku with Dream-7B, while maintaining generation quality, such as matching LLaDA-8B's 54.0% MMLU accuracy. This work implies that AI practitioners can significantly improve the computational efficiency and practical deployability of DLMs by integrating dynamic early stopping mechanisms based on predictive confidence, rather than fixed-step decoding. |
| Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered
  Smartphone Agents (Read more on [arXiv](https://arxiv.org/abs/2508.19493) or [HuggingFace](https://huggingface.co/papers/2508.19493))| Yue Yao, Yibo Shi, Shidong Pan, Zhixin Lin, Jungang | This paper introduces SAPA-Bench, the first large-scale benchmark specifically designed to evaluate privacy awareness in MLLM-powered smartphone agents. The main objective is to thoroughly understand the privacy awareness capabilities of these agents, which often access sensitive user data during automated tasks. The methodology involved constructing SAPA-Bench with 7,138 annotated real-world scenarios and proposing five specialized privacy metrics (PRR, PLR, PLAR, PCAR, RA) to benchmark seven mainstream agents. Primary results showed that most benchmarked agents exhibited unsatisfying privacy awareness, with performance remaining below 60% even with explicit hints; Gemini 2.0-flash achieved the best Risk Awareness (RA) of 67%. This highlights the critical need for specialized privacy-focused training, tighter alignment strategies, and the design of effective prompt frameworks to enhance multimodal agents' risk-response capabilities for secure deployment. |
| AudioStory: Generating Long-Form Narrative Audio with Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2508.20088) or [HuggingFace](https://huggingface.co/papers/2508.20088))| Yixiao Ge, Shijie Ma, Yuying Ge, Yuxin Guo, wybertwang | AudioStory is a unified framework integrating large language models (LLMs) with text-to-audio (TTA) systems to generate structured, long-form narrative audio. The primary objective is to address the challenge of generating temporally coherent and compositionally reasoned long-form audio narratives, which current short-form TTA models struggle with. AudioStory employs LLMs for interleaved reasoning generation, decomposing complex instructions into temporally ordered sub-tasks with contextual cues, and utilizes a decoupled bridging mechanism with specialized semantic and residual tokens to condition a diffusion-based audio generator, all trained through a progressive end-to-end strategy. Extensive experiments on the AudioStory-10K benchmark demonstrate AudioStory's superiority, achieving an instruction-following CLAP score of 0.392 and a Frechet Audio Distance (FAD) of 3.00, outperforming prior TTA baselines like TangoFlux (CLAP 0.317, FAD 3.49). This framework provides AI practitioners with a robust method for developing advanced long-form audio generation systems by synergizing LLM reasoning with high-fidelity audio synthesis, relevant for applications such as dynamic soundscapes and audiobooks. |
| StepWiser: Stepwise Generative Judges for Wiser Reasoning (Read more on [arXiv](https://arxiv.org/abs/2508.19229) or [HuggingFace](https://huggingface.co/papers/2508.19229))| Olga Golovneva, Weizhe Yuan, Wenting Zhao, Wei Xiong, sainbar | STEPWISER introduces a novel RL-trained generative judge that performs meta-reasoning about intermediate steps to enhance LLM reasoning and judgment accuracy. The primary objective is to address the critical challenge of supervising the logical validity of multi-step LLM reasoning by reframing reward modeling as a reasoning task. The methodology combines self-segmentation of Chain-of-Thought into coherent chunks, stepwise data annotation using Monte-Carlo Q-value estimates for relative progress, and online reinforcement learning via GRPO to train the judge. Experiments show that STEPWISER's RL-trained generative judge significantly outperforms SFT-trained discriminative baselines, achieving a 61.9% average ProcessBench score on the 7B model using Rel-Effective signals, compared to 39.7% for the discriminative baseline. This demonstrates that explicit meta-reasoning, trained with online RL on dense stepwise signals, provides a more effective strategy for improving policy model reasoning during training and inference-time search, including self-correction and high-quality data selection. |
| MotionFlux: Efficient Text-Guided Motion Generation through Rectified
  Flow Matching and Preference Alignment (Read more on [arXiv](https://arxiv.org/abs/2508.19527) or [HuggingFace](https://huggingface.co/papers/2508.19527))| An-An Liu, Chao Xue, Diqiong Jiang, Dan Song, Zhiting Gao | MotionFlux is an efficient text-guided motion generation framework utilizing rectified flow matching and online preference alignment. The paper addresses the challenges of precise semantic alignment between linguistic descriptions and motion, alongside the slow inference inefficiencies of current text-to-motion systems. Its core methodology combines a high-speed generation framework based on deterministic rectified flow matching with TMR++ Aligned Preference Optimization (TAPO), a self-supervised online preference learning system that uses TMR++ as a proxy reward model to construct preference data. Experimental results show MotionFlux-ultra achieves a state-of-the-art Average Inference Time per Sentence (AITS) of 0.005, an R-Precision Top 1 of 0.536, and an FID of 0.078 on the HumanML3D dataset, outperforming baselines in speed, semantic alignment, and motion quality. This advancement offers AI practitioners a scalable solution for real-time, high-fidelity text-to-motion synthesis, minimizing reliance on extensive human annotation for preference alignment. |
| Taming the Chaos: Coordinated Autoscaling for Heterogeneous and
  Disaggregated LLM Inference (Read more on [arXiv](https://arxiv.org/abs/2508.19559) or [HuggingFace](https://huggingface.co/papers/2508.19559))| Chunlei Han, Sida Zhao, Zefang Chu, Ruogu Du, Rongzhi Li | HeteroScale is a coordinated autoscaling framework designed to optimize Large Language Model (LLM) inference on heterogeneous and disaggregated Prefill-Decode (P/D) architectures. The main objective is to address challenges in P/D disaggregated LLM serving, including inefficient heterogeneous hardware utilization, network bottlenecks, and architectural imbalance. Key methodologies involve a topology-aware scheduler, novel network-aware abstractions, and a metric-driven policy that uses decode Tokens-Per-Second (TPS) as the primary robust signal to jointly scale prefill and decode pools. Deployed in a massive production environment, HeteroScale increased average GPU utilization by 26.6 percentage points and SM activity by 9.2 percentage points, saving hundreds of thousands of GPU-hours daily while maintaining service level objectives. For AI practitioners, this demonstrates that coordinated, metric-driven autoscaling with topology awareness is crucial for achieving significant resource efficiency and stability in large-scale heterogeneous LLM serving environments. |
| DeepScholar-Bench: A Live Benchmark and Automated Evaluation for
  Generative Research Synthesis (Read more on [arXiv](https://arxiv.org/abs/2508.20033) or [HuggingFace](https://huggingface.co/papers/2508.20033))| Ion Stoica, Ankita Sundar, Harshit Gupta, Negar Arabzadeh, Liana Patel | This paper introduces DeepScholar-Bench, a live benchmark and automated evaluation framework for generative research synthesis, specifically for generating related work sections of academic papers. Its primary objective is to address the limitations of existing benchmarks by providing a holistic and scalable evaluation for complex, evolving research synthesis tasks. The framework uses recent ArXiv papers as its dataset, defines an automated evaluation across knowledge synthesis, retrieval quality, and verifiability, and employs LLM-as-a-judge metrics validated with over 200 human annotations. DeepScholar-base, a reference pipeline, is also introduced. Evaluation shows that no existing system, including open-source models, search AIs, and OpenAI DeepResearch, exceeds a score of 0.19 across all metrics, indicating significant room for improvement. DeepScholar-base achieves competitive or higher performance, with up to 6.3x higher verifiability compared to OpenAI's DeepResearch. These findings highlight the inherent difficulty of generative research synthesis and underscore the importance of DeepScholar-Bench as a foundation for developing more capable AI systems in this domain. |
