

## Papers for 2025-08-29

| Title | Authors | Summary |
|-------|---------|---------|
| Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable
  Text-to-Image Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2508.20751) or [HuggingFace](https://huggingface.co/papers/2508.20751))| Jiazi Bu, Yujie Zhou, Zhimin Li, yuhangzang, CodeGoat24 | This paper introduces PREF-GRPO, a reinforcement learning method using pairwise preference fitting to stabilize text-to-image (T2I) generation, and a fine-grained evaluation benchmark named UNIGENBENCH. The main objective is to mitigate the "reward hacking" problem, which the authors attribute to "illusory advantage"—an issue where minimal pointwise reward score differences are amplified during normalization, destabilizing training. The key methodology shifts from traditional reward score maximization to pairwise preference fitting, where a model compares pairs of generated images, and their resulting win rates are used as the reward signal for Group Relative Policy Optimization (GRPO). On the proposed UNIGENBENCH, PREF-GRPO achieved a 5.84% increase in the overall score over the score-maximization baseline, including a 12.04% improvement in Logical Reasoning. For AI practitioners, this research provides a more stable training paradigm for RL-based T2I models by demonstrating that optimizing for relative preferences, rather than absolute scores, effectively mitigates training instability and reward hacking. |
| rStar2-Agent: Agentic Reasoning Technical Report (Read more on [arXiv](https://arxiv.org/abs/2508.20722) or [HuggingFace](https://huggingface.co/papers/2508.20722))| Weijiang Xu, Yi Zhu, Yifei Liu, Ning Shang, lynazhang | rStar2-Agent is a 14B math reasoning model trained with a novel agentic reinforcement learning approach to achieve frontier-level performance. The main objective is to make agentic reinforcement learning (RL) effective at scale for complex reasoning by overcoming challenges like high rollout costs, environment noise from coding tools, and inefficient training. The methodology combines an efficient RL infrastructure with a high-throughput code environment, a new RL algorithm called GRPO-RoC (Group Relative Policy Optimization with Resampling-on-Correct) to filter noisy trajectories, and a multi-stage training recipe starting with non-reasoning Supervised Fine-Tuning (SFT). The resulting rStar2-Agent-14B model achieves an 80.6% pass@1 score on the AIME24 benchmark in just 510 RL steps, surpassing the much larger 671B DeepSeek-R1 model while generating significantly shorter responses. The principal implication for AI practitioners is that this work provides a compute-efficient recipe and a scalable infrastructure for training smaller models with agentic RL to achieve reasoning capabilities that meet or exceed those of much larger models, offering a practical path for developing powerful agents with limited GPU resources. |
| USO: Unified Style and Subject-Driven Generation via Disentangled and
  Reward Learning (Read more on [arXiv](https://arxiv.org/abs/2508.18966) or [HuggingFace](https://huggingface.co/papers/2508.18966))| Jiahe Tian, Mengqi Huang, wuwx, cb1cyf, fenfan | The USO model unifies style-driven and subject-driven image generation into a single framework using a novel cross-task co-disentanglement and reward learning paradigm. The primary objective is to determine if these traditionally separate tasks can be unified and mutually enhanced by jointly learning to disentangle content from style features. The methodology involves a two-stage training process on a specially curated triplet dataset (<style_ref, content_ref, stylized_result>): first, a style-alignment stage fine-tunes a T2I model using a Hierarchical Projector on SigLIP embeddings, followed by a content-style disentanglement stage that introduces a separate VAE encoder for content and is further optimized via Style Reward Learning (SRL). USO achieves state-of-the-art performance on the USO-Bench, attaining the highest scores for both subject consistency and style similarity, including a CSD score of 0.557 for style-driven tasks and a CLIP-I score of 0.623 for subject-driven tasks. The principal implication for AI practitioners is that a single, efficient customization model can handle both style transfer and subject preservation without requiring separate architectures, as the cross-task training approach demonstrates that learning feature isolation for one task improves feature exclusion for its complementary task. |
| AWorld: Orchestrating the Training Recipe for Agentic AI (Read more on [arXiv](https://arxiv.org/abs/2508.20404) or [HuggingFace](https://huggingface.co/papers/2508.20404))| Qintong Wu, Dong Wang, Chenyi Zhuang, Chengyue Yu, IcyFish | AWORLD is an open-source, distributed framework designed to accelerate the "learning from practice" paradigm for agentic AI by parallelizing agent-environment interaction. The primary objective is to overcome the bottleneck of inefficient experience generation (rollouts) in complex benchmarks, thereby making large-scale reinforcement learning for agents computationally feasible. The methodology involves a Kubernetes-based distributed architecture that executes agent-environment interactions concurrently across a cluster, separating the high-throughput rollout phase from the model training phase, which uses the GRPO algorithm for updates. The framework achieves a 14.6x speedup in experience collection compared to sequential execution; leveraging this, a trained Qwen3-32B agent improved its average pass@1 score on the GAIA benchmark from 21.59% to 32.23% and achieved a 16.33% score on the most difficult Level 3 tasks, surpassing listed proprietary models. The principal implication for AI practitioners is that for complex agentic tasks, the primary bottleneck has shifted from training computation to environment interaction, and this work provides a practical, open-source infrastructure for implementing the massively parallel rollouts necessary to train high-performing agents. |
| TCIA: A Task-Centric Instruction Augmentation Method for Instruction
  Finetuning (Read more on [arXiv](https://arxiv.org/abs/2508.20374) or [HuggingFace](https://huggingface.co/papers/2508.20374))| Simin Ma, kqsong, songwang41, huuuyeah, shujian2025 | TCIA is a framework for systematically augmenting instructions to fine-tune LLMs, preserving task relevance and diversity through a structured query-constraint representation. The objective is to overcome task drift and diversity collapse in automated instruction generation for fine-tuning LLMs on specialized, real-world tasks. The method decomposes instructions into base queries and explicit constraints, then applies a Breadth-First Search (BFS) algorithm to generate new variants by adding, removing, or replacing constraints retrieved from a task-organized database. Models fine-tuned with TCIA improved performance by an average of 8.7% across four real-world applications and maintained a near-100% on-task instruction ratio, while the baseline WizardLM's ratio dropped below 60% after three augmentation hops. AI practitioners can leverage TCIA to cost-effectively adapt open-source LLMs for specific enterprise applications, achieving superior task-specific performance without degrading general instruction-following abilities. |
| Mixture of Contexts for Long Video Generation (Read more on [arXiv](https://arxiv.org/abs/2508.21058) or [HuggingFace](https://huggingface.co/papers/2508.21058))| Junfei Xiao, Yuwei Guo, Lvmin Zhang, Ceyuan Yang, Shengqu Cai | The paper introduces Mixture of Contexts (MoC), a learnable sparse attention mechanism that recasts long video generation as an internal information retrieval task to improve efficiency and temporal consistency. The main objective is to scale diffusion transformers for coherent, minute-long video generation by overcoming the quadratic computational complexity of standard self-attention. The key methodology is the MoC module, which replaces dense attention by partitioning the token stream into content-aligned chunks; for each query, a top-k router dynamically selects relevant chunks based on mean-pooled key similarity, while mandatorily attending to text and local-shot anchors and enforcing a causal routing mask to prevent feedback loops. On multi-shot video generation (180k tokens), MoC reduces attention FLOPs by over 7x and achieves a 2.2x end-to-end speedup with 85% sparsity, while improving motion diversity (Dynamic Degree from 0.46 to 0.56) and maintaining quality compared to a dense attention baseline. The principal implication for AI practitioners is that MoC provides a framework for building scalable long-sequence transformers by replacing computationally expensive dense attention with an efficient, learned, and dynamic sparse routing mechanism, demonstrating that reallocating compute to salient historical context is a practical path to long-term memory in generative models. |
| CogVLA: Cognition-Aligned Vision-Language-Action Model via
  Instruction-Driven Routing & Sparsification (Read more on [arXiv](https://arxiv.org/abs/2508.21046) or [HuggingFace](https://huggingface.co/papers/2508.21046))| Liqiang Nie, Jie He, Rui Shao, Renshan Zhang, Wei Li | CogVLA is a cognition-aligned Vision-Language-Action model that uses a three-stage, instruction-driven routing and sparsification architecture to improve both performance and computational efficiency for robotic manipulation. The primary objective is to overcome the high computational overhead and cross-modal semantic degradation in existing VLA models by jointly optimizing the perception-reasoning-action pipeline. The methodology consists of: 1) EFA-Routing, which uses instructions to selectively aggregate visual tokens in the encoder; 2) LFP-Routing, which prunes instruction-irrelevant tokens within the LLM; and 3) V-L-A Coupled Attention (CAtten), which ensures coherent, parallel action decoding from compressed inputs. Experimentally, CogVLA achieves a state-of-the-art success rate of 97.4% on the LIBERO benchmark while reducing inference latency by 2.8x and training costs by 2.5x compared to OpenVLA. The principal implication for AI practitioners is that integrating instruction-driven, multi-stage sparsification across modalities provides a concrete framework for developing highly efficient and performant VLA models suitable for scalable deployment on resource-constrained systems. |
| MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World
  Tasks via MCP Servers (Read more on [arXiv](https://arxiv.org/abs/2508.20453) or [HuggingFace](https://huggingface.co/papers/2508.20453))| Shashank Biju, Hemani Patel, Qi Chang, Zhenting Wang, ankits0052 | MCP-Bench introduces a benchmark for evaluating LLM agents on complex, multi-step, real-world tasks using a live ecosystem of 28 Model Context Protocol (MCP) servers and 250 tools. The objective is to assess advanced agentic capabilities, including tool use, cross-tool coordination, and long-horizon planning, which are not adequately evaluated by existing benchmarks that rely on isolated APIs and shallow workflows. The methodology features an automated task synthesis pipeline that discovers tool dependency chains and generates 104 tasks with fuzzy, natural language instructions; evaluation is conducted via a hybrid framework combining rule-based metrics (schema compliance, execution success) and a rubric-based LLM-as-a-Judge to score task completion, tool usage, and planning effectiveness. Primary results from evaluating 20 LLMs show that while schema understanding is high across top models (>98% compliance), higher-level reasoning remains a challenge; the best-performing model, gpt-5, achieved an overall score of 0.749, whereas smaller models like llama-3-1-8b-instruct scored only 0.428, with their performance degrading further in multi-server settings. The principal implication for AI practitioners is that while basic tool execution has largely converged, building agents for complex, multi-domain workflows requires frontier models, as long-horizon planning and robust orchestration across multiple servers are key differentiating capabilities and significant bottlenecks for less advanced models. |
| OneReward: Unified Mask-Guided Image Generation via Multi-Task Human
  Preference Learning (Read more on [arXiv](https://arxiv.org/abs/2508.21066) or [HuggingFace](https://huggingface.co/papers/2508.21066))| Yitong Wang, Shiyin Wang, Yuan Gong, wujie10, XionghuiWang | This research introduces OneReward, a unified reinforcement learning framework that fine-tunes a single generative model for multiple mask-guided editing tasks using a single vision-language model (VLM) as a reward signal. The primary objective is to develop a single, proficient mask-guided image generation model capable of handling diverse sub-tasks like image fill, object removal, and text rendering without relying on task-specific supervised fine-tuning. The framework trains a VLM on multi-dimensional human preference data to act as a unified reward model which provides task- and criterion-specific feedback by evaluating image pairs; this reward signal is then used in a multi-task reinforcement learning pipeline to directly optimize a pre-trained flow matching model. The resulting model, Seedream 3.0 Fill, demonstrated superior performance over competitors, achieving a 69.04% usability rate on the image fill task, outperforming the next best model by 16.93 percentage points. For AI practitioners, this framework provides a method to consolidate multi-task generative model alignment into a single training process with one reward model, reducing the complexity and resource requirements for building versatile image editing tools. |
| Turning the Spell Around: Lightweight Alignment Amplification via
  Rank-One Safety Injection (Read more on [arXiv](https://arxiv.org/abs/2508.20766) or [HuggingFace](https://huggingface.co/papers/2508.20766))| Bernard Ghanem, George Turkiyyah, Hasan Abed Al Kader Hammoud, Harethah Abu Shairah | This paper introduces RANK-ONE SAFETY INJECTION (ROSI), a fine-tuning-free method to amplify an LLM's safety alignment by permanently modifying its weights. The primary objective is to investigate if a model's inherent safety mechanisms can be systematically amplified by steering its activations towards a refusal-mediating subspace. The methodology involves extracting a "safety direction" vector from the activation differences between harmful and harmless prompt pairs and then applying a rank-one update to all residual stream write matrices. Experiments show ROSI consistently increases safety refusal rates—for example, boosting YI-6B-CHAT's harm refusal by +18.2 points—while preserving utility on benchmarks like MMLU and significantly reducing jailbreak success rates. For AI practitioners, the principal implication is that this lightweight, interpretable weight-editing technique provides an efficient, low-cost "last-mile" procedure to enhance the safety of both aligned and uncensored models without requiring expensive retraining. |
| Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability
  in Knowledge and Safety with DuET-PD (Read more on [arXiv](https://arxiv.org/abs/2508.17450) or [HuggingFace](https://huggingface.co/papers/2508.17450))| Roy Ka-Wei Lee, Nancy F. Chen, Zhengyuan Liu, Daniel Wai Kit Chin, Incomple | This paper introduces DuET-PD, a novel framework for evaluating LLM persuasion dynamics in multi-turn dialogues, and proposes Holistic DPO to balance robustness and receptiveness. The main objective is to measure and foster appropriate stance-change behavior in LLMs during multi-turn dialogues across knowledge (MMLU-Pro) and safety (SALAD-Bench) domains. DuET-PD employs a systematic multi-turn evaluation protocol using MMLU-Pro and SALAD-Bench MCQs, subjecting models to positive (corrective) or negative (misleading) persuasion appeals across three turns, with iterative stance checks and confidence recording, and explores mitigation via Holistic Direct Preference Optimization (DPO). Results demonstrate that even state-of-the-art GPT-40 achieves only 27.32% accuracy in MMLU-Pro under sustained misleading persuasions, while Holistic DPO significantly improves Llama-3.1-8B-Instruct's NEG-Acc@3 in safety contexts from 4.21% to 76.54%. This work implies that AI practitioners must prioritize balanced training approaches to cultivate epistemic integrity in LLMs, ensuring they resist misinformation while remaining receptive to valid corrections in high-stakes applications. |
| Dress&Dance: Dress up and Dance as You Like It - Technical Preview (Read more on [arXiv](https://arxiv.org/abs/2508.21070) or [HuggingFace](https://huggingface.co/papers/2508.21070))| Yu-Xiong Wang, Minh Phuoc Vo, Aayush Bansal, Jun-Kun Chen | Dress&Dance is a novel video diffusion framework generating high-resolution, temporally consistent virtual try-on videos with user-controlled motion and garment options. Its main objective is to generate high-quality 5-second, 24 FPS virtual try-on videos at 1152 x 720 resolution of a user wearing desired garments animated by a reference video. The key methodology involves CondNet, a novel attention-based conditioning network that unifies multi-modal inputs via cross-attention to enhance garment registration and motion fidelity, supported by a multi-stage progressive training strategy and an auto-regressive video refiner. Dress&Dance significantly outperforms open-source baselines and achieves comparable or better quality than commercial models, recording a PSNR of 22.41 and SSIM of 0.9038 on a captured dataset, exceeding TPD+CogVideoX I2V's PSNR of 14.47 and SSIM of 0.8305. This framework demonstrates to AI practitioners how unified multi-modal conditioning and progressive training strategies can enable state-of-the-art, high-fidelity, and controllable video synthesis in complex virtual try-on applications. |
| OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn
  Dialogue with Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.21061) or [HuggingFace](https://huggingface.co/papers/2508.21061))| Alex Endert, Eunyee Koh, Shunan Guo, Adam Coscia | OnGoal is an LLM chat interface augmenting multi-turn dialogue with goal-tracking visualizations to help users evaluate and review conversational goals. The primary objective was to assess how a linear chat interface can support users in managing conversational goals during extended LLM dialogues. This was achieved through a three-stage LLM-assisted goal pipeline (infer, merge, evaluate) utilizing GPT-40 and prompt engineering, integrated with in-situ visualizations, ex-situ timeline views, and text highlighting, and evaluated via a 20-participant user study comparing it to a baseline chat. Participants using OnGoal reported lower mental demand (2.7 vs 3.9 in baseline, strong evidence) and effort (3.2 vs 4.1 in baseline, weak evidence) for tasks, and spent less time reading (56.8s vs 66.5s in baseline, weak evidence) compared to a baseline interface. For AI practitioners, OnGoal demonstrates that integrating real-time goal tracking and visualization can enhance user engagement and reduce cognitive load, though the study identified that LLM-assisted goal evaluation accuracy (rated 2.9 by users) was significantly lower than goal inference/merging accuracy (rated 4.0). |
| Multi-View 3D Point Tracking (Read more on [arXiv](https://arxiv.org/abs/2508.21060) or [HuggingFace](https://huggingface.co/papers/2508.21060))| Irem Demir, Siyuan Li, Marko Mihajlovic, Haofei Xu, Frano Rajič | This research introduces MVTracker, a novel data-driven, feed-forward model for tracking arbitrary 3D points in dynamic scenes using multiple camera views. The primary objective is to develop an accurate online tracker that handles long-range correspondences and occlusions with a practical number of cameras (e.g., four), avoiding the depth ambiguities of monocular systems and the extensive hardware requirements of previous multi-view optimization methods. The methodology involves fusing per-view features into a unified 3D point cloud, leveraging a k-nearest-neighbors (kNN) correlation mechanism with explicit 3D offset vectors, and using a spatiotemporal transformer to iteratively refine trajectories. On the Panoptic Studio and DexYCB real-world benchmarks, MVTracker achieves median trajectory errors of 3.1 cm and 2.0 cm respectively, outperforming existing monocular and multi-view baselines. For AI practitioners, MVTracker provides a practical tool for robotics and dynamic scene reconstruction, enabling robust online 3D tracking in sparse camera setups without requiring per-sequence optimization. |
| FakeParts: a New Family of AI-Generated DeepFakes (Read more on [arXiv](https://arxiv.org/abs/2508.21052) or [HuggingFace](https://huggingface.co/papers/2508.21052))| Xi Wang, Awais Hussain Sani, Samy Aimeur, Soobash Daiboo, Gaetan Brison | This paper introduces FakeParts, a class of deepfakes with subtle, localized manipulations, and presents the FakePartsBench dataset to evaluate detectors against this emerging threat. The main objective is to define this new forgery class and systematically benchmark the performance of both human and state-of-the-art algorithmic detectors to expose current vulnerabilities. The methodology consists of creating the 25,000-video FakePartsBench dataset using spatial (e.g., inpainting), temporal (e.g., interpolation), and style-based manipulations, then evaluating multiple detection models and conducting a human perception study. The primary result shows that FakeParts reduce human detection accuracy by over 30% compared to traditional deepfakes and that algorithmic detectors exhibit a trade-off, with foundation-model-based systems handling partial fakes better while non-foundation models perform better on fully synthetic content. For AI practitioners, this research provides a critical benchmark dataset that highlights the urgent need to develop robust, generalized detection systems capable of identifying both fully synthetic videos and fine-grained partial manipulations. |
| Provable Benefits of In-Tool Learning for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.20755) or [HuggingFace](https://huggingface.co/papers/2508.20755))| Vivien Cabannes, Charles Arnal, Ambroise Odonnat, Sam Houliston | This paper demonstrates the provable and empirical benefits of in-tool learning over in-weight learning for factual recall in large language models (LLMs). The core objective was to formalize the tradeoff between internalizing knowledge via parameter updates and accessing external sources of truth, seeking the most efficient way for LLMs to acquire and utilize information. The authors established theoretical lower bounds for in-weight memorization and upper bounds for tool-augmented recall via a formal circuit construction, then validated these with controlled experiments on synthetic datasets using small transformers and fine-tuning pretrained LLMs. Results show that in-weight learning parameter requirements scale linearly with facts (empirically, y = 8.14x + 5171), whereas in-tool learning saturates parameter needs beyond approximately 1,000 facts, preserving general capabilities like HellaSwag accuracy (≥98%) with minimal distributional shift (TV distance < 0.04). This work implies that AI practitioners should prioritize developing LLMs with robust tool-use capabilities for scalable knowledge access and retention, rather than relying on rote memorization to expand model capacity. |
| Collaborative Multi-Modal Coding for High-Quality 3D Generation (Read more on [arXiv](https://arxiv.org/abs/2508.15228) or [HuggingFace](https://huggingface.co/papers/2508.15228))| Ziwei Liu, Liang Pan, Zhaoxi Chen, Ziang Cao | TriMM is a feed-forward 3D generative model that introduces a collaborative multi-modal coding scheme to unify photometric (RGB, RGBD) and geometric (point cloud) data into a shared latent space for high-quality asset generation. The primary objective is to overcome the limitations of single-modality 3D generative models by creating a framework that synergistically leverages the complementary strengths of diverse data sources to address training data scarcity. The key methodology involves using modality-specific encoders (e.g., DINOv2, PointNet) to map heterogeneous inputs into a unified triplane representation, which is then used to train a triplane latent diffusion model guided by a specialized reconstruction loss. On the OmniObject3D benchmark, TriMM, trained on 80K objects, achieves a PSNR of 14.13 and a Chamfer Distance of 0.096, outperforming models like TRELLIS which was trained on 500K objects. For AI practitioners, this framework provides a practical method to improve 3D generative model performance by integrating varied data types, offering a direct pathway to mitigate the challenge of limited high-quality 3D training data. |
| ROSE: Remove Objects with Side Effects in Videos (Read more on [arXiv](https://arxiv.org/abs/2508.18633) or [HuggingFace](https://huggingface.co/papers/2508.18633))| Hantang Liu, Zixiang Gao, Jianshu Zeng, Yutong Feng, Chenxuan Miao | ROSE is a diffusion transformer-based framework for video object removal that explicitly handles physical side effects like shadows and reflections by training on a large-scale, synthetically generated paired dataset. The primary objective is to develop a video object removal model capable of accurately eliminating not only the target object but also its environmental side effects (e.g., shadows, reflections, lighting changes), which existing methods struggle with due to the lack of physically-correct paired training data. The methodology involves: a fully-automatic pipeline using a 3D rendering engine to generate a synthetic dataset of paired videos that capture physical interactions; a reference-based erasing paradigm where the model is conditioned on the complete original video and mask; and an auxiliary difference mask predictor that provides explicit supervision for localizing all object-affected areas. On its synthetic paired benchmark designed to evaluate side effect removal, ROSE achieves a mean PSNR of 31.12, significantly outperforming prior methods like DiffuEraser (26.50). The principal implication for AI practitioners is that leveraging synthetic data from 3D rendering engines is an effective strategy for creating physically-grounded, paired datasets to train robust models for complex visual editing tasks, overcoming real-world data scarcity for phenomena like shadows and reflections. |
