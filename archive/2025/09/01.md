

## Papers for 2025-09-01

| Title | Authors | Summary |
|-------|---------|---------|
| R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs
  via Bi-Mode Annealing and Reinforce Learning (Read more on [arXiv](https://arxiv.org/abs/2508.21113) or [HuggingFace](https://huggingface.co/papers/2508.21113))| Han Hu, Shiming Xiang, Bolin Ni, Qi Yang, Jie Jiang | This paper presents R-4B, a multimodal large language model that adaptively engages a step-by-step "thinking" process based on query complexity. The primary objective is to create a computationally efficient MLLM that can autonomously switch between complex reasoning for difficult problems and direct responses for simple ones, thus reducing unnecessary computational overhead. The methodology involves a two-stage process: first, "bi-mode annealing" to train a base model on a curated mix of thinking and non-thinking data, followed by "Bi-mode Policy Optimization" (BPO), a reinforcement learning framework to teach the model when to activate the thinking mode. The resulting R-4B-RL model achieves state-of-the-art results, scoring 68.1% on the MMMU_val benchmark, outperforming comparable open-source models and achieving performance comparable to larger models on reasoning-intensive benchmarks. The principal implication for AI practitioners is that this auto-thinking framework provides a practical method to build more resource-efficient MLLMs that dynamically allocate reasoning resources, optimizing the trade-off between performance and inference cost. |
| EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for
  General Robot Control (Read more on [arXiv](https://arxiv.org/abs/2508.21112) or [HuggingFace](https://huggingface.co/papers/2508.21112))| Zhaoqing Chen, Qizhi Chen, Haoming Song, sundrops, delinqu | The paper introduces EO-1, a unified 3B parameter foundation model, and the EO-Data1.5M dataset, which leverages interleaved vision-text-action pretraining to enhance generalist robot control and embodied reasoning. The research objective is to design a training paradigm for robot policies that supports flexible and mutually-informed integration of reasoning and action. The key methodology is a unified decoder-only transformer architecture that synergizes discrete auto-regressive decoding for text with continuous flow matching for actions, trained on EO-Data1.5M, a new dataset of 1.5 million curated, interleaved vision-text-action sequences. EO-1 demonstrates superior performance on multiple benchmarks, achieving a 98.2% success rate on the LIBERO simulation benchmark, significantly outperforming prior state-of-the-art models. For AI practitioners, the principal implication is that pretraining on large-scale, carefully constructed *interleaved* multimodal data, rather than on siloed robotic and web datasets, is critical for developing vision-language-action models with robust open-world generalization and integrated reasoning capabilities. |
| A.S.E: A Repository-Level Benchmark for Evaluating Security in
  AI-Generated Code (Read more on [arXiv](https://arxiv.org/abs/2508.18106) or [HuggingFace](https://huggingface.co/papers/2508.18106))| Libo Chen, Lei Zhang, Bin Wang, wanng, KekeLian | This paper introduces A.S.E, a repository-level benchmark for evaluating the security, quality, and stability of LLM-generated code in realistic software engineering contexts. The research objective is to assess LLM security performance on complex, multi-file code generation tasks derived from real-world repositories with documented CVEs, addressing the limitations of snippet-based evaluations. Its methodology employs a reproducible, containerized evaluation framework that uses expert-defined static analysis rules and in-repository build validation to deterministically measure vulnerability remediation. Primary results from evaluating 26 models show that while a top model like Claude-3.7-Sonnet achieves a high code quality score of 91.58, it has a significant security deficit, scoring only 46.72 on security. The principal implication for AI practitioners is that current state-of-the-art LLMs generate functionally correct but insecure code, and concise "fast-thinking" decoding strategies outperform complex reasoning for security patching, highlighting the need for context-aware security validation before deployment. |
| Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation (Read more on [arXiv](https://arxiv.org/abs/2508.20470) or [HuggingFace](https://huggingface.co/papers/2508.20470))| Qi Jia, Liang Jin, Runze Zhang, Guoguang Du, lixiaochuan | The paper introduces Droplet3D, a framework that leverages commonsense priors from video data to enhance controllable 3D content generation from joint image and text inputs. The research objective is to mitigate 3D data scarcity by fine-tuning a pre-trained video generation model to inherit spatial consistency and rich semantic knowledge for 3D tasks. The core methodology involves training on Droplet3D-4M, a new large-scale dataset of 4 million 3D models, each paired with a 360Â° orbital rendered video and dense, multi-view-level text annotations. Droplet3D significantly outperforms prior methods in text-and-image-to-3D generation, achieving a PSNR of 28.36 on the GSO dataset, compared to the next-best baseline's 22.31. For AI practitioners, this work validates that adapting large video foundation models with curated multi-view 3D datasets is a powerful strategy for creating high-fidelity 3D assets with superior control and generalization, even extending to scene-level generation. |
| TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head
  Synthesis (Read more on [arXiv](https://arxiv.org/abs/2508.13618) or [HuggingFace](https://huggingface.co/papers/2508.13618))| Pengcheng Chen, Zihan Ye, Yexin Liu, Hejin Huang, Shunian Chen | This paper introduces TalkVid, a large-scale (1,244 hours, 7,729 speakers) and diverse dataset, alongside TalkVid-Bench, a stratified benchmark, to address generalization failures in audio-driven talking head synthesis. The primary objective is to mitigate the brittleness of state-of-the-art models when confronted with the full spectrum of human diversity in ethnicity, language, and age. The key methodology is a principled, multi-stage automated pipeline that sources high-resolution videos and rigorously filters them for motion stability, aesthetic quality, and facial detail, with the pipeline's efficacy validated against human judgments. Experiments show that a model trained on TalkVid achieves superior performance, recording a Frechet Video Distance (FVD) of 178.396 on the TalkVid-Bench language dimension, significantly better than models trained on prior datasets like HDTF (FVD 205.990). The principal implication for AI practitioners is that training on demographically diverse, high-quality data is essential for building robust and equitable models, while the provided benchmark enables crucial auditing of algorithmic bias across subgroups that aggregate metrics would otherwise obscure. |
| UItron: Foundational GUI Agent with Advanced Perception and Planning (Read more on [arXiv](https://arxiv.org/abs/2508.21767) or [HuggingFace](https://huggingface.co/papers/2508.21767))| Yufeng Zhong, Wenkang Han, Liming Zheng, Jing Huang, Zhixiong Zeng | This paper introduces Ultron, an open-source foundational model for GUI agents designed for advanced perception and planning across mobile and PC environments. The research aims to address key challenges in GUI agent development, including the scarcity of high-quality trajectory data, the lack of interactive infrastructure, and the poor performance of existing models in Chinese application scenarios. Ultron's methodology involves a three-stage training paradigm: supervised fine-tuning for perception and planning, followed by a curriculum reinforcement learning (CuRL) framework using Group Relative Policy Optimization (GRPO) to enable complex reasoning and exploration. On an offline evaluation benchmark for top-tier Chinese mobile apps, Ultron-72B achieves a 47.4% task success rate, significantly outperforming the UI-TARS-72B model's 32.8%. The principal implication for AI practitioners is that developing robust, real-world GUI agents requires a systemic approach combining targeted data engineering, the creation of interactive environments for reinforcement learning, and domain-specific data collection to overcome the limitations of general-purpose models. |
| Think in Games: Learning to Reason in Games via Reinforcement Learning
  with Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.21365) or [HuggingFace](https://huggingface.co/papers/2508.21365))| Yifan Lu, Zining Zhu, Yuan Sui, Yu Gu, Yi Liao | The paper introduces Think-In-Games (TiG), a framework using reinforcement learning to teach Large Language Models (LLMs) procedural knowledge for strategic decision-making in complex game environments. The research objective is to bridge the gap between an LLM's declarative knowledge and the procedural knowledge required for dynamic, interactive tasks by enabling the model to learn directly from environmental feedback. The methodology reformulates RL-based decision-making as a language modeling task, employing Group Relative Policy Optimization (GRPO) to iteratively refine language-guided policies based on a simple, rule-based binary reward signal derived from gameplay data. The framework enabled a Qwen-3-14B model to achieve 90.91% accuracy on the in-game action prediction task, outperforming the significantly larger Deepseek-R1 baseline (86.67%). The principal implication for AI practitioners is that online RL with simple, rule-based reward functions can efficiently instill domain-specific procedural reasoning in LLMs, allowing smaller, more deployable models to achieve superior performance in interactive applications. |
| A Survey of Scientific Large Language Models: From Data Foundations to
  Agent Frontiers (Read more on [arXiv](https://arxiv.org/abs/2508.21148) or [HuggingFace](https://huggingface.co/papers/2508.21148))| Jiamin Wu, Wanghan Xu, Wei Li, Chenglong Ma, Ming Hu | This paper surveys the evolution of scientific large language models (Sci-LLMs), reframing their development as a co-evolution between models and their underlying scientific data substrates. The objective is to provide a data-centric synthesis of the Sci-LLM landscape by formulating a unified taxonomy of scientific data, reviewing models and datasets, and outlining a roadmap toward autonomous agentic systems. The methodology involves a systematic review and meta-analysis of over 270 pre-/post-training datasets and over 190 evaluation benchmarks, alongside formulating a novel hierarchical model of scientific knowledge. The analysis reveals that the current landscape is dominated by text-only models (approx. 74%), with 7B parameter models being the most common size (32%), and shows that leading LLMs' performance drops from over 80% on general benchmarks to as low as 2-10% on expert-level scientific reasoning tests. The principal implication for AI practitioners is that progress requires a shift from scaling generalist models to developing specialized systems that can handle heterogeneous scientific data and function as autonomous agents within a closed-loop discovery process. |
| TiKMiX: Take Data Influence into Dynamic Mixture for Language Model
  Pre-training (Read more on [arXiv](https://arxiv.org/abs/2508.17677) or [HuggingFace](https://huggingface.co/papers/2508.17677))| Jiyao Deng, Yuanfan Guo, Fengze Liu, Binbin Liu, Yifan Wang | TiKMiX is a framework that dynamically adjusts data mixtures in LLM pre-training by using a "Group Influence" metric to optimize the evolving impact of data domains on model performance. The research objective is to develop a computationally efficient method to dynamically adjust data mixture proportions during pre-training to align with the model's changing learning preferences, thereby improving final performance. The methodology introduces Group Influence, an extension of influence functions that calculates a data domain's collective impact using accumulated gradients, and uses it in two schemes: TiKMiX-D for direct multi-objective optimization of influence, and TiKMiX-M, which trains a LightGBM surrogate model to predict optimal mixtures by modeling non-linear interactions. Primary results demonstrate that Group Influence strongly correlates with downstream performance (Pearson Ï = 0.789); the TiKMiX-M variant achieved an average performance gain of 2.0 points across nine benchmarks over the REGMIX baseline, while the TiKMiX-D variant performed comparably while using only 20% of the computational resources. The principal implication for AI practitioners is that Group Influence offers a computationally efficient diagnostic to periodically re-weight pre-training data, significantly improving model performance and training efficiency by better aligning the data mixture with the model's state, mitigating the "under-digestion" of data from static ratios. |
| Efficient Code Embeddings from Code Generation Models (Read more on [arXiv](https://arxiv.org/abs/2508.21290) or [HuggingFace](https://huggingface.co/papers/2508.21290))| Han Xiao, Scott Martens, Michael GÃ¼nther, Saba Sturua, dariakryvosheieva | This paper introduces `jina-code-embeddings`, a family of efficient code embedding models (0.5B and 1.5B parameters) derived from fine-tuning autoregressive code generation models. The primary objective is to create compact, high-performance embedding models specifically for code retrieval tasks by adapting pre-trained decoder-only backbones. The methodology involves initializing models with pre-trained Qwen2.5-Coder weights and fine-tuning them using a contrastive InfoNCE loss objective on diverse code-text pairs, employing task-specific instruction prefixes and last-token pooling for embedding generation. The resulting 1.5B parameter model achieves an average score of 79.04% on the MTEB code retrieval benchmark, outperforming larger general-purpose models like `gemini-embedding-001` (77.38%). The principal implication for AI practitioners is that these smaller, specialized models enable the development of resource-efficient yet state-of-the-art code retrieval systems, such as for RAG applications, without the significant overhead of larger models. |
| Morae: Proactively Pausing UI Agents for User Choices (Read more on [arXiv](https://arxiv.org/abs/2508.21456) or [HuggingFace](https://huggingface.co/papers/2508.21456))| Amy Pavel, Jeffrey P. Bigham, Dingzeyu Li, Yi-Hao Peng | This paper introduces Morae, an accessible UI agent that proactively pauses automation to allow blind and low-vision (BLV) users to make choices. The research objective is to address the reduced user agency caused by existing UI agents that fully automate tasks without consulting users at critical decision points. Morae employs a large multimodal model to interpret user queries against UI representations and uses a "Dynamic Verification of Ambiguous Choices" mechanism to identify when to pause and generate interactive UIs for user input. In a study with 10 BLV participants, Morae enabled users to make significantly more preference-aligned choices (mean of 4.03) compared to OpenAI Operator (mean of 2.98). The principal implication for AI practitioners is that incorporating mixed-initiative models which proactively detect ambiguity and solicit user clarification, rather than aiming for complete end-to-end automation, is crucial for developing more effective and empowering UI agents. |
| AHELM: A Holistic Evaluation of Audio-Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.21376) or [HuggingFace](https://huggingface.co/papers/2508.21376))| Siwei Yang, Zijun Wang, Chi Heem Wong, Haoqin Tu, Tony Lee | This paper introduces AHELM, a holistic benchmark to systematically evaluate Audio-Language Models (ALMs) across 10 aspects including capabilities, fairness, and safety. The research objective is to create a standardized evaluation framework to address the limitations of existing benchmarks, which typically measure only one or two capabilities and lack consistent testing protocols. The methodology involves aggregating 14 datasets and introducing two new ones (PARADE for bias, CoRe-Bench for reasoning), standardizing prompts and inference parameters, and evaluating 14 ALMs against 3 baseline systems composed of an Automatic Speech Recognizer (ASR) paired with a Language Model (LM). The results show that while Gemini 2.5 Pro is the top-ranked model, it exhibits group unfairness (p=0.01) on ASR tasks, and baseline ASR+LM systems perform competitively, with one ranking 5th overall, highlighting that end-to-end ALMs are not universally superior. The key implication for AI practitioners is that for many speech-based tasks, a simpler, engineered system combining a dedicated ASR with an LM can be more robust and performant than a single, complex ALM, necessitating careful comparative benchmarking before deployment. |
| HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data
  for Mobile Dexterous Manipulation (Read more on [arXiv](https://arxiv.org/abs/2508.20085) or [HuggingFace](https://huggingface.co/papers/2508.20085))| Tianhai Liang, Pu Hua, Langzhe Gu, Tianming Wei, Zhecheng Yuan | HERMES is a framework for mobile bimanual dexterous manipulation that learns from single-shot, multi-source human motion data using reinforcement learning and a robust vision-based sim2real transfer pipeline. The primary objective is to translate heterogeneous human motion data into deployable, physically plausible policies for a mobile dexterous robot, enabling autonomous execution of complex, long-horizon manipulation tasks in unstructured real-world environments. The framework employs a unified reinforcement learning approach with a generalizable reward function to train a state-based expert policy from a single human motion trajectory, which is then distilled into a vision-based student policy via DAgger using augmented depth images; this is integrated with a navigation foundation model refined by a closed-loop Perspective-n-Point (PnP) localizer for precise mobile manipulation. HERMES successfully executed diverse real-world bimanual dexterous manipulation tasks, achieving an average success rate of 67.8%, which represents a +54.5% performance gain compared to a baseline using unprocessed depth inputs. The principal implication for AI practitioners is that combining a generalizable RL reward with DAgger distillation to a depth-image policy and a hybrid control scheme is an effective strategy for transferring skills from minimal human data to high-DoF robots, providing a practical pipeline for sim2real deployment in complex mobile manipulation scenarios. |
| CLIPSym: Delving into Symmetry Detection with CLIP (Read more on [arXiv](https://arxiv.org/abs/2508.14197) or [HuggingFace](https://huggingface.co/papers/2508.14197))| Raymond A. Yeh, Md Ashiqur Rahman, Tinghan Yang | CLIPSym is a framework leveraging the pre-trained CLIP model to achieve state-of-the-art performance in image symmetry detection. The main objective is to determine how a pre-trained vision-language model can be effectively adapted for the geometric task of detecting reflection and rotation symmetries. The key methodology involves using CLIP's image and text encoders with a novel Semantic-Aware Prompt Grouping (SAPG) technique, which aggregates diverse object-based prompts, and a rotation-equivariant decoder based on a Transformer and G-Convolution to generate symmetry heatmaps. The primary result is that CLIPSym outperforms previous methods on the DENDI dataset, achieving an F1-score of 66.5% for reflection detection, which is a 2.0% improvement over the prior state-of-the-art. The principal implication for AI practitioners is that large pre-trained vision-language models can be successfully fine-tuned for specialized geometric tasks, and that performance can be significantly enhanced by combining principled equivariant decoder architectures with sophisticated prompting strategies that leverage the model's semantic understanding. |
