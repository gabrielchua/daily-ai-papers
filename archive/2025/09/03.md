

## Papers for 2025-09-03

| Title | Authors | Summary |
|-------|---------|---------|
| The Landscape of Agentic Reinforcement Learning for LLMs: A Survey (Read more on [arXiv](https://arxiv.org/abs/2509.02547) or [HuggingFace](https://huggingface.co/papers/2509.02547))| Hejia Geng, Guibin Zhang, henggg, Artemis0430, JeremyYin | This survey formalizes Agentic Reinforcement Learning (Agentic RL) as a paradigm that reframes LLMs from static sequence generators into autonomous, decision-making agents optimized for sequential tasks in dynamic environments. The paper's main objective is to define and structure this emerging field by synthesizing over 500 recent works, contrasting the multi-step, partially observable Markov decision process (POMDP) of Agentic RL with the degenerate single-step MDP of traditional LLM-RL. Its methodology involves a systematic literature review to construct a twofold taxonomy based on core agentic capabilities (e.g., planning, tool use, memory) and application domains, while cataloging relevant RL algorithm families like PPO, DPO, and GRPO. The survey consolidates results demonstrating Agentic RL's effectiveness, citing findings such as DeepCoder-14B achieving a +8% Pass@1 gain on LiveCodeBench by using outcome-based rewards. The principal implication for AI practitioners is to approach LLM training not just as single-turn preference alignment but as the development of a learnable policy for long-horizon, interactive tasks, enabling more robust and autonomous agentic behavior through direct optimization in complex environments. |
| UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.02544) or [HuggingFace](https://huggingface.co/papers/2509.02544))| Haoyang Zou, zhwang4ai, JoeYing, jzfeng, MingComplex | This paper presents UI-TARS-2, a native GUI-centered agent advanced through a systematic methodology combining a data flywheel and multi-turn reinforcement learning. The research aims to solve open problems in GUI agent development, including data scarcity, scalable multi-turn RL, the limitations of GUI-only operation, and environment instability. Its key methodology integrates four pillars: a data flywheel for scalable data generation, a stabilized multi-turn RL framework using enhanced proximal policy optimization (PPO), a hybrid GUI environment integrating file systems and terminals, and a unified sandbox platform for large-scale rollouts. Empirical evaluation shows UI-TARS-2 achieves state-of-the-art performance, reaching 88.2 on the Online-Mind2Web benchmark and demonstrating strong out-of-domain generalization. The principal implication for AI practitioners is that this systematic approach, particularly the data flywheel and stabilized RL infrastructure, provides an effective methodology for training robust, generalizable GUI agents capable of handling diverse and complex real-world interactive scenarios. |
| SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn
  Tool-Integrated Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.02479) or [HuggingFace](https://huggingface.co/papers/2509.02479))| Qian Liu, Longtao Zheng, Zhenghai Xue, xszheng2020, R1ch0rd | SimpleTIR is a plug-and-play algorithm that stabilizes multi-turn Tool-Integrated Reasoning (TIR) training via Reinforcement Learning (RL) by filtering problematic trajectories. The primary objective is to address the training instability and performance collapse in multi-turn TIR, which is caused by distributional drift from external tool feedback leading to low-probability tokens and gradient norm explosions, without requiring a supervised fine-tuning (SFT) "cold-start". The core methodology involves identifying and filtering out entire trajectories that contain "void turns"—defined as LLM responses that yield neither a complete code block nor a final answer—thus preventing high-magnitude gradients from backpropagating during the policy update. The primary result shows that SimpleTIR elevates the AIME24 score from a text-only baseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model. For AI practitioners, the principal implication is that implementing a simple, heuristic-based trajectory filtering rule to remove incomplete or non-progressive conversational turns can effectively stabilize end-to-end RL training for tool-using agents, enabling the development of more robust and diverse reasoning capabilities directly from base models. |
| ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long
  Video Understanding (Read more on [arXiv](https://arxiv.org/abs/2508.21496) or [HuggingFace](https://huggingface.co/papers/2508.21496))| Xuanyu Zheng, Ruohui Wang, Mercury7353, datamonkey, HLSv | This paper introduces ELV-Halluc, a benchmark for evaluating Semantic Aggregation Hallucination (SAH) in long-video understanding, where models misattribute correctly perceived frame-level semantics across different temporal events. The primary objective is to systematically measure and mitigate SAH, which becomes more critical as video length and semantic complexity increase. The methodology involves creating a benchmark with an adversarial triplet question-pair design (ground-truth, in-video hallucination, out-of-video hallucination) to isolate and quantify SAH, and then using Direct Preference Optimization (DPO) on a curated 8K-pair dataset to reduce it. The primary result is that this DPO-based approach successfully mitigated SAH, achieving a 27.7% reduction in the SAH ratio on a Qwen2.5-VL-7B model while also improving general video understanding on the VideoMME benchmark. The principal implication for AI practitioners is that to improve the reliability of long-video models, it is crucial to employ targeted mitigation strategies like DPO with intra-video adversarial examples, as simply increasing frame sampling can sometimes worsen this specific type of hallucination. |
| LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model (Read more on [arXiv](https://arxiv.org/abs/2509.00676) or [HuggingFace](https://huggingface.co/papers/2509.00676))| Jianwei Yang, Chunyuan Li, Benjamin-eecs, drogozhang, russwang | This paper demonstrates that training a vision-language model on critic data via reinforcement learning simultaneously produces a strong policy model, unifying evaluation and generation capabilities. The research investigates whether a model trained for critic tasks, such as judging response preferences, can also excel as a generative policy model across diverse benchmarks. The key methodology involves reformulating pairwise preference critic datasets into a verifiable RL task and fine-tuning a base generative model (Qwen-2.5-VL-7B) using Group Relative Policy Optimization (GRPO), thereby creating LLaVA-Critic-R1. The primary result is that LLaVA-Critic-R1 not only excels as a critic but also improves as a policy model, achieving an average performance gain of +5.7% over its base model across 26 visual reasoning benchmarks. For AI practitioners, the principal implication is that a single, unified model can be trained to perform both generation and self-evaluation, offering a simplified and scalable approach to building self-improving multimodal systems that benefit from effective test-time scaling via self-critique without an external evaluator. |
| POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models
  for Document Conversion (Read more on [arXiv](https://arxiv.org/abs/2509.01215) or [HuggingFace](https://huggingface.co/papers/2509.01215))| Haicheng Wang, Le Tian, Zhongyin Zhao, YxxxB, YuanLiuuuuuu | This paper introduces a two-stage, distillation-free framework using synthetic data and iterative self-improvement to train a vision-language model for document conversion. The objective is to create a fully automated pipeline for generating high-quality training data to accurately extract plain text, tables, and mathematical formulas without relying on teacher models. The methodology involves an initial warm-up stage training on large-scale synthetic data, followed by an iterative self-improvement stage where the model annotates real-world documents, which are then filtered using rule-based strategies (e.g., F1-score for text, structural validation for tables) and used for retraining. The resulting 3B parameter POINTS-Reader model achieves a score of 0.259 on the OmniDocBench benchmark (lower is better), and significantly outperforms the expert GOT-OCR model on the table metric by 0.197 (0.335 vs 0.532). For AI practitioners, this work demonstrates a viable, resource-efficient methodology to bootstrap high-quality, specialized document understanding models by leveraging unlabeled real-world data and rule-based filtering, reducing dependence on large-scale proprietary models. |
| VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use (Read more on [arXiv](https://arxiv.org/abs/2509.01055) or [HuggingFace](https://huggingface.co/papers/2509.01055))| Zhiheng Lyu, Zhuofeng Li, Yi Lu, JasperHaozhe, DongfuJiang | VERLTOOL is a unified, modular framework designed to efficiently train tool-using LLM agents via Agentic Reinforcement Learning with Tool use (ARLT) across diverse multi-modal domains. The primary objective is to overcome the fragmentation, synchronous execution bottlenecks, and limited extensibility of existing ARLT systems by providing a unified and efficient training infrastructure. The methodology is centered on a decoupled architecture featuring a dedicated tool server with standardized APIs, a modular plugin design for easy tool integration, and asynchronous rollout execution to eliminate idle waiting time during training. VERLTOOL demonstrates competitive performance across six ARLT tasks and its asynchronous architecture achieves a near 2x speedup (e.g., 1.97x on the DeepSearch task) in rollout execution compared to synchronous methods. For AI practitioners, VERLTOOL offers a scalable, open-source infrastructure that reduces development overhead and accelerates the training of multi-modal, tool-augmented agents through its efficient, extensible, and modular design. |
| Baichuan-M2: Scaling Medical Capability with Large Verifier System (Read more on [arXiv](https://arxiv.org/abs/2509.02208) or [HuggingFace](https://huggingface.co/papers/2509.02208))| Jayok6, yuanshuai, sdujq, anselcmy, fairyang | This paper presents Baichuan-M2, a 32B-parameter medical LLM trained via a dynamic reinforcement learning framework that simulates real-world clinical interactions. The primary objective is to address the performance gap between medical LLMs on static benchmarks and their utility in dynamic clinical decision-making by creating a high-fidelity, interactive training environment. The key methodology involves a novel verifier system comprising a `Patient Simulator` built from de-identified medical records and a `Clinical Rubrics Generator` for dynamic, multi-dimensional evaluation, using an improved Group Relative Policy Optimization (GRPO) algorithm for training. The primary result is that Baichuan-M2 outperforms all other open-source models on HealthBench, achieving a score of 34.7 on the HealthBench Hard benchmark, a threshold previously surpassed only by GPT-5. The principal implication for AI practitioners is that developing domain-specific, interactive simulation environments for reinforcement learning is crucial for aligning LLM capabilities with complex, real-world applications, offering a more effective path to high performance than reliance on static datasets alone. |
| Kwai Keye-VL 1.5 Technical Report (Read more on [arXiv](https://arxiv.org/abs/2509.01563) or [HuggingFace](https://huggingface.co/papers/2509.01563))| SXxtyz, Chengru, bhsc24, dingboyang, biaoYang | This report presents Keye-VL-1.5, an 8-billion parameter multimodal model optimized for video understanding. The primary objective was to overcome the inherent trade-off between spatial resolution and temporal coverage in video processing within Multimodal Large Language Models (MLLMs). Key methodology includes a novel Slow-Fast video encoding strategy that dynamically allocates computational resources based on inter-frame similarity, a progressive four-stage pre-training to extend context length to 128K tokens, and a comprehensive post-training pipeline using GSPO-based reinforcement learning. The model achieves state-of-the-art performance on video-centric benchmarks, notably scoring 66.0% on Video-MMMU, outperforming comparable models by over 6.5 absolute percentage points. For AI practitioners, the Slow-Fast encoding strategy provides a computationally efficient method for processing long-form video, enabling MLLMs to better handle dynamic, information-dense visual content. |
| Implicit Actor Critic Coupling via a Supervised Learning Framework for
  RLVR (Read more on [arXiv](https://arxiv.org/abs/2509.02522) or [HuggingFace](https://huggingface.co/papers/2509.02522))| Lu Wang, Yukun Chen, Ze Gong, Longze Chen, Geaming | The paper proposes PACS, a framework that reformulates Reinforcement Learning with Verifiable Rewards (RLVR) as a supervised learning task to improve LLM reasoning capabilities. The objective is to address the sparse reward signals and unstable policy updates common in existing RL-based RLVR methods. PACS achieves this by treating the verifiable outcome reward as a target label and training a score function, parameterized by the policy model, to predict this reward using a cross-entropy loss, which a gradient analysis shows implicitly couples actor and critic roles. On the AIME 2024 benchmark with a Qwen2.5-7B model, PACS achieved a 59.78% pass@256 rate, outperforming PPO and GRPO by 13.32 and 14.36 percentage points respectively. For AI practitioners, this research offers a simpler, more stable, and higher-performing alternative to complex RL algorithms for post-training LLMs on tasks with verifiable outcomes. |
| DCPO: Dynamic Clipping Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2509.02333) or [HuggingFace](https://huggingface.co/papers/2509.02333))| Kai Lu, Chengfeng Dou, sdujq, GuoPD, yangshui | DCPO (Dynamic Clipping Policy Optimization) is a reinforcement learning algorithm that enhances the reasoning capabilities of large language models by improving data utilization and exploration efficiency in RLVR. The primary objective is to overcome the zero-gradient and sample inefficiency issues inherent in methods like GRPO, which are caused by fixed clipping bounds and per-step reward standardization. DCPO's methodology integrates a dynamic clipping strategy that adaptively adjusts bounds based on token-specific probabilities and a smooth advantage standardization technique that aggregates rewards across cumulative training steps for more stable updates. The proposed method demonstrated superior performance, achieving an Avg@32 score of 38.8 on the AIME24 benchmark with a 7B model, significantly outperforming GRPO (32.1) and DAPO (31.6), and increasing the average response utilization ratio by 28% over GRPO. For AI practitioners, DCPO offers a more data-efficient and robust framework for RL fine-tuning, enabling the development of stronger reasoning models by mitigating training instability and making better use of generated samples. |
| Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task
  Arithmetic (Read more on [arXiv](https://arxiv.org/abs/2509.01363) or [HuggingFace](https://huggingface.co/papers/2509.01363))| Bernard Ghanem, Mohammad Zbeeb, hammh0a | This work demonstrates that reasoning capabilities can be extracted as a compact task vector from a reinforcement learning-tuned model and transferred to compatible models via simple tensor arithmetic. The research investigates if reasoning abilities learned via reinforcement learning can be isolated from shared knowledge and reused as a transferable task vector. The methodology defines a "reasoning vector" as the parameter-space difference between two identically initialized models, one trained with Group Relative Policy Optimization (GRPO) and the other with Supervised Fine-Tuning (SFT) on the same dataset (`v_reason = θ_GRPO - θ_SFT`). Primary results show that adding this vector to a 1.5B QWEN2.5 model improved performance on GSM8K by +4.9% and BigBenchHard by +12.3%, while subtracting the vector degraded GSM8K performance by -11.8%. For AI practitioners, this provides a computationally inexpensive, training-free method to enhance the reasoning of compatible models by arithmetically applying pre-computed vectors from existing open-source checkpoints. |
| GenCompositor: Generative Video Compositing with Diffusion Transformer (Read more on [arXiv](https://arxiv.org/abs/2509.02460) or [HuggingFace](https://huggingface.co/papers/2509.02460))| Lingen Li, Guangzhi Wang, Xiaodong Cun, Xiaoyu521, Ysz2022 | GenCompositor introduces a generative video compositing framework using a Diffusion Transformer (DiT) to automate the integration of dynamic foreground videos into background videos with user-specified trajectories and scales, while preserving background consistency. The main objective is to create a model that can adaptively inject foreground identity and motion information into a target video in an interactive manner. The core methodology is a DiT pipeline featuring a background preservation branch for consistency, a full self-attention DiT fusion block for integrating dynamic elements, and a novel Extended Rotary Position Embedding (EROPE) to handle pixel-unaligned video inputs. The model demonstrates superior performance over existing solutions, achieving a PSNR of 42.0010 on video harmonization tasks, and introduces VideoComp, a new 61K-video dataset curated for this task. The principal implication for AI practitioners is the EROPE technique, which provides an effective, parameter-free method for generative models to handle layout-unaligned video conditions, enabling more flexible and powerful automated video editing tools. |
| Jointly Reinforcing Diversity and Quality in Language Model Generations (Read more on [arXiv](https://arxiv.org/abs/2509.02534) or [HuggingFace](https://huggingface.co/papers/2509.02534))| Tianlu, jcklcn, spermwhale, danyaljj, dogtooth | The paper introduces Diversity-Aware Reinforcement Learning (DARLING), an online RL framework that jointly optimizes for response quality and semantic diversity in large language models. The main objective is to counteract the loss of output diversity that occurs during standard LM post-training by developing a method to simultaneously reinforce high-quality and semantically distinct generations. DARLING's key methodology involves using a learned semantic classifier to generate a diversity signal from model rollouts, which is then multiplicatively combined with a quality reward within a Group Relative Policy Optimization (GRPO) framework to amplify updates for novel, high-quality responses. Primary results show that on verifiable competition math tasks, DARLING improved both solution quality (pass@1) and variety (pass@k), outperforming a quality-only GRPO baseline by an average of +3.51% on pass@1 and +7.62% on pass@128 for Qwen3-4B-Base models. The principal implication for AI practitioners is that this method can be integrated into post-training pipelines to mitigate diversity collapse, enhancing model performance in creative, exploratory, and multi-path problem-solving tasks without sacrificing response quality. |
| OpenVision 2: A Family of Generative Pretrained Visual Encoders for
  Multimodal Learning (Read more on [arXiv](https://arxiv.org/abs/2509.01644) or [HuggingFace](https://huggingface.co/papers/2509.01644))| Zirui Wang, Letian Zhang, Xianhang Li, Yanqing Liu, cihangxie | OpenVision 2 introduces a family of visual encoders pretrained using a purely generative captioning objective, simplifying its predecessor by removing the text encoder and contrastive loss. The main objective is to evaluate if this generative-only paradigm can match the multimodal performance of combined contrastive-generative models while significantly enhancing training efficiency. Its methodology consists of an image encoder feeding visual tokens, with approximately two-thirds randomly masked, directly to a text decoder trained autoregressively to predict high-quality synthetic captions. The primary result is competitive performance with substantial efficiency gains; for instance, the ViT-L/14 model reduces training time by ~1.5x (from 83h to 57h) and memory usage by ~1.8x (from 24.5GB to 13.8GB) compared to the original OpenVision. The principal implication for AI practitioners is that a caption-only generative objective is a computationally efficient and effective alternative to CLIP-style contrastive learning for building scalable, general-purpose vision encoders, lowering the resource barrier for training large multimodal models. |
| M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via
  Self-Supervision (Read more on [arXiv](https://arxiv.org/abs/2509.01360) or [HuggingFace](https://huggingface.co/papers/2509.01360))| Yan-Jie Zhou, Heng Guo, Chengyu Fang, Zheng Jiang, Che Liu | M³Ret is a unified visual encoder trained via self-supervision on a large-scale, hybrid-modality medical dataset to achieve zero-shot image retrieval without modality-specific designs. The primary objective is to determine if a single framework can learn transferable visual representations across heterogeneous 2D (X-ray, ultrasound), 3D (CT), and video (endoscopy) data using only visual signals. The methodology involves pretraining a Vision Transformer on a curated dataset of 867,653 medical images using Masked Autoencoder (MAE) and SimDINO self-supervised learning paradigms with a unified 4D patchification input strategy. The model sets a new state-of-the-art, with the SimDINO variant achieving a Recall@5 of 0.674 on ChestXray14, and demonstrates strong cross-modal generalization by performing retrieval on unseen MRI tasks despite no MRI exposure during pretraining. The principal implication for AI practitioners is that a single, modality-agnostic visual encoder can be successfully pretrained without paired text data or specialized architectures, offering a scalable and effective pathway for building foundational models for medical image understanding. |
| Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm
  Simulators for Conditional Synthetic Data Generation (Read more on [arXiv](https://arxiv.org/abs/2509.02040) or [HuggingFace](https://huggingface.co/papers/2509.02040))| Xiaolei Huang, Weisi Liu, kwangju | This paper introduces Genetic Prompt, a framework using LLMs to simulate genetic algorithms on semantic text attributes for high-quality synthetic data generation. The primary objective is to automatically amplify the diversity and generator adaptability of synthetic data to improve the training of robust downstream models. The key methodology treats textual attributes like readability and style as "genes," employs an active learning strategy to select semantically distant parent samples, and prompts an LLM to perform crossover and mutation on these genes to create new data. The primary result shows consistent outperformance over baselines; for example, on the Conll04 relation extraction task using a GPT-4o generator, Genetic Prompt achieved a Micro-F1 score of 85.3, significantly outperforming the next-best baseline's 73.3. For AI practitioners, this framework provides a robust method to augment datasets, especially in class-imbalanced scenarios, to significantly boost downstream model performance by generating diverse, high-quality training examples. |
| Benchmarking Optimizers for Large Language Model Pretraining (Read more on [arXiv](https://arxiv.org/abs/2509.01440) or [HuggingFace](https://huggingface.co/papers/2509.01440))| mjaggi, MatPag, Andron00e | This paper presents a comprehensive benchmark of 11 optimization methods for Large Language Model pretraining across various model sizes, batch sizes, and training durations. The main objective is to systematically evaluate recent optimization techniques against the dominant AdamW baseline in standardized LLM pretraining scenarios to identify the most effective methods and provide guidance to practitioners. The methodology involves pretraining Llama-like dense and Mixture-of-Experts models (from 124M to 720M parameters) on the FineWeb dataset, with extensive and controlled hyperparameter tuning for each optimizer across different training compute budgets. Primary results show that AdEMAMix and MARS consistently outperform AdamW and other optimizers, particularly at larger scales; for a 720M parameter model trained on 48B tokens, AdEMAMix achieves the lowest final validation loss of approximately 2.8. The principal implication for AI practitioners is that AdamW is no longer the default optimal choice for LLM pretraining; alternatives like AdEMAMix can provide superior performance, and this paper offers an evidence-based framework and tuned configurations to select a better optimizer for a given training scenario. |
| The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in
  LLMs with Camlang (Read more on [arXiv](https://arxiv.org/abs/2509.00425) or [HuggingFace](https://huggingface.co/papers/2509.00425))| Solomon Tsai, Zhujun Jin, Yixuan Liu, Fenghua Liu, yulongchen | This paper introduces Camlang, a novel constructed language, to demonstrate that state-of-the-art LLMs fail at metalinguistic deductive reasoning, in contrast to humans who can systematically apply explicit grammar rules to learn a new language. The main objective is to determine whether LLMs can learn and apply explicit grammatical rules for an unfamiliar language to perform reasoning, or if their success relies on pattern matching from training data. The key methodology involves the creation of Camlang, a typologically plausible but novel language, accompanied by a grammar book and dictionary, and the development of the Camlang-CSQA-v0 benchmark by translating CommonsenseQA questions. Primary results show that while GPT-5 achieves 98% accuracy in English, its performance drops to 47% in Camlang, far below the human baseline of 87%; human verification further reveals that models achieve near-zero (0-2.13%) Strict Human-Verified accuracy, indicating correct answers stem from shallow heuristics, not grammatical mastery. The principal implication for AI practitioners is that current LLMs cannot be relied upon to systematically interpret and apply novel, explicit rule sets (e.g., API documentation, legal text, game rules), as they fundamentally struggle with the deductive metalinguistic competence required for such tasks. |
| Fantastic Pretraining Optimizers and Where to Find Them (Read more on [arXiv](https://arxiv.org/abs/2509.02046) or [HuggingFace](https://huggingface.co/papers/2509.02046))| Percy Liang, Tengyu Ma, David Hall, Kaiyue Wen | This paper systematically benchmarks ten pretraining optimizers, revealing that their speedups over a well-tuned AdamW baseline are significantly smaller than reported and diminish with increasing model scale. The main objective is to conduct a fair comparison of modern optimizers for large language model pretraining by addressing methodological flaws in prior work, specifically unequal hyperparameter tuning and limited evaluation setups. The methodology consists of a rigorous three-phase hyperparameter tuning framework, performing coordinate descent sweeps across ten optimizers on four model scales (0.1B-1.2B parameters) and four data-to-model ratios (1–8× the Chinchilla optimum). The primary result is that the speedup of matrix-based optimizers is inversely proportional to model scale, decreasing from a 1.4× speedup over AdamW for 0.1B parameter models to merely 1.1× for 1.2B parameter models. The principal implication for AI practitioners is that reported speedups of new optimizers should be treated with skepticism; rigorous, independent hyperparameter tuning for both the baseline and new optimizers is crucial, as undertuned baselines account for most of the claimed performance gains. |
| Universal Deep Research: Bring Your Own Model and Strategy (Read more on [arXiv](https://arxiv.org/abs/2509.00244) or [HuggingFace](https://huggingface.co/papers/2509.00244))| Pavlo Molchanov, Peter Belcak | The paper introduces Universal Deep Research (UDR), a generalist agentic framework that translates user-defined natural language research strategies into executable code to control any underlying language model for structured information retrieval. The main objective is to overcome the rigidity of existing deep research agents by allowing users to create and refine custom research strategies without model fine-tuning. UDR's methodology uses an LLM to generate a complete Python script from the user's strategy in a single pass; this script is then run in a sandboxed environment to orchestrate tool use and specific LLM reasoning calls. The primary result is a system that decouples agentic orchestration from core reasoning, successfully executing complex workflows within a constant context length of just 8k tokens. For AI practitioners, the principal implication is an architectural pattern for building more efficient, deterministic, and auditable agents by offloading control logic to CPU-executable code, which minimizes expensive LLM orchestration calls and reduces GPU usage. |
| FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in
  Diverse Adventure Games (Read more on [arXiv](https://arxiv.org/abs/2509.01052) or [HuggingFace](https://huggingface.co/papers/2509.01052))| Dongmin Park, Jaehyeon Son, Heeseung Yun, Junseo Kim, ahnpersie | This research introduces FlashAdventure, a benchmark of 34 adventure games for evaluating GUI agents on full story arcs, and proposes the COAST framework to address the long-term observation-behavior gap. The main objective is to assess the capability of LLM-powered GUI agents to complete entire narrative-driven story arcs and to address the challenge of managing long-term dependencies between information observation and subsequent action. The key methodology involves the FlashAdventure benchmark, an automated evaluator named CUA-as-a-Judge, and a novel agentic framework, COAST (Clue-Oriented Agent for Sequential Tasks), which utilizes a Seek-Map-Solve cycle with a long-term clue memory to generate and execute subtasks. Primary results show that current GUI agents demonstrate near-zero success rates on full story arcs; the proposed COAST framework improved the milestone completion rate by 2.78 percentage points over the Claude-3.7-Sonnet baseline, achieving 19.89%, yet this remains significantly lower than the human performance benchmark (97.06% success rate). The principal implication for AI practitioners is that developing robust GUI agents for complex sequential tasks requires explicit architectural designs for long-term memory management and planning, as current models struggle with the "observation-behavior gap"; practitioners should consider structured approaches like COAST's clue-oriented cycle rather than relying solely on large context windows. |
| Discrete Noise Inversion for Next-scale Autoregressive Text-based Image
  Editing (Read more on [arXiv](https://arxiv.org/abs/2509.01984) or [HuggingFace](https://huggingface.co/papers/2509.01984))| Amin Heyrani Nobar, Ngan Hoai Nguyen, Ligong Han, Xiaoxiao He, quandao10 | This paper introduces VARIN, the first training-free, noise inversion-based editing technique specifically designed for discrete next-scale visual autoregressive (VAR) models. The main objective is to enable prompt-guided image editing in VARs by developing a method to invert their non-differentiable argmax sampling process, thereby allowing for precise image reconstruction and controlled modification. The key methodology is a novel pseudo-inverse function called Location-aware Argmax Inversion (LAI), which estimates inverse Gumbel noises from a source image's token maps; these recovered noises are then used to guide the generative process toward a target prompt. In experiments on the PIE-Bench dataset, VARIN achieved a Whole CLIP Similarity of 25.05, outperforming the discrete model DICE (23.79) in edit alignment while being approximately twice as fast. For AI practitioners, this provides a method to perform efficient, training-free text-based editing on next-scale autoregressive architectures like HART, offering a computationally faster alternative to many diffusion-based editing workflows. |
| MobiAgent: A Systematic Framework for Customizable Mobile Agents (Read more on [arXiv](https://arxiv.org/abs/2509.00531) or [HuggingFace](https://huggingface.co/papers/2509.00531))| Wangbo Gong, Yisheng Zhao, Xi Zhao, fengerhu, sjtuzc | This paper presents MobiAgent, a systematic framework for developing, accelerating, and evaluating GUI-based mobile agents. The main objective is to address the significant accuracy and efficiency challenges that current Vision-Language Model (VLM) agents face in real-world mobile task execution. The key methodology involves a three-part system: the MobiMind multi-role agent models, the AgentRR record-and-replay acceleration framework which uses a latent memory model to cache and reuse task trajectories, and the MobiFlow DAG-based benchmark for evaluation. The primary result is that MobiAgent outperforms models like GPT-5 and UI-TARS-1.5-7B in task completion, while its AgentRR framework achieves a 2-3x performance improvement by attaining a 60%-85% action replay rate under realistic user task distributions. The principal implication for AI engineers is that deploying a record-and-replay acceleration layer provides a highly effective, full-stack solution to mitigate the high inference latency of VLMs, making mobile agents more practical and efficient for recurring real-world tasks. |
| MedDINOv3: How to adapt vision foundation models for medical image
  segmentation? (Read more on [arXiv](https://arxiv.org/abs/2509.02379) or [HuggingFace](https://huggingface.co/papers/2509.02379))| Xiaofeng Yang, Yuheng Li, wy20030128, yuxianglai117, mcl0222 | The paper presents MedDINOv3, a framework for adapting vision foundation models (FMs) to medical image segmentation by combining architectural refinements with domain-adaptive pretraining. The research aims to determine how to effectively transfer large-scale, natural-image FMs to medical segmentation tasks, overcoming challenges like the ViT-CNN performance gap and the substantial domain shift. The key methodology involves first refining a plain Vision Transformer (ViT) architecture with multi-scale token aggregation from intermediate layers and high-resolution training, then performing a three-stage, domain-adaptive pretraining on a curated 3.87 million slice CT dataset (CT-3M) using a DINOv3-style recipe. The primary result is that MedDINOv3 outperforms or matches strong baselines on four benchmarks, achieving an 87.38% Dice Similarity Coefficient (DSC) on the AMOS22 dataset, surpassing the nnU-Net baseline by 2.57%. The principal implication for AI practitioners is that general-purpose FMs can outperform highly specialized architectures in domains like medical imaging when combined with targeted architectural enhancements and large-scale, domain-specific self-supervised pretraining. |
| AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with
  Knowledge Augmentation for Robust Constitutional Alignment of Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.02133) or [HuggingFace](https://huggingface.co/papers/2509.02133))| Rahul Karthikeyan, Shivam Dubey, Aryan Kasat, Snehasis Mukhopadhyay, amanchadha | The AMBEDKAR framework introduces an inference-time, fairness-aware speculative decoding method to mitigate caste and religious biases in LLMs by aligning their outputs with principles from the Indian Constitution. The primary objective is to develop a computationally efficient, model-agnostic technique to reduce sociocultural biases specific to the Indian context, which existing mitigation strategies often overlook. The key methodology inverts speculative decoding, using a draft model to propose tokens and a constitutionally-aligned verifier model to select the token with the minimum Jensen-Shannon divergence between its generation probabilities under original and counterfactually perturbed prompts. This approach yields an absolute bias reduction of up to 26.41% compared to baseline models, with a per-token latency increase of only 6.29%. For AI practitioners, this provides a deployable, low-latency "fairness-by-speculation" mechanism to enforce normative constraints at inference time without retraining, making it applicable for steering generation towards fairness even in black-box models. |
| Improving Large Vision and Language Models by Learning from a Panel of
  Peers (Read more on [arXiv](https://arxiv.org/abs/2509.01610) or [HuggingFace](https://huggingface.co/papers/2509.01610))| Simon Jenni, Jing Shi, Jefferson Hernandez, kushalkafle, vicenteor | The paper introduces Panel-of-Peers (PoP), a framework for iteratively improving Large Vision-Language Models (LVLMs) through collaborative, self-generated feedback, eliminating the need for human-labeled preference data. The research objective is to develop a scalable self-improvement paradigm for LVLMs that bootstraps their capabilities using only unlabeled prompts. The core methodology involves a panel of peer LVLMs that both generate candidate responses and evaluate each other's outputs along multiple axes (e.g., correctness, helpfulness) to create a synthetic preference dataset, which is then used to iteratively fine-tune all models in the panel. The PoP framework demonstrated significant performance gains, increasing the average score across 15 vision-language benchmarks by 9 absolute points (from 48% to 57%) over three self-improvement iterations. For AI practitioners, this provides a cost-effective method to enhance LVLM performance and enable cross-model knowledge transfer (e.g., teaching an OCR-deficient model to read) without requiring expensive human annotation. |
| ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association (Read more on [arXiv](https://arxiv.org/abs/2509.01584) or [HuggingFace](https://huggingface.co/papers/2509.01584))| Daniel Cremers, Xi Wang, Shenhan Qian, zhangganlin | ViSTA-SLAM is a real-time, monocular visual SLAM system designed to provide accurate dense 3D reconstruction and tracking without requiring camera intrinsics. The research objective is to create an efficient and accurate dense SLAM system that is broadly applicable across diverse camera setups by eliminating the need for pre-calibration. The methodology consists of a lightweight Symmetric Two-view Association (STA) frontend that estimates relative poses and local pointmaps from image pairs, which are then integrated into a backend Sim(3) pose graph with loop closure for global optimization. The system achieves state-of-the-art accuracy, with an average Absolute Trajectory Error (ATE) RMSE of 0.052 on the TUM-RGBD dataset, while its frontend model is only 35% the size of comparable methods. This presents AI practitioners with an efficient and broadly applicable framework for dense 3D perception in robotics and AR, demonstrating that a symmetric, lightweight design can outperform larger models and remove the dependency on pre-calibrated sensors. |
| Towards More Diverse and Challenging Pre-training for Point Cloud
  Learning: Self-Supervised Cross Reconstruction with Decoupled Views (Read more on [arXiv](https://arxiv.org/abs/2509.01250) or [HuggingFace](https://huggingface.co/papers/2509.01250))| Junchi Yan, Shaofeng Zhang, Xiangdong Zhang | Point-PQAE is a self-supervised generative framework that pre-trains point cloud models via cross-reconstruction between two decoupled views. The objective is to create a more challenging and informative pre-training task than standard single-view self-reconstruction to learn more robust 3D representations. The core methodology introduces a point cloud crop mechanism to generate two views and a view-relative positional embedding (VRPE) which enables a positional query block to reconstruct one view from the other's latent representation. The method outperforms the Point-MAE baseline by an average of 6.7% on ScanObjectNN classification under the MLP-LINEAR protocol. For AI practitioners, this cross-reconstruction approach provides a more powerful pre-training strategy for developing 3D vision models, yielding superior frozen feature quality for downstream tasks in label-free settings. |
| SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction (Read more on [arXiv](https://arxiv.org/abs/2509.00581) or [HuggingFace](https://huggingface.co/papers/2509.00581))| bindsch, amanchadha, shollercoaster | The paper introduces SQL-of-Thought, a multi-agent framework that improves text-to-SQL generation through structured reasoning and guided error correction. The primary objective is to develop a robust text-to-SQL system by combining multi-agent decomposition with Chain-of-Thought (CoT) reasoning and a systematic, interpretable correction mechanism. The methodology involves a pipeline of specialized agents for schema linking, subproblem identification, CoT-based query plan generation, and SQL synthesis, along with a novel correction loop guided by a predefined error taxonomy to rectify failures. SQL-of-Thought achieves a state-of-the-art execution accuracy of 91.59% on the Spider benchmark. The principal implication for AI practitioners is that decomposing complex code generation tasks into a multi-agent framework with explicit reasoning steps (e.g., query planning) and taxonomy-guided error correction is more effective than relying on monolithic models or simple execution-based feedback. |
| C-DiffDet+: Fusing Global Scene Context with Generative Denoising for
  High-Fidelity Object Detection (Read more on [arXiv](https://arxiv.org/abs/2509.00578) or [HuggingFace](https://huggingface.co/papers/2509.00578))| Vito Renó, Abdenour Hadid, Bekhouche, xkruvox, ldb0071 | C-DiffDet+ enhances diffusion-based object detection by integrating global scene context with local proposal features to improve performance on fine-grained tasks. The objective is to overcome the limitations of local feature conditioning in diffusion detectors by explicitly leveraging global information to disambiguate objects with subtle visual cues. The key methodology involves a Global Context Encoder (GCE) that generates a scene-level embedding, which is then fused with local Region of Interest features using a cross-attention-based Context-Aware Fusion (CAF) module within the denoising process. On the CarDD benchmark, C-DiffDet+ achieves a state-of-the-art mean Average Precision of 64.8%, a 1.4% improvement over the DiffusionDet baseline, with a notable 6.8% absolute increase in AP for small objects. For AI practitioners, this work demonstrates that augmenting local features with a global context vector via cross-attention is a highly effective strategy for improving detection accuracy in domains where scene-level understanding is critical, such as industrial defect detection or medical imaging. |
| Metis: Training Large Language Models with Advanced Low-Bit Quantization (Read more on [arXiv](https://arxiv.org/abs/2509.00404) or [HuggingFace](https://huggingface.co/papers/2509.00404))| Hengjie Cao, wenzi001, ZhouJixian, cnyangyifeng, ChenMengyi | This paper introduces Metis, a framework that enables stable and effective low-bit (FP8/FP4) training of large language models by managing anisotropic parameter distributions through spectral decomposition. The main objective is to overcome the training instability and performance degradation caused by the wide numerical ranges in LLM parameters when using low-bit quantization. The methodology combines spectral decomposition via randomized SVD to separate dominant and long-tail components, an adaptive spectral learning rate to rebalance updates, and a dual-range regularizer to narrow parameter distributions. Primary results show that Metis enables FP4 training to achieve performance comparable to FP32, while FP8 training surpasses the FP32 baseline, with a 1.1B parameter GPT-2 model achieving a test loss of 3.95 versus 4.00 for the FP32 baseline. The principal implication for AI practitioners is that training LLMs with aggressive FP4/FP8 quantization is now feasible, significantly reducing memory and computational costs while maintaining or even improving model performance compared to standard FP32 training. |
| FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable
  Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2508.20586) or [HuggingFace](https://huggingface.co/papers/2508.20586))| Zhen Wang, Zhuandi He, Shiyue Zhang, Yanwei Lei, zhengchong | FastFit is a cacheable diffusion model architecture that accelerates multi-reference virtual try-on by decoupling static reference features from the iterative denoising process. The primary objective is to create a virtual try-on framework that supports coherent multi-garment outfit composition and fundamentally solves the computational inefficiency caused by redundant feature re-computation in existing diffusion models. The key methodology is a novel "Cacheable UNet" which uses static, learnable "Reference Class Embeddings" instead of timestep embeddings for garment inputs and a "Semi-Attention" mechanism; this enables reference item features to be pre-computed into a "Reference KV Cache" and reused losslessly across all denoising steps. The framework achieves an average 3.5× speedup over comparable methods while surpassing state-of-the-art models on fidelity metrics, attaining a FID score of 9.311 on the proposed DressCode-MR multi-reference dataset. For AI practitioners, the cacheable architecture provides a generalizable strategy for accelerating subject-driven generative models by isolating time-independent conditional inputs from the iterative generation loop, enabling significant, lossless reduction in inference latency. |
