

## Papers for 2025-09-04

| Title | Authors | Summary |
|-------|---------|---------|
| Robix: A Unified Model for Robot Interaction, Reasoning and Planning (Read more on [arXiv](https://arxiv.org/abs/2509.01106) or [HuggingFace](https://huggingface.co/papers/2509.01106))| Zixuan Wang, Wei Li, Heng Dong, Mengxi Zhang, Huang Fang | Robix is a unified vision-language model designed as a high-level cognitive layer for hierarchical robot systems, integrating robot reasoning, task planning, and natural language interaction. Its primary objective is to overcome limitations in embodied reasoning and multimodal interaction, enabling generalist robots to handle complex, long-horizon tasks in dynamic environments. The methodology involves a three-stage training strategy: continued pretraining for foundational embodied reasoning, supervised finetuning to model human-robot interaction as a unified reasoning-action sequence, and reinforcement learning for improved consistency and coherence. In online evaluations of the VLM-VLA robot system, Robix-32B-RL achieved an average task progress of 92.5%, outperforming Gemini-2.5-Pro by 4.3 percentage points and GPT-40 by 28.1 percentage points. This demonstrates a robust pathway for AI practitioners developing general-purpose embodied intelligence that requires adaptable and human-like interaction in real-world settings. |
| Open Data Synthesis For Deep Research (Read more on [arXiv](https://arxiv.org/abs/2509.00375) or [HuggingFace](https://huggingface.co/papers/2509.00375))| Zheng Liu, Hongjin Qian, Kun Luo, ZiyiXia | This paper introduces InfoSeek, an open-source framework for synthesizing large-scale, structurally complex Deep Research tasks formalized as Hierarchical Constraint Satisfaction Problems (HCSPs). The main objective is to provide a scalable method for generating verifiable Deep Research questions, addressing the limitations of existing benchmarks which lack structural depth and complexity for multi-step, multi-source reasoning. InfoSeek utilizes a dual-agent system to recursively build Research Trees from webpages, blurring nodes into sub-problems and converting them into natural language questions, yielding over 50K training examples and 16.5K reasoning trajectories via reject sampling. Experiments demonstrate that a 3B LLM trained on InfoSeek achieves 16.5% accuracy on the BrowseComp-Plus benchmark, outperforming larger 32B models (e.g., Qwen3-32B at 3.5%) and lightweight commercial APIs (e.g., Gemini 2.5 Flash at 15.5%), and is comparable to Gemini 2.5 Pro (19.0%). For AI practitioners, the InfoSeek dataset's preservation of meta-information, such as intermediate steps and retrieval labels, facilitates the development of advanced optimization strategies, including compound reward design and trajectory-level exploration for training Deep Research agents. |
| LMEnt: A Suite for Analyzing Knowledge in Language Models from
  Pretraining Data to Representations (Read more on [arXiv](https://arxiv.org/abs/2509.03405) or [HuggingFace](https://huggingface.co/papers/2509.03405))| Yoav Gur-Arieh, Ido Cohen, Alon Gilae-Dotan, Daniela Gottesman, mega | LMEnt is an open-source suite designed for analyzing knowledge acquisition and representation in Language Models from pretraining data to their internal representations. The primary objective is to facilitate the study of how knowledge representations are formed and shaped during LM pretraining, specifically addressing the interplay between data composition, training dynamics, and knowledge mechanisms. Key methodologies include annotating a 7.3M-entity Wikipedia corpus with fine-grained entity mentions (hyperlinks, entity linking, coreference), building an Elasticsearch index for entity-based retrieval by Wikidata QID, and releasing 12 pretrained OLMO-2 models (170M-1B parameters) with 4K intermediate checkpoints. A primary result shows LMEnt's entity-based retrieval outperforms string-based methods by as much as 80.4% in retrieving relevant document chunks, maintaining over 97% precision. For AI practitioners, LMEnt provides a controlled and transparent environment to investigate knowledge representations, plasticity, editing, attribution, and learning dynamics in LMs, enhancing the ability to control and improve model factuality and reasoning. |
| Mixture of Global and Local Experts with Diffusion Transformer for
  Controllable Face Generation (Read more on [arXiv](https://arxiv.org/abs/2509.00428) or [HuggingFace](https://huggingface.co/papers/2509.00428))| Kai Li, Yue Li, Xing Fu, Shun Zhang, Xuechao Zou | Face-MoGLE is a unified Diffusion Transformer framework for high-quality, controllable, and photorealistic face generation. Its primary objective is to address challenges in balancing semantic controllability and photorealism by decoupling semantic controls from generation pipelines. The framework utilizes a Diffusion Transformer backbone with a Mixture of Global and Local Experts (MoGLE) architecture, integrating semantic-decoupled latent modeling and a dynamic gating network that adaptively blends expert outputs based on spatial and temporal awareness. Face-MoGLE achieved a Fr√©chet Inception Distance (FID) of 22.24 for multimodal face generation on MM-CelebA-HQ, surpassing state-of-the-art models, and demonstrated robust zero-shot generalization. Additionally, generated images from Face-MoGLE achieved an Area Under the Curve (AUC) of 0.50 against the NPR deepfake detector, indicating high perceptual realism. This enables AI practitioners to develop advanced generative modeling applications requiring precise facial attribute manipulation, high visual fidelity, and enhanced resilience to deepfake detection. |
| MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware
  Alignment and Disentanglement (Read more on [arXiv](https://arxiv.org/abs/2509.01977) or [HuggingFace](https://huggingface.co/papers/2509.01977))| Hualiang Wang, Qiaoqiao Jin, Mushui Liu, Siming Fu, Dong She | MOSAIC is a representation-centric framework for multi-subject personalized image generation that uses explicit semantic correspondence and orthogonal feature disentanglement. The primary objective is to overcome identity blending and attribute leakage in multi-subject generation by improving subject interaction modeling and disentangling conflated features. Its methodology introduces SemAlign-MS, a dataset with fine-grained semantic point correspondences, and utilizes Semantic Correspondence Attention Loss (SCAL) for precise point-to-point alignment and Multi-Reference Disentanglement Loss (MDL) for orthogonal feature separation. Quantitatively, MOSAIC achieves state-of-the-art performance, for instance, securing 76.30 CLIP-I, 32.40 CLIP-T, and 56.83 DINO on DreamBench multi-subject scenarios, and maintaining high fidelity with 4+ reference subjects where other methods degrade. This directly implies that AI practitioners can now leverage MOSAIC for highly consistent and scalable multi-subject synthesis applications, pushing the boundaries of personalized image generation beyond previous limitations. |
