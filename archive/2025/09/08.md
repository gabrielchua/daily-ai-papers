

## Papers for 2025-09-08

| Title | Authors | Summary |
|-------|---------|---------|
| Why Language Models Hallucinate (Read more on [arXiv](https://arxiv.org/abs/2509.04664) or [HuggingFace](https://huggingface.co/papers/2509.04664))| Edwin Zhang, Santosh S. Vempala, Ofir Nachum, Adam Tauman Kalai | This paper theoretically analyzes how language model training statistically induces hallucinations and argues they persist because evaluation benchmarks reward guessing over expressing uncertainty. The main objective is to identify the statistical causes of hallucinations in both pretraining and post-training stages, framing them as a natural outcome of the training pipeline. The key methodology involves a reduction from generative modeling to a binary classification problem ("Is-It-Valid") to derive lower bounds on error rates, complemented by a meta-analysis of influential evaluation benchmarks. Primary results show the generative error rate is lower-bounded by twice the binary misclassification rate on validity, and a review of 10 popular benchmarks reveals that 9 use binary grading which provides no credit for uncertain ("IDK") responses. The principal implication for AI practitioners is that mitigating hallucinations requires modifying the scoring of mainstream evaluations to stop penalizing and instead reward appropriate expressions of uncertainty, for instance by incorporating explicit confidence targets and penalties for incorrect answers. |
| Symbolic Graphics Programming with Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.05208) or [HuggingFace](https://huggingface.co/papers/2509.05208))| Kaipeng Zhang, Zeju Qiu, Haoquan Zhang, Yamei Chen, YangyiH | This research introduces a benchmark and a reinforcement learning (RL) framework with cross-modal rewards to evaluate and enhance the capability of Large Language Models (LLMs) to generate Symbolic Graphics Programs (SGPs) from text. The main objective is to assess existing LLMs' SGP generation abilities and develop a method to improve open-source models' performance to a level competitive with proprietary systems. The key methodology involves finetuning the Qwen-2.5-7B model using a critic-free RL algorithm where the reward signal is derived from a format-validity gate combined with text-image (SigLIP) and image-image (DINO) alignment scores. The RL-finetuned model substantially improves its overall compositional generation score on the SGP-COMPBENCH from 8.8 to 60.8, outperforming other open-source models and approaching frontier model performance. For AI practitioners, the principal implication is that reinforcement learning with rewards from vision foundation models provides a scalable, data-efficient method for distilling visual grounding into LLMs, enabling smaller models to perform precise, visually-aligned program synthesis without requiring paired ground-truth datasets. |
| Set Block Decoding is a Language Model Inference Accelerator (Read more on [arXiv](https://arxiv.org/abs/2509.04185) or [HuggingFace](https://huggingface.co/papers/2509.04185))| Jeremy Reizenstein, Daniel Haziza, Marton Havasi, Heli Ben-Hamu, Itai Gat | Set Block Decoding (SBD) is a paradigm that accelerates language model inference by generating multiple, non-consecutive future tokens in parallel within a standard autoregressive architecture. The paper's objective is to accelerate the computationally expensive decoding stage by integrating standard next token prediction (NTP) with masked token prediction (MATP) without requiring architectural changes or sacrificing performance. The key methodology involves fine-tuning existing NTP models with a combined loss, enabling the use of advanced solvers from discrete diffusion literature, such as the Entropy Bounded (EB) Sampler, to dynamically select tokens for parallel generation. The primary result is a demonstrated 3-5x reduction in the number of forward passes required for generation on fine-tuned Llama-3.1 8B and Qwen-3 8B models, while maintaining equivalent performance to standard NTP training. For AI practitioners, the principal implication is that SBD offers a practical method to achieve significant inference speedups by simply fine-tuning existing models, as it maintains compatibility with exact KV-caching and does not degrade model accuracy. |
| WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.04744) or [HuggingFace](https://huggingface.co/papers/2509.04744))| Amit Namburi, Yash Vishe, Gagan Mundada, ZacharyNovack, XinXuNLPer | This paper introduces WildScore, the first in-the-wild benchmark for evaluating Multimodal Large Language Models (MLLMs) on symbolic music reasoning. The primary objective is to assess MLLMs' ability to interpret real-world music scores and answer complex musicological queries. The methodology involves constructing a multiple-choice question answering dataset from genuine musical compositions and user-generated questions sourced from public forums, categorized using a systematic musicological taxonomy. The empirical evaluation shows that the state-of-the-art model, GPT-4.1-mini, achieves a peak accuracy of 68.31%, indicating that current MLLMs struggle with tasks requiring deep symbolic abstraction and rhythmic interpretation. For AI practitioners, this highlights a critical gap in MLLM capabilities for specialized, dense symbolic domains and suggests that future models require improved pretraining on schematic notation and more robust vision-language alignment for such tasks. |
| LatticeWorld: A Multimodal Large Language Model-Empowered Framework for
  Interactive Complex World Generation (Read more on [arXiv](https://arxiv.org/abs/2509.05263) or [HuggingFace](https://huggingface.co/papers/2509.05263))| Zhan Zhao, Wei Jia, Tongwei Gu, Zhengxia Zou, Yinglin Duan | LatticeWorld is a framework that leverages lightweight multimodal LLMs and the Unreal Engine to generate interactive, large-scale 3D worlds from textual and visual instructions. The primary objective is to develop an effective and efficient framework for generating complex, dynamic, and interactive 3D worlds by integrating the spatial understanding and structured generation capabilities of LLMs with an industry-grade rendering pipeline, using multimodal user inputs. The methodology involves a two-stage LLM process: a fine-tuned LLaMA-2-7B model (`LLML`) first generates a sequential symbolic representation of the scene layout from text and vision (height map) inputs, followed by a second LLM (`LLMc`) that generates detailed environmental and agent configurations in JSON format; these intermediate representations are then procedurally rendered in Unreal Engine 5. The framework demonstrates superior performance in layout generation accuracy and visual fidelity compared to other generative models. Quantitatively, LatticeWorld achieves over a 90x increase in production efficiency, reducing the creation time for a complex environment from 55 days for a human artist to less than 0.6 days. For AI practitioners, this framework provides a method to rapidly generate diverse, physically-realistic, and interactive 3D simulation environments for training and testing embodied AI agents, reducing reliance on manual content creation and accelerating the development cycle for applications in autonomous systems and robotics. |
| LuxDiT: Lighting Estimation with Video Diffusion Transformer (Read more on [arXiv](https://arxiv.org/abs/2509.03680) or [HuggingFace](https://huggingface.co/papers/2509.03680))| Sanja Fidler, Igor Gilitschenski, Zan Gojcic, Kai He, Ruofan Liang | LuxDiT is a video diffusion transformer model that generates high-dynamic-range (HDR) environment maps from single LDR images or videos. The main objective is to develop a data-driven method for accurately estimating HDR scene illumination from casually captured LDR visual inputs, overcoming the scarcity of paired ground-truth HDR data. The method fine-tunes a pre-trained video diffusion transformer (DiT) conditioned on visual input tokens, representing HDR outputs using a dual-tonemapped LDR format to handle high dynamic range, and employs a two-stage training process involving pre-training on synthetic data for physical correctness followed by low-rank adaptation (LoRA) on real HDR panoramas for semantic alignment. The model outperforms existing state-of-the-art methods; on the Laval Outdoor dataset, it reduces the mean peak angular error for sunlight direction by nearly 50% compared to the DiffusionLight baseline, from 44.4 to 23.7 degrees. The principal implication for AI practitioners is the ability to generate realistic HDR lighting for applications like virtual object insertion, augmented reality, and synthetic data generation directly from standard LDR images or videos, thereby removing the need for specialized capture equipment. |
| WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool (Read more on [arXiv](https://arxiv.org/abs/2509.05296) or [HuggingFace](https://huggingface.co/papers/2509.05296))| Wenzheng Chang, Yifan Wang, Jianjun Zhou, Zizun Li, ghy0324 | WinT3R is a feed-forward model for real-time, high-quality 3D reconstruction and camera pose estimation from streaming images. The main objective is to resolve the trade-off between reconstruction quality and real-time performance in online methods by improving inter-frame information exchange and efficiently incorporating global context. The key methodology involves a sliding window mechanism for direct interaction between adjacent image tokens and a global camera token pool that maintains a compact representation of all historical camera information for robust pose prediction. The model achieves state-of-the-art results on multiple benchmarks, demonstrating superior reconstruction accuracy and the fastest processing speed among compared methods at 17.2 FPS. For AI practitioners, the principal implication is a novel and efficient architecture using a camera token pool as a lightweight global memory, enabling high-fidelity, real-time 3D perception in applications like robotics and AR without significant computational overhead. |
| On Robustness and Reliability of Benchmark-Based Evaluation of LLMs (Read more on [arXiv](https://arxiv.org/abs/2509.04013) or [HuggingFace](https://huggingface.co/papers/2509.04013))| Kevin Roitero, Stefano Mizzaro, Vincenzo Della Mea, Riccardo Lunardi | This study evaluates the robustness of 34 LLMs to linguistic variation, finding that while relative model rankings are preserved, absolute performance on six standard benchmarks degrades significantly when questions are paraphrased. The primary objective is to assess the reliability of benchmark-based evaluations and the robustness of LLMs by measuring performance variations when benchmark questions are systematically reworded while preserving semantic meaning. The methodology involved automatically generating five paraphrases for all questions in six multiple-choice benchmarks, then evaluating 34 LLMs in a zero-shot setting to measure changes in accuracy, response consistency, and ranking stability (Kendall's τ). The primary results show that while relative model rankings remain highly stable (Kendall’s τ > 0.9 across all benchmarks), absolute performance degrades and models show significant inconsistency, with 15-30% of questions receiving two or more different answers across the paraphrased versions. The principal implication for AI practitioners is that standard benchmark scores overestimate an LLM's true generalization capabilities; therefore, evaluations should incorporate robustness testing against linguistic variations to obtain a more reliable assessment of real-world performance. |
| MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in
  3D CT Disease Detection, Understanding and Reporting (Read more on [arXiv](https://arxiv.org/abs/2509.03800) or [HuggingFace](https://huggingface.co/papers/2509.03800))| Vanessa Wildman, Jike Zhong, Yuxiang Lai, Yenho Chen, Yuheng Li | This paper introduces MedVista3D, a multi-scale semantic-enriched vision-language pretraining framework for 3D CT analysis designed to reduce diagnostic errors. The main objective is to overcome the limitations of existing models by jointly enabling precise localized disease detection, global volume-level reasoning, and semantically consistent reporting within a single framework. Its methodology employs a multi-scale alignment loss to learn both local organ-level and global volume-level representations simultaneously, complemented by Large Language Model (LLM) rewrites and a Radiology Semantic Matching Bank (RSMB) for semantically robust text-image alignment. MedVista3D achieves state-of-the-art results, with one variant obtaining an AUC of 0.782 on global zero-shot disease classification on the CT-RATE dataset, outperforming the CT-CLIP baseline by 7.4 percentage points. For AI practitioners, the principal implication is that integrating multi-scale alignment with semantic text enhancement provides a robust pretraining strategy for building 3D medical foundation models with superior generalization and transferability to diverse downstream tasks like classification, retrieval, and segmentation. |
| U-ARM : Ultra low-cost general teleoperation interface for robot
  manipulation (Read more on [arXiv](https://arxiv.org/abs/2509.02437) or [HuggingFace](https://huggingface.co/papers/2509.02437))| Junda Huang, Zewei Ye, Chenyang Shi, Zhaoye Zhou, Yanwen Zou | This paper presents U-Arm, an open-source, ultra-low-cost leader-follower teleoperation framework for robot manipulation with a bill of materials (BOM) of $50.5 for the 6-DoF version. The research objective is to create a rapidly adaptable and user-friendly teleoperation system compatible with a wide range of commercial robotic arms to facilitate large-scale, high-quality data collection. The methodology involves designing three structurally distinct, 3D-printed leader arm configurations that are mechanically isomorphic to common robot joint arrangements, using modified servos for joint angle sensing, and applying a calibration and filtering algorithm to improve control. Experimental results show U-Arm achieves 39% higher data collection efficiency compared to a Joycon controller across multiple manipulation tasks, while maintaining a comparable success rate. For AI practitioners, this work provides a low-cost, open-source hardware and software solution that significantly reduces the barrier to acquiring real-world human demonstration data for training robot learning policies. |
| Behavioral Fingerprinting of Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.04504) or [HuggingFace](https://huggingface.co/papers/2509.04504))| Xing Li, Zhiyuan Yang, Ying Zhang, Hui-Ling Zhen, Zehua Pei | This paper introduces "Behavioral Fingerprinting," a framework using a judge LLM to evaluate Large Language Models on cognitive and interactive styles beyond standard performance metrics. The research objective is to create multi-faceted profiles that reveal "how a model thinks" by probing dimensions like reasoning, robustness, sycophancy, and world model integrity. The methodology utilizes a curated Diagnostic Prompt Suite, with responses from 18 target models being automatically scored by a powerful judge model (Claude-opus-4.1) against detailed, prompt-specific rubrics. Results show that while core reasoning abilities are converging among top-tier models, alignment-related behaviors diverge significantly, with sycophancy resistance scores ranging from 1.00 (complete resistance) to 0.25 (high sycophancy). The principal implication for AI practitioners is that alignment traits are a direct consequence of specific developer strategies, not an emergent property of scale, making these behavioral fingerprints critical for selecting models whose interactive styles match application-specific safety and reliability requirements. |
| Bootstrapping Task Spaces for Self-Improvement (Read more on [arXiv](https://arxiv.org/abs/2509.04575) or [HuggingFace](https://huggingface.co/papers/2509.04575))| Yoram Bachrach, Andrei Lupu, Minqi Jiang | The paper introduces Exploratory Iteration (EXIT), an autocurriculum reinforcement learning method that trains LLMs for multi-step self-improvement by dynamically creating and prioritizing single-step iteration tasks from the model's own solution histories. The objective is to develop a sample-efficient RL training framework that enables LLMs to perform multi-step self-improvement at inference time without the high cost and arbitrary depth limits of naively training on full K-step rollouts. EXIT uses Group-Relative Policy Optimization (GRPO) to train an LLM on single-step self-improvement tasks by maintaining a buffer of partial solution histories, sampling from it based on group return variance as a learning potential metric, and augmenting the task space with explicit self-divergence prompts. Across domains, EXIT produced policies with strong inference-time improvement; on a collection of math test sets, the full EXIT method achieved a final accuracy of 20.4% after 16 self-improvement steps, a net improvement of +2.0 percentage points over its initial response and superior to the standard GRPO baseline. For AI practitioners, EXIT offers a method to fine-tune LLMs for iterative refinement by decomposing long-horizon improvement into prioritized single-step tasks, enhancing performance in complex, scaffolded applications like automated ML engineering without requiring additional compute over standard RL fine-tuning. |
