

## Papers for 2025-09-09

| Title | Authors | Summary |
|-------|---------|---------|
| Reverse-Engineered Reasoning for Open-Ended Generation (Read more on [arXiv](https://arxiv.org/abs/2509.06160) or [HuggingFace](https://huggingface.co/papers/2509.06160))| Wangchunshu Zhou, Minghao Liu, Qixin Xu, Haoran Que, Haozhe Wang | This paper introduces REverse-Engineered Reasoning (REER), a novel paradigm for instilling deep reasoning in LLMs for open-ended generation by computationally discovering latent thought processes from existing high-quality outputs. The main objective is to cultivate deep reasoning for non-verifiable, creative tasks without relying on reinforcement learning or costly instruction distillation. The key methodology formulates the discovery of a reasoning trajectory as a gradient-free local search problem, where the optimal trajectory is the one that minimizes the perplexity of a known-good solution, a process used to create the DeepWriting-20K dataset. The resulting model, DeepWriter-8B, demonstrates performance competitive with proprietary systems, achieving a score of 91.28 on LongBench-Write, which surpasses GPT-4o's score of 83.1. The principal implication for AI practitioners is that this method offers a scalable and cost-effective pathway to generate high-quality, structured reasoning data for subjective domains, enabling smaller models to acquire sophisticated planning and generation capabilities. |
| WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents (Read more on [arXiv](https://arxiv.org/abs/2509.06501) or [HuggingFace](https://huggingface.co/papers/2509.06501))| Aili Chen, Jingyang Li, Chi Zhang, Yunji Li, Junteng Liu | This paper presents WebExplorer, a framework for autonomously synthesizing challenging training data for long-horizon web agents via model-based exploration and iterative query evolution. The primary objective is to overcome the scarcity of complex, multi-step training data required to develop highly capable information-seeking agents. The key methodology first uses an LLM to autonomously explore the web and generate initial query-answer pairs, then iteratively increases query difficulty through a "long-to-short" evolution process that systematically removes salient clues and adds obfuscation; this data is then used for supervised fine-tuning and reinforcement learning. The resulting 8B-parameter model, WebExplorer-8B, achieves state-of-the-art performance at its scale, scoring 15.7% on the BrowseComp-en benchmark, outperforming the much larger WebSailor-72B model. For AI practitioners, this work provides a practical and scalable method for generating difficult training data, demonstrating that iterative obfuscation is an effective strategy for developing advanced web agents without reliance on expensive manual data curation. |
| Revolutionizing Reinforcement Learning Framework for Diffusion Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.06949) or [HuggingFace](https://huggingface.co/papers/2509.06949))| Ke Shen, Ye Tian, Bowen Li, Ling Yang, Yinjie Wang | This paper introduces TraceRL, a trajectory-aware reinforcement learning framework to enhance the reasoning capabilities of diffusion language models (DLMs). The main objective is to create a unified and effective RL framework that aligns the training objective with the model's inference trajectory, addressing performance limitations on complex reasoning tasks across different DLM architectures. The key methodology is TraceRL, which optimizes based on intermediate generation steps (the trajectory) rather than only the final output, and incorporates a diffusion-based value model to reduce training variance and stabilize optimization. The primary result is the TraDo series of models; TraDo-8B-Instruct achieves a relative accuracy improvement of 6.1% over Qwen2.5-7B-Instruct on mathematical reasoning benchmarks. The principal implication for AI practitioners is the provision of an open-source framework and a novel RL method (TraceRL) that enables the development of diffusion models with reasoning abilities superior to stronger, larger autoregressive models, while retaining the inherent inference speed advantages of parallel decoding. |
| Does DINOv3 Set a New Medical Vision Standard? (Read more on [arXiv](https://arxiv.org/abs/2509.06467) or [HuggingFace](https://huggingface.co/papers/2509.06467))| Bailiang Jian, Jinpeng Lu, Haoyuan Shi, Yinda Chen, Che Liu | This paper comprehensively benchmarks the DINOv3 vision foundation model, pre-trained on natural images, to determine if it establishes a new standard for medical imaging tasks. The primary objective is to evaluate whether DINOv3's features can directly transfer to medical domains without specific pre-training, assessing its performance and scalability across 2D/3D classification and segmentation. Using a frozen backbone and linear probing methodology, the study reveals that DINOv3 excels in tasks visually similar to natural images, such as CT classification where DINOv3-B achieves a 0.798 AUC on the CT-RATE dataset, significantly outperforming the 0.731 AUC of the domain-specific CT-CLIP. However, its performance degrades severely on modalities with large domain shifts, such as Whole-Slide Images (WSI), Electron Microscopy (EM), and PET segmentation. The principal implication for AI practitioners is that while general-purpose foundation models offer a powerful baseline for certain medical modalities like CT and X-ray, they fail catastrophically where domain-specific, fine-grained textural or functional features are paramount, necessitating caution and domain-specific adaptation for reliable deployment. |
| Reinforced Visual Perception with Tools (Read more on [arXiv](https://arxiv.org/abs/2509.01656) or [HuggingFace](https://huggingface.co/papers/2509.01656))| Mingyang Fu, Zhihan Hu, Zixian Ma, Dongping Chen, Zetong Zhou | This paper introduces REVPT, a framework that enhances multi-modal LLMs' visual perception by training them to use visual tools through reinforcement learning. The primary objective is to overcome the limitations of supervised fine-tuning, such as poor generalization and reliance on expensive data, by enabling models to dynamically reason about and select appropriate visual tools. The methodology involves a two-stage process: an initial supervised "cold-start" phase to teach basic tool use, followed by fine-tuning using the Group Relative Policy Optimization (GRPO) algorithm with a suite of four visual tools. REVPT models achieve state-of-the-art results, with the 7B model outperforming its base instruct counterpart by 9.44% on the CV-Bench benchmark. For AI practitioners, this work demonstrates that an RL-based approach to tool usage can unlock significant performance gains in visual reasoning, providing a more robust and adaptive alternative to static supervised training for integrating specialized vision models. |
| Reinforcement Learning Foundations for Deep Research Systems: A Survey (Read more on [arXiv](https://arxiv.org/abs/2509.06733) or [HuggingFace](https://huggingface.co/papers/2509.06733))| Wei Han, Hannan Cao, Jingru Lin, Zhi Chen, Wenjun Li | This survey systematizes the reinforcement learning (RL) foundations for building deep research systems, covering data synthesis, RL methods, agentic architectures, training frameworks, and evaluation benchmarks. The paper's objective is to provide the first comprehensive survey dedicated to RL for deep research systems, motivating the use of RL over supervised fine-tuning (SFT) and preference-based methods (DPO) for training agents on complex, multi-step, tool-interactive tasks. The methodology is a literature survey that analyzes and categorizes recent work (post-February 2025) across three core axes: (1) data synthesis and curation for RL, (2) RL methods for agentic research, and (3) agentic RL training frameworks, supplemented by reviews of agent architectures and benchmarks. The survey distills a standard agentic RL pipeline consisting of an optional SFT cold start, templated rollouts with explicit tool tags, outcome-based rewards, and PPO/GRPO optimization with a reference-KL penalty. It highlights specific algorithmic improvements, such as Duplicating Sampling Policy Optimization (DUPO) from WebSailor, which reportedly provides a ~2-3x training speed-up over DAPO through dynamic sampling. The principal implication for AI practitioners is to adopt an RL-centric approach for developing deep research agents, starting with the identified baseline pipeline and focusing innovations on curriculum design, reward shaping for search efficiency, and leveraging scalable, asynchronous training frameworks to overcome rollout and credit assignment bottlenecks. |
| Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.06461) or [HuggingFace](https://huggingface.co/papers/2509.06461))| Baolong Bi, Lingrui Mei, Yiwei Wang, Shenghua Liu, Yuyao Ge | This paper introduces CARVE, a training-free method that enhances Vision-Language Model (VLM) reasoning by contrastively filtering visual noise using the model's own attention mechanisms. The main objective is to mitigate the negative impact of visual complexity on VLM performance by developing a method to isolate task-relevant visual signals, based on an initial investigation into the correlation between visual complexity, attention entropy, and reasoning accuracy. The key methodology, Contrastive Attention Refinement for Visual Enhancement (CARVE), contrasts attention maps from a general instruction (capturing visual noise) with those from a task-specific question (capturing semantic signal) to produce a refined attention mask, which is then used to crop the image to relevant regions for a final inference pass. Primary results demonstrate consistent performance enhancement across multiple models and datasets, notably improving the LLAVA1.5-7B model's accuracy on the V* benchmark from 38.7% to 66.5%, a relative increase of 71.83%. The principal implication for AI practitioners is that CARVE can be used as a plug-and-play, inference-time technique to significantly improve the accuracy of existing VLMs on visually complex tasks without requiring any model retraining, offering a resource-efficient method to boost performance. |
| UniVerse-1: Unified Audio-Video Generation via Stitching of Experts (Read more on [arXiv](https://arxiv.org/abs/2509.06155) or [HuggingFace](https://huggingface.co/papers/2509.06155))| Xinyao Liao, Ling-Hao Chen, Aojie Li, Wei Zuo, Duomin Wang | The paper introduces UniVerse-1, an open-source model for simultaneous audio-video generation by fusing pre-trained expert models. The research objective is to develop a unified, Veo-like system capable of generating temporally and semantically coordinated audio-visual content to bridge the gap in open-source research. The core methodology is a "Stitching of Experts" (SoE) technique that deeply integrates a pre-trained video model (WAN2.1) and a music model (Ace-step) at the block level, combined with an online annotation pipeline and an independent noise sampling strategy. On the newly introduced Verse-Bench benchmark, UniVerse-1 achieves a superior identity preservation (ID) score of 0.89 in video generation tasks. The principal implication for AI practitioners is that the SoE framework provides an efficient pathway to construct complex multimodal generative models by leveraging existing unimodal foundations, significantly reducing the need for training entirely new systems from scratch. |
| Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage,
  but Not Direct the Play? (Read more on [arXiv](https://arxiv.org/abs/2509.03516) or [HuggingFace](https://huggingface.co/papers/2509.03516))| Rui Chen, Huijuan Huang, Xinting Hu, Yuan Wang, lioooox | The paper introduces T2I-COREBENCH, a comprehensive and complex benchmark designed to evaluate the composition and reasoning capabilities of text-to-image models. The main objective is to systematically assess T2I model performance on tasks requiring high compositional density and multi-step logical inference, moving beyond simple explicit instruction following. The methodology involves a 12-dimensional evaluation taxonomy for composition and reasoning, with 1,080 complex prompts evaluated via an automated MLLM-based checklist that asks specific yes/no questions for fine-grained analysis. Primary results from evaluating 27 models reveal a significant performance disparity between the two capabilities; the top open-source model, Qwen-Image, scored 78.0 in composition but only 49.3 in reasoning, demonstrating that reasoning is a critical bottleneck. The principal implication for AI practitioners is that current T2I models excel at rendering explicit elements ("painting") but struggle significantly with inferring and generating implicit, logically-derived content ("thinking"), identifying a key area for future model architecture and training improvements. |
| Interleaving Reasoning for Better Text-to-Image Generation (Read more on [arXiv](https://arxiv.org/abs/2509.06945) or [HuggingFace](https://huggingface.co/papers/2509.06945))| Shixiang Tang, Shaosheng Cao, Zheyong Xie, Shuang Chen, Wenxuan Huang | The paper introduces Interleaving Reasoning Generation (IRG), a framework that improves text-to-image synthesis by alternating between text-based reasoning and image generation for iterative refinement. The research aims to determine if a multi-turn, reflective process can enhance T2I models' instruction following, visual quality, and fine-grained detail preservation. The methodology involves a two-step process: first, generating a text-based plan to guide an initial image synthesis, and second, reflecting on this image to produce textual refinements that guide the generation of a final, improved image, all trained via a two-stage strategy on a curated IRGL-300K dataset. IRG achieves state-of-the-art results across multiple benchmarks, scoring 0.85 on GenEval, which is a 5-10 point absolute gain over previous methods. For AI practitioners, this work demonstrates that decomposing a complex generation task into an explicit "reason-generate-reflect-refine" loop, trained on specific sub-tasks, is a powerful paradigm for enhancing the fidelity and controllability of generative models. |
| Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI
  Agents (Read more on [arXiv](https://arxiv.org/abs/2509.06917) or [HuggingFace](https://huggingface.co/papers/2509.06917))| James Zou, Jonathan K. Pritchard, Joe R. Davis, Jiacheng Miao | Paper2Agent is an automated multi-agent framework that converts research papers and their codebases into interactive, reliable AI agents accessible via natural language. The primary objective is to overcome technical barriers in research dissemination by transforming passive artifacts (papers, code) into active, executable systems. The methodology uses a multi-agent workflow to analyze a paper's codebase, encapsulate its core functionalities into validated tools, and package them into a standardized Model Context Protocol (MCP) server for integration with LLM-based agents. The framework demonstrated its efficacy by creating an agent for the AlphaGenome paper that achieved 100% accuracy in reproducing numerical results on 15 tutorial-based queries and 15 novel queries. For AI practitioners, this work provides a concrete framework for "agentifying" research code, enabling the packaging of complex computational methods into standardized, reproducible, and LLM-callable tools that accelerate the translation of research into practical applications. |
| Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM
  Step-Provers (Read more on [arXiv](https://arxiv.org/abs/2509.06493) or [HuggingFace](https://huggingface.co/papers/2509.06493))| Xia Xiao, Kun Yuan, Yanchen Nie, Zeyu Zheng, Ran Xin | The paper introduces BFS-Prover-V2, a system that scales LLMs for automated theorem proving using a dual approach for training and inference. The primary objective is to overcome the dual scaling challenges in LLM-based provers: training-time performance plateaus in reinforcement learning and inference-time combinatorial search complexity. The methodology combines a multi-stage, off-policy RL framework inspired by AlphaZero, which uses adaptive perplexity-based data filtering and periodic retraining to escape local optima, with a planner-enhanced multi-agent search architecture where a high-level planner decomposes theorems into subgoals for specialized prover agents. The system achieves state-of-the-art results for step-provers, solving 95.08% of the MiniF2F benchmark and 41.4% on the ProofNet test set. For AI practitioners, this work provides a concrete framework for sustaining long-term improvement in RL-trained LLMs by using perplexity-based curriculum learning and periodic retraining to overcome performance stagnation, a technique applicable to other complex, multi-turn reasoning domains. |
| R^textbf{2AI}: Towards Resistant and Resilient AI in an
  Evolving World (Read more on [arXiv](https://arxiv.org/abs/2509.06786) or [HuggingFace](https://huggingface.co/papers/2509.06786))| Bowen Zhou, Chaochao Lu, Jie Fu, Xiang Wang, Youbang Sun | This paper introduces R²AI (Resistant and Resilient AI), a conceptual framework that treats AI safety as an intrinsic capability that coevolves with intelligence, inspired by biological immunity. The primary objective is to address the persistent gap between rapidly advancing AI capabilities and lagging safety progress by proposing a proactive framework that moves beyond reactive, post-hoc safety measures. The key methodology is an architectural framework comprising four components: a Fast Safe Model for real-time reactions, a Slow Safe Model for reflective reasoning, a "Safety Wind Tunnel" for continuous adversarial coevolution, and feedback loops with the external deployment environment. The primary result is the proposed framework itself, which is motivated by the quantitative finding that leading foundation models exhibit a significant safety-capability gap, with some models showing capability scores near 90 but safety scores around 65. The principal implication for AI practitioners is to shift from applying static, post-development safety patches to architecting systems with integrated, continually learning safety mechanisms, such as a dual fast-slow architecture and a dedicated adversarial simulation environment for the entire model lifecycle. |
| Llama-GENBA-10B: A Trilingual Large Language Model for German, English
  and Bavarian (Read more on [arXiv](https://arxiv.org/abs/2509.05668) or [HuggingFace](https://huggingface.co/papers/2509.05668))| Hoi-Fong Mak, Gokul Ramakrishnan, Stefan Schweter, Jophin John, Michael Hoffmann | This paper introduces Llama-GENBA-10B, a 10-billion parameter trilingual foundation model for German, English, and Bavarian, developed via continual pretraining on a Llama 3.1-8B base. The primary objective was to create a high-performing model that mitigates English-centric bias and supports the low-resource Bavarian dialect by addressing data scarcity and optimizing the tokenizer. The methodology involved scaling the architecture using block expansion, creating a custom tokenizer with a 20% vocabulary increase, and performing staged continual pretraining on a 164B token corpus (82B English, 82B German, 80M Bavarian) on a single Cerebras CS-2 system. The instruction-tuned variant achieves state-of-the-art performance in Bavarian, outperforming models like gemma-2-9b-it and Apertus-8B-Instruct-2509, while the base model scores a competitive 0.7364 on Winogrande. For AI practitioners, this work provides a resource-efficient and energy-documented (35.23 MWh) blueprint for extending existing foundation models to effectively support low-resource languages without requiring training from scratch. |
| Test-Time Scaling in Reasoning Models Is Not Effective for
  Knowledge-Intensive Tasks Yet (Read more on [arXiv](https://arxiv.org/abs/2509.06861) or [HuggingFace](https://huggingface.co/papers/2509.06861))| See-Kiong Ng, Bryan Hooi, James Xu Zhao | This paper demonstrates that increasing inference-time computation via test-time scaling in reasoning models does not consistently improve accuracy and can increase hallucinations on knowledge-intensive tasks.  The study's primary objective is to determine if test-time scaling is an effective strategy for improving the performance of large reasoning models on knowledge-intensive tasks that require high factual accuracy.  The methodology involves evaluating 12 reasoning models (e.g., GPT-5 mini, Gemini 2.5 Flash) on two benchmarks (SimpleQA, FRAMES) by systematically increasing inference-time computation through techniques like adjusting a `reasoning effort` parameter, increasing a `thinking budget` in tokens, or using "budget forcing," while measuring accuracy and hallucination ratio.  The primary results indicate that more computation does not reliably improve accuracy and can increase hallucinations; for example, on SimpleQA, GPT-5-mini's hallucination ratio increased by over 15% as its reasoning length grew from 300 to 3300 tokens. The analysis shows that reduced hallucinations often result from abstention, while increased hallucinations stem from the model attempting previously unanswered questions.  The principal implication for AI practitioners is that naively increasing inference-time computation is not a reliable strategy for enhancing factual accuracy and may be counterproductive. While enabling a baseline level of thinking is beneficial compared to non-thinking, simply extending it further is not an effective optimization for factual robustness in knowledge-intensive applications. |
| D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.06771) or [HuggingFace](https://huggingface.co/papers/2509.06771))| Dhanvin Sanjay Namboodiri, Rishi Bharat Junghare, Shahid Shafi Dar, Mohammad Zia Ur Rehman, Sai Kartheek Reddy Kasu | This research introduces the D-HUMOR dataset and a reasoning-augmented multimodal framework, TCRNet, for detecting, categorizing, and rating the intensity of dark humor in memes. The primary objective is to develop a robust method for understanding multimodal dark humor by addressing the challenge of interpreting implicit, context-dependent cues in memes. The key methodology involves using a Large Vision-Language Model with a novel Role-Reversal Self-Loop to generate and refine structured explanations, which are then fused with image and OCR text features via a Tri-stream Cross-Reasoning Network using pairwise attention. The proposed TCRNet model achieves state-of-the-art performance, attaining 75.00% accuracy in binary dark humor detection, outperforming all unimodal and zero-shot VLM baselines. The principal implication for AI practitioners is that for complex, subjective multimodal tasks, integrating an explicit, structured reasoning stream is critical; ablation studies show that removing this reasoning component causes a severe performance drop in target identification Macro-F1 from 60.54% to 35.11%, demonstrating that fusing raw multimodal data alone is insufficient. |
| MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI
  Agents (Read more on [arXiv](https://arxiv.org/abs/2509.06477) or [HuggingFace](https://huggingface.co/papers/2509.06477))| Zhengxi Lu, Weiqing He, Yaozhen Liang, Guangyi Liu, Pengxiang Zhao | This paper introduces MAS-Bench, a benchmark for evaluating hybrid mobile agents that combine GUI operations with programmatic shortcuts like APIs, deep links, and RPA scripts. The main objective is to systematically assess an agent's ability to discover, utilize, and autonomously generate shortcuts to improve task performance on real-world mobile applications. The methodology involves 139 complex tasks, a knowledge base of 88 predefined shortcuts, and a novel two-stage framework to evaluate the quality of agent-generated shortcuts based on their impact on a baseline agent's performance. The primary result shows that hybrid agents significantly outperform GUI-only agents, with the shortcut-augmented MAS-MobileAgent achieving a 64.1% success rate on single-app tasks compared to its 44.6% GUI-only counterpart. The principal implication for AI practitioners is that integrating a well-defined shortcut knowledge base is a validated strategy for substantially improving mobile agent success rates and efficiency, and MAS-Bench provides a standardized platform to guide and measure these improvements. |
| Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in
  the TPTP Ecosystem (Read more on [arXiv](https://arxiv.org/abs/2509.06809) or [HuggingFace](https://huggingface.co/papers/2509.06809))| Damien Sileo, Valentin Quesnel | This paper presents a framework for generating guaranteed-valid mathematical reasoning datasets by applying saturation-based theorem proving to the TPTP axiom library, avoiding the use of LLMs in the generation loop. The objective is to address the scarcity of high-quality, logically sound data by creating a scalable engine that produces three difficulty-controlled tasks—Conjecture Entailment, Minimal Premise Selection, and Proof Graph Reconstruction—to rigorously benchmark and improve LLM deductive capabilities. The core methodology uses the E-prover to exhaustively derive theorems from TPTP axioms, filters them for mathematical "interestingness" using AGInTRater, and deconstructs the resulting derivation graphs into tasks validated by the Vampire prover. Zero-shot experiments reveal a severe performance degradation with increased logical complexity; for instance, the gpt-5 model's average accuracy across all tasks dropped from 93% on "Easy" problems to 44% on "Very Hard" problems, with performance collapsing on the structural Proof Reconstruction task. The principal implication for AI practitioners is that this framework provides a scalable source of high-quality symbolic data for fine-tuning models to address a core deficit in multi-step, structural reasoning that is not overcome by model scaling alone. |
