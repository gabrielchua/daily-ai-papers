

## Papers for 2025-09-10

| Title | Authors | Summary |
|-------|---------|---------|
| Parallel-R1: Towards Parallel Thinking via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.07980) or [HuggingFace](https://huggingface.co/papers/2509.07980))| Xinyu Yang, Xiaoyang Wang, Wenhao Yu, Hongming Zhang, Tong Zheng | The paper introduces Parallel-R1, the first reinforcement learning (RL) framework designed to instill parallel thinking capabilities in LLMs for complex mathematical reasoning. Its primary objective is to overcome the cold-start problem of training this behavior by enabling models to learn the exploration of multiple concurrent reasoning paths without relying on complex, pre-generated data for difficult problems. The methodology employs a progressive curriculum that first uses supervised fine-tuning (SFT) on prompt-generated data from simpler tasks (GSM8K) to teach the parallel format, followed by RL on more challenging datasets (DAPO) to generalize the skill. The framework achieves an 8.4% accuracy improvement over a sequential RL model, and when used as a "mid-training exploration scaffold," it yields a 42.9% performance improvement over the baseline on the AIME25 benchmark. For AI practitioners, the principal implication is that a temporary, forced-exploration phase using a structured reasoning pattern can serve as an effective scaffold to discover more robust policies and unlock higher performance ceilings during RL training. |
| Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual
  Search (Read more on [arXiv](https://arxiv.org/abs/2509.07969) or [HuggingFace](https://huggingface.co/papers/2509.07969))| Tianjian Li, Tao Liu, Wei Li, Junyi Li, Xin Lai | Mini-o3 is a visual search system that achieves state-of-the-art performance by scaling up multi-turn reasoning and interaction depth through a novel reinforcement learning strategy. The research objective is to develop a Vision-Language Model capable of executing deep, trial-and-error reasoning over tens of interaction turns to solve complex visual search tasks where existing models fail. The methodology involves training on a new challenging dataset (Visual Probe) using a two-phase approach: Supervised Fine-Tuning on diverse "cold-start" trajectories, followed by reinforcement learning with GRPO enhanced by an "over-turn masking" strategy that avoids penalizing trajectories that exceed the training turn limit. The primary result shows that Mini-o3 achieves 48.0% accuracy on the VisualProbe-Hard benchmark, a significant improvement over the 35.1% from the previous state-of-the-art model, demonstrating effective scaling of reasoning depth at inference despite a limited training budget of only six turns. For AI practitioners, the over-turn masking technique is a key implication, providing a practical method to train agents on a fixed, short-turn budget while enabling them to generalize to much longer, more complex reasoning chains during deployment. |
| Visual Representation Alignment for Multimodal Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.07979) or [HuggingFace](https://huggingface.co/papers/2509.07979))| Heeseong Shin, Hyungyu Choi, Junwan Kim, Jaewoo Jung, Heeji Yoon | This paper introduces VIRAL, a regularization method that aligns internal visual representations of MLLMs with features from vision foundation models to improve fine-grained visual understanding. The primary objective is to mitigate the degradation of detailed visual information in Multimodal Large Language Models (MLLMs) that occurs under the conventional text-only supervision paradigm of instruction tuning. The key methodology involves adding an auxiliary alignment loss, based on cosine similarity, that explicitly regularizes the MLLM's intermediate visual representations (e.g., at the 16th layer) to match the feature outputs from a separate, pre-trained Vision Foundation Model (VFM) like DINOv2. The primary result shows that applying VIRAL to LLaVA-1.5-7B with a SigLIPv2 encoder improves accuracy on the CV-Bench2D benchmark from 58.90% to 62.66%. For AI practitioners, the principal implication is that this lightweight regularization technique can be integrated into existing MLLM fine-tuning pipelines to enhance visual grounding and performance on vision-centric tasks without requiring architectural changes to the model. |
| Reconstruction Alignment Improves Unified Multimodal Models (Read more on [arXiv](https://arxiv.org/abs/2509.07295) or [HuggingFace](https://huggingface.co/papers/2509.07295))| XuDong Wang, Luke Zettlemoyer, Trevor Darrell, Ji Xie | This paper introduces Reconstruction Alignment (RecA), a self-supervised post-training method to improve Unified Multimodal Models (UMMs) by training them to reconstruct images from their own visual encoder embeddings. The research objective is to resolve the misalignment between visual understanding and generation capabilities in UMMs that arises from training on sparse text-image pairs. The key methodology involves conditioning a UMM on its own visual understanding embeddings, used as dense "visual prompts," and optimizing it via a self-supervised reconstruction loss to regenerate the input image. With only 27 GPU-hours, post-training a 1.5B parameter model with RecA substantially improved its GenEval score from 0.73 to 0.90 and its DPGBench score from 80.93 to 88.15. For AI practitioners, the principal implication is that RecA offers a computationally efficient, data-agnostic post-training strategy to significantly enhance the generation and editing fidelity of existing UMMs without requiring additional labeled data or architectural modifications. |
| UMO: Scaling Multi-Identity Consistency for Image Customization via
  Matching Reward (Read more on [arXiv](https://arxiv.org/abs/2509.06818) or [HuggingFace](https://huggingface.co/papers/2509.06818))| Fei Ding, Mengqi Huang, Wenxu Wu, fenfan, cb1cyf | UMO is a framework that uses reinforcement learning with a multi-to-multi matching reward to improve multi-identity consistency and reduce confusion in image customization. The objective is to scale high-fidelity identity preservation for multiple subjects in customized image generation by addressing the identity confusion that arises when using multiple reference images. The core methodology is Reference Reward Feedback Learning (ReReFL), which applies a Multi-Identity Matching Reward (MIMR) to a diffusion model; MIMR formulates a global assignment problem between generated faces and reference identities, solved with the Hungarian algorithm to positively reward correct matches and penalize confusion during fine-tuning. UMO significantly enhances performance; when applied to the UNO model on the XVerseBench multi-subject task, it improved the ID-Sim score from 31.82 to 69.09 and the ID-Conf (identity confusion) score from 61.06 to 78.06. Practitioners can apply UMO's reinforcement learning framework as a fine-tuning step on existing image customization models to substantially boost multi-identity fidelity and reduce face-swapping errors, enabling more robust personalized generation at scale. |
| Curia: A Multi-Modal Foundation Model for Radiology (Read more on [arXiv](https://arxiv.org/abs/2509.06830) or [HuggingFace](https://huggingface.co/papers/2509.06830))| Elodie Ferreres, Helene Philippe, Antoine Saporta, Julien Khlaut, Corentin Dancette | The paper introduces Curia, a multi-modal radiological foundation model developed by pre-training a Vision Transformer on 200 million CT and MRI images using self-supervised learning. The main objective is to create a single, generalizable model for a wide range of radiological tasks to overcome the limitations of specialized, single-task models. The key methodology involves using the DINOv2 self-supervised learning algorithm on a large-scale, real-world clinical dataset (150,000 exams) and then evaluating the frozen model backbone by training only lightweight prediction heads for 19 downstream tasks. The primary result is that Curia meets or surpasses the performance of existing foundation models and radiologists on the introduced CuriaBench benchmark, achieving 98.40% accuracy on CT organ recognition and exhibiting strong cross-modal generalization. The principal implication for AI practitioners is that large-scale, self-supervised pre-training on unlabeled, domain-specific data yields a powerful feature extractor that enables the rapid development of high-performance, data-efficient models for diverse clinical applications using simple, lightweight classifiers. |
| F1: A Vision-Language-Action Model Bridging Understanding and Generation
  to Actions (Read more on [arXiv](https://arxiv.org/abs/2509.06951) or [HuggingFace](https://huggingface.co/papers/2509.06951))| Zherui Qiu, Jia Zeng, Hao Li, Weijie Kong, aopolin-lv | F1 is a pretrained Vision-Language-Action (VLA) framework that integrates goal-conditioned visual foresight generation into the decision-making pipeline to improve robotic manipulation. The primary objective is to overcome the limitations of reactive state-to-action policies by creating a foresight-driven model capable of robust performance in dynamic and long-horizon environments. F1 utilizes a Mixture-of-Transformer (MoT) architecture with dedicated experts for perception, foresight generation, and control, reformulating action generation as a foresight-guided inverse dynamics problem trained via a three-stage recipe. On 9 real-world tasks, F1 achieved an 82.2% average success rate, substantially outperforming the best-performing reactive baseline (π₀ at 65.2%), and ranked first across all suites of the LIBERO simulation benchmark. The principal implication for AI practitioners is that explicitly generating and conditioning on future visual states as intermediate planning targets is a highly effective strategy for enhancing the robustness and generalization of visuomotor policies in complex, dynamic scenarios. |
| Staying in the Sweet Spot: Responsive Reasoning Evolution via
  Capability-Adaptive Hint Scaffolding (Read more on [arXiv](https://arxiv.org/abs/2509.06923) or [HuggingFace](https://huggingface.co/papers/2509.06923))| Yongcheng Zeng, Erxue Min, Zexu Sun, zhaojinm, ChillingDream | SEELE is a novel supervision-aided Reinforcement Learning with Verifiable Rewards (RLVR) framework that dynamically adjusts problem difficulty. This work addresses the problem of dynamically adjusting off-policy guidance difficulty in RLVR to match evolving model capabilities and optimize learning efficiency. The methodology is grounded in a theoretical analysis showing maximum RLVR learning efficiency at 50% rollout accuracy, achieved by appending dynamically adjusted hints to problems. A multi-round sampling framework employs an Item Response Theory (IRT) model to predict accuracy based on hint length, iteratively adjusting hint length to maintain the 50% target. SEELE outperforms GRPO by +11.8 points on average across six math reasoning benchmarks, offering AI practitioners a method to enhance RLVR training efficiency by precisely aligning problem difficulty with model capability. |
| Language Self-Play For Data-Free Training (Read more on [arXiv](https://arxiv.org/abs/2509.07414) or [HuggingFace](https://huggingface.co/papers/2509.07414))| Vijai Mohan, Yuandong Tian, Qi Ma, Mengting Gu, Jakub Grudzien Kuba | The paper introduces Language Self-Play (LSP), a reinforcement learning framework enabling large language models to improve performance without external training data by playing against themselves. The main objective is to overcome the data dependency bottleneck in large language model training, allowing models to improve autonomously. LSP models improvement as a competitive game between a "Challenger" generating increasingly difficult queries and a "Solver" responding, instantiated by a single LLM (Llama-3.2-3B-Instruct) in a self-play mechanism, regularized by KL-divergence and quality self-rewards. Experiments on the AlpacaEval benchmark show LSP-trained models achieve a 40.6% overall win-rate against a base model without using any external training data, comparable to data-driven baselines (GRPO at 40.9%), and further boosting an RL-trained model's win-rate from 40.9% to 43.1%. This work implies that AI practitioners can develop and enhance LLMs, particularly for challenging and conversational tasks, without continuous reliance on new human-generated datasets, potentially reducing data acquisition costs and improving model autonomy. |
| Causal Attention with Lookahead Keys (Read more on [arXiv](https://arxiv.org/abs/2509.07301) or [HuggingFace](https://huggingface.co/papers/2509.07301))| Quanquan Gu, Huizhuo Yuan, Peng Sun, Zhuoqing Song | Causal Attention with Lookahead Keys (CASTLE) is a novel attention mechanism designed to address the limitations of standard causal attention in pretraining by allowing token keys to incorporate information from subsequent tokens while preserving autoregressive properties. Its methodology introduces "lookahead keys" that are continually updated as context unfolds, leveraging a hybrid design of causal and lookahead keys and a SiLU function in attention weight calculation. Efficient parallel training at O(L^2d) complexity and O(td) inference are enabled by a derived mathematical equivalence, avoiding explicit lookahead key materialization. CASTLE consistently outperforms standard causal attention, with the CASTLE-XL model achieving a 0.0348 lower validation loss than Baseline-XL, alongside improved downstream task performance. This provides AI practitioners with a more token-efficient approach for large language model pretraining, enhancing performance and natural language understanding under fixed training-token budgets. |
| Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human
  Preference (Read more on [arXiv](https://arxiv.org/abs/2509.06942) or [HuggingFace](https://huggingface.co/papers/2509.06942))| Yingfang Zhang, Shiyi Zhang, Zhantao Yang, Zhimin Li, Xiangwei Shen | The paper introduces Direct-Align and Semantic Relative Preference Optimization (SRPO), a framework for efficiently aligning the entire diffusion model trajectory with fine-grained human preferences using text-conditioned rewards. The objective is to overcome the limitations of existing alignment methods that are computationally expensive and restricted to late-stage diffusion steps, by developing an efficient technique to optimize the full generation trajectory and enable online reward adjustment without offline re-tuning. The key methodology, Direct-Align, recovers a clean image from any noisy timestep in a single, differentiable step by using a predefined noise prior, while SRPO formulates reward as the difference between scores from positive and negative text-conditioned prompts for online regularization. When fine-tuning the FLUX.1.dev model, SRPO increased the human-evaluated "Excellent" rate for overall preference from 5.3% to 29.4% and achieved convergence in just 10 minutes. For AI practitioners, this means large diffusion models can be rapidly fine-tuned for specific aesthetic qualities (e.g., photorealism) by augmenting prompts with control words, eliminating the need for costly offline reward model training and enabling highly efficient, targeted alignment. |
| SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric
  Knowledge (Read more on [arXiv](https://arxiv.org/abs/2509.07968) or [HuggingFace](https://huggingface.co/papers/2509.07968))| Dipanjan Das, Sasha Goldshtein, Giovanni D'Antonio, Gal Yona, Lukas Haas | This paper introduces SimpleQA Verified, a curated 1,000-prompt benchmark for evaluating the parametric factuality of LLMs by addressing critical limitations in OpenAI's SimpleQA. The objective was to create a more reliable evaluation set by correcting noisy labels, topical biases, and question redundancy. The methodology involved a multi-stage filtering process on the original dataset, including semantic de-duplication, source reconciliation, topic re-balancing, and an adversarially selected difficulty filter, alongside improvements to the autorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a state-of-the-art F1-score of 55.6, outperforming other frontier models, including GPT-5. The principal implication for AI practitioners is the availability of a higher-fidelity benchmark and evaluation code to more accurately measure parametric knowledge, track genuine progress in model factuality, and mitigate hallucinations. |
| Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with
  Quantization-Aware Scheduling (Read more on [arXiv](https://arxiv.org/abs/2509.01624) or [HuggingFace](https://huggingface.co/papers/2509.01624))| Diana Marculescu, Natalia Frumkin | Q-Sched introduces a post-training quantization-aware scheduler for few-step diffusion models, achieving high image fidelity with reduced model size by modifying the sampling trajectory. The objective is to enable efficient, high-fidelity image generation with few-step diffusion models by integrating quantization without degrading performance. Q-Sched modifies the diffusion model scheduler by optimizing two learnable scalar preconditioning coefficients applied to the input noise and quantized noise prediction, using a novel reference-free Joint Alignment-Quality (JAQ) loss. Q-Sched achieves a 15.5% FID improvement over FP16 4-step Latent Consistency Models while reducing model size by 4x for W4A8 quantization. AI practitioners can leverage Q-Sched to deploy few-step diffusion models with significantly reduced model size and inference cost, enabling high-fidelity image generation on resource-constrained hardware. |
| ΔL Normalization: Rethink Loss Aggregation in RLVR (Read more on [arXiv](https://arxiv.org/abs/2509.07558) or [HuggingFace](https://huggingface.co/papers/2509.07558))| Lili Qiu, Yuqing Yang, Yike Zhang, Xufang Luo, Zhiyuan He | ΔL Normalization is an unbiased, minimum-variance loss aggregation technique for Reinforcement Learning with Verifiable Rewards (RLVR) that stabilizes training and improves model performance. The paper's objective is to address the bias and excessive variance issues present in existing length-dependent and length-independent loss aggregation methods for RLVR by developing an estimator that is both unbiased and minimizes gradient variance. The key methodology involves reformulating the problem as constructing a minimum-variance unbiased estimator for the true policy gradient, proposing ΔL Normalization that uses length-dependent scaling $x_i = \frac{1}{M} \frac{L_i^\alpha}{\sum_{j=1}^G L_j^\alpha}$ for sample-level gradients. Empirical results show ΔL Normalization consistently outperforms baselines; for instance, on the CountDown 3B model, it achieved an Avg@8 score of 0.847 and a Pass@8 of 0.938, surpassing GRPO Norm's 0.811 and 0.928, respectively, and exhibiting the highest training monotonicity with scores consistently above 0.94. AI practitioners can leverage ΔL Normalization to enhance the stability and convergence of RLVR models, especially when dealing with the dynamic and highly variable response lengths characteristic of reasoning tasks. |
