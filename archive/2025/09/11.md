

## Papers for 2025-09-11

| Title | Authors | Summary |
|-------|---------|---------|
| A Survey of Reinforcement Learning for Large Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2509.08827) or [HuggingFace](https://huggingface.co/papers/2509.08827))| Runze Liu, Youbang Sun, Bingxiang He, Yuxin Zuo, Kaiyan Zhang | This survey systematically reviews the application of Reinforcement Learning (RL) for transforming Large Language Models (LLMs) into Large Reasoning Models (LRMs) with advanced reasoning capabilities. The paper's objective is to synthesize the foundational components, core problems, and applications of RL for enhancing LLM reasoning, identifying key trends and future directions for scaling these methods. The paper conducts a comprehensive literature review, structuring its analysis around a taxonomy of RL for LRMs that includes reward design, policy optimization (e.g., Group Relative Policy Optimization - GRPO), sampling strategies, training resources, and key applications like coding and agentic tasks. The survey identifies a primary trend of using Reinforcement Learning with Verifiable Rewards (RLVR) to significantly boost performance on complex logical tasks, citing a finding where one-shot RLVR more than doubled MATH500 accuracy for a Qwen2.5-Math-1.5B model, while also noting that the debate on whether RL discovers new abilities or merely sharpens existing ones remains a central, unresolved issue. The principal implication for AI practitioners is that they should prioritize RLVR with rule-based, automatically verifiable rewards over alignment-focused RLHF for developing reasoning-intensive models, as this approach enables scalable capability enhancement; critic-free algorithms like GRPO are identified as a robust and computationally efficient method for implementing such training pipelines. |
| RewardDance: Reward Scaling in Visual Generation (Read more on [arXiv](https://arxiv.org/abs/2509.08826) or [HuggingFace](https://huggingface.co/papers/2509.08826))| Liang Li, Ming Li, Zilyu Ye, Yu Gao, Jie Wu | RewardDance is a scalable reward modeling framework for visual generation that employs a generative paradigm to overcome the limitations of traditional regressive approaches and mitigate reward hacking. The research objective is to establish scalability as a core principle for designing effective visual Reward Models (RMs) that consistently improve generation quality via Reinforcement Learning from Human Feedback (RLHF). The key methodology reformulates reward prediction as a next-token prediction task, where the reward score is the VLM's probability of generating a "yes" token in a comparative evaluation, enabling systematic scaling of both model size (up to 26B parameters) and input context (including instructions, references, and Chain-of-Thought). The primary result is a direct correlation between RM scale and generation quality; for the Seedream-3.0 model, increasing the RM size from 1B to 26B improved the text-to-image alignment score by +10.7 points (from a 74.1 baseline to 84.8). For AI practitioners, the principal implication is that investing in larger, context-rich generative RMs is a robust strategy for enhancing the performance and alignment of visual generation models, as larger RMs are more resistant to reward hacking and provide a more effective training signal. |
| 3D and 4D World Modeling: A Survey (Read more on [arXiv](https://arxiv.org/abs/2509.07996) or [HuggingFace](https://huggingface.co/papers/2509.07996))| Ao Liang, Youquan Liu, Jianbiao Mei, Wesley Yang, Lingdong Kong | This survey presents the first comprehensive review and structured taxonomy for 3D and 4D world modeling, focusing on native geometric representations. The paper's objective is to address fragmented literature by establishing precise definitions for world models and organizing existing approaches that leverage video, occupancy, and LiDAR data into a coherent framework. The authors introduce a hierarchical taxonomy that categorizes models based on their core data representation into three primary classes: video-based (VideoGen), occupancy-based (OccGen), and LiDAR-based (LiDARGen). The survey benchmarks numerous models, revealing significant progress in generation fidelity; for instance, in occupancy reconstruction on nuScenes, the Triplane-VAE-based T³Former achieves a state-of-the-art mIoU of 85.50%. For practitioners, this work provides a unified reference for selecting appropriate 3D/4D modeling techniques, datasets, and evaluation metrics for applications like autonomous driving and robotics, highlighting key challenges such as long-horizon physical fidelity and cross-modal coherence. |
| AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making
  through Multi-Turn Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.08755) or [HuggingFace](https://huggingface.co/papers/2509.08755))| Honglin Guo, Baodai Huang, Chenyang Liao, Jixuan Huang, Zhiheng Xi | This paper introduces AgentGym-RL, a framework, and ScalingInter-RL, a training method, for training LLM agents for long-horizon decision-making using multi-turn reinforcement learning. The main objective is to create a unified, interactive RL framework to train LLM agents from scratch for multi-turn decision-making without relying on supervised fine-tuning as a preliminary step. The methodology consists of two parts: 1) AgentGym-RL, a modular and extensible framework separating agent, environment, and training components across diverse scenarios like web navigation and scientific tasks, and 2) ScalingInter-RL, a training approach that progressively increases the maximum number of agent-environment interaction turns, balancing initial exploitation with later exploration. The primary result is that an open-source 7B parameter model trained with the framework and method achieves an average performance improvement of 33.65 points across five task domains, matching or outperforming larger proprietary models like OpenAI-03 and Gemini-2.5-Pro. The principal implication for AI practitioners is that investing compute in targeted RL post-training on smaller models can be more effective and efficient for developing capable agents than simply scaling the model's parameter count, as the trained 7B model outperformed models nearly ten times its size. |
| P3-SAM: Native 3D Part Segmentation (Read more on [arXiv](https://arxiv.org/abs/2509.06784) or [HuggingFace](https://huggingface.co/papers/2509.06784))| Yunhan Yang, Jiachen Xu, Xinhao Yan, Yang Li, murcherful | The paper introduces P3-SAM, a native 3D point-promptable model for fully automatic part segmentation of complex 3D objects, trained on a new 3.7 million model dataset. The objective is to overcome the imprecision and automation limitations of prior methods that lift 2D segmentation to 3D. The methodology utilizes a Point Transformer V3 feature extractor and a two-stage multi-head segmentor with an IoU predictor to generate precise masks from a single point prompt; automation is achieved by using Farthest Point Sampling for prompt generation and Non-Maximum Suppression for merging predicted masks. The model achieves state-of-the-art performance, demonstrating an average IOU of 81.14% on the PartObj-Tiny benchmark for segmentation with connectivity. For AI practitioners, P3-SAM provides a fully automated, class-agnostic tool for direct integration into 3D asset management and generative pipelines, removing the need for manual prompting or category specification for geometric decomposition. |
| Hunyuan-MT Technical Report (Read more on [arXiv](https://arxiv.org/abs/2509.05209) or [HuggingFace](https://huggingface.co/papers/2509.05209))| Yang Du, Mingyang Song, Bingxin Qu, Zheng Li, Mao Zheng | This paper introduces Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B, open-source 7B parameter models for multilingual translation, detailing a holistic training pipeline from pre-training to a weak-to-strong reinforcement learning stage. The primary objective is to develop a parameter-efficient multilingual translation model that excels in diverse scenarios, particularly low-resource and Mandarin-minority language pairs, and to introduce a novel weak-to-strong fusion model for enhanced test-time performance. The methodology consists of a multi-stage process: general and MT-oriented pre-training, followed by supervised fine-tuning (SFT), reinforcement learning (RL) with a composite quality and terminology-aware reward function, and an advanced weak-to-strong RL stage to train a fusion model (Chimera) that aggregates multiple candidate translations. The models demonstrate state-of-the-art performance, with Hunyuan-MT-7B achieving an XCOMET-XXL score of 0.6082 on the Mandarin⇔Minority translation benchmark, a relative improvement of approximately 4.7% over the next-best system, Gemini-2.5-Pro (0.5811). For AI practitioners, the paper provides a replicable training recipe for specializing foundation models for translation and shows that a weak-to-strong fusion approach offers a practical alternative to Chain-of-Thought for improving translation quality in quality-sensitive, non-real-time applications. |
| <think> So let's replace this phrase with insult... </think> Lessons
  learned from generation of toxic texts with LLMs (Read more on [arXiv](https://arxiv.org/abs/2509.08358) or [HuggingFace](https://huggingface.co/papers/2509.08358))| Alexander Panchenko, Daniil Moskovskiy, Sergey Pletenev | This paper demonstrates that Large Language Models (LLMs) are currently unsuitable for generating synthetic toxic data to train text detoxification systems. The research aims to assess if LLM-generated toxic text can replace human-annotated data for creating detoxification training corpora. Using activation-patched LLMs like Llama 3 and Qwen3 to toxify neutral text, the authors trained BART models and found that models trained on synthetic data significantly underperformed, showing a performance drop of up to 30% (a -0.159 decrease in the joint metric) compared to a baseline trained on human data. This performance degradation is caused by a critical "lexical diversity gap," where LLMs generate a small, repetitive vocabulary of insults, failing to capture the variety of human toxicity. For AI practitioners, this implies that generating high-quality synthetic data for sensitive and nuanced domains like detoxification is non-trivial, and reliance on diverse, human-annotated datasets remains essential for building robust and generalizable models. |
| EnvX: Agentize Everything with Agentic AI (Read more on [arXiv](https://arxiv.org/abs/2509.08088) or [HuggingFace](https://huggingface.co/papers/2509.08088))| Wenzheng Tom Tang, Yikun Wang, Yingxuan Yang, Zimian Peng, Linyao Chen | EnvX is a framework that transforms GitHub repositories into collaborative, intelligent agents through a structured, tool-driven agentization process. The main objective is to automate the conversion of static open-source repositories into autonomous agents that can be invoked via natural language and collaborate to solve complex software tasks. The key methodology involves a three-phase process: TODO-guided environment initialization from repository documentation, human-aligned agentic automation for task execution, and an Agent-to-Agent (A2A) protocol for multi-agent communication. On the GitTaskBench benchmark, EnvX achieves a 51.85% task pass rate with the Claude 3.7 Sonnet model, outperforming prior frameworks. The principal implication for practitioners is the ability to interact with and orchestrate complex code repositories as callable services through natural language, reducing manual integration effort and enabling automated, multi-repository workflows. |
| HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI
  Assistants (Read more on [arXiv](https://arxiv.org/abs/2509.08494) or [HuggingFace](https://huggingface.co/papers/2509.08494))| Jacy Reese Anthis, Jacob Haimes, Daniel Samuelson, Benjamin Sturgeon | This paper introduces HUMANAGENCYBENCH (HAB), a scalable, LLM-automated benchmark that evaluates AI assistants' support for human agency across six distinct dimensions. The research objective is to operationalize and systematically measure how contemporary LLM-based assistants support or reduce human agency, defined as a person's capacity to willfully shape their future through action. The methodology employs a three-stage LLM pipeline: an LLM first simulates a diverse set of user queries (tests) for each of the six agency dimensions, another LLM validates them, and a final evaluator LLM scores the subject models' responses against a deduction-based rubric. Primary results indicate low-to-moderate agency support overall, with significant variance; for instance, while Anthropic's models scored highest on average, they performed worst on the "Avoid Value Manipulation" dimension, where Meta's Llama-4-Scout achieved the highest score (66.9%). The principal implication for AI practitioners is that current alignment techniques, such as instruction-following via RLHF, do not inherently produce agency-supporting behaviors and can create trade-offs, necessitating a shift towards more robust sociotechnical alignment targets beyond standard preference optimization. |
