

## Papers for 2025-09-12

| Title | Authors | Summary |
|-------|---------|---------|
| HuMo: Human-Centric Video Generation via Collaborative Multi-Modal
  Conditioning (Read more on [arXiv](https://arxiv.org/abs/2509.08519) or [HuggingFace](https://huggingface.co/papers/2509.08519))| Zhuowei Chen, Bingchuan Li, Jiawei Liu, Tianxiang Ma, Liyang Chen | HuMo is a unified framework for human-centric video generation (HCVG) conditioned on text, reference images, and audio. The objective is to address data scarcity and the difficulty of collaborative control by developing a system that jointly manages subject preservation, audio-visual synchronization, and text adherence. The methodology involves a two-stage data processing pipeline to create a paired triplet dataset and a progressive training paradigm that first learns subject preservation via minimal-invasive image injection, then incorporates audio-visual sync using cross-attention and a novel "focus-by-predicting" strategy for facial regions. HuMo surpasses specialized state-of-the-art methods, with the 17B model achieving a Text-Video Alignment (TVA) score of 3.939 and an Identity-Curve (ID-Cur) score of 0.731 on the subject preservation task, outperforming prior models. For AI practitioners, the principal implication is a reusable, progressive training framework and data pipeline for building multi-modal generative models, demonstrating how to decouple and then jointly optimize for complex, heterogeneous control signals (text, image, audio) in a unified architecture. |
| SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.09674) or [HuggingFace](https://huggingface.co/papers/2509.09674))| Zhaohui Yang, Yuhao Zhang, Jiale Yu, Yuxin Zuo, Haozhan Li | SimpleVLA-RL is an efficient online reinforcement learning framework that scales Vision-Language-Action (VLA) models by improving data efficiency, generalization, and task performance using simple outcome-based rewards. The primary objective is to determine if reinforcement learning, leveraging a simple, scalable reward mechanism, can enhance the long-horizon action planning of VLA models to overcome the data scarcity and poor generalization inherent in supervised fine-tuning (SFT). The key methodology involves an online RL framework built upon Group Relative Policy Optimization (GRPO) that uses binary (success/failure) outcome rewards, VLA-specific interactive trajectory sampling, and exploration enhancements such as dynamic sampling and increased sampling temperature. The primary result shows that in a data-scarce setting on the LIBERO-Long benchmark (using only one demonstration), the method increased the success rate from 17.3% to 91.7%, significantly outperforming the SFT baseline. The principal implication for AI practitioners is that this RL framework allows for the development of more robust and generalizable robotic policies with substantially less expert demonstration data, enabling cost-effective scaling through simulation and improving sim-to-real transfer. |
| EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for
  Speech-to-Speech LLMs (Read more on [arXiv](https://arxiv.org/abs/2509.09174) or [HuggingFace](https://huggingface.co/papers/2509.09174))| Kaiqi Kou, Xiangnan Ma, Zhanchen Dai, Yuhao Du, Yuhao Zhang | The paper introduces EchoX, a training framework that mitigates the acoustic-semantic gap in speech-to-speech LLMs by using a novel "Echo training" stage to align semantic representations with speech token generation. The main objective is to address the degradation in knowledge and reasoning capabilities that occurs when adapting text-based LLMs for end-to-end speech-to-speech tasks. The key methodology is a three-stage framework: first, building a speech-to-text LLM; second, training a text-to-codec module; and third, an "Echo training" stage that uses hidden states from the speech-to-text LLM to predict speech token targets dynamically generated by the text-to-codec module. The primary result is that with approximately six thousand hours of audio data, EchoX achieves a score of 40.6 on the Web Questions benchmark, a performance competitive with models trained on orders of magnitude more data. The principal implication for AI practitioners is that this Echo training strategy offers a data-efficient method to construct SLLMs that better preserve the reasoning abilities of the base text LLM by explicitly bridging the representation gap between semantics and acoustics. |
| Kling-Avatar: Grounding Multimodal Instructions for Cascaded
  Long-Duration Avatar Animation Synthesis (Read more on [arXiv](https://arxiv.org/abs/2509.09595) or [HuggingFace](https://huggingface.co/papers/2509.09595))| Wentao Hu, Zekun Wang, Wenyuan Zhang, Jiwen Liu, Yikang Ding | Kling-Avatar is a cascaded framework using a Multimodal Large Language Model (MLLM) to generate coherent, long-duration avatar animations from multimodal instructions. The objective is to synthesize high-fidelity avatar videos that follow high-level semantic intent from audio, image, and text inputs, overcoming the narrative incoherence of methods that only track low-level cues. The key methodology is a two-stage pipeline where an MLLM Director first creates a semantic "blueprint video," which then guides the parallelized generation of high-resolution sub-clips conditioned on anchor keyframes from the blueprint. In human preference-based Good/Same/Bad (GSB) evaluations, Kling-Avatar achieved an overall score of 2.39 against the OmniHuman-1 baseline, showing superior performance across lip sync, visual quality, and identity consistency. The principal implication for AI practitioners is the validation of an architecture that decouples high-level semantic planning (via MLLM) from parallelized, fine-grained video synthesis, enabling the generation of controllable and temporally stable long-duration content. |
| Harnessing Uncertainty: Entropy-Modulated Policy Gradients for
  Long-Horizon LLM Agents (Read more on [arXiv](https://arxiv.org/abs/2509.09265) or [HuggingFace](https://huggingface.co/papers/2509.09265))| Xintao Wang, Yingru Li, Yuqian Fu, Jiacai Liu, Jiawei Wang | This paper introduces Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates policy gradients using step-wise uncertainty to improve credit assignment for long-horizon LLM agents. The primary objective is to solve the problem where standard policy gradient magnitudes are inherently coupled with policy entropy, leading to inefficient updates for confident actions and instability from uncertain ones in sparse-reward environments. The key methodology involves a "Self-Calibrating Gradient Scaling" mechanism that modulates the advantage signal based on step-wise entropy and a "Future Clarity Bonus" that intrinsically rewards actions leading to more predictable subsequent states. Experiments demonstrate that EMPG significantly outperforms strong baselines, for example, improving the success rate of a DAPO-trained agent on the WebShop benchmark from 79.6% to 82.7%. For AI practitioners, EMPG provides a general-purpose, value-free module to enhance the performance and stability of RL-trained agents on complex tasks by using the model's own uncertainty to create a more effective learning signal, thus overcoming common training plateaus. |
| FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning
  Dataset and Comprehensive Benchmark (Read more on [arXiv](https://arxiv.org/abs/2509.09680) or [HuggingFace](https://huggingface.co/papers/2509.09680))| Shuai Bai, Linjiang Huang, Chengqi Duan, Aldrich Yu, Rongyao Fang | This paper introduces FLUX-Reason-6M, a 6-million-image reasoning dataset, and PRISM-Bench, a corresponding benchmark, to advance text-to-image (T2I) synthesis. The primary objective is to create a large-scale, reasoning-focused dataset and a comprehensive evaluation framework to train and assess complex T2I generation capabilities, addressing the limitations of existing datasets. The methodology involves synthesizing images with a powerful T2I model, then using advanced Vision-Language Models (VLMs) for multi-stage filtering, multi-label categorization across six characteristics (e.g., Composition, Text rendering), and generating detailed "Generation Chain-of-Thought" (GCoT) captions. Evaluation on PRISM-Bench across 19 models shows that leading closed-source models significantly outperform open-source alternatives, with GPT-Image-1 achieving the highest overall score of 86.3 when evaluated by GPT-4.1, yet all models struggle with long-text instruction following and text rendering. For AI practitioners, this work provides a public, large-scale dataset engineered to imbue T2I models with complex reasoning and a robust benchmark with evaluation code to identify and address critical performance gaps, especially in compositional and long-prompt understanding. |
| VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action
  Model (Read more on [arXiv](https://arxiv.org/abs/2509.09372) or [HuggingFace](https://huggingface.co/papers/2509.09372))| Zirui Ge, Can Cui, Lingxiao Li, Pengxiang Ding, Yihao Wang | VLA-Adapter is a novel, lightweight paradigm that enables tiny-scale Vision-Language-Action models to achieve state-of-the-art performance without extensive robotic data pre-training. The paper's main objective is to investigate how to effectively bridge vision-language representations to the action space to reduce reliance on large-scale VLMs. The key methodology involves a Policy network with a "Bridge Attention" mechanism that selectively integrates conditions from both all-layer raw VLM features and learnable "ActionQuery" latents to guide action generation. The primary result shows that on the LIBERO-Long benchmark, the 0.5B parameter VLA-Adapter achieves a 95.0% success rate, outperforming the 7B parameter OpenVLA-OFT model, while offering a 3x higher throughput of 219.2Hz. The principal implication for AI practitioners is that this paradigm drastically lowers the computational cost and training time (8 hours on one consumer-grade GPU) for developing high-performance VLA models, making them more accessible for deployment. |
| Can Understanding and Generation Truly Benefit Together -- or Just
  Coexist? (Read more on [arXiv](https://arxiv.org/abs/2509.09666) or [HuggingFace](https://huggingface.co/papers/2509.09666))| Hui Han, Junyan Ye, Zongjian Li, Kaiqing Lin, Zhiyuan Yan | The paper introduces UAE, an autoencoder-inspired framework that unifies multimodal understanding (Image-to-Text, I2T) and generation (Text-to-Image, T2I) using a shared reconstruction objective optimized via reinforcement learning. The main objective is to create a mutually beneficial, co-evolving system where gains in understanding directly improve generation and vice versa, by treating understanding as encoding and generation as decoding within a single reconstruction loop. The core methodology is Unified-GRPO, a three-stage reinforcement learning scheme that first initializes the system with a reconstruction loss, then alternately fine-tunes the encoder to produce more informative captions and the decoder to better reconstruct from them. The primary result is that the UAE model achieves a state-of-the-art overall unified score of 86.09 on the new Unified-Bench, surpassing models like GPT-4o-Image (85.95) by demonstrating superior bidirectional information flow. For AI practitioners, the principal implication is that framing multimodal tasks within a reconstruction-based autoencoder paradigm, optimized via RL, provides an effective method for building more deeply integrated and synergistic unified models where components mutually reinforce each other's capabilities. |
| SpatialVID: A Large-Scale Video Dataset with Spatial Annotations (Read more on [arXiv](https://arxiv.org/abs/2509.09676) or [HuggingFace](https://huggingface.co/papers/2509.09676))| Jian Gao, Youtian Lin, Rujie Zheng, Yufeng Yuan, Jiahao Wang | This paper introduces SpatialVID, a large-scale video dataset with explicit spatial and semantic annotations for training spatial intelligence models. The primary objective is to address the scarcity of large-scale, in-the-wild video data with ground-truth camera motion and rich 3D information. The methodology consists of a multi-stage pipeline that collects over 21,000 hours of raw video, filters it into 2.7 million high-quality clips, and enriches them with per-frame camera poses, depth maps, dynamic masks, and structured captions using tools like MegaSaM and large language models. The primary result is a dataset of 7,089 hours (127 million annotated frames) featuring diverse scenes and camera movements, which demonstrates superior quality and motion diversity compared to existing large-scale datasets like Panda-70M. For AI practitioners, SpatialVID serves as a key asset for training and evaluating models in 3D reconstruction, novel view synthesis, and controllable video generation by providing direct, high-quality supervision signals for 3D geometry and camera dynamics. |
| Visual Programmability: A Guide for Code-as-Thought in Chart
  Understanding (Read more on [arXiv](https://arxiv.org/abs/2509.09286) or [HuggingFace](https://huggingface.co/papers/2509.09286))| Ethan Chern, Jiadi Su, Fei Zhang, Yan Ma, Bohao Tang | This paper introduces Visual Programmability, a learnable property for Vision-Language Models (VLMs) to adaptively choose between Code-as-Thought (CaT) and direct visual analysis for chart understanding. The research objective is to overcome the generalization failures of fixed-strategy models by teaching a VLM to dynamically select the optimal reasoning pathway based on a task's suitability for programmatic representation. The methodology involves an adaptive framework where a VLM's selection policy is trained using reinforcement learning (GRPO) guided by a novel dual-reward system that promotes both factual accuracy and strategic flexibility. The resulting 7B parameter adaptive model achieved a 62.8% average accuracy across four diverse benchmarks, outperforming fixed-strategy baselines by dynamically modulating its code usage from 76.0% on highly structured charts to 10.1% on complex, "in-the-wild" charts. For AI practitioners, this work provides a concrete framework using a dual-reward RL system to build more robust models that can learn to select the appropriate reasoning tool for a given task, mitigating common failure modes like numerical hallucination and strategy collapse. |
| Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust
  Text-based Person Retrieval (Read more on [arXiv](https://arxiv.org/abs/2509.09118) or [HuggingFace](https://huggingface.co/papers/2509.09118))| Kaicheng Yang, Ziyong Feng, Xiang An, Yifan Zhang, Tianlu Zheng | This work introduces the GA-DMS framework and the 5M-pair WebPerson dataset to improve robust text-based person retrieval by mitigating textual noise and enhancing fine-grained cross-modal alignment. The primary objective is to overcome CLIP's limitations for person retrieval, specifically the scarcity of person-centric data and vulnerability to noisy web-crawled text tokens. The methodology combines a novel Gradient-Attention Similarity Score (GASS) to guide a dual-masking strategy—suppressing noisy tokens while using a masked prediction objective for informative ones—with a new dataset, WebPerson, constructed using MLLMs for automated image filtering and captioning. The GA-DMS model, pre-trained on the 5M WebPerson dataset, achieves state-of-the-art performance, including a Rank-1 accuracy of 77.60% on the CUHK-PEDES benchmark. For AI practitioners, this provides a scalable MLLM-based pipeline for creating domain-specific datasets and a gradient-guided masking technique to train vision-language models that are more robust to imperfect text annotations. |
| 2D Gaussian Splatting with Semantic Alignment for Image Inpainting (Read more on [arXiv](https://arxiv.org/abs/2509.01964) or [HuggingFace](https://huggingface.co/papers/2509.01964))| Guangming Lu, Xiaoming Li, Chaofeng Chen, learn12138 | This paper introduces a novel image inpainting framework that leverages 2D Gaussian Splatting to reconstruct missing image regions from a continuous representation, guided by semantic alignment. The research objective is to achieve local coherence and global consistency by encoding a masked image into patch-level 2D Gaussian parameters via a U-Net, reconstructing it with a differentiable rasterizer, and integrating semantic priors from a DINOv2 model. The framework demonstrates competitive performance, achieving a state-of-the-art LPIPS of 0.028 on the CelebA-HQ dataset for small masks. The principal implication for AI practitioners is the establishment of an efficient encoder-rendering paradigm for image restoration, presenting Gaussian-based representations as a viable alternative to traditional discrete pixel-synthesis or diffusion-based methods. |
| LoCoBench: A Benchmark for Long-Context Large Language Models in Complex
  Software Engineering (Read more on [arXiv](https://arxiv.org/abs/2509.09614) or [HuggingFace](https://huggingface.co/papers/2509.09614))| Jianguo Zhang, Rithesh Murthy, Zhiwei Liu, Zuxin Liu, Jielin Qiu | LoCoBench introduces a comprehensive benchmark to evaluate long-context LLMs on complex, multi-file software engineering tasks. The objective is to address the evaluation gap for long-context capabilities by testing reasoning across entire codebases and maintaining architectural consistency, which existing short-context benchmarks neglect. The methodology involves a systematic 5-phase pipeline that generates 8,000 evaluation scenarios with context lengths scaling from 10K to 1M tokens, assessed using a framework of 17 metrics including novel ones for architectural coherence and long-context utilization. The primary results reveal substantial performance gaps in current models, with the top-performing model, Gemini-2.5-Pro, achieving a LoCoBench Score (LCBS) of 2.312 out of 5. The principal implication for AI practitioners is that model selection for complex software development requires specific, multi-dimensional evaluation, as model capabilities vary significantly across different long-context tasks, and claimed context window size is not a reliable indicator of performance on realistic, multi-file workflows. |
| OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and
  Embodiment-aware Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.09332) or [HuggingFace](https://huggingface.co/papers/2509.09332))| Yuzheng Zhuang, Zhanguang Zhang, Shiguang Wu, Dafeng Chi, Yuecheng Liu | OmniEVA is an embodied planner that enhances multimodal reasoning and physical feasibility using a task-adaptive 3D grounding mechanism and an embodiment-aware reinforcement learning strategy. The research objective is to address the "Geometric Adaptability Gap" and "Embodiment Constraint Gap" in MLLMs by creating a planner that dynamically leverages 3D information and generates physically executable plans. The methodology introduces a Task-Adaptive Gated Router (TAGR) to selectively fuse 3D positional embeddings with visual features, and employs a Task- and Embodiment-aware GRPO (TE-GRPO) algorithm that optimizes for both semantic correctness and physical feasibility. The system achieves state-of-the-art performance on 7 of 8 embodied reasoning benchmarks, and its embodiment-aware fine-tuning improves the success rate on the challenging Mobile Placement (Hard) task by 50% over a model without this training. The principal implication for AI practitioners is that incorporating dynamic, context-aware 3D feature fusion and explicitly modeling physical robot constraints during RL-based fine-tuning can significantly improve the real-world executability and performance of embodied agents. |
| The Choice of Divergence: A Neglected Key to Mitigating Diversity
  Collapse in Reinforcement Learning with Verifiable Reward (Read more on [arXiv](https://arxiv.org/abs/2509.07430) or [HuggingFace](https://huggingface.co/papers/2509.07430))| Xiaoyu Tan, Zhijian Zhou, Jason Klein Liu, Jiaran Hao, Long Li | This paper introduces Diversity-Preserving Hybrid RL (DPH-RL), a framework that uses mass-covering f-divergences to mitigate diversity collapse in reinforcement learning with verifiable reward (RLVR). The primary objective is to resolve the paradox where RLVR improves single-attempt accuracy (Pass@1) but degrades multi-attempt performance (Pass@k) by replacing the standard mode-seeking reverse KL-divergence. The proposed methodology partitions data into "exploration" and "perfection" sets, applying a standard PPO objective to the former and a loss derived from forward-KL or JS-divergence to the latter, creating a "rehearsal mechanism" that preserves the initial policy's knowledge base. On SQL generation tasks using a Llama-3.1-8B model, the DPH-JS variant improved the in-domain Pass@8 score by 4.3% relative to the GRPO baseline while also preserving performance on out-of-domain tasks. For AI practitioners, this work demonstrates that selecting a mass-covering divergence like forward-KL is a powerful and efficient tool for improving model generalization and solution diversity in RL fine-tuning without needing an online reference model. |
| Modality Alignment with Multi-scale Bilateral Attention for Multimodal
  Recommendation (Read more on [arXiv](https://arxiv.org/abs/2509.09114) or [HuggingFace](https://huggingface.co/papers/2509.09114))| Dong-Ho Lee, Chan-Yang Ju, renkelin | i) This paper proposes MambaRec, a multimodal recommendation framework that improves performance by jointly optimizing local feature alignment with a novel attention module and enforcing global distribution consistency across modalities. ii) The main objective is to address the insufficient modeling of fine-grained cross-modal associations and the lack of global distribution-level consistency in existing multimodal recommendation systems, which leads to suboptimal fusion quality and representational bias. iii) The key methodology combines a Dilated Refinement Attention Module (DREAM) using multi-scale dilated convolutions for local feature alignment, with a global regularization approach that applies Maximum Mean Discrepancy (MMD) and contrastive loss to ensure semantic consistency between modality distributions. iv) MambaRec demonstrates superior performance over existing models; for instance, on the Sports dataset, it achieved a Recall@20 of 0.1147, outperforming the next-best model, MGCN, which scored 0.1106. v) The principal implication for AI practitioners is that explicitly enforcing both local, fine-grained feature alignment (via mechanisms like DREAM) and global distribution consistency (via losses like MMD) is a critical strategy for mitigating modality-specific noise and improving the robustness of multimodal recommendation models. |
| Reasoning Introduces New Poisoning Attacks Yet Makes Them More
  Complicated (Read more on [arXiv](https://arxiv.org/abs/2509.05739) or [HuggingFace](https://huggingface.co/papers/2509.05739))| Jamie Hayes, Harsh Chaudhari, Yiren Zhao, Ilia Shumailov, Hanna Foerster | This paper introduces a decomposed Chain-of-Thought (CoT) poisoning attack where malicious logic is fragmented across multiple "clean prompt, dirty CoT, clean answer" training samples to backdoor reasoning models. The primary objective is to investigate if manipulating only the CoT trace during fine-tuning can reliably steer a model's final output at inference time without traditional triggers. The authors use LoRA to fine-tune a Qwen-32B model, injecting samples that chain different problems together by connecting their CoT reasoning paths. The results show that while the attack can successfully corrupt the generated thought trace in up to 63.75% of test cases, this poison rarely transfers to the final output, achieving only a 14.00% answer-poisoning success rate. For AI practitioners, the principal implication is that an LLM's explicit CoT is not a faithful representation of its internal reasoning used to generate a final answer, revealing an emergent robustness where models can self-correct or ignore a corrupted thought trace. |
