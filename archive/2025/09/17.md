

## Papers for 2025-09-17

| Title | Authors | Summary |
|-------|---------|---------|
| WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for
  Open-Ended Deep Research (Read more on [arXiv](https://arxiv.org/abs/2509.13312) or [HuggingFace](https://huggingface.co/papers/2509.13312))| Houquan Zhou, Shen Huang, Bo Zhang, Xin Guan, Zijian Li | WebWeaver is a dual-agent AI framework designed for open-ended deep research that dynamically refines a report outline while gathering web-scale evidence. The primary objective is to overcome the failures of static research pipelines and one-shot generation methods, such as "loss in the middle" and hallucinations in long-context scenarios. The key methodology involves a "planner" agent that iteratively interleaves evidence acquisition with outline optimization to create a source-grounded plan, and a "writer" agent that performs hierarchical, section-by-section synthesis by retrieving only necessary evidence from a memory bank for each part. WebWeaver establishes a new state-of-the-art across major benchmarks, achieving a 93.37% citation accuracy on DeepResearch Bench. The principal implication for AI practitioners is that an iterative, dual-agent architecture separating dynamic planning from focused, memory-grounded synthesis is a superior strategy for complex, long-form generation tasks, with the provided WebWeaver-3k dataset demonstrating these skills can be finetuned into smaller models. |
| Scaling Agents via Continual Pre-training (Read more on [arXiv](https://arxiv.org/abs/2509.13310) or [HuggingFace](https://huggingface.co/papers/2509.13310))| Chenxi Wang, Zhuo Chen, Guangyu Li, Zhen Zhang, Liangcai Su | This paper introduces Agentic Continual Pre-training (Agentic CPT), an intermediate training stage designed to create a pre-aligned agentic foundation model, named AgentFounder, to improve agent capabilities before downstream fine-tuning. The main objective is to determine if embedding agentic behaviors directly into a foundation model through a scalable, offline data synthesis pipeline is more effective than relying solely on post-training methods like SFT or RL. The methodology involves a two-stage CPT process using two novel data synthesis techniques: First-order Action Synthesis (FAS) to create planning and reasoning data without API calls, and Higher-order Action Synthesis (HAS) to remodel existing trajectories into multi-step decision-making problems. The resulting model, AgentFounder-30B, achieves state-of-the-art performance, scoring 39.9% on BrowseComp-en, which is a significant improvement over the prior open-source best of 30.0%. For AI practitioners, the principal implication is that developing a specialized agentic base model via CPT is a more efficient and powerful strategy for building high-capability agents, as it facilitates easier downstream alignment and achieves higher performance ceilings compared to fine-tuning general-purpose foundation models. |
| WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic
  Data and Scalable Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.13305) or [HuggingFace](https://huggingface.co/papers/2509.13305))| Yida Zhao, Rui Ye, Huifeng Yin, Zhongwang Zhang, Kuan Li | This paper presents WebSailor-V2, a complete post-training pipeline that uses novel synthetic data and scalable reinforcement learning (RL) to create high-performance open-source web agents. The primary objective is to bridge the performance gap between open-source agents and proprietary systems by tackling insufficiencies in data diversity and the instability of RL training environments. The core methodology involves two innovations: (1) SailorFog-QA-2, a new dataset constructed from a dense knowledge graph to generate tasks with complex uncertainties, and (2) a dual-environment RL framework that combines a high-fidelity simulator for rapid iteration with a managed real-world environment for stable policy training. The resulting WebSailor-V2 agent, built on a Qwen3-30B-A3B model, achieves a state-of-the-art score of 35.3 on BrowseComp-EN, outperforming even the much larger 671B DeepSeek-V3.1. The principal implication for AI practitioners is that investing in sophisticated synthetic data generation and a stable, robust training infrastructure is more critical for developing capable agents than focusing on model scale or specific RL algorithm choice. |
| Towards General Agentic Intelligence via Environment Scaling (Read more on [arXiv](https://arxiv.org/abs/2509.13311) or [HuggingFace](https://huggingface.co/papers/2509.13311))| Guangyu Li, Jialong Wu, Baixuan Li, Shihao Cai, Runnan Fang | This paper presents a scalable pipeline for developing general agentic intelligence by automatically constructing and scaling diverse, verifiable tool-use environments. The primary objective is to create a systematic framework for environment generation and agent training to overcome the limitations of manual or non-scalable data collection methods. The methodology involves programmatically materializing over 30,000 APIs into executable tools grounded in database-structured environments, generating agentic tasks via simulated human-agent interplay, and employing a two-stage agent experience learning process for fine-tuning. The trained AgentScaler-30B-A3B model achieves state-of-the-art results among open-source models under 1T parameters, attaining an overall accuracy of 67.7% on the ACEBench-en benchmark. The principal implication for AI practitioners is the provision of a fully simulated, verifiable, and scalable pipeline for generating high-quality agent training data, which facilitates the development of robust tool-using agents with more compact models. |
| WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon
  Agents (Read more on [arXiv](https://arxiv.org/abs/2509.13309) or [HuggingFace](https://huggingface.co/papers/2509.13309))| Wenbiao Yin, Donglei Yu, Xuanzhong Chen, Guoxin Chen, Zile Qiao | This paper introduces WebResearcher, a framework for deep-research AI agents that employs an iterative, state-reconstructing paradigm called IterResearch to overcome the reasoning limitations of linear context accumulation. The primary objective is to enable sustained long-horizon reasoning by mitigating the "context suffocation" and "noise contamination" that degrade performance in mono-contextual agent architectures. The methodology models research as a Markov Decision Process where the agent's workspace is periodically reconstructed from a synthesized report of findings, and the agent is trained on data generated by WebFrontier, a scalable multi-agent data synthesis engine. The system achieves state-of-the-art performance, scoring 36.7% on the Humanity's Last Exam (HLE) benchmark, significantly surpassing prior systems like OpenAI Deep Research (26.6%). For AI practitioners, this demonstrates that for complex, long-horizon tasks, an iterative synthesis and state-reconstruction architecture is superior to linear context accumulation, providing a robust pattern for building more capable agents by preventing cognitive overload and error propagation. |
| ReSum: Unlocking Long-Horizon Search Intelligence via Context
  Summarization (Read more on [arXiv](https://arxiv.org/abs/2509.13313) or [HuggingFace](https://huggingface.co/papers/2509.13313))| Litu Ou, Liwen Zhang, Yida Zhao, Kuan Li, Xixi Wu | The paper introduces ReSum, a paradigm using periodic context summarization to enable LLM-based web agents to handle long-horizon tasks that exceed standard context window limits. The primary objective is to overcome fixed context window constraints in complex search problems requiring extensive exploration, without significant architectural changes. The key methodology involves periodically invoking a specialized summary tool (`ReSumTool-30B`) to condense the interaction history into a compact state, from which the agent resumes reasoning, with a tailored reinforcement learning algorithm (ReSum-GRPO) for paradigm adaptation. Primary results show that the ReSum-GRPO trained WebResummer-30B model achieves 33.3% Pass@1 on the BrowseComp-zh benchmark, surpassing existing open-source web agents with only 1K training samples. For AI practitioners, ReSum offers a lightweight, plug-and-play modification to existing ReAct-based agents to mitigate context overflow failures and improve performance on complex, multi-step tasks. |
| Single-stream Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2509.13232) or [HuggingFace](https://huggingface.co/papers/2509.13232))| Zihan Ding, Zhongwen Xu | This paper introduces Single-stream Policy Optimization (SPO), a group-free policy gradient method designed to improve the efficiency and scalability of fine-tuning Large Language Models (LLMs) with reinforcement learning. The research objective is to address the critical flaws of group-based methods like GRPO, namely computational waste from degenerate learning signals and synchronization bottlenecks in distributed training. SPO's methodology replaces on-the-fly, per-group baselines with three components: a persistent, KL-adaptive Bayesian value tracker for low-variance baseline estimation, global advantage normalization across the batch, and prioritized prompt sampling for an adaptive curriculum. The primary result shows that when training a Qwen3-8B model, SPO improves the average maj@32 score by +3.4 percentage points over GRPO across five math benchmarks, and simulations indicate it can achieve a 4.35× training throughput speedup in agentic settings. The principal implication for AI practitioners is that SPO offers a more robust, scalable, and efficient alternative to group-based RL, simplifying the training infrastructure and accelerating convergence, especially for agentic tasks with variable-length trajectories. |
| Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset
  Generation (Read more on [arXiv](https://arxiv.org/abs/2509.12815) or [HuggingFace](https://huggingface.co/papers/2509.12815))| Lixin Xu, Shuhui Yang, Xinhai Liu, Yang Li, Biwen Lei | Hunyuan3D Studio is an end-to-end AI pipeline that automates the generation of game-ready 3D assets, including optimized geometry, PBR textures, and animation, from a single image or text description. The objective is to automate the traditionally labor-intensive 3D asset creation workflow by integrating a suite of advanced neural modules into a single, cohesive system. The methodology consists of a seven-stage sequential pipeline that includes modules for controllable image generation, diffusion-based geometry synthesis, part-level decomposition (X-Part), autoregressive polygon generation (PolyGen), and semantic UV unwrapping (SeamGPT). The proposed shape decomposition module, X-Part, achieves a state-of-the-art Chamfer Distance (CD) of 0.11 and an F-score of 0.80 on the ObjaversePart-Tiny benchmark, outperforming all baselines. For AI practitioners, the principal implication is the demonstration of a unified framework that integrates multiple specialized generative models to automate a complex, multi-stage production pipeline, providing a seamless bridge from creative intent to a final technical asset. |
| 3D Aware Region Prompted Vision Language Model (Read more on [arXiv](https://arxiv.org/abs/2509.13317) or [HuggingFace](https://huggingface.co/papers/2509.13317))| Xiaolong Li, Zhijian Liu, Yukang Chen, Yang Fu, An-Chieh Cheng | SR-3D is a vision-language model that enables 3D-aware spatial reasoning by unifying 2D and multi-view data through shared visual tokens enriched with 3D positional embeddings. The research aims to develop a unified VLM capable of accurate 3D spatial reasoning from flexible, sparse region prompts by leveraging strong, pre-existing 2D foundational model priors. Its core methodology involves integrating canonicalized 3D positional embeddings, derived from depth maps, into visual features and using a dynamic "tile-then-stitch" region extractor for high-resolution analysis across single- or multi-view inputs. The model demonstrates state-of-the-art performance, achieving 90.3% accuracy on the BLINKDepth benchmark for point-level depth understanding and showing strong zero-shot generalization from 2D pre-training to 3D tasks. For AI practitioners, the principal implication is the ability to develop systems with sophisticated 3D spatial awareness using only sparse, single-frame annotations, drastically reducing the annotation burden for applications in robotics and scene understanding. |
| EconProver: Towards More Economical Test-Time Scaling for Automated
  Theorem Proving (Read more on [arXiv](https://arxiv.org/abs/2509.12603) or [HuggingFace](https://huggingface.co/papers/2509.12603))| Shansan Gong, Jiahao Xu, Zhenwen Liang, Linfeng Song, Mukai Li | The paper introduces ECONPROVER, a framework to reduce the computational cost of LLM-based automated theorem provers by dynamically applying Chain-of-Thought reasoning and diversifying parallel proof attempts. The research objective is to improve the test-time computational efficiency of state-of-the-art automated theorem proving (ATP) models by mitigating high token costs from scaling strategies without performance loss. The EconRL methodology integrates Dynamic CoT Switching, trained via Direct Preference Optimization (DPO) to selectively apply complex reasoning, and Diverse Parallel-scaled Reinforcement Learning, which uses PPO to train specialized, difficulty-aware reasoning heads to increase proof diversity. Experiments on the miniF2F benchmark show ECONPROVER-GD achieves an 84.0% pass rate, comparable to its baseline's 84.4%, while consuming only 12% of the total sampling token cost. For AI practitioners, this work provides a validated approach to deploy more economical and computationally efficient ATP systems by implementing dynamic reasoning allocation and diversity-focused parallel sampling, enabling high-performance models in resource-constrained environments. |
| Exact Coset Sampling for Quantum Lattice Algorithms (Read more on [arXiv](https://arxiv.org/abs/2509.12341) or [HuggingFace](https://huggingface.co/papers/2509.12341))| Yifan Zhang | This paper presents a fully correct "pair-shift difference" construction to replace a contested step in a recent windowed-QFT quantum algorithm for lattice problems. The primary objective is to resolve a periodicity and support mismatch in the original algorithm's Step 9, ensuring the correct generation of a uniform random vector `u` that satisfies the modular linear relation `⟨b*, u⟩ = 0 (mod P)`. The key methodology involves creating a coherent, shifted copy of the quantum state, subtracting it from the original to deterministically cancel unknown offset vectors, and performing a mandatory ancilla cleanup to produce an exact uniform superposition over a CRT-coset. The procedure results in measurement outcomes that are exactly and uniformly distributed over the desired solution set, which contains `M₂ⁿ / P` elements, while preserving the algorithm's overall poly(log M₂) gate complexity. For practitioners developing quantum algorithms, this work provides a robust, reversible, and provably correct subroutine that fixes a critical flaw, offering a broadly applicable technique for handling unknown offsets in quantum signal processing pipelines. |
| Multimodal Reasoning for Science: Technical Report and 1st Place
  Solution to the ICML 2025 SeePhys Challenge (Read more on [arXiv](https://arxiv.org/abs/2509.06079) or [HuggingFace](https://huggingface.co/papers/2509.06079))| Wentao Zhang, Junbo Niu, Bohan Zeng, Ruitao Wu, Hao Liang | The paper introduces a caption-assisted reasoning framework that converts visual information from scientific diagrams into structured text to improve multimodal problem-solving. The primary objective is to mitigate the performance degradation that powerful reasoning models exhibit in multimodal scenarios by bridging the gap between visual perception and textual logic. The key methodology is a pipeline where a vision-language model first generates a detailed, structured caption from an image, which is then used by a large language model for reasoning, optionally followed by format optimization and critical review stages. The method achieved 1st place in the ICML 2025 SeePhys Challenge, improving accuracy on the SeePhys-mini benchmark from a 58.0% baseline to 66.0%, and on the MathVerse benchmark, it boosted the accuracy of the Claude-Opus-4 model from 60.2% to 85.5% on vision-intensive tasks. The principal implication for AI practitioners is that for problems involving diagrams, decoupling visual perception from reasoning via a captioning module can be more effective than using end-to-end multimodal models, as it allows specialized, powerful text-only LLMs to handle the complex reasoning phase more robustly. |
| Multiple Instance Learning Framework with Masked Hard Instance Mining
  for Gigapixel Histopathology Image Analysis (Read more on [arXiv](https://arxiv.org/abs/2509.11526) or [HuggingFace](https://huggingface.co/papers/2509.11526))| Bo Liu, Fengtao Zhou, Heng Fang, Sheng Huang, Wenhao Tang | This paper presents MHIM-MIL, a Multiple Instance Learning (MIL) framework that improves gigapixel histopathology image analysis by mining hard instances through a masked, momentum-teacher approach. The research objective is to enhance MIL models by shifting the training focus from easy-to-classify salient instances to more challenging hard instances to learn better decision boundaries. The key methodology employs a Siamese architecture where a momentum teacher calculates class-aware instance probabilities to mask easy instances, thereby forcing a student model to train on the remaining hard instances, stabilized by a consistency loss and a Global Recycle Network to mitigate feature loss. The framework demonstrates superior performance across multiple tasks, with MHIM (TransMIL) improving the C-index by 1.8% over the baseline on the TCGA-BLCA-UNI survival analysis task while reducing training time and memory by 20% and 50%, respectively. For AI practitioners, this work provides an effective strategy for implementing hard instance mining in weakly-supervised MIL settings, demonstrating that masking easy instances can improve model generalization and efficiency compared to conventional attention mechanisms. |
| Optimal Brain Restoration for Joint Quantization and Sparsification of
  LLMs (Read more on [arXiv](https://arxiv.org/abs/2509.11177) or [HuggingFace](https://huggingface.co/papers/2509.11177))| Luca Benini, Yawei Li, Hang Guo | The paper introduces Optimal Brain Restoration (OBR), a training-free framework that enables joint quantization and sparsification of LLMs by computing a closed-form compensation to reconcile their conflicting weight distribution requirements. The primary objective is to combine aggressive low-bit quantization (e.g., 4-bit) with high-ratio sparsity (e.g., 50%) for LLMs, overcoming the inherent conflict where quantization favors flat distributions and pruning prefers high-variance ones. OBR formulates a second-order Hessian-based objective to minimize task degradation, which is made tractable through row-wise decoupling and solved in a closed form via group error compensation, systematically redistributing errors from pruning and quantization to more robust weights. The method enables W4A4KV4 quantization with 50% sparsity on large models, achieving up to a 4.72× inference speedup and 6.4× memory reduction compared to an FP16-dense baseline by leveraging INT4-sparse hardware support. For AI practitioners, OBR offers a direct, post-training method to highly compress existing LLMs for efficient deployment on modern GPUs with sparse tensor cores, significantly reducing latency and memory footprint without any model retraining. |
