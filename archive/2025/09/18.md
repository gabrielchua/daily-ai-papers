

## Papers for 2025-09-18

| Title | Authors | Summary |
|-------|---------|---------|
| Hala Technical Report: Building Arabic-Centric Instruction & Translation
  Models at Scale (Read more on [arXiv](https://arxiv.org/abs/2509.14008) or [HuggingFace](https://huggingface.co/papers/2509.14008))| Bernard Ghanem, Mohammad Zbeeb, Hasan Abed Al Kader Hammoud | This paper presents HALA, a family of Arabic-centric models developed via an efficient "translate-and-tune" pipeline to address the scarcity of high-quality Arabic instruction data. The main objective is to create a scalable method for building specialized Arabic models by translating large English instruction corpora and fine-tuning foundation models. The key methodology involves bootstrapping a lightweight translator by fine-tuning a 1.2B model on a bilingual corpus created using a quantized (FP8) teacher, then using this new translator to generate a million-scale Arabic instruction dataset for subsequent model training and slerp-based merging. The primary results show that this approach achieves state-of-the-art performance on Arabic benchmarks, with the HALA-1.2B model scoring 51.4% on average, a +5.1 absolute point improvement over its base. For AI practitioners, this research provides a validated, compute-efficient recipe for developing high-performing models in under-resourced languages by leveraging existing English-language assets and specialized data generation pipelines. |
| SAIL-VL2 Technical Report (Read more on [arXiv](https://arxiv.org/abs/2509.14033) or [HuggingFace](https://huggingface.co/papers/2509.14033))| Zijian Kang, Yue Liao, Fangxun Shu, Yongjie Ye, Weijie Yin | SAIL-VL2 is an open-suite vision-language foundation model designed for comprehensive multimodal understanding and reasoning. The primary objective was to develop powerful yet efficient LVMs by optimizing knowledge injection through efficient architectures and training strategies for strong multimodal performance. The methodology includes a large-scale data curation pipeline, a progressive training framework (SAIL-ViT pre-training, multimodal pre-training, and thinking-fusion SFT-RL hybrid), and architectural advances like sparse Mixture-of-Experts (MoE) designs. SAIL-VL2 achieves state-of-the-art performance at 2B and 8B parameter scales across 106 diverse benchmarks, with SAIL-VL2-2B ranking first on the OpenCompass leaderboard among officially released open-source models under 4B parameters, and SAIL-VL2-8B-Thinking scoring 54.4 on OpenCompass multimodal reasoning. SAIL-VL2 serves as an efficient and extensible open-source foundation, advancing state-of-the-art performance and empowering the broader multimodal ecosystem. |
| PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era (Read more on [arXiv](https://arxiv.org/abs/2509.12989) or [HuggingFace](https://huggingface.co/papers/2509.12989))| Zihao Dongfang, Kaiyu Lei, Ziqiao Weng, Chenfei Liao, Xu Zheng | The paper "PANORAMA" presents a comprehensive overview of omnidirectional vision in embodied AI, outlining its rise, challenges, and future roadmap. Its primary objective is to address fundamental gaps in integrating panoramic visual technology with embodied intelligence by overcoming data bottlenecks, enhancing model capabilities, and exploring new application domains. The key methodology involves proposing PANORAMA, a four-subsystem architecture for data acquisition, perception, application, and acceleration, along with a six-stage roadmap for its implementation. The paper identifies 23 representative omnidirectional datasets and highlights advancements in generation, perception, and understanding, with domain adaptation techniques yielding significant performance improvements. This research implies that AI practitioners should prioritize creating large-scale multi-task omnidirectional datasets, developing projection-agnostic and unified models, and exploring real-world applications to advance embodied AI. |
| GenExam: A Multidisciplinary Text-to-Image Exam (Read more on [arXiv](https://arxiv.org/abs/2509.14232) or [HuggingFace](https://huggingface.co/papers/2509.14232))| Yu Qiao, Changyao Tian, Xiangyu Zhao, Penghao Yin, Zhaokai Wang | GenExam introduces the first benchmark for multidisciplinary text-to-image exams. Its primary objective is to rigorously assess AI models' ability to integrate understanding, reasoning, and generation in complex, exam-style graph-drawing problems, serving as a yardstick for general AGI development. The methodology involves 1,000 samples across 10 subjects, each with ground-truth images and fine-grained scoring points, evaluated by an MLLM-as-a-judge framework for semantic correctness and visual plausibility. Experiments show state-of-the-art models like GPT-Image-1 achieve a highest strict score of only 12.1%, with many others near 0%, highlighting the benchmark's difficulty. For AI practitioners, this indicates a critical need to prioritize multidisciplinary knowledge integration, rigorous reasoning, and fine-grained visual coherence in developing advanced generative models. |
| Scrub It Out! Erasing Sensitive Memorization in Code Language Models via
  Machine Unlearning (Read more on [arXiv](https://arxiv.org/abs/2509.13755) or [HuggingFace](https://huggingface.co/papers/2509.13755))| Zhou Yang, Di Wang, Zhikun Zhang, Yao Wan, Zhaoyang Chu | This paper pioneers machine unlearning for erasing sensitive memorization in Code Language Models (CLMs). The main objective is to determine if sensitive information memorized by CLMs can be erased effectively and efficiently. The methodology introduces CODEERASER, an advanced gradient ascent-based unlearning variant that selectively unlearns sensitive memorized code segments via gradient ascent while preserving structural integrity and functional correctness of surrounding code through gradient descent and KL divergence-based constraints. Experiments on Qwen2.5-Coder-7B demonstrated CODEERASER reduced memorization by 93.89% on targeted data, retaining 99.00% of original model utility with an average processing time of 46.88 seconds per sample. This provides AI practitioners with a practical and efficient technique to actively mitigate data privacy risks stemming from sensitive memorization in CLMs. |
| THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.13761) or [HuggingFace](https://huggingface.co/papers/2509.13761))| Yicheng Pan, Jiefeng Ma, Pengfei Hu, Zhenrong Zhang, Qikai Chang | THOR introduces a novel framework for Tool-Integrated Hierarchical Optimization via RL to address challenges in LLM mathematical reasoning, focusing on TIR data construction, fine-grained optimization, and inference enhancement. The methodology involves TIRGen, an actor-critic pipeline for generating high-quality TIR datasets, a hierarchical RL strategy for joint trajectory-level problem-solving and step-level code generation, and a self-correction mechanism leveraging immediate tool feedback during inference. Quantitatively, THOR-7B improves the average mathematical benchmark score for non-reasoning models from 35.7% to 61.2%, and THOR-Thinking-8B boosts reasoning models from 74.5% to 79.8% (Table 1). For AI practitioners, THOR offers a generalizable and efficient approach to build more robust LLMs with superior mathematical reasoning and code generation capabilities by effectively integrating external tools and hierarchical reinforcement learning. |
| Wan-Animate: Unified Character Animation and Replacement with Holistic
  Replication (Read more on [arXiv](https://arxiv.org/abs/2509.14055) or [HuggingFace](https://huggingface.co/papers/2509.14055))| Mingyang Huang, Siqi Hu, Li Hu, Xin Gao, Gang Cheng | Wan-Animate is a unified, state-of-the-art framework for high-fidelity character animation and replacement, enabling holistic replication of motion, expression, and environmental context. Its primary objective is to provide a comprehensive solution for character animation that unifies control over motion, expression, and seamless environment interaction. The methodology builds upon the DiT-based Wan-I2V model, employing a modified input paradigm, spatially-aligned skeleton signals for body motion, implicit facial features for expressions, and an auxiliary Relighting LoRA for environmental integration. Quantitatively, Wan-Animate achieves state-of-the-art performance, for instance, in facial animation with an FVD of 94.65, demonstrating superior realism and temporal coherence. For AI practitioners, this open-source framework provides a high-caliber model that establishes a new performance baseline, accelerating development and enabling diverse applications in video generation and character synthesis. |
| Improving Context Fidelity via Native Retrieval-Augmented Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.13683) or [HuggingFace](https://huggingface.co/papers/2509.13683))| Xiangru Tang, Shiqi Li, Xinyu Wang, Jinlin Wang, Suyuchen Wang | This paper introduces CARE, a novel native retrieval-augmented reasoning framework that teaches large language models (LLMs) to explicitly integrate in-context evidence within their reasoning process, enhancing context fidelity. The core objective is to improve LLM context fidelity and reduce hallucinations in knowledge-intensive tasks by enabling models to dynamically identify and incorporate relevant input context evidence during reasoning. CARE employs a two-phase training process: supervised fine-tuning establishes evidence integration patterns using self-curated data with <RETRIEVAL> tokens, followed by reinforcement learning with Group Relative Policy Optimization (GRPO) and a curriculum learning strategy, using accuracy, format, and retrieval-aware rewards. Experiments demonstrate CARE consistently outperforms baselines, achieving a +15.29% average F1 improvement over the original LLaMA-3.1 8B model on real-world QA, with significant gains of +29.42% on 2WikiMQA and +18.92% on MuSiQue. This approach provides AI practitioners with a method to develop more accurate, reliable, and efficient LLM systems for knowledge-intensive tasks by leveraging native retrieval, reducing reliance on expensive external retrieval infrastructure and minimizing context hallucination. |
| SteeringControl: Holistic Evaluation of Alignment Steering in LLMs (Read more on [arXiv](https://arxiv.org/abs/2509.13450) or [HuggingFace](https://huggingface.co/papers/2509.13450))| Zhun Wang, Nathan W. Henry, David Park, Nicholas Crispino, Vincent Siu | This paper introduces STEERINGCONTROL, a benchmark designed to holistically evaluate the effectiveness and behavioral entanglement of representation steering methods in LLMs. The research aims to systematically assess whether these methods can control primary alignment behaviors—bias, harmful generation, and hallucination—while minimizing unintended effects on secondary behaviors like sycophancy and reasoning. The methodology involves evaluating five popular training-free steering methods (DIM, ACE, CAA, PCA, LAT) on Qwen-2.5-7B and Llama-3.1-8B across 17 curated datasets, using aggregate metrics for "Effectiveness" and "Entanglement." The primary results show that steering performance is highly dependent on the specific combination of method, model, and target behavior, and that a significant tradeoff exists between effectiveness and entanglement; for example, steering for refusal on Qwen-2.5-7B with the DIM method increased refusal ASR by 72.7% but simultaneously decreased performance on the sycophancy task by 55.5%. The principal implication for AI practitioners is that applying representation steering methods can introduce severe, unintended side effects, making it critical to perform comprehensive, multi-behavioral evaluations rather than optimizing for a single target, as no single steering method is universally optimal. |
| MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,
  Results, Discussion, and Outlook (Read more on [arXiv](https://arxiv.org/abs/2509.14142) or [HuggingFace](https://huggingface.co/papers/2509.14142))| Bowen Zhou, Yaxiong Chen, Jiajun Zhang, Shengwu Xiong, Peng Xu | This paper reports on the MARS2 2025 Challenge, which benchmarked multimodal reasoning capabilities of large language models using new, specialized datasets (`Lens`, `AdsQA`) across three complex reasoning tracks. The challenge's objective was to evaluate synergistic effects among reasoning tasks and probe non-stepwise complex reasoning, pushing MLLMs beyond standard perception into specialized, real-world scenarios like spatial awareness and advertisement analysis. The methodology involved three competition tracks evaluated with task-specific metrics, where top-performing participants primarily employed multi-stage alignment strategies combining supervised fine-tuning (SFT) and reinforcement learning (e.g., GRPO) on foundational models. The results reveal significant remaining challenges in multimodal reasoning, with the winning solution for the Visual Grounding in Real-world Scenarios (VG-RS) track achieving a 66.70% accuracy (Acc.@0.5), indicating a substantial performance gap. For AI practitioners, the key implication is that deploying MLLMs for specialized, high-fidelity reasoning tasks necessitates significant investment in domain-specific data synthesis, targeted fine-tuning, and advanced alignment techniques, as general-purpose models are insufficient for these complex applications. |
