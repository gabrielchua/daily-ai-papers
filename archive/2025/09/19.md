

## Papers for 2025-09-19

| Title | Authors | Summary |
|-------|---------|---------|
| ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform
  Data (Read more on [arXiv](https://arxiv.org/abs/2509.15221) or [HuggingFace](https://huggingface.co/papers/2509.15221))| Zehao Li, QiushiSun, heroding77, ownerEli, zyliu | The ScaleCUA paper introduces a large-scale, cross-platform dataset and a family of open-source models to advance general-purpose computer use agents (CUAs). The research objective is to address the constraints of data scarcity and limited model transferability by creating a comprehensive GUI-centric training corpus. The methodology involves a dual-loop "Cross-Platform Interactive Data Pipeline" that combines automated agent interaction with human expert annotation across six operating systems (Windows, macOS, Linux, Android, iOS, Web) to collect over 17M grounding samples and 19K trajectories. The primary result is that the trained ScaleCUA-32B model sets new state-of-the-art performance, achieving 94.4% on MMBench-GUI L1-Hard and 47.4% on WebArena-Lite-v2, outperforming prior baselines by a significant margin (+26.6 on WebArena-Lite-v2). The principal implication for AI practitioners is that scaling with diverse, cross-platform, in-domain data is a highly effective strategy for building more capable and generalizable visual GUI agents, with the released dataset and models providing a direct foundation for future development. |
| FlowRL: Matching Reward Distributions for LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.15207) or [HuggingFace](https://huggingface.co/papers/2509.15207))| Hengli Li, Dinghuai Zhang, jayyoung0802, daixuancheng, xuekai | FlowRL is a reinforcement learning framework that improves LLM reasoning by matching the full reward distribution via flow balancing, rather than pursuing simple reward maximization. The primary objective is to mitigate the mode collapse seen in methods like PPO and GRPO, thereby promoting diverse and generalizable reasoning trajectories. The methodology transforms scalar rewards into a normalized target distribution using a learnable partition function and minimizes the reverse KL divergence between the policy and this target, implemented via a GFlowNet-inspired trajectory balance objective with importance sampling. On math reasoning benchmarks, FlowRL achieved an average improvement of 10.0% over GRPO and 5.1% over PPO, demonstrating superior performance. For AI practitioners, this implies that adopting a distribution-matching objective can enhance the fine-tuning of LLMs for complex reasoning tasks, leading to models that explore a broader set of valid solutions and exhibit better generalization instead of overfitting to a single dominant reasoning path. |
| Reasoning over Boundaries: Enhancing Specification Alignment via
  Test-time Delibration (Read more on [arXiv](https://arxiv.org/abs/2509.14760) or [HuggingFace](https://huggingface.co/papers/2509.14760))| Zhilin Wang, Dongrui Liu, Xuyang Hu, Yafu Li, zzzhr97 | This paper introduces "specification alignment" for LLMs, proposing a test-time deliberation method called ALIGN3 and a benchmark called SPECBENCH to improve and evaluate adherence to dynamic, scenario-specific rules. The main objective is to formalize and address the challenge of "specification alignment," defined as an LLM's ability to simultaneously adhere to bespoke, scenario-specific behavioral and safety specifications. The key methodology includes ALIGN3, a lightweight Test-Time Deliberation (TTD) method employing a three-step process of behavior optimization, safety-guided refinement, and holistic audit, and SPECBENCH, a benchmark covering 5 scenarios, 103 specifications, and 1,500 prompts, measured by a new metric, the Specification Alignment Rate (SAR). Primary results show that TTD enhances alignment; specifically, ALIGN3 improved the SAR of the Qwen3-14B model by 11.89% (from a 51.03% baseline to 62.92%) with minimal token overhead, effectively advancing the safety-helpfulness trade-off. The principal implication for AI practitioners is that lightweight, test-time deliberation methods offer a flexible and cost-effective alternative to retraining for enforcing complex, evolving operational specifications, enabling better model control in diverse real-world applications. |
| Evolving Language Models without Labels: Majority Drives Selection,
  Novelty Promotes Variation (Read more on [arXiv](https://arxiv.org/abs/2509.15194) or [HuggingFace](https://huggingface.co/papers/2509.15194))| Kishan Panaganti, Wenhao Yu, Haolin Liu, invokerliang, yujunzhou | This paper introduces EVOL-RL, a framework that enables label-free language model self-improvement by preventing the entropy collapse common in majority-vote-based methods. The research objective is to develop a method that allows LLMs to "evolve"—achieving broad-based, generalizable improvements—on unlabeled data by explicitly balancing selection and variation. The methodology, EVOL-RL, combines a majority-voted answer for selection with a novelty-aware reward that promotes semantically diverse reasoning paths for variation, implemented within the GRPO optimization framework. The primary result shows that EVOL-RL significantly outperforms a majority-only TTRL baseline; for example, training on label-free AIME24 data boosts a Qwen3-4B model's pass@1 accuracy on the AIME25 benchmark from 4.6% to 16.4%. For AI practitioners, this provides a practical technique to continuously self-improve models using unlabeled data streams, critically maintaining solution diversity and enhancing out-of-domain generalization without external verifiers. |
| Understand Before You Generate: Self-Guided Training for Autoregressive
  Image Generation (Read more on [arXiv](https://arxiv.org/abs/2509.15185) or [HuggingFace](https://huggingface.co/papers/2509.15185))| Xihui Liu, Wenlong Zhang, Yuqing Wang, GoodEnough, YueXY233 | The paper introduces ST-AR, a self-guided training framework that integrates masked image modeling and contrastive learning into autoregressive models to enhance their visual understanding and image generation quality. The primary objective is to address fundamental limitations in autoregressive visual modeling—local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency—by compelling the model to learn high-level semantics during the generative training process. The key methodology, Self-guided Training for AutoRegressive models (ST-AR), augments the standard next-token prediction loss with three self-supervised objectives: a masked image modeling loss on attention maps to expand receptive fields, an inter-step contrastive loss for temporal semantic consistency, and an inter-view contrastive loss for spatial invariance, all guided by an exponential moving average (EMA) teacher network. Primary results demonstrate significant performance gains on ImageNet; ST-AR achieves approximately a 49% FID improvement for the LlamaGen-XL model (from 19.42 to 9.81) and increases the linear probing top-1 accuracy of LlamaGen-B from 18.68% to 45.27%, all without relying on pre-trained representation models. The principal implication for AI practitioners is that autoregressive models can be substantially improved by directly integrating self-supervised losses into the training loop, creating models with superior generation fidelity and visual understanding without altering the core architecture or inference process. The most impactful finding is that this approach yields state-of-the-art results (e.g., 2.37 FID for LlamaGen-XL with ST-AR) that are competitive with diffusion models, demonstrating a direct and effective path to enhancing existing autoregressive frameworks for image synthesis. |
| FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial
  Search and Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.13160) or [HuggingFace](https://huggingface.co/papers/2509.13160))| Jiashuo Liu, Jianpeng Jiao, Liang Hu, WenhaoHuang, zhangysk | This paper introduces FinSearchComp, a benchmark for evaluating LLM agents on realistic, open-domain financial search and reasoning. The primary objective is to assess an agent's ability to perform complex, multi-step searches over time-sensitive, domain-specific financial data, simulating real-world analyst workflows. The methodology involves a benchmark of 635 questions, curated by 70 financial experts, divided into three tasks (Time-Sensitive Data Fetching, Simple Historical Lookup, Complex Historical Investigation) across global and Greater China markets, with evaluation conducted using an LLM-as-a-Judge protocol. The primary results show that even the top-performing model on the global subset, Grok 4 (web), scored 68.9%, significantly trailing the human expert baseline of 75.0, while on the Greater China subset, all models performed more than 34 percentage points below human experts. The principal implication for AI practitioners is that current agents struggle with freshness awareness, multi-source reconciliation, and temporal reasoning, indicating that improving search depth, data validation, and integration with specialized financial plugins is critical for developing robust and reliable financial applications. |
| RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation (Read more on [arXiv](https://arxiv.org/abs/2509.15212) or [HuggingFace](https://huggingface.co/papers/2509.15212))| SpaceProduct, Sicong, yaniii, huangsiteng, yumingj | RynnVLA-001 is a vision-language-action (VLA) model that improves robot manipulation by pretraining on large-scale human demonstration videos. The objective is to overcome the scarcity of robot-specific training data by developing a pretraining strategy that effectively transfers manipulation knowledge from abundant, ego-centric human videos to a robotic agent. The key methodology is a two-stage pretraining curriculum: an Image-to-Video model first learns visual dynamics from 12M human videos, and is then finetuned to jointly predict future visual frames and human keypoint trajectories, using an ActionVAE to compress action chunks into a compact latent space. When finetuned on the same downstream manipulation dataset, RynnVLA-001 achieved a 90.6% average success rate, significantly outperforming baselines like Pi0 (70.4%) and GR00T N1.5 (55.6%). The principal implication for AI practitioners is that a multi-stage video generative pretraining pipeline, which progressively bridges from visual prediction on human data to trajectory-aware modeling, provides a more effective weight initialization for VLA models than using standard image-text pretrained models or training from scratch. |
| AToken: A Unified Tokenizer for Vision (Read more on [arXiv](https://arxiv.org/abs/2509.14476) or [HuggingFace](https://huggingface.co/papers/2509.14476))| Mingze Xu, Liangchen Song, afshin525, byeongjooahn, Jiasenlu | ATOKEN is a unified visual tokenizer that achieves high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets within a single framework. The research objective is to create a general-purpose visual tokenizer that overcomes the fragmentation between reconstruction- and understanding-specific models and bridges different visual modalities. The key methodology involves a pure transformer architecture with 4D Rotary Position Embeddings (RoPE) to encode diverse inputs into a shared, sparse 4D latent space, trained using an adversarial-free objective that combines perceptual and Gram matrix losses. The model achieves strong performance across modalities, for instance, attaining 0.21 rFID for image reconstruction with 82.2% zero-shot ImageNet accuracy. The principal implication for AI practitioners is that ATOKEN can serve as a single, foundational visual component for next-generation multimodal AI systems, simplifying architectures by unifying generation and understanding capabilities across diverse visual data types. |
| WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model
  via Training-Free Guidance (Read more on [arXiv](https://arxiv.org/abs/2509.15130) or [HuggingFace](https://huggingface.co/papers/2509.15130))| Ruibo Li, Tong Zhao, ChiZhang, 2hiTee, ChenxiSong | WorldForge is a training-free, inference-time framework that enables precise 3D/4D trajectory control over pre-trained video diffusion models for tasks like scene generation and re-rendering. The research objective is to inject fine-grained geometric and motion control into existing video diffusion models without the need for costly retraining or fine-tuning, thereby preserving their rich generative priors. The methodology leverages a unified guidance strategy composed of three modules: Intra-Step Recursive Refinement (IRR) to inject trajectory cues at each denoising step, Flow-Gated Latent Fusion (FLF) to selectively apply guidance to motion-relevant latent channels, and Dual-Path Self-Corrective Guidance (DSG) to mitigate artifacts by correcting the denoising path. The framework demonstrates state-of-the-art performance, achieving a Fréchet Inception Distance (FID) of 96.08 on static 3D scene generation, substantially improving upon the 111.49 score of the next-best baseline. For AI practitioners, the key implication is the ability to unlock and steer the emergent 3D/4D capabilities of existing large-scale video models in a plug-and-play manner, enabling controllable content generation without specialized model training. |
| MultiEdit: Advancing Instruction-based Image Editing on Diverse and
  Challenging Tasks (Read more on [arXiv](https://arxiv.org/abs/2509.14638) or [HuggingFace](https://huggingface.co/papers/2509.14638))| Xijun Gu, Lin Liu, HaoxingChen, dreamzz5, Mingsong07 | The paper introduces MultiEdit, a large-scale dataset of over 107K samples for training and benchmarking instruction-based image editing (IBIE) models on diverse and complex tasks. The primary objective is to address the limitations of existing datasets by creating a high-quality resource covering challenging scenarios like reference-based editing, in-image text manipulation, and GUI editing. The methodology involves a novel pipeline using a SOTA MLLM to generate visual-adaptive instructions directly from source images and a SOTA ImageGen model to produce the corresponding high-fidelity edited images. As a primary result, fine-tuning the UltraEdit model on MultiEdit-Train improved its DINO score on the MultiEdit-Test benchmark by approximately 7.2% while surpassing the SOTA model Step1X-Edit on the same metric. For AI practitioners, the principal implication is that the MultiEdit dataset enables the fine-tuning of foundational models to significantly enhance their performance on sophisticated, fine-grained editing tasks for more complex real-world applications without degrading capabilities on standard benchmarks. |
| Unleashing the Potential of Multimodal LLMs for Zero-Shot
  Spatio-Temporal Video Grounding (Read more on [arXiv](https://arxiv.org/abs/2509.15178) or [HuggingFace](https://huggingface.co/papers/2509.15178))| Rynson W. H. Lau, Gerhard Hancke, yuhaoliu, zaiquan | i) This paper introduces a zero-shot framework for Spatio-Temporal Video Grounding (STVG) by exploiting and enhancing the latent grounding capabilities of Multimodal Large Language Models (MLLMs). ii) The primary objective is to overcome MLLMs' suboptimal grounding performance in complex videos, which stems from their inability to fully integrate specific attribute and action cues from a text query. iii) The key methodology involves a Decomposed Spatio-Temporal Highlighting (DSTH) strategy that decouples a query into attribute and action sub-queries, and uses a novel logit-guided re-attention (LRA) module for test-time optimization of spatial and temporal visual prompts, complemented by a Temporal-Augmented Assembling (TAS) strategy to ensure temporal consistency. iv) On the HCSTVG-v1 benchmark, the proposed method with the LLaVA-OneVision-7B model achieves a 24.8% m_vIoU, outperforming the previous zero-shot SOTA (E3M) which scored 19.1%. v) The principal implication for AI practitioners is that MLLMs' inherent, yet often overlooked, grounding capabilities associated with specific internal tokens can be unlocked and directed through test-time prompt optimization, enabling effective zero-shot performance on complex multimodal tasks without requiring model fine-tuning or grounding-specific training data. |
| Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question
  Answering with LLMs (Read more on [arXiv](https://arxiv.org/abs/2509.15020) or [HuggingFace](https://huggingface.co/papers/2509.15020))| Katharina von der Wense, MinhDucBui, mario-sanz | This paper demonstrates that the tokenization of the space preceding an answer label in multiple-choice question answering (MCQA) significantly impacts LLM performance and evaluation reliability. The research objective is to investigate how two different tokenization schemes—tokenizing the space separately from the answer letter versus together with it—affect model accuracy and calibration. The methodology involves evaluating 15 LLMs across six MCQA datasets by comparing the next-token probabilities of the answer labels under both tokenization strategies. Results show that tokenizing the space together with the answer letter consistently improves performance, yielding accuracy gains of up to 11% and significantly better model calibration. The principal implication for AI practitioners is that evaluation protocols for MCQA must be standardized, as this seemingly minor implementation detail can alter model rankings and produce inconsistent benchmark results. |
| EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal
  Ultrasound Intelligence (Read more on [arXiv](https://arxiv.org/abs/2509.14977) or [HuggingFace](https://huggingface.co/papers/2509.14977))| Qinghua Huang, WeiWang, lidachen, Ruimed, chaoyinshe | The paper introduces EchoVLM, a vision-language model specialized for universal ultrasound intelligence using a dynamic Mixture-of-Experts (MoE) architecture. The main objective is to address the poor performance and generalization of existing general-purpose VLMs when applied to multi-organ, multi-task ultrasound diagnostics. The methodology involves specializing the Qwen2-VL foundation model by integrating a dual-path MoE architecture and training it on a newly curated large-scale dataset of 1.47 million ultrasound images across seven anatomical regions. EchoVLM achieved significant improvements on the ultrasound report generation task, outperforming the baseline Qwen2-VL with a 10.15 point increase in BLEU-1 score. The principal implication for AI practitioners is that domain-specific adaptation using specialized architectures like MoE is a highly effective strategy for enhancing the performance of large foundation models in complex, specialized fields such as medical imaging. |
| FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution
  Remote Sensing Change Detection (Read more on [arXiv](https://arxiv.org/abs/2509.06482) or [HuggingFace](https://huggingface.co/papers/2509.06482))| Zhewei Zhang, Yuhan Jiang, Shuangxi Miao, pedramghamisi, zx-Xie | The paper introduces FSG-Net, a network that synergistically leverages frequency-spatial analysis to improve change detection in high-resolution remote sensing images. The primary objective is to systematically disentangle genuine semantic changes from nuisance variations (e.g., illumination, season) and to bridge the semantic gap between deep and shallow features for precise boundary delineation. The methodology consists of three core components: a Discrepancy-Aware Wavelet Interaction Module (DAWIM) to suppress pseudo-changes in the frequency domain, a Synergistic Temporal-Spatial Attention Module (STSAM) to enhance change saliency in the spatial domain, and a Lightweight Gated Fusion Unit (LGFU) to selectively integrate multi-level features. The proposed FSG-Net establishes a new state-of-the-art, achieving an F1-score of 94.16% on the CDD benchmark, outperforming previous methods. The principal implication for AI practitioners is that employing a dual-domain approach—first using wavelet decomposition to process different frequency components distinctly for noise suppression, then using spatial attention for feature enhancement—is a highly effective strategy for tasks requiring robust differentiation between semantic and stylistic variations in bi-temporal imagery. |
