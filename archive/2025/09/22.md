

## Papers for 2025-09-22

| Title | Authors | Summary |
|-------|---------|---------|
| RPG: A Repository Planning Graph for Unified and Scalable Codebase
  Generation (Read more on [arXiv](https://arxiv.org/abs/2509.16198) or [HuggingFace](https://huggingface.co/papers/2509.16198))| Steven Liu, Xin Zhang, Kyleraha, Cipherxzc, Luo2003 | RPG introduces a structured, graph-based representation for unified and scalable repository generation, overcoming natural language ambiguity in planning. The paper addresses the fundamental challenge of generating complete software repositories from scratch by bridging the gap between high-level user intent and intricate file/dependency networks, which prior natural language-based planning methods fail to address. The proposed methodology involves the Repository Planning Graph (RPG) to encode functional goals, file structures, data flows, and functions, and ZeroRepo, a graph-driven framework with proposal-level planning, implementation-level refinement, and graph-guided code generation. On the RepoCraft benchmark, ZeroRepo achieved 81.5% functional coverage and a 69.7% pass rate, producing repositories averaging 36K LOC, approximately 3.9x larger than the strongest baseline. This enables AI practitioners to leverage RPG for enhanced LLM understanding of repositories, accelerating agent localization, and facilitating near-linear scaling of functionality and code size for long-horizon and large-scale repository development. |
| MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid
  Vision Tokenizer (Read more on [arXiv](https://arxiv.org/abs/2509.16197) or [HuggingFace](https://huggingface.co/papers/2509.16197))| jialingt, haosoul122, haotiz, bpan, FrozzZen | Manzano is a simple and scalable unified multimodal LLM featuring a hybrid vision tokenizer for both visual understanding and generation. This work aims to significantly reduce the performance trade-off between these capabilities in unified multimodal LLMs. Its core methodology employs a shared visual encoder with separate continuous and discrete adapters for understanding and generation, respectively, feeding into a unified autoregressive LLM and an auxiliary diffusion decoder, all trained jointly. Manzano-30B achieved state-of-the-art results among unified models, scoring 94.3 on DocVQA, outperforming all other unified models presented. This implies AI practitioners can leverage Manzano's architecture and training recipe to develop highly capable unified multimodal systems that mitigate the typical performance degradation when combining understanding and generation tasks. |
| Latent Zoning Network: A Unified Principle for Generative Modeling,
  Representation Learning, and Classification (Read more on [arXiv](https://arxiv.org/abs/2509.15591) or [HuggingFace](https://huggingface.co/papers/2509.15591))| Wenyu Wang, Junyi Zhu, Xuefei Ning, Enshu Liu, fjxmlzn | Latent Zoning Network (LZN) proposes a unified principle and framework for generative modeling, representation learning, and classification by integrating diverse data types into a shared Gaussian latent space. The paper investigates whether a single principle can unify these three core ML tasks, which currently rely on largely disjoint state-of-the-art solutions. LZN's methodology involves mapping data to disjoint latent zones in a shared Gaussian latent space via encoders and flow matching for "latent computation," and aligning these zones across data types using "latent alignment." LZN improves FID on CIFAR10 for unconditional image generation from 2.76 to 2.59, and achieves 9.3% higher Top-1 accuracy than MoCo and 0.2% higher than SimCLR in unsupervised representation learning on ImageNet. AI practitioners can leverage LZN's unified framework to simplify ML pipelines and enable greater synergy across diverse tasks by providing a principled way for shared representations. |
| BaseReward: A Strong Baseline for Multimodal Reward Model (Read more on [arXiv](https://arxiv.org/abs/2509.16127) or [HuggingFace](https://huggingface.co/papers/2509.16127))| jianfeipan, xuwang, KaiWu123, achernarcursa, yifanzhang114 | This paper presents BaseReward, a powerful multimodal reward model (MRM), and provides an empirically-backed recipe for its construction. The primary objective is to systematically investigate crucial components of the MRM development pipeline, including modeling paradigms, reward head architecture, data curation, and training strategies, to establish a clear guide for building high-performance models. The methodology involves exhaustive experimental analysis comparing various architectures and training configurations, including an ablation study on over ten preference datasets. The resulting model, BaseReward, establishes a new state-of-the-art on major benchmarks, surpassing the previous SOTA on the MM-RLHF-Reward Bench by approximately 11.9% in accuracy. The principal implication for practitioners is that a highly effective MRM can be built using a simple Naive-RM architecture with an optimized two-layer MLP reward head, trained on a carefully curated mixture of both multimodal and text-only preference data, which surprisingly enhances multimodal judgment. |
| SPATIALGEN: Layout-guided 3D Indoor Scene Generation (Read more on [arXiv](https://arxiv.org/abs/2509.14981) or [HuggingFace](https://huggingface.co/papers/2509.14981))| Yongsen Mao, Yixun Liang, Heng Li, Chuan Fang, bertjiazheng | SPATIALGEN is a novel framework for layout-guided 3D indoor scene generation, addressing challenges in visual quality, diversity, and semantic consistency for high-fidelity 3D indoor environments. The primary objective is to generate realistic and semantically consistent 3D indoor scenes from a 3D layout, optionally conditioned on text or a reference image. Its key methodology involves a multi-view multi-modal diffusion model leveraging a new large-scale synthetic dataset of 12,328 scenes and 4.7M panoramic renderings, utilizing alternating cross-view and cross-modal attention for consistent synthesis of appearance, geometry, and semantics. Quantitatively, SPATIALGEN significantly outperforms prior score distillation methods on its dataset, achieving an Image Reward of -0.238 and CLIP Similarity of 26.84, demonstrating superior realism and plausibility. This work provides AI practitioners with a comprehensive dataset and a robust, controllable framework for generating high-fidelity 3D indoor scenes, enabling advancements in applications such as virtual reality, interior design, and robotics. |
| BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent (Read more on [arXiv](https://arxiv.org/abs/2509.15566) or [HuggingFace](https://huggingface.co/papers/2509.15566))| Jiahui Yang, Shaokang Wang, Pei Fu, Ruoceng Zhang, Shaojie Zhang | BTL-UI introduces a brain-inspired "Blink-Think-Link" (BTL) framework to enhance AI-driven human-GUI interaction automation. The primary objective is to develop GUI agents whose interaction logic better mimics human cognitive processes to overcome limitations in current AI-GUI interaction models. BTL decomposes GUI interactions into Blink (rapid visual attention), Think (high-level reasoning), and Link (executable command generation) phases, utilizing automated Blink Data Generation for ROI annotations and a novel rule-based BTL Reward, optimized via GRPO. The BTL-UI-7B model achieved an average GUI grounding accuracy of 89.1% on the corrected ScreenSpot-V2 dataset, establishing a new state-of-the-art, and demonstrated SOTA performance in all metrics for AndroidControl-Low tasks. This framework provides a robust and biologically plausible approach for developing advanced GUI agents, offering multi-dimensional training guidance and improved generalizability for digital assistants. |
| Lynx: Towards High-Fidelity Personalized Video Generation (Read more on [arXiv](https://arxiv.org/abs/2509.15496) or [HuggingFace](https://huggingface.co/papers/2509.15496))| Linjie Luo, Jing Liu, gutianpei, tzhi-bytedance, shensang | Lynx is a high-fidelity, adapter-based framework for personalized video generation from a single input image. The primary objective is to synthesize videos that faithfully preserve subject identity while maintaining temporal coherence and visual realism. Lynx extends a Diffusion Transformer (DiT) foundation model with two lightweight adapters: an ID-adapter using a Perceiver Resampler for ArcFace embeddings and a Ref-adapter integrating dense VAE features via cross-attention from a frozen reference pathway, trained with spatio-temporal frame packing. On a benchmark of 800 test cases, Lynx demonstrated superior face resemblance (0.779 facexlib cosine similarity) and overall video quality (0.956), while achieving competitive prompt following and motion naturalness. This adapter-based design provides AI/ML engineers with a scalable and robust framework for developing identity-preserving video synthesis applications without extensive model fine-tuning. |
| A Vision-Language-Action-Critic Model for Robotic Real-World
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.15937) or [HuggingFace](https://huggingface.co/papers/2509.15937))| Jiangmiao, simonlin123, andyzsz123, haoranzhang, fuxian | VLAC is a vision-language-action-critic model for efficient real-world robotic reinforcement learning. The paper aims to overcome sparse rewards and inefficient exploration in real-world robotic RL for VLA models. VLAC unifies actor and critic roles within a single autoregressive architecture, built on InternVL, and is trained on over 4,000 hours of language-annotated manipulation data to generate dense progress delta rewards and actions, integrating into an asynchronous real-world RL loop with human-in-the-loop protocols. VLAC increased robotic manipulation success rates from approximately 30% to 90% within 200 real-world interaction episodes, with human-in-the-loop interventions further boosting sample efficiency by 50% and achieving up to 100% final success. For AI practitioners, this work provides a practical recipe demonstrating that large multimodal priors combined with structured intrinsic progress feedback enable feasible, data-efficient, and incrementally improvable real-world online RL. |
| RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes (Read more on [arXiv](https://arxiv.org/abs/2509.15123) or [HuggingFace](https://huggingface.co/papers/2509.15123))| Narendra Ahuja, Hao Zhang, fangli3 | This paper introduces ROS-Cam, a novel RGB-only supervised method for accurate and efficient camera parameter optimization in dynamic scenes. The main objective is to estimate camera parameters (focal length, rotation, translation) in dynamic scenes, using only a single RGB video, without external ground truth supervision. The methodology consists of patch-wise tracking filters for robust pseudo-supervision, outlier-aware joint optimization with a Cauchy distribution-modeled uncertainty parameter and an Average Cumulative Projection (ACP) error, and a two-stage optimization strategy for enhanced stability and speed. ROS-Cam demonstrates superior performance, achieving a PSNR of 33.55 on the NeRF-DS dataset compared to casualSAM's 21.23, and reducing average runtime for NeRF-DS from casualSAM's 10.5 hours to 0.83 hours. This work provides AI practitioners with a robust and efficient solution for camera pose estimation in dynamic environments, significantly reducing the reliance on costly ground truth data for dynamic scene reconstruction. |
| Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in
  Instruction-Guided Expressive Text-To-Speech Systems (Read more on [arXiv](https://arxiv.org/abs/2509.13989) or [HuggingFace](https://huggingface.co/papers/2509.13989))| Hung-yi Lee, Kuan-Yu Chen, Tzu-Chieh Wei, Huang-Cheng Chou, Yi-Cheng Lin | This paper quantifies the instruction-perception gap in instruction-guided expressive text-to-speech (ITTS) systems using a novel human evaluation framework. The study's objective is to determine if natural-language instructions for ITTS systems reliably align with listener perceptions, particularly for graded emotion intensity and adverbs of degree. A novel evaluation framework was introduced, incorporating adverbs of degree and graded emotion intensity, alongside speaker age and word-level emphasis, utilizing the newly compiled Expressive VOice Control (E-VOC) corpus of over 165 human raters for large-scale subjective evaluations, complemented by objective acoustic analyses. The evaluation of five ITTS systems revealed that gpt-4o-mini-tts exhibited the most reliable alignment across acoustic dimensions, achieving an F1-score of 0.285 for speaker age; however, analyzed systems frequently generated "Adult" voices regardless of "Child" or "Elderly" instructions, and fine-grained control remained a significant challenge. For AI practitioners, these findings imply that current ITTS models have substantial room for improvement in perceptually aligning fine-grained expressive controls and diverse speaker age attributes with user instructions, which is crucial for their reliable deployment in applications requiring precise speech synthesis. |
| Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided
  Role-playing Agents (Read more on [arXiv](https://arxiv.org/abs/2509.15233) or [HuggingFace](https://huggingface.co/papers/2509.15233))| Chao Zhang, Xueqiao Zhang, RoyalVane, YifanZhu, raul678 | Video2Roleplay introduces a multimodal dataset and framework for video-guided role-playing agents (RPAs) to incorporate dynamic role profiles via video modality. The objective is to bridge the gap in existing RPAs by integrating video, supported by the new Role-playing-Video60k dataset comprising 60k videos and 700k dialogues. The methodology involves adaptive temporal sampling of video frames for dynamic profiles and fine-tuned character dialogues with video summary contexts for static profiles, integrated into a comprehensive RPA framework. Experimental results demonstrate the framework (InternVL2.5-8B w/ Video SFT) achieved an average performance score of 72.28 and a human-likeness score of 69.98, outperforming general and role-playing expertise baselines. This work implies AI practitioners can develop more immersive and human-like RPAs by leveraging video modality and dynamic role profiles, enhancing performance and user engagement in social applications and digital humans. |
| WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained
  Speech Recognition Transformers (Read more on [arXiv](https://arxiv.org/abs/2509.10452) or [HuggingFace](https://huggingface.co/papers/2509.10452))| Karun Kumar, Akshat Pandey, tetrisd | This paper introduces WhisTLE, a deeply supervised, text-only domain adaptation method for pretrained encoder-decoder Automatic Speech Recognition (ASR) models. The objective is to adapt ASR models like Whisper to new domains using only text data, addressing scenarios where paired speech-text data is unavailable. The methodology involves training a variational autoencoder (VAE) to model the ASR encoder's latent outputs from text; this text-to-latent encoder is then used as a drop-in replacement to fine-tune the ASR decoder. Across four ASR models and four out-of-domain datasets, WhisTLE combined with text-to-speech (TTS) synthesis reduces the word error rate (WER) by 12.3% relative to TTS-only adaptation. The principal implication for AI practitioners is that this method allows for the effective adaptation of pretrained ASR models to specialized domains using text-only corpora, improving accuracy on domain-specific terminology without altering the inference architecture or increasing runtime costs. |
| Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn
  Dialogue (Read more on [arXiv](https://arxiv.org/abs/2509.15061) or [HuggingFace](https://huggingface.co/papers/2509.15061))| Hui Zhang, Sicheng Xie, Tianyi Lu, Xinghao Zhu, leolin9248 | Ask-to-Clarify is a framework that enables embodied agents to resolve ambiguous human instructions through multi-turn dialogue and subsequently generate low-level actions for real-world tasks. The primary objective is to build collaborative embodied agents that actively clarify instructions with human users rather than passively executing potentially ambiguous commands. Its methodology involves a two-stage knowledge-insulation training strategy, integrating a Vision-Language Model (VLM) for dialogue-based ambiguity resolution and a diffusion model for end-to-end action generation, with a connection module linking them and a signal detector routing inference. The framework achieved strong average success rates of 95.0% on "Put the fruit," 98.3% on "Pour the water," and 90.0% on "Stack the blocks" tasks, significantly outperforming existing VLAs which performed poorly or failed. This work implies a crucial advancement for AI practitioners aiming to develop robust, interactive, and reliable embodied agents for real-world applications where instruction ambiguity is common. |
