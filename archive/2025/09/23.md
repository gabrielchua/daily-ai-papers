

## Papers for 2025-09-23

| Title | Authors | Summary |
|-------|---------|---------|
| LIMI: Less is More for Agency (Read more on [arXiv](https://arxiv.org/abs/2509.17567) or [HuggingFace](https://huggingface.co/papers/2509.17567))| happyZYM, evanlin2570, weizhihao1, mhjiang0408, YangXiao-nlp | The LIMI (Less Is More for Intelligent Agency) paper demonstrates that agentic intelligence emerges more effectively from minimal, strategically curated training data than from conventional large-scale datasets. The research investigates whether sophisticated agentic capabilities can be cultivated more efficiently with a small, high-quality dataset, challenging the paradigm that more data yields better agency. The methodology involved fine-tuning the GLM-4.5 model on a strategically curated dataset of only 78 training samples derived from complex, multi-turn software development and scientific research workflows. The primary result is that LIMI achieves a 73.5% performance score on AgencyBench, dramatically outperforming baseline models and showing a 53.7% improvement over a model trained on 10,000 samples while using 128 times less data. The principal implication for AI practitioners is that the development of autonomous AI systems should prioritize the strategic curation of high-quality agentic demonstrations over the sheer volume of training data, suggesting a fundamental shift in data strategy for building agents. |
| OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion
  Transformer Models (Read more on [arXiv](https://arxiv.org/abs/2509.17627) or [HuggingFace](https://huggingface.co/papers/2509.17627))| Pengze Zhang, Tianxiang Ma, Xu Bai, Xinghui Li, Jinshu Chen | This paper presents OmniInsert, a unified framework for mask-free video insertion of any reference subject into a source video using a diffusion transformer model. The objective is to address the key challenges of data scarcity, subject-scene equilibrium, and insertion harmonization in the Mask-free Video Insertion (MVI) task. The methodology includes a new data generation pipeline called InsertPipe, a Condition-Specific Feature Injection (CFI) mechanism, and a four-stage Progressive Training (PT) strategy that incorporates a Subject-Focused Loss and Insertive Preference Optimization (IPO). On the newly introduced InsertBench benchmark, OmniInsert demonstrated superior performance over commercial baselines, achieving a 68.34% preference rate in a comprehensive user study. The principal implication for AI practitioners is the provision of a complete system—including a data pipeline, a model architecture, and a multi-stage training strategy—that effectively balances the disparate learning difficulties of subject insertion and background preservation, offering a blueprint for complex conditional video editing tasks. |
| Qwen3-Omni Technical Report (Read more on [arXiv](https://arxiv.org/abs/2509.17765) or [HuggingFace](https://huggingface.co/papers/2509.17765))| Lhma-aslp, Cyanbox, jinzheng-he, faychu, ZhifangGuo | The Qwen3-Omni technical report introduces a unified multimodal model that achieves state-of-the-art performance across text, image, audio, and video without performance degradation compared to its single-modal counterparts. The primary objective was to resolve the common modality trade-off, where improving one modality degrades others, by developing a single, integrated model. This was achieved using a Thinker-Talker Mixture-of-Experts (MoE) architecture, a novel Audio Transformer (AuT), and a multi-stage pretraining strategy that mixes unimodal and cross-modal data from an early phase. The key result demonstrates non-degradation, with the Qwen3-Omni-30B-A3B model scoring 81.69 on the MMLU benchmark, slightly outperforming its 81.24-scoring text-only counterpart, while also achieving state-of-the-art results on 32 audio/audiovisual benchmarks. For practitioners, this implies that a single, efficient model can replace multiple specialized systems for complex multimodal applications without sacrificing performance, thereby simplifying the deployment stack. |
| OnePiece: Bringing Context Engineering and Reasoning to Industrial
  Cascade Ranking System (Read more on [arXiv](https://arxiv.org/abs/2509.18091) or [HuggingFace](https://huggingface.co/papers/2509.18091))| Jiahua Wu, Ethan7, vicowang, TangJiakai5704, KID-22 | This paper introduces OnePiece, a unified framework that integrates Large Language Model (LLM) principles of context engineering and multi-step reasoning into industrial cascaded ranking systems. The primary objective is to operationalize these LLM mechanisms to achieve significant performance improvements beyond merely transplanting Transformer architectures into existing Deep Learning Recommendation Models (DLRMs). The core methodology combines structured context engineering to unify interaction history and reference signals into a token sequence, block-wise latent reasoning for iterative representation refinement, and progressive multi-task training that uses user feedback chains (e.g., click, order) for supervision. Deployed in a large-scale commercial search system, OnePiece demonstrated significant online A/B test gains, including a +2.90% increase in advertising revenue over a strong production baseline. The principal implication for AI practitioners is that redesigning input representation and the optimization process to emulate LLM-style reasoning provides a practical and more effective path to improving industrial ranking systems than solely focusing on architectural changes. |
| TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning
  for Video LLMs (Read more on [arXiv](https://arxiv.org/abs/2509.18056) or [HuggingFace](https://huggingface.co/papers/2509.18056))| Shaohui Jiao, Hangyi Kuang, Shaoyong Jia, Jing Cheng, lyhisme | TempSamp-R1 is a novel reinforcement fine-tuning framework designed to enhance temporal video understanding in Multimodal Large Language Models (MLLMs). The framework addresses limitations of existing on-policy reinforcement learning methods by improving MLLMs' performance on video temporal grounding tasks that require precise spatio-temporal understanding. TempSamp-R1 leverages ground-truth annotations as off-policy supervision, integrates a non-linear soft advantage computation via asymmetric transformation for stable policy updates, and employs a hybrid Chain-of-Thought training paradigm. The method achieves state-of-the-art performance, evidenced by a +5.3% improvement in R1@0.5 on ActivityNet Captions, reaching 56.0%, and also shows robust few-shot generalization. This approach offers AI practitioners a more stable and data-efficient fine-tuning paradigm, which is critical for developing precise temporal reasoning capabilities in applications like video retrieval and assistive robotics, particularly when annotated data is limited. |
| GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.17437) or [HuggingFace](https://huggingface.co/papers/2509.17437))| Hou Pong Chan, Weiwen Xu, Swrooy, 26hzhang, Guizhen | This paper introduces a two-stage reinforcement learning framework to improve geometric reasoning in Multimodal Large Language Models (MLLMs) by first correcting foundational visual perception deficits. The objective is to overcome the "perceptual bottleneck," where poor visual understanding of geometric concepts limits the efficacy of high-level reasoning training. The methodology consists of a two-stage RL process: first, training on a curated Geo-Perception Question-Answering (GeoPQA) dataset to enhance visual perception, followed by a second stage focused on complex geometric reasoning. Applying this framework to the Qwen2.5-VL-3B-Instruct model improved geometric reasoning accuracy by 9.7% and problem-solving by 9.1% on MathVista compared to direct reasoning training alone. The principal implication for practitioners is that for vision-intensive domains, establishing a strong perceptual foundation in a model is a critical prerequisite for effective higher-level reasoning training. |
| EpiCache: Episodic KV Cache Management for Long Conversational Question
  Answering (Read more on [arXiv](https://arxiv.org/abs/2509.17396) or [HuggingFace](https://huggingface.co/papers/2509.17396))| Minsik Cho, Richa Dixit, Han-Byul Kim, Arnav Kundu, minsoo2333 | EPICACHE is a training-free Key-Value (KV) cache management framework for long conversational question answering that uses episodic clustering and an adaptive layer-wise budget allocation to operate under fixed memory constraints. The research objective is to address the unbounded memory growth from KV caching in long-context LLMs by designing a system that enforces a constant memory footprint without degrading multi-turn conversational accuracy. The methodology involves three stages: offline clustering of conversation history into topical episodes, block-wise prefill using episode-specific medoids as patched prompts to guide eviction based on a pre-calculated layer-wise sensitivity score, and online retrieval of the relevant compressed episodic cache for decoding. Across three LongConvQA benchmarks, EPICACHE improves accuracy by up to 40% over baselines, reduces latency by up to 2.4x, and cuts peak memory usage by up to 3.5x compared to full KV caching. For AI practitioners, this framework provides a practical method to deploy LLMs for extended multi-turn conversations on resource-constrained systems by bounding peak memory and significantly reducing inference latency, making such applications more efficient and feasible. |
| SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering
  Tasks? (Read more on [arXiv](https://arxiv.org/abs/2509.16941) or [HuggingFace](https://huggingface.co/papers/2509.16941))| Yannis Yiming He, Edwin Pan, Jeff Da, Xiang Deng, nlauffer | The paper introduces SWE-BENCH PRO, a challenging, contamination-resistant benchmark designed to evaluate AI agents on complex, enterprise-level software engineering tasks. The main objective is to assess the capability of current AI agents to solve long-horizon, multi-file coding problems that are more representative of real-world software development than existing benchmarks. The key methodology involves curating 1,865 problems from 41 diverse repositories (including copyleft-licensed public and proprietary commercial codebases) and using a three-stage human-in-the-loop process to augment problem statements and verify test suites, ensuring task resolvability and minimizing data contamination. The primary result is that state-of-the-art models struggle significantly, with GPT-5 achieving the highest reported Pass@1 resolve rate at 23.3% on the public set, a substantial drop from the >70% performance seen on simpler benchmarks. The principal implication for AI practitioners is that current agentic systems have critical limitations in handling the complexity and scale of industrial software engineering tasks, indicating that substantial advancements are needed in areas like context management, algorithmic correctness, and multi-file code manipulation to achieve professional-level autonomy. |
| DiffusionNFT: Online Diffusion Reinforcement with Forward Process (Read more on [arXiv](https://arxiv.org/abs/2509.16117) or [HuggingFace](https://huggingface.co/papers/2509.16117))| Qinsheng Zhang, Haoxiang Wang, Haotian Ye, Huayu Chen, Kaiwen Zheng | This paper introduces DiffusionNFT, a novel online reinforcement learning paradigm for diffusion models that performs policy optimization directly on the forward process. The objective is to develop a likelihood-free RL framework that avoids the drawbacks of reverse-process methods, such as solver restrictions and forward-reverse inconsistency. DiffusionNFT employs a flow matching objective that contrasts positive and negative generations to define an implicit policy improvement direction, integrating reinforcement signals directly into the supervised learning objective. The method demonstrates up to 25x greater efficiency than FlowGRPO, improving the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO achieves 0.95 in over 5k steps. For AI practitioners, this provides a more efficient and simplified method to finetune diffusion models using RL, as it decouples training from sampling, allows the use of any black-box solver, and eliminates the need for Classifier-Free Guidance. |
| ByteWrist: A Parallel Robotic Wrist Enabling Flexible and
  Anthropomorphic Motion for Confined Spaces (Read more on [arXiv](https://arxiv.org/abs/2509.18084) or [HuggingFace](https://huggingface.co/papers/2509.18084))| Jiafeng Xu, Jingchao Qiao, Liqun Huang, Jiawen Tian, cuizhongren | This paper introduces ByteWrist, a compact, anthropomorphic parallel robotic wrist designed for high-dexterity manipulation in confined spaces. The primary objective is to overcome the structural limitations of existing serial and parallel wrists by developing a novel mechanism that achieves precise Roll-Pitch-Yaw motion while maintaining high compactness, efficiency, and stiffness. The methodology involves a mechanical design featuring a nested three-stage motor-driven linkage system, the derivation of its complete forward and inverse kinematic models, and a numerical solution for the Jacobian matrix to enable precise control. In a comparative confined-space grasping experiment, the ByteWrist-equipped robot completed the task in 234 seconds, approximately twice as fast as a Kinova-based serial wrist system which took 476 seconds. The principal implication for AI practitioners is that this hardware provides a more dexterous and anthropomorphic platform, crucial for collecting high-quality manipulation data and successfully deploying vision-language-action (VLA) models in complex, human-centric tasks. |
| FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning
  Models on Automatically Verifiable Textual and Visual Questions (Read more on [arXiv](https://arxiv.org/abs/2509.17177) or [HuggingFace](https://huggingface.co/papers/2509.17177))| tengdai722, stephaniezhou, xuanricheng, miguelhuchen, lilaczheng | This report presents a moderate-scale, contamination-free evaluation of Large Reasoning Models (LRMs) on novel textual and visual tasks, assessing performance and behavioral characteristics like reasoning faithfulness and tool use. The primary objective is to evaluate how recent LRMs perform and behave on new, automatically verifiable problems, and to understand the utility and characteristics of their test-time thinking processes. The methodology involves creating new textual and visual datasets, including the new ROME benchmark for vision, and conducting an LLM-assisted, rubric-guided analysis to quantify behaviors like inconsistent answers and tool hallucination from reasoning traces. Key results show that while LRMs generally outperform non-thinking counterparts on complex textual tasks, with the GPT-5 series achieving over 90% accuracy on deciphering, they exhibit significant behavioral issues; for example, Gemini 2.5 Pro hallucinates web search in ~40% of cases on long-tailed factual questions. The principal implication for AI practitioners is that LRM reasoning traces are not inherently reliable, as models frequently display misaligned thinking where the reasoning contradicts the final answer and pretend to use external tools, which necessitates robust verification before deployment in mission-critical applications. |
| VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video
  Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2509.17985) or [HuggingFace](https://huggingface.co/papers/2509.17985))| Sunghyun Cho, Janghyeok Han, Geonung Kim | VideoFrom3D is a novel framework that synthesizes high-quality, stylized 3D scene videos from coarse geometry by leveraging the complementary strengths of image and video diffusion models. The primary objective is to generate temporally coherent and stylistically consistent videos from minimal inputs—coarse geometry, a camera trajectory, and a reference image—addressing the quality limitations of existing video diffusion models on complex scenes. The methodology employs a two-stage approach: a Sparse Anchor-view Generation (SAG) module uses an image diffusion model to create high-quality, consistent keyframes, which are then interpolated by a Geometry-guided Generative Inbetweening (GGI) module using a video diffusion model conditioned on optical flow and structural guidance. The proposed method demonstrated superior performance over baselines, achieving a visual quality MUSIQ score of 68.615, the highest among all compared approaches. The principal implication for AI practitioners is the validation of a hybrid architecture that uses high-fidelity image models for keyframe generation and video models for motion interpolation, providing a practical method to enhance video quality and consistency in controlled generative tasks. |
| ARE: Scaling Up Agent Environments and Evaluations (Read more on [arXiv](https://arxiv.org/abs/2509.17158) or [HuggingFace](https://huggingface.co/papers/2509.17158))| Matteo Bettini, Gerard Moreno-Torres Bertran, Amine Benhalloum, Pierre Andrews, HugoLaurencon | This paper introduces Meta Agents Research Environments (ARE), a platform for building scalable, asynchronous agent environments, and the Gaia2 benchmark, designed to evaluate advanced agent capabilities in dynamic settings. The main objective is to bridge the gap between model development and real-world deployment by enabling the creation and evaluation of agents in complex, time-driven environments that surface failure modes invisible in static benchmarks. The key methodology involves using the event-based ARE platform to construct the Gaia2 benchmark, which evaluates agents on 1,120 scenarios within a simulated mobile environment using a `pass@1` metric; success is determined by a verifier that compares an agent's sequence of `write` actions against a ground-truth oracle action graph for consistency, causality, and timing. The primary results show that no single model dominates, with GPT-5 (high) achieving the top overall score of 42.1% but scoring 0.0% on time-sensitive tasks, indicating an inverse scaling relationship where stronger reasoning correlates with higher latency, making agents less practical for interactive deployments. The principal implication for AI practitioners is that progress requires moving beyond current scaffolds to develop adaptive compute strategies and architectures that balance reasoning capability with latency and cost, as deploying the most powerful model is not always optimal for real-world, time-constrained applications. |
| Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from
  Token and Parameter Levels (Read more on [arXiv](https://arxiv.org/abs/2509.16596) or [HuggingFace](https://huggingface.co/papers/2509.16596))| Qi Zhang, Shuo Li, Yang Nan, Umean, Junjie-Ye | This paper investigates how supervised fine-tuning (SFT) impacts large language model (LLM) factual knowledge, aiming to understand knowledge change mechanisms and mitigate undesirable effects during fine-tuning. The study evaluates closed-book question answering (CBQA) performance across LLaMA-2 and LLaMA-3 models, employing token-level analysis via Kullback-Leibler (KL) divergence and parameter-level analysis through selective restoration of highly updated parameters. Results show that LLMs fine-tuned on 1,920 samples can perform up to 14% worse than those fine-tuned on 240 samples, and up to 90% of parameter updates during SFT do not contribute to knowledge enhancement. Restoring specific parameter updates, for instance, yielded a 10.48% performance gain on CBQA for LLaMA-3-8B with certain datasets. This work offers practical guidance for AI practitioners to develop more effective fine-tuning strategies by optimizing data scale and quality, and considering targeted parameter restoration to preserve prior knowledge and enhance model performance. |
| Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG
  Applications (Read more on [arXiv](https://arxiv.org/abs/2509.17671) or [HuggingFace](https://huggingface.co/papers/2509.17671))| Fatma Betül Terzioğlu, Reyhan Bayraktar, ozayezerceli, MElHuseyni, selvatas | Turk-LettuceDetect introduces specialized token-level hallucination detection models for Turkish Retrieval-Augmented Generation (RAG) applications. The primary objective is to mitigate hallucination in LLM-generated content for Turkish RAG systems, addressing challenges in a morphologically complex, low-resource language. The methodology formulates hallucination detection as a token-level classification task, fine-tuning ModernBERT-base-tr, TurkEmbed4STS, and lettucedect-210m-eurobert-tr-v1 encoder architectures on a machine-translated Turkish RAGTruth dataset. Experimentally, the ModernBERT-based model achieved an F1-score of 0.7266 on the complete test set, exhibiting strong performance in structured tasks. This work implies that AI practitioners can deploy computationally efficient hallucination detection for Turkish RAG, as the models support long contexts up to 8,192 tokens, enhancing reliability for real-time applications in such languages. |
| QWHA: Quantization-Aware Walsh-Hadamard Adaptation for
  Parameter-Efficient Fine-Tuning on Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.17428) or [HuggingFace](https://huggingface.co/papers/2509.17428))| Jae-Joon Kim, Yulhwa Kim, Beomseok Kang, Seojune Lee, Hyesung Jeon | QWHA is a novel quantization-aware parameter-efficient fine-tuning (QA-PEFT) framework using Walsh-Hadamard Transform (WHT)-based adapters with an advanced initialization scheme for large language models (LLMs). The research aims to effectively integrate Fourier-related transform (FT)-based adapters into quantized LLMs for QA-PEFT, addressing limitations of existing methods by mitigating quantization errors during initialization and enhancing fine-tuning. QWHA employs a WHT-based adapter with a single transform and a two-stage initialization: AdaAlloc adaptively allocates parameters to channels based on quantization error, followed by Refinement to optimize selected parameter values, all driven by minimizing layer output error. QWHA consistently outperforms baselines in low-bit quantization accuracy, achieving 60.98% accuracy on CSQA for LLaMA-3.2-3B with 2-bit quantization, a 6.09% increase over CLoQ (54.89%), and reducing training time from 9.8 hours (LoCA) to 3.9 hours (QWHA) for LLaMA-3.1-8B (batch size 16). This provides a computationally efficient and accurate QA-PEFT solution, enabling AI practitioners to fine-tune highly quantized LLMs with improved performance and reduced training overhead. |
| Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.18083) or [HuggingFace](https://huggingface.co/papers/2509.18083))| Damien Sileo, Valentin Quesnel, Valentin Lacombe | The paper introduces Reasoning Core, a scalable RLVR environment designed to advance foundational symbolic reasoning in LLMs. Its main objective is to provide a continuous supply of high-quality, verifiable training data for core formal domains like PDDL planning, first-order logic, and system equation solving. The methodology employs procedural generation with a continuous "difficulty knob" and integrates external specialized tools for solution verification via offline parallel generation. Initial zero-shot evaluations of GPT-5 confirmed the benchmark's challenging nature, showing its average reward on 'logic_nli' dropped from approximately 60% on easy tasks to less than 10% on hard tasks. This environment provides AI practitioners with a critical, scalable resource for training LLMs to achieve more general and robust reasoning skills through RLVR. |
| Understanding Embedding Scaling in Collaborative Filtering (Read more on [arXiv](https://arxiv.org/abs/2509.15709) or [HuggingFace](https://huggingface.co/papers/2509.15709))| Yonghui Yang, Fengbin Zhu, Haoyue Bai, Zhou Kaiyu, Zhuangzhuang He | This research investigates the performance of collaborative filtering models as embedding dimensions are scaled, uncovering novel "double-peak" and "logarithmic" phenomena that challenge the conventional single-peak assumption. The study aims to understand why increasing embedding dimensions does not always improve performance by analyzing the impact of noisy user-item interactions on different model architectures. The authors conducted large-scale experiments on 10 datasets with 4 models (BPR, NeuMF, LightGCN, SGL) by exponentially increasing embedding dimensions, complemented by a theoretical analysis of noise robustness. Key results show that noise-resistant models like SGL exhibit a logarithmic performance increase, while models like BPR often show a double-peak trend; for instance, on one dataset, scaling the embedding dimension led to a 25.57% NDCG@20 improvement over a standard 128-dimension model. For AI practitioners, the principal implication is that scaling embedding dimensions can significantly boost performance, but this is contingent on the dataset's noise level and the chosen model's inherent robustness, with graph-based and self-supervised models being more scalable. |
| Synthetic bootstrapped pretraining (Read more on [arXiv](https://arxiv.org/abs/2509.15248) or [HuggingFace](https://huggingface.co/papers/2509.15248))| Emmanuel Candès, Tatsunori Hashimoto, Hong Liu, Aonan Zhang, Zitong Yang | Synthetic Bootstrapped Pretraining (SBP) is a procedure that improves language model performance by learning inter-document relations from a fixed corpus to synthesize a vast new dataset for joint training. The main objective is to determine if explicitly modeling inter-document correlations, which standard pretraining overlooks, can improve model performance in data-constrained scenarios. The methodology involves a three-step process: (1) identifying semantically similar document pairs in the pretraining corpus using approximate nearest neighbor search, (2) tuning a conditional language model (a "synthesizer") on these pairs to learn the relationship `p(doc2|doc1)`, and (3) using this synthesizer to generate a large new text corpus for joint training with the original data. In compute-matched experiments training a 3B parameter model on up to 1T tokens, SBP consistently outperforms a strong repetition baseline and delivers roughly 47% of the QA accuracy improvement attainable by an oracle model with access to 20x more unique data. The principal implication for AI practitioners is that SBP provides a self-contained framework to overcome data scarcity and enhance model pretraining by more effectively utilizing an existing, fixed corpus, enabling model self-improvement without needing external teacher models or new data sources. |
| MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late
  Interaction (Read more on [arXiv](https://arxiv.org/abs/2509.18095) or [HuggingFace](https://huggingface.co/papers/2509.18095))| Xintao Chen, Chun-cheng Jason Chen, Mengting Gu, Qi Ma, MrZilinXiao | METAEMBED is a framework for multimodal retrieval using learnable "Meta Tokens" to generate compact, structured multi-vector embeddings, enabling a test-time trade-off between retrieval accuracy and computational cost. The objective is to create a scalable multimodal retrieval system that resolves the trade-off between the limited expressiveness of single-vector embeddings and the high computational cost of traditional multi-vector methods. The key methodology involves appending a fixed number of learnable Meta Tokens to the input of a Vision-Language Model (VLM) and training them with a proposed Matryoshka Multi-Vector Retrieval (MMR) objective, which uses contrastive learning on nested prefixes of the output embeddings to organize information by granularity. The primary result is state-of-the-art performance on the MMEB benchmark, where the METAEMBED-32B variant achieved a 78.7% Precision@1 score. The framework's test-time scalability was demonstrated as increasing the retrieval budget from (1 query, 1 candidate) vector to (16 query, 64 candidate) vectors improved this model's performance by 6.6 percentage points. The principal implication for AI practitioners is the ability to deploy a single, powerful retrieval model that can be dynamically configured at inference time to meet different latency and memory budgets by simply adjusting the number of embedding vectors used for indexing and scoring, eliminating the need for retraining multiple models. |
| ContextFlow: Training-Free Video Object Editing via Adaptive Context
  Enrichment (Read more on [arXiv](https://arxiv.org/abs/2509.17818) or [HuggingFace](https://huggingface.co/papers/2509.17818))| Yue Ma, Xiujun Ma, Xuanhua He, Yiyang Chen | ContextFlow is a training-free framework enabling high-fidelity video object editing for Diffusion Transformers (DiTs) by using adaptive context enrichment. The primary objective is to achieve precise and temporally coherent training-free video object editing—including insertion, swapping, and deletion—in DiT models by resolving the challenges of inaccurate inversion and contextual conflicts found in prior methods. The key methodology involves three components: 1) using a high-order Rectified Flow (RF-Solver) for near-lossless video inversion to establish a robust editing foundation, 2) employing "Adaptive Context Enrichment," a dual-path sampling process that enriches the editing path's self-attention context by concatenating Key-Value pairs from a parallel reconstruction path, and 3) applying this guidance only to task-specific vital layers identified via a novel data-driven "Guidance Responsiveness Metric." The method significantly outperforms existing training-free approaches and surpasses several training-based models; for object swapping, ContextFlow achieves a state-of-the-art CLIP-Score of 0.3391 and an overall consistency of 0.2648, outperforming all listed baselines. The principal implication for AI practitioners is that enriching self-attention context with reference Key-Value pairs, targeted at systematically identified influential layers, provides a superior mechanism for guiding generative transformers over hard feature injection, enabling higher-fidelity control over pretrained models without requiring any finetuning. |
| AuditoryBench++: Can Language Models Understand Auditory Knowledge
  without Hearing? (Read more on [arXiv](https://arxiv.org/abs/2509.17641) or [HuggingFace](https://huggingface.co/papers/2509.17641))| Jaeho Lee, Hyeonjun Kim, Hyunjong Ok, suhoyoo | This paper introduces AuditoryBench++, a text-only benchmark for evaluating auditory knowledge in language models, and proposes AIR-CoT, a reasoning method that injects auditory embeddings during inference to improve performance. The primary objective is to evaluate if language models can reason about auditory properties without direct audio input and to develop a method to enhance this capability. The proposed AIR-CoT method employs a two-stage process: first, it fine-tunes an LLM to detect text spans requiring auditory knowledge using special tokens; second, during inference, it pauses generation at these spans to dynamically inject audio embeddings produced by a CLAP text encoder and a projector MLP. The AIR-CoT method significantly outperforms baseline models, achieving 82.67% accuracy on the Auditory Context Reasoning task, an absolute improvement of over 11.88 percentage points compared to the strongest off-the-shelf LLM. The principal implication for AI practitioners is that this work provides a framework for enhancing LLMs with specialized, non-textual commonsense by training them to recognize knowledge gaps and dynamically inject modality-specific embeddings, enabling more robust reasoning in text-only environments without requiring full multimodal inputs at inference time. |
| Mano Report (Read more on [arXiv](https://arxiv.org/abs/2509.17336) or [HuggingFace](https://huggingface.co/papers/2509.17336))| Minghui Wu, Hanning Wang, Chenxu Zhao, Anyang Su, Tianyu Fu | The paper introduces Mano, a multi-modal foundation model-based agent designed for robust automation of Graphical User Interface (GUI) tasks. The primary objective is to overcome the limitations of existing vision-language models (VLMs) in GUI automation, such as domain mismatch and insufficient sequential decision-making capabilities. The methodology involves a three-stage training pipeline (supervised fine-tuning, offline reinforcement learning, and online reinforcement learning) applied to the UITARS-1.5-7B model, utilizing a custom simulated environment for data generation and a dedicated verification module for error recovery. Mano achieves state-of-the-art performance, including a success rate of 41.6% on the OSWorld-Verified benchmark for computer usage tasks. The principal implication for AI practitioners is that integrating a progressive, multi-stage reinforcement learning framework with high-fidelity, domain-specific simulated data provides an effective pathway to enhance VLM robustness and sequential reasoning for practical GUI agent deployment. |
| Cross-Attention is Half Explanation in Speech-to-Text Models (Read more on [arXiv](https://arxiv.org/abs/2509.18010) or [HuggingFace](https://huggingface.co/papers/2509.18010))| Luisa Bentivogli, Matteo Negri, Marco Gaido, Dennis Fucci, Sara Papi | This research systematically evaluates the explanatory power of cross-attention in speech-to-text (S2T) models by comparing its scores against feature attribution-based saliency maps. The study's main objective is to determine the extent to which cross-attention reliably reflects input-output dependencies and can serve as a valid proxy for explainability in S2T systems. The methodology involves computing Pearson correlation between cross-attention scores and saliency maps generated by the SPES attribution method on both raw spectrogram inputs and encoder outputs across various Conformer-based models. Results demonstrate that even under optimal aggregation, cross-attention accounts for only about 50% of input relevance and explains a maximum of 52-75% of encoder output saliency, highlighting a fundamental gap. For AI practitioners, this implies that cross-attention is an incomplete explanatory proxy; for downstream applications like timestamping, averaging attention across layers and heads is recommended to better approximate true input relevance. |
| DIWALI - Diversity and Inclusivity aWare cuLture specific Items for
  India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian
  Context (Read more on [arXiv](https://arxiv.org/abs/2509.17399) or [HuggingFace](https://huggingface.co/papers/2509.17399))| Maunendra Sankar Desarkar, mrajbrahma, pramitsahoo | This paper introduces DIWALI, a dataset of ~8,800 Indian culture-specific items across 17 facets, to evaluate LLM cultural adaptation capabilities. The main objective is to create a comprehensive, sub-regionally granular dataset for Indian culture and use it to systematically assess the cultural competence of LLMs on a text adaptation task from an American to an Indian context. The methodology involves curating the DIWALI dataset via prompting and web searches with manual verification, then evaluating seven open-weight LLMs by prompting them to adapt text from datasets like GSM8k. Performance is measured using a custom Adaptation Score (CSI matching), LLM-as-Judge, and human evaluation. Primary results demonstrate that DIWALI provides a more sensitive evaluation than existing datasets; for instance, Llama-2-7b-chat-hf scored an exact match Adaptation Score of 0.855 using DIWALI versus only 0.028 using the CANDLE dataset. Human evaluations confirm that LLMs perform shallow, surface-level adaptations, with the best model achieving an average cultural relevance score of only 2.68 out of 5. The principal implication for AI practitioners is that standard LLMs are unreliable for nuanced cultural adaptation tasks, exhibiting significant sub-regional biases and failing to capture deep contextual meaning. Relying solely on automatic metrics or even LLM-as-Judge can be misleading, and practitioners must use specialized, culturally-grounded datasets like DIWALI for meaningful evaluation and development of culturally competent AI systems. |
| When Big Models Train Small Ones: Label-Free Model Parity Alignment for
  Efficient Visual Question Answering using Small VLMs (Read more on [arXiv](https://arxiv.org/abs/2509.16633) or [HuggingFace](https://huggingface.co/papers/2509.16633))| Anand Mishra, Piyush Arora, Navlika Singh, abhiram4572 | This paper presents the Model Parity Aligner (MPA), a label-free framework to enhance small vision-language models (S-VLMs) for visual question answering (VQA) by leveraging knowledge from large VLMs (L-VLMs). The objective is to systematically improve S-VLM performance on complex VQA tasks using only unlabeled images, thereby eliminating the need for expensive data annotation. The core methodology involves a three-stage pipeline: an L-VLM first generates pseudo-annotated question-answer pairs from images (Pseudo Annotator); a Parity Identifier then filters these pairs to retain only those where the L-VLM answers correctly but the S-VLM fails, thus isolating knowledge gaps; finally, the S-VLM is fine-tuned exclusively on this high-signal subset (Parity Leveler). The framework consistently improves S-VLM performance across four VQA benchmarks, with a notable quantitative result being a +15.2% absolute accuracy improvement for TinyLLaVA-2B on ChartQA. The principal implication for AI practitioners is that MPA provides a computationally efficient method to create performant, specialized S-VLMs suitable for resource-constrained deployment by leveraging large, even closed-source, models without requiring labeled data or access to model logits. |
| From Uniform to Heterogeneous: Tailoring Policy Optimization to Every
  Token's Nature (Read more on [arXiv](https://arxiv.org/abs/2509.16591) or [HuggingFace](https://huggingface.co/papers/2509.16591))| Bin Cui, Mengzhang Cai, Siwei Wen, Mengjie Liu, starriver030515 | This paper introduces Heterogeneous Adaptive Policy Optimization (HAPO), a token-aware algorithm that tailors reinforcement learning optimization to individual tokens based on their entropy. The research objective is to overcome the limitations of uniform optimization in existing RLHF algorithms by developing a framework that treats tokens heterogeneously based on their functional role. HAPO's methodology consists of four entropy-driven components: Adaptive Temperature Sampling for rollouts, Token-Level Group Average for advantage calculation, Differential Advantage Redistribution using both entropy and importance ratios, and Asymmetric Adaptive Clipping for the loss function. The proposed method demonstrates consistent outperformance of baselines, with the HAPO-trained Qwen2.5-Math-7B model achieving a 3.07 point average accuracy gain over vanilla DAPO. The principal implication for AI practitioners is that applying these fine-grained, entropy-aware controls at every stage of the RL training pipeline can significantly boost LLM reasoning performance with negligible additional computational overhead. |
| CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End
  Code Review Evaluation in Python Projects (Read more on [arXiv](https://arxiv.org/abs/2509.14856) or [HuggingFace](https://huggingface.co/papers/2509.14856))| Hang Yu, Zihan Liao, Xunjin Zheng, Hanyang Guo, Geralt-Targaryen | This paper introduces CodeFuse-CR-Bench, a comprehensiveness-aware benchmark designed for the end-to-end evaluation of Large Language Models on repository-level code review (CR) tasks. The primary objective is to bridge the "reality gap" between existing, context-poor CR benchmarks and the holistic, context-rich nature of real-world software development by providing a more realistic evaluation framework. The methodology involves the construction of a benchmark with 601 high-quality instances from 70 Python projects, each containing multi-faceted context, and a novel evaluation framework that combines rule-based metrics (location, syntax) with model-based judgments from a custom-trained Reward Model and an LLM-as-a-Judge. The primary result shows that no single LLM dominates all aspects of CR, but Gemini 2.5 Pro achieves the highest comprehensive performance score (52.37%), demonstrating superior robustness and context utilization compared to other state-of-the-art models. The principal implication for AI practitioners is that developing effective, practical AI-powered CR assistants requires holistic, multi-dimensional evaluation, and Gemini 2.5 Pro's strong performance with minimal retrieved context makes it a highly efficient model for scalable, real-world implementation. |
| From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI
  Ecosystem (Read more on [arXiv](https://arxiv.org/abs/2509.09873) or [HuggingFace](https://huggingface.co/papers/2509.09873))| Ahmed E. Hassan, Gopi Krishnan Rajbahadur, Bram Adams, James Jewitt, hao-li | This research presents the first end-to-end empirical audit of license propagation in the AI supply chain, revealing systemic non-compliance from datasets to models to software applications. The study's objective is to quantify the scale and nature of "license drift"—the process by which legal obligations are discarded—across the full dataset-to-model-to-application lineage. The methodology involves constructing a dependency graph linking 364k datasets and 1.6M models from Hugging Face to 140k downstream GitHub applications, using AST-based static analysis and a custom, ML-aware compatibility matrix to detect license violations. The analysis reveals that 35.5% of model-to-application transitions violate the upstream model's license, primarily by relicensing models with restrictive, use-based clauses under fully permissive terms. The principal implication for AI practitioners is that they must diligently vet the entire license lineage of components, as automated tooling can fix many license declaration errors but cannot resolve fundamental incompatibilities inherited from upstream assets, which create significant and often unacknowledged legal risk. |
| VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery (Read more on [arXiv](https://arxiv.org/abs/2509.17191) or [HuggingFace](https://huggingface.co/papers/2509.17191))| Shiya Huang, Zeyu Zhang, Biao Wu, Tengfei Cheng, Jinchao Ge | The paper introduces VaseVL, an SFT-then-RL framework, and the VaseVQA benchmark to equip MLLMs with expert-level reasoning for ancient Greek pottery analysis. The objective is to develop a system that moves beyond standard supervised fine-tuning to address the brittle, superficial reasoning MLLMs exhibit in specialized domains. The methodology uses an SFT model as a baseline, diagnoses its performance gaps across a taxonomy of question types, and then applies Group Relative Policy Optimization (GRPO) with type-conditioned rewards and a KL penalty to specifically target these identified weaknesses. The VaseVL model improves upon the SFT-only baseline in complex reasoning tasks, most notably increasing the BLEU@1 score for the descriptive `Decoration` question type from 2.57 to 9.82. The principal implication for AI practitioners is a generalizable "diagnosis-guided reward engineering" template for adapting foundation models to expert domains, showing how a targeted RL phase can patch specific reasoning failures post-SFT without sacrificing factual recall. |
| SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward
  Learning (Read more on [arXiv](https://arxiv.org/abs/2509.16548) or [HuggingFace](https://huggingface.co/papers/2509.16548))| Zhaopeng Tu, Xiaobo Liang, Juntao Li, Xinyu Shi, dyyyyyyyy | SCAN introduces a self-denoising Monte Carlo annotation framework for robust process reward learning. The primary objective is to overcome the high noise ratio and scalability issues of synthetic data for Process Reward Models (PRMs) without external strong supervision. It employs a self-confidence metric, selective sampling for efficient data synthesis, and a noise-tolerant loss with confidence-wise reweighting for robust training. SCAN-Pro achieved a 39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench, surpassing baselines including those trained on PRM800K, and generated high-quality annotations with only 6% of vanilla MC estimation's inference cost. This approach enables AI practitioners to train high-performing PRMs scalably and cost-effectively from noisy synthetic data. |
