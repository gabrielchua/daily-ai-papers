

## Papers for 2025-09-24

| Title | Authors | Summary |
|-------|---------|---------|
| Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR (Read more on [arXiv](https://arxiv.org/abs/2509.18174) or [HuggingFace](https://huggingface.co/papers/2509.18174))| Zeina Aldallal, Ahmad Bastati, Mohamed Motasim Hamed, Muhammad Hreden, Khalil Hennara | Baseer is a vision-language model fine-tuned from Qwen2.5-VL-3B-Instruct for high-accuracy Arabic document-to-markdown OCR. The research objective was to develop a specialized model to overcome the inherent complexities of Arabic script, such as its cursive nature and right-to-left orientation, to achieve state-of-the-art performance in document OCR. The key methodology involved a decoder-only fine-tuning strategy on a pre-trained MLLM, keeping the vision encoder frozen, using a hybrid dataset of 500,000 image-text pairs composed of 300,000 synthetic and 200,000 real-world documents. The primary result is that on the newly introduced Misraj-DocOCR benchmark, Baseer achieves a state-of-the-art Word Error Rate (WER) of 0.25 and a Tree Edit Distance Similarity (TEDS) of 66, outperforming commercial systems in both textual and structural metrics. The principal implication for AI practitioners is that domain-specific, decoder-only fine-tuning of a general-purpose MLLM is a highly effective strategy for creating high-performance, specialized models for morphologically complex languages without retraining the entire architecture. |
| Reinforcement Learning on Pre-Training Data (Read more on [arXiv](https://arxiv.org/abs/2509.19249) or [HuggingFace](https://huggingface.co/papers/2509.19249))| Evander Yang, Guanhua Huang, Zenan Xu, Kejiao Li, Siheng Li | This paper introduces Reinforcement Learning on Pre-Training data (RLPT), a paradigm for improving LLMs by applying a self-supervised, next-segment reasoning objective directly to unlabeled corpora. The main objective is to create a scalable RL framework that enhances model reasoning without relying on human annotations, thereby overcoming the bottleneck of finite high-quality data. The methodology involves rewarding the policy for predicting subsequent text segments—using Autoregressive and Middle Segment Reasoning tasks—with a generative reward model assessing the semantic consistency between the predicted segment and the ground truth. Primary results demonstrate that applying RLPT to a Qwen3-4B-Base model yields absolute improvements of 8.1 on GPQA-Diamond and 6.6 Pass@1 on AIME24, with performance following a favorable scaling law. For AI practitioners, the principal implication is that RLPT offers a compute-driven method to enhance the reasoning capabilities of base models using existing pre-training data, providing a stronger foundation for subsequent fine-tuning stages like RLVR. |
| Do You Need Proprioceptive States in Visuomotor Policies? (Read more on [arXiv](https://arxiv.org/abs/2509.18644) or [HuggingFace](https://huggingface.co/papers/2509.18644))| Yushen Liang, Yufeng Liu, Di Zhang, Wenbo Lu, Juntu Zhao | This research demonstrates that removing proprioceptive state inputs from visuomotor policies significantly enhances their spatial generalization capabilities. The study's objective is to investigate if eliminating proprioceptive states can prevent imitation learning policies from overfitting to specific training trajectories. The key methodology involves implementing a "State-free Policy" that predicts actions in a relative end-effector (EEF) action space, conditioned solely on visual observations from dual wide-angle wrist cameras to ensure full task observation. The primary result shows a dramatic improvement in generalization, with the average success rate increasing from 0% to 85% in height generalization and from 6% to 64% in horizontal generalization. For AI practitioners, the principal implication is that visuomotor policies can achieve greater robustness and data efficiency by omitting proprioceptive state inputs, provided the system ensures comprehensive visual context and operates in a relative action space. |
| MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and
  Training Recipe (Read more on [arXiv](https://arxiv.org/abs/2509.18154) or [HuggingFace](https://huggingface.co/papers/2509.18154))| Wenshuo Ma, Fuwei Huang, Chongyi Wang, Zefan Wang, Tianyu Yu | This paper presents MiniCPM-V 4.5, an 8B parameter Multimodal Large Language Model (MLLM) optimized for high efficiency and performance via novel architectural, data, and training strategies. The main objective is to address the efficiency bottlenecks in MLLM training and inference to improve scalability and accessibility. The key methodology involves three components: a unified 3D-Resampler for compact video and image encoding, a unified learning paradigm for document understanding via dynamic visual corruption, and a hybrid reinforcement learning strategy for controllable short and long reasoning. On the VideoMME benchmark, MiniCPM-V 4.5 achieves state-of-the-art performance among models under 30B while using only 46.7% of the GPU memory and 8.7% of the inference time required by the Qwen2.5-VL 7B model. The principal implication for AI practitioners is that the 3D-Resampler architecture provides a highly efficient method for processing high-frame-rate video by significantly reducing the number of visual tokens, thus lowering computational costs for deploying capable video understanding systems. |
| MAPO: Mixed Advantage Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2509.18849) or [HuggingFace](https://huggingface.co/papers/2509.18849))| Xuankun Rong, Jian Liang, Yiyang Fang, Quan Zhang, Wenke Huang | MAPO is a policy optimization strategy that dynamically adjusts the advantage function in GRPO to improve foundation model reasoning by accounting for trajectory certainty. The paper's objective is to solve the "advantage reversion" and "advantage mirror" problems in Group Relative Policy Optimization (GRPO), where a fixed advantage formulation provides poor learning signals for samples with varying difficulty. The key methodology introduces "trajectory certainty" to dynamically reweight the advantage function, mixing a standard deviation-based z-score normalization for uncertain trajectories with a proposed mean-based Advantage Percent Deviation (APD) for high-certainty trajectories. On the Geo3K math reasoning benchmark, MAPO improved the Qwen2.5-VL-7B model's accuracy to 54.41, outperforming the baseline GRPO's 51.91. For AI practitioners, MAPO offers a hyperparameter-free modification to the GRPO advantage calculation that can yield more stable and accurate performance in RL-based post-training for complex reasoning tasks. |
| VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with
  Voxel-Aligned Prediction (Read more on [arXiv](https://arxiv.org/abs/2509.19297) or [HuggingFace](https://huggingface.co/papers/2509.19297))| Haoxiao Wang, Hengyu Liu, Zeyu Zhang, Yeqing Chen, Weijie Wang | VolSplat introduces a voxel-aligned paradigm for feed-forward 3D Gaussian Splatting that predicts Gaussians from a 3D voxel grid instead of individual 2D pixels. The main objective is to overcome the limitations of pixel-aligned methods, such as multi-view alignment errors, view-biased density distributions, and a rigid coupling of Gaussian density to input image resolution. The key methodology involves unprojecting 2D image features into a 3D voxel grid using predicted depth maps, refining this grid with a sparse 3D U-Net, and then directly predicting Gaussian parameters for each occupied voxel. The method achieves state-of-the-art results, attaining a PSNR of 31.30 on the RealEstate10K dataset, significantly outperforming the previous best pixel-aligned method's PSNR of 27.47. For AI practitioners, this voxel-aligned framework offers a more scalable and robust approach to feed-forward 3D reconstruction, enabling the creation of geometrically consistent and adaptively dense 3D representations from sparse views without being constrained by input image resolution. |
| Hyper-Bagel: A Unified Acceleration Framework for Multimodal
  Understanding and Generation (Read more on [arXiv](https://arxiv.org/abs/2509.18824) or [HuggingFace](https://huggingface.co/papers/2509.18824))| Jianbin Zheng, Huafeng Kuang, Manlin Zhang, Xin Xia, Yanzuo Lu | This paper proposes Hyper-Bagel, a unified framework to accelerate inference in multimodal models for both understanding and generation. The core objective is to reduce the computational overhead caused by iterative autoregressive decoding and diffusion denoising in models handling complex interleaved contexts. The framework uses a divide-and-conquer strategy, employing speculative decoding for next-token prediction and a multi-stage distillation process involving Classifier-Free Guidance (CFG) distillation and Distribution Matching Distillation via ODE (DMDO) for diffusion denoising. Key results demonstrate a greater than 2x speedup in understanding tasks and, for generation, a 16.67x speedup in text-to-image synthesis with a 6-NFE model that preserves the original model's output quality. For AI practitioners, this research provides a method to significantly reduce the inference latency and cost of large unified multimodal models, enabling their practical deployment in cost-sensitive or real-time applications without sacrificing performance. |
| Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model
  Self-Distillation (Read more on [arXiv](https://arxiv.org/abs/2509.19296) or [HuggingFace](https://huggingface.co/papers/2509.19296))| Yifeng Jiang, Jiahui Huang, Jiawei Ren, Tianchang Shen, Sherwin Bahmani | Lyra is a generative framework for feed-forward 3D and 4D scene reconstruction into an explicit 3D Gaussian Splatting (3DGS) representation from a single image or video. The research objective is to distill the implicit 3D knowledge from a pre-trained video diffusion model into an explicit 3DGS decoder, eliminating the need for real-world multi-view training datasets. This is achieved through a self-distillation framework where a 3DGS decoder (student), operating in the video model's latent space, is supervised by the RGB video outputs of a frozen, pre-trained video diffusion model (teacher). The model achieves state-of-the-art results, including a PSNR of 21.79 on the RealEstate10K dataset for single-image to 3D generation. The principal implication for AI practitioners is the ability to create explicit, interactive 3D/4D environments for simulation in domains like robotics and autonomous driving without requiring multi-view data capture or per-scene optimization. |
| What Characterizes Effective Reasoning? Revisiting Length, Review, and
  Structure of CoT (Read more on [arXiv](https://arxiv.org/abs/2509.19284) or [HuggingFace](https://huggingface.co/papers/2509.19284))| Anthony Hartshorn, Parag Jain, Cheng Zhang, Julia Kempe, Yunzhen Feng | This paper re-evaluates what characterizes effective Chain-of-Thought (CoT) reasoning, finding that structural quality, specifically the fraction of failed steps, is a more robust predictor of correctness than lexical properties like length or review. The research objective is to systematically determine if CoT length and review behaviors improve reasoning accuracy and to identify the underlying structural properties that drive performance across ten Large Reasoning Models. The methodology involves generating multiple CoT traces on math and scientific reasoning benchmarks, introducing a graph-based metric called the Failed-Step Fraction (FSF), and performing conditional correlation analyses alongside two causal interventions: test-time selection and controlled CoT editing. The primary result is that lower FSF is the most consistent and strongest predictor of correctness; in a test-time selection intervention, reranking candidate CoTs by FSF yielded accuracy gains of up to 10% on the AIME benchmark. The principal implication for AI practitioners is that structure-aware metrics like FSF offer a more effective mechanism for test-time selection and quality control than simple lexical heuristics, enabling more efficient use of computational resources by focusing on reasoning quality over quantity. |
| Large Language Models Discriminate Against Speakers of German Dialects (Read more on [arXiv](https://arxiv.org/abs/2509.13835) or [HuggingFace](https://huggingface.co/papers/2509.13835))| Katharina von der Wense, Anne Lauscher, Valentin Hofmann, Carolin Holtermann, Minh Duc Bui | This research demonstrates that large language models exhibit significant negative stereotypical biases against speakers of German dialects. The study investigates whether LLMs reproduce human societal stereotypes by assessing biases across traits like education level and personality, analyzing seven regional German dialects. Using an association task and a decision-making task, the methodology measures both "dialect naming bias" (explicit labels) and "dialect usage bias" (implicit textual cues). The primary results show that all evaluated LLMs exhibit significant biases; for example, in the association task, GPT-5 Mini achieved a dialect usage bias score of 1.0 for the "uneducated" trait, indicating a perfect stereotypical correlation. The principal implication for AI practitioners is that models can display explicit discriminatory behavior based on linguistic demographics, with bias being amplified by explicit dialect labels, which poses significant risks for fairness in real-world applications like personnel selection. |
| OpenGVL - Benchmarking Visual Temporal Progress for Data Curation (Read more on [arXiv](https://arxiv.org/abs/2509.17321) or [HuggingFace](https://huggingface.co/papers/2509.17321))| Viktor Petrenko, Igor Kulakov, Gracjan Góral, Emilia Wiśnios, Paweł Budzianowski | The paper introduces OpenGVL, an open-source benchmark for evaluating the ability of Vision-Language Models (VLMs) to predict temporal task progress in robotics and for automated data curation. The main objective is to benchmark open-source VLMs against proprietary models for this task and to provide a practical tool for assessing the quality of large-scale robotics datasets. The methodology involves prompting VLMs in zero-shot and two-shot settings to predict task completion percentages for shuffled image frames from robot trajectories, evaluating performance using the Value-Order Correlation (VOC) metric. The primary result shows that open-source models significantly underperform, achieving only approximately 70% of the performance of closed-source counterparts on these temporal reasoning tasks. For AI practitioners, OpenGVL provides a practical, automated framework to programmatically assess and filter large robotics datasets, enabling the identification of issues such as ambiguous task definitions, execution failures, and out-of-distribution samples before using the data for model training. |
| CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target
  for Better Flow Matching (Read more on [arXiv](https://arxiv.org/abs/2509.19300) or [HuggingFace](https://huggingface.co/papers/2509.19300))| Rui Qian, Jiasen Lu, Liangchen Song, Pengsheng Guo, Chen Chen | CAR-Flow introduces a lightweight, shift-only reparameterization technique that conditions the source and target distributions in flow-matching models to improve generative performance. The primary objective is to alleviate the dual burden on conditional flow-matching networks, which must simultaneously learn long-range mass transport and semantic conditioning, by explicitly aligning the distributions based on the condition. The key methodology is Condition-Aware Reparameterization (CAR-Flow), which applies learnable, condition-dependent additive shifts to the initial source and/or final target distributions, thereby shortening the required probability transport path for the main velocity network. The primary result is that on ImageNet-256, augmenting the SiT-XL/2 model with CAR-Flow reduces the FID score from 2.07 to 1.68 while introducing less than 0.6% additional parameters and demonstrating faster convergence. The principal implication for AI practitioners is that integrating CAR-Flow's simple, lightweight shift modules into existing flow-matching frameworks provides a practical and computationally inexpensive method to improve sample fidelity and training efficiency for large-scale conditional image generation. |
| HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel
  View Synthesis (Read more on [arXiv](https://arxiv.org/abs/2509.17083) or [HuggingFace](https://huggingface.co/papers/2509.17083))| Dan Xu, ZipW | HyRF introduces a hybrid representation for novel view synthesis that combines explicit Gaussians with grid-based neural fields to reduce model size while maintaining high rendering quality. The research objective is to mitigate the significant memory overhead of 3D Gaussian Splatting (3DGS) without compromising its real-time performance or visual fidelity. The key methodology involves decomposing the scene representation into (1) a compact set of explicit Gaussians storing only essential high-frequency parameters like position and diffuse color, and (2) decoupled grid-based neural fields that predict remaining geometric and view-dependent appearance properties. The primary result is a model size reduction of over 20x compared to 3DGS while achieving state-of-the-art rendering quality; for instance, on the Deep Blending dataset, HyRF achieves a 30.37 PSNR with a 34 MB model, surpassing 3DGS's 29.41 PSNR with a 676 MB model. For AI practitioners, this implies that high-quality, real-time 3D rendering systems based on Gaussian splatting can be deployed in memory-constrained environments, such as on-device applications, where the large footprint of standard 3DGS would be prohibitive. |
| Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal
  Gemini 2.5 Model for Remote Sensing Applications (Read more on [arXiv](https://arxiv.org/abs/2509.19087) or [HuggingFace](https://huggingface.co/papers/2509.19087))| Genady Beryozkin, Maxim Neumann, Dahun Kim, Yotam Gigi, Ganesh Mallya | This paper presents a training-free, zero-shot method for adapting generalist large multimodal models (LMMs) trained on RGB-only inputs to process and leverage multi-spectral remote sensing data. The main objective is to enable an RGB-trained model like Gemini 2.5 to understand and utilize novel multi-spectral sensor data for remote sensing tasks without any retraining or fine-tuning. The methodology transforms multi-spectral bands into several pseudo-color images (e.g., NDVI, NDWI) and provides them as input to the model alongside a detailed text prompt that describes how each image was generated from specific spectral bands and what physical properties it represents. On the BigEarthNet 19-class benchmark, this zero-shot approach improved the F1 score of Gemini 2.5 to 0.453, representing a +0.053 gain over the previous state-of-the-art zero-shot result. The principal implication for AI practitioners is that the capabilities of generalist LMMs can be extended to specialized, non-standard sensor modalities through input transformation and detailed prompt engineering, bypassing the need for expensive domain-specific model training. |
| VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via
  Travel Video Itinerary Reconstruction (Read more on [arXiv](https://arxiv.org/abs/2509.19002) or [HuggingFace](https://huggingface.co/papers/2509.19002))| So Fukuda, Ayako Sato, Lingfang Zhang, Eiki Murata, Hao Wang | This paper introduces VIR-Bench, a novel benchmark for evaluating the long-range geospatial-temporal understanding of Multimodal Large Language Models (MLLMs) through travel video itinerary reconstruction. The main objective is to assess MLLMs' capabilities on macro-scale scenarios involving multi-day, inter-city travel, addressing a gap left by existing micro-scale video benchmarks. The methodology involves a new dataset of 200 travel videos with manually annotated visiting order graphs and decomposes the evaluation into two zero-shot tasks: node prediction (identifying locations) and edge prediction (inferring temporal/spatial relationships). Results reveal that even the best proprietary model, Gemini-2.5-Pro, achieves only a 52.8% F1 score for Point of Interest (POI) node prediction and a 66.8% F1 for transition edge prediction, underscoring the task's difficulty. The primary implication for AI practitioners is that current MLLMs possess critical limitations in long-horizon temporal reasoning from video, with transition edge prediction being a major bottleneck, indicating that robust, video-based planning applications require significant architectural improvements or specialized fine-tuning. |
