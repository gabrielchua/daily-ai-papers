

## Papers for 2025-09-25

| Title | Authors | Summary |
|-------|---------|---------|
| Video models are zero-shot learners and reasoners (Read more on [arXiv](https://arxiv.org/abs/2509.20328) or [HuggingFace](https://huggingface.co/papers/2509.20328))| rgeirhos, kswersky, nmatares, yuxuanli, ThaddaeusWiedemer | This research demonstrates that the generative video model Veo 3 possesses emergent zero-shot capabilities across a wide range of vision tasks, from perception to reasoning. The study investigates if large generative video models are developing general-purpose vision understanding, akin to the evolution of Large Language Models in natural language processing. The methodology involved prompting the Veo 3 model with an initial image and text instructions for 62 qualitative and 7 quantitative tasks in a zero-shot setting, comparing its performance against its predecessor, Veo 2, and other models. Based on an analysis of 18,384 generated videos, Veo 3 showed substantial improvement over Veo 2, achieving a 78% pass@10 rate on 5x5 maze solving compared to Veo 2's 14%, and exhibited early forms of "chain-of-frames" visual reasoning. The principal implication for AI practitioners is that prompting large video models is an emerging paradigm for solving diverse computer vision problems, potentially reducing the need for task-specific training and underscoring the future importance of visual and textual prompt engineering. |
| SIM-CoT: Supervised Implicit Chain-of-Thought (Read more on [arXiv](https://arxiv.org/abs/2509.20317) or [HuggingFace](https://huggingface.co/papers/2509.20317))| Yuhang Cao, Xiaoyi Dong, Yuhang Zang, LiuXR, Wiselnn | SIM-CoT is a training module that stabilizes implicit Chain-of-Thought reasoning in LLMs by applying step-level supervision through an auxiliary decoder, improving performance without inference overhead. The research objective is to diagnose and resolve the "latent instability" issue, where increasing implicit reasoning tokens causes training collapse due to homogeneous latent representations, thereby closing the performance gap with explicit CoT methods. The key methodology involves using a temporary auxiliary decoder during training to align each implicit latent token with its corresponding explicit textual reasoning step, providing fine-grained supervision that is removed at inference. SIM-CoT significantly boosts baseline performance, improving upon the Coconut method by +8.2% on the GSM8k-Aug dataset for GPT-2 and surpassing the explicit CoT baseline by 2.1% with 2.3x greater token efficiency. For AI practitioners, SIM-CoT offers a practical, plug-and-play technique to build more stable, accurate, and token-efficient reasoning models while enabling interpretability of the latent steps for debugging. |
| Advancing Speech Understanding in Speech-Aware Language Models with GRPO (Read more on [arXiv](https://arxiv.org/abs/2509.16990) or [HuggingFace](https://huggingface.co/papers/2509.16990))| Avihu, rhoory, NimrodShabtay1986, hagaia, avishai-elmakies | This paper applies Group Relative Policy Optimization (GRPO) to enhance the performance of Speech-Aware Language Models (SALLMs) on open-ended speech understanding tasks. The research objective is to evaluate GRPO's effectiveness against supervised fine-tuning (SFT) for improving SALLM performance on Spoken Question Answering (SQA) and Automatic Speech Translation (AST). The methodology involves fine-tuning Granite Speech models (2B and 8B) using a GRPO variant where the reward is calculated via standard metrics like BLEU between generated and ground-truth text. On the CoVoST2 AST task with the 8B model, GRPO improved the BLEU score by 10.9% over SFT (35.08 vs 31.62), a scenario where SFT degraded performance relative to the base model. The principal implication for AI practitioners is that GRPO with a metric-based reward function serves as a highly effective fine-tuning method for generative SALLM tasks, particularly for larger models where SFT may not yield improvements. |
| EmbeddingGemma: Powerful and Lightweight Text Representations (Read more on [arXiv](https://arxiv.org/abs/2509.20354) or [HuggingFace](https://huggingface.co/papers/2509.20354))| Marksherwood, osanseviero, ssmoot, SindhuRaghuram97, hschechter | This paper introduces EmbeddingGemma, a 308M parameter open text embedding model that achieves state-of-the-art performance for lightweight models. The research objective was to develop a general-purpose embedding model that offers an exceptional performance-to-cost ratio, making it suitable for resource-constrained applications. The key methodology involves initializing the model from the encoder of a T5Gemma model, training with a combination of noise-contrastive estimation, a spread-out regularizer, and geometric embedding distillation from a larger teacher model, and finally, "souping" (averaging) checkpoints trained on varied data mixtures. A primary result is its state-of-the-art performance on the MTEB benchmark for models under 500M parameters, achieving a mean task score of 61.15 on MTEB(Multilingual, v2), which is comparable to models twice its size. The principal implication for AI practitioners is the availability of a compact, open-source model for high-performance text representation tasks that is robust to quantization (down to 4-bit) and embedding truncation, enabling efficient deployment in low-latency, high-throughput, and on-device systems. |
| LLMs4All: A Review on Large Language Models for Research and
  Applications in Academic Disciplines (Read more on [arXiv](https://arxiv.org/abs/2509.19580) or [HuggingFace](https://huggingface.co/papers/2509.19580))| Yanfang, lalor, Sweson, ZehongWang, mtybilly | This paper presents a comprehensive review of state-of-the-art Large Language Models (LLMs) and their applications, limitations, and performance across diverse academic disciplines. The primary objective is to survey the integration of LLMs into arts, letters, law, economics, business, science, and engineering, and to provide a structured taxonomy of their use cases and performance benchmarks. The methodology consists of a systematic literature review and synthesis of existing LLMs (e.g., GPT series, Claude 3, Llama 3), their architectural designs, and their performance on key evaluation benchmarks like MMLU, HumanEval, and MATH. The review's synthesis of benchmark data reveals that no single LLM dominates all domains; for instance, while Claude 3.5 Sonnet achieves a top score of 93.7% on the HumanEval coding benchmark, reasoning-specific models like DeepSeek R1 excel in quantitative tasks, scoring 97.3% on the MATH benchmark. The principal implication for AI practitioners is the necessity of task-specific model selection, as the paper demonstrates that model performance varies significantly across reasoning, coding, and general-purpose tasks, highlighting a trade-off between specialized capabilities and broad applicability. |
| EditVerse: Unifying Image and Video Editing and Generation with
  In-Context Learning (Read more on [arXiv](https://arxiv.org/abs/2509.20360) or [HuggingFace](https://huggingface.co/papers/2509.20360))| Tianyu Wang, sooyek, Shaldon, CaiYuanhao, juxuan27 | EditVerse is a unified transformer framework that performs instruction-guided image and video editing and generation by representing all modalities as an interleaved token sequence to enable in-context learning. The main objective is to develop a single, scalable model for diverse image and video editing and generation tasks, addressing the challenges of architectural limitations and video data scarcity. The key methodology involves a transformer architecture with full self-attention that processes a unified sequence of interleaved text and vision tokens, utilizing a four-dimensional Rotary Positional Embedding (RoPE) to encode sequential, temporal, and spatial information. The primary result shows that on the proposed EditVerseBench, the model achieves a VLM editing quality score of 7.65, outperforming open-source models like InsV2V (5.21) and demonstrating emergent abilities like performing tasks not seen in the video editing training data. The principal implication for AI practitioners is that a unified, self-attention-based architecture enables effective knowledge transfer from data-rich domains (image editing) to data-scarce domains (video editing), providing a viable strategy to overcome data limitations and build more generalist multimodal foundation models. |
| PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video
  Generation (Read more on [arXiv](https://arxiv.org/abs/2509.20358) or [HuggingFace](https://huggingface.co/papers/2509.20358))| Yiming Huang, thomagram, frankzydou, MorPhLingXD, chenwang | PhysCtrl introduces a diffusion-based generative physics network to produce controllable, physically-plausible videos by generating 3D point trajectories conditioned on material properties and external forces. The main research objective is to develop a framework for physics-grounded image-to-video generation that allows explicit control over physical parameters and applied forces, addressing the common lack of physical plausibility in data-driven video models. The key methodology is a diffusion transformer model trained on a 550K synthetic animation dataset to learn physical dynamics across multiple materials. The model represents dynamics as 3D point trajectories and incorporates a novel spatiotemporal attention block and a physics-based loss derived from the Material Point Method (MPM) deformation gradient update to enforce physical constraints. The primary result is that the trajectory generation model significantly outperforms existing methods on generative dynamics tasks, achieving a volume Intersection over Union (vIoU) of 77.03% on an elastic object test set, substantially higher than the 53.78% achieved by the next-best baseline. The principal implication for AI practitioners is that this framework provides a scalable method for injecting strong, controllable physics priors into generative video pipelines by using 3D point trajectories as an intermediate control signal, enabling the creation of high-fidelity, physically plausible animations without direct reliance on computationally expensive online physics simulators. |
| Logics-Parsing Technical Report (Read more on [arXiv](https://arxiv.org/abs/2509.19760) or [HuggingFace](https://huggingface.co/papers/2509.19760))| Fan Yang, Shuzhao Li, Xiangyang Chen, ZjuCv, xiuwenzhu | This paper introduces Logics-Parsing, an end-to-end Large Vision-Language Model augmented with reinforcement learning for advanced document parsing. The primary objective is to overcome the limitations of existing LVLMs in handling documents with complex layouts and non-linear reading orders, such as multi-column newspapers. The methodology employs a two-stage "SFT-then-RL" training strategy, where a base model is first fine-tuned on a large, diverse dataset (>300K images) and then optimized using Layout-Centric Reinforcement Learning (LC-RL) with a multi-component reward function that directly evaluates text, layout, and reading order. On the newly proposed LogicsParsingBench benchmark, the model achieves a state-of-the-art aggregate edit distance of 0.124 on English documents, outperforming existing pipeline, expert, and general LVLM-based methods. For AI practitioners, this work provides an effective framework demonstrating that augmenting standard SFT with a targeted RL stage using explicit structural rewards is critical for building document AI systems that can accurately process structurally complex content. |
| Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal
  Understanding and Generation (Read more on [arXiv](https://arxiv.org/abs/2509.19244) or [HuggingFace](https://huggingface.co/papers/2509.19244))| Zhe Lin, xternalz, kl3141, JoshuaGu, jacklishufan | Lavida-O is a unified Masked Diffusion Model (MDM) that integrates high-resolution image generation, editing, object grounding, and image understanding within a single framework. The objective is to develop a single MDM that overcomes the limitations of prior multimodal MDMs by effectively combining image understanding and generation capabilities to achieve state-of-the-art performance on complex, interleaved tasks. The model employs an Elastic Mixture-of-Transformers (Elastic-MoT) architecture, which couples a lightweight (2.4B) generation branch with a larger (8B) understanding branch for parameter efficiency, and introduces explicit planning and self-reflection mechanisms to leverage its understanding capabilities to improve generation quality. Lavida-O achieves state-of-the-art performance across multiple benchmarks; for text-to-image generation, it obtains a FID score of 6.68 on the MJHQ-30k dataset, significantly outperforming prior unified MDMs like MMaDa (32.85). For AI practitioners, the Elastic-MoT architecture presents a parameter-efficient method for augmenting large, pretrained understanding models with generative capabilities, enabling the development of unified systems that can perform complex tasks like instruction-based editing through internal reasoning and self-correction, reducing the need for separate specialist models. |
| On the Use of Agentic Coding: An Empirical Study of Pull Requests on
  GitHub (Read more on [arXiv](https://arxiv.org/abs/2509.14745) or [HuggingFace](https://huggingface.co/papers/2509.14745))| Hajimu Iida, Brittany Reid, Yutaro Kashiwa, Miku Watanabe, hao-li | This empirical study analyzes 567 GitHub pull requests (PRs) from the agentic coding tool Claude Code to assess their integration characteristics and required human oversight compared to human-generated PRs. The main objective is to investigate the differences between agent-assisted and human-written PRs in terms of purpose, acceptance rates, rejection reasons, and the nature of subsequent revisions. The methodology involves a comparative analysis of 567 agent-generated PRs and a matched set of 567 human-generated PRs using manual classification and statistical analysis of repository data. Results show that while 83.8% of agent-assisted PRs are accepted, 45.1% of those merged require revisions, with bug fixes being the most common revision type at 45.1%. The principal implication for AI practitioners is that agent-generated code requires diligent human oversight to correct functional bugs, align with project-specific conventions, and ensure documentation consistency, making it crucial to provide agents with explicit contextual guidelines to minimize integration friction. |
