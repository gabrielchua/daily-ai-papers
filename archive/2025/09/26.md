

## Papers for 2025-09-26

| Title | Authors | Summary |
|-------|---------|---------|
| SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines (Read more on [arXiv](https://arxiv.org/abs/2509.21320) or [HuggingFace](https://huggingface.co/papers/2509.21320))| Jiabei Xiao, Han Deng, Chen Tang, Uanu, Cohesion98 | The paper introduces SciReasoner, a scientific reasoning foundation model that aligns natural language with heterogeneous scientific data representations across multiple disciplines. The research objective is to create a single, unified model that can perform a wide range of scientific tasks (103 in total), from property prediction to sequence design, while generating explicit and verifiable reasoning chains. The methodology involves pretraining on a 206B-token scientific corpus, followed by supervised fine-tuning and a novel reinforcement learning stage that employs "Adaptive Scientific Reasoning" to selectively apply chain-of-thought to complex tasks, along with task-grouped reward shaping to stabilize training. The SciReasoner-8B model achieves state-of-the-art performance on 54 tasks, for instance, attaining a 56.63% Top1 match accuracy on SMILES-to-IUPAC molecular translation, significantly outperforming specialist models (29.00%). For AI practitioners, the principal implication is the ability to use a single, powerful backbone for diverse scientific AI applications, reducing the fragmentation of specialist models and improving cross-domain generalization for complex workflows. |
| VCRL: Variance-based Curriculum Reinforcement Learning for Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.19803) or [HuggingFace](https://huggingface.co/papers/2509.19803))| Yuewei Zhang, Guofeng Quan, Wenfeng Feng, Chuzhan, Nothing2Say | VCRL is a curriculum reinforcement learning framework that uses the variance of rollout group rewards to dynamically select appropriately difficult training samples for improving LLM reasoning. The primary objective is to enhance the efficiency and performance of rollout-based reinforcement learning by creating a curriculum that adapts to the model's current abilities, unlike methods that treat all samples equally. The core methodology involves calculating the normalized variance of rewards from multiple generation rollouts for each sample, using this variance as a proxy for difficulty to filter the training batch to include only high-variance samples, and employing a memory bank with replay learning to maintain batch quality. Experiments show VCRL significantly outperforms baselines; on the Qwen3-8B-Base model across five math benchmarks, it achieved an average score of 57.76, a 4.67-point improvement over the strongest baseline, GSPO. For AI practitioners, this provides an efficient and dynamic curriculum learning strategy for RL fine-tuning that focuses computation on the most informative samples—those at the frontier of the model's capabilities—thereby improving training stability and final performance. |
| MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and
  Open Resources (Read more on [arXiv](https://arxiv.org/abs/2509.21268) or [HuggingFace](https://huggingface.co/papers/2509.21268))| Jing Wang, Sicong Leng, Swrooy, 26hzhang, jxjessieli | This paper introduces MMR1, a framework that enhances multimodal reasoning by using a novel Variance-Aware Sampling (VAS) strategy to stabilize reinforcement learning, complemented by the release of large-scale open data and models. The primary objective is to mitigate the gradient vanishing problem in Group Relative Policy Optimization (GRPO) for multimodal models, which occurs when reward variance is low, thereby stabilizing training and improving reasoning performance. The core methodology is Variance-Aware Sampling (VAS), a dynamic data selection strategy guided by a Variance Promotion Score (VPS) that combines outcome variance and reasoning trajectory diversity to maintain an informative reward signal. The proposed 7B parameter MMR1 model achieves state-of-the-art performance, attaining an average score of 58.4 across five mathematical and logical reasoning benchmarks, outperforming comparable reasoning-oriented models. For AI practitioners, the principal implication is that implementing VAS can stabilize RL training and improve model performance by dynamically selecting data that maximizes reward variance, thus ensuring more consistent policy gradients without modifying the core RL algorithm. |
| Tree Search for LLM Agent Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.21240) or [HuggingFace](https://huggingface.co/papers/2509.21240))| Xiangxiang Chu, Guanhua Chen, Yong Wang, Ziyu Ma, Yux1ang | This paper introduces Tree-GRPO, a tree-search-based reinforcement learning framework that improves the sample efficiency and performance of multi-turn LLM agents. The research aims to overcome the high rollout costs and sparse supervision signals of conventional chain-based RL methods for agent training. The key methodology, Tree-based Group Relative Policy Optimization (Tree-GRPO), replaces independent rollouts with a tree search where nodes represent complete agent interaction steps, allowing for shared prefixes and estimating grouped relative advantages at intra-tree and inter-tree levels to create process supervision from outcome rewards. Experimental results demonstrate that with a highly constrained budget (equivalent to two rollouts), Tree-GRPO achieves a 112% relative performance improvement over the chain-based baseline on multi-hop QA tasks. For AI practitioners, this method provides a way to train more capable and complex LLM agents with significantly lower token and tool-call budgets, making agentic RL more efficient and cost-effective. |
| Seedream 4.0: Toward Next-generation Multimodal Image Generation (Read more on [arXiv](https://arxiv.org/abs/2509.20427) or [HuggingFace](https://huggingface.co/papers/2509.20427))| Yunpeng Chen, Team Seedream, Cakeyan, wuwx, wujie10 | Seedream 4.0 is an efficient, high-performance multimodal image generation system unifying text-to-image (T2I) synthesis, image editing, and multi-image composition into a single framework. The objective was to develop a scalable model capable of fast, high-resolution (1K-4K) image generation and complex multimodal editing. The methodology involves a highly efficient diffusion transformer (DiT) with a powerful VAE for reduced image tokenization, joint post-training with a Vision Language Model (VLM) for T2I and editing tasks, and inference acceleration via adversarial distillation, quantization, and speculative decoding. The system achieves state-of-the-art results, ranking first on the Artificial Analysis Arena for both T2I (Elo score: 1,220) and image editing (Elo score: 1,198) as of September 18, 2025. For AI practitioners, this provides a unified, production-ready tool that integrates high-speed, high-resolution generation with advanced editing capabilities, extending its use to professional applications like generating charts, formulas, and other knowledge-based content. |
| Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D
  Assets (Read more on [arXiv](https://arxiv.org/abs/2509.21245) or [HuggingFace](https://huggingface.co/papers/2509.21245))| Bowen Zhang, Team Hunyuan3D, SeanYoungxh, AuWang, Huiwenshi | Hunyuan3D-Omni is a unified framework that enhances controllable 3D asset generation by integrating multiple geometric conditioning signals into a single diffusion model. The primary objective is to improve geometric accuracy and fine-grained control in image-to-3D generation by developing a single model that accepts point clouds, voxels, bounding boxes, and skeletal poses as conditioning priors. The methodology extends the Hunyuan3D 2.1 architecture by introducing a lightweight "Unified Control Encoder" which processes all control signals as point clouds and concatenates their features with image embeddings before feeding them into a Diffusion Transformer (DiT). Qualitative results demonstrate that the model accurately aligns generated meshes with conditioning signals like skeletons and resolves single-view ambiguities using point clouds; however, the paper does not provide specific quantitative performance metrics. For AI practitioners, the principal implication is the ability to add multi-modal, fine-grained geometric controls to existing image-to-3D pipelines via a single lightweight encoder, increasing robustness and enabling precise asset creation without training separate models for each control type. |
| AutoIntent: AutoML for Text Classification (Read more on [arXiv](https://arxiv.org/abs/2509.21138) or [HuggingFace](https://huggingface.co/papers/2509.21138))| Denis Kuznetsov, Darina Rustamova, Samoed, voorhs | AutoIntent is a modular, end-to-end AutoML framework for text classification that automates embedding selection, classifier optimization, and threshold tuning. The primary objective is to create a comprehensive AutoML solution for intent classification that supports multi-label and out-of-scope (OOS) detection, features often lacking in existing tools. The methodology is a sequential, three-stage optimization pipeline (embedding, scoring, decision) that uses Optuna for hierarchical hyperparameter tuning across a diverse set of transformer-based and classical models. In experiments, AutoIntent achieved an out-of-scope F1-measure of 76.79 on the CLINC150 dataset, significantly outperforming AutoGluon's 48.53. The principal implication for AI practitioners is the availability of a tool that automates the construction of robust intent classification systems, particularly for conversational AI applications requiring reliable OOS handling. |
| TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them (Read more on [arXiv](https://arxiv.org/abs/2509.21117) or [HuggingFace](https://huggingface.co/papers/2509.21117))| Zhuohao Yu, Xuanwang Zhang, Tingyuan Zhu, Yunze Song, Yidong Wang | TrustJudge is a probabilistic framework that mitigates fundamental inconsistencies in LLM-as-a-judge evaluations by preserving information entropy in scoring and resolving ambiguities in pairwise comparisons. The paper's objective is to identify, formalize, and alleviate two key inconsistencies in LLM-as-a-judge evaluations: Score-Comparison Inconsistency (where single-score ratings conflict with pairwise preferences) and Pairwise Transitivity Inconsistency (circular or contradictory preferences). The key methodology involves two components: 1) distribution-sensitive scoring, which calculates a continuous expected score from a fine-grained probability distribution over ratings to prevent information loss, and 2) likelihood-aware aggregation, which resolves transitivity violations by aggregating bidirectional preference probabilities or using response perplexity to break ties. When using Llama-3.1-70B-Instruct as the judge, TrustJudge reduced Score-Comparison Inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise Transitivity Inconsistency by 10.82% (from 15.22% to 4.40%), while simultaneously improving evaluation accuracy. For AI practitioners, TrustJudge provides a training-free, model-agnostic method to significantly improve the reliability of automated evaluations and generate more consistent preference data for reward modeling and alignment techniques like DPO, without requiring additional human annotation. |
| CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy
  Optimization in Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.20712) or [HuggingFace](https://huggingface.co/papers/2509.20712))| Wenping Hu, Yuntao Li, Minxuan Lv, Leiyu Pan, Zhenpeng Su | CE-GPPO is a novel reinforcement learning algorithm that controls policy entropy by reintroducing and scaling gradients from tokens typically discarded by PPO's clipping mechanism. The main objective is to mitigate entropy instability—either collapse or explosion—during the reinforcement learning fine-tuning of large language models by analyzing and managing the gradients from low-probability tokens that fall outside the standard PPO clipping interval. The key methodology is an algorithm named Controlling Entropy via Gradient-Preserving Policy Optimization (CE-GPPO), which uses a stop-gradient operation to incorporate gradients from clipped tokens and introduces tunable coefficients (β1 and β2) to scale their magnitude, thereby enabling explicit control over the exploration-exploitation balance. The primary result shows that on the DeepSeek-R1-Distill-Qwen-7B model, CE-GPPO achieved a 67.5% average score across five mathematical reasoning benchmarks, outperforming the strong DAPO baseline's score of 64.5%. For AI practitioners, CE-GPPO provides a more stable and effective alternative to PPO-style algorithms for fine-tuning LLMs, as it prevents both premature entropy collapse and excessive exploration, leading to consistently better performance on complex reasoning tasks. |
| Does FLUX Already Know How to Perform Physically Plausible Image
  Composition? (Read more on [arXiv](https://arxiv.org/abs/2509.21278) or [HuggingFace](https://huggingface.co/papers/2509.21278))| Chen Zhao, Shaocong Zhang, Zhuming Lian, Edennnnn, Shilin-LU | This paper introduces SHINE, a training-free framework that enables modern diffusion models like FLUX to perform high-fidelity, physically plausible image composition. The research objective is to unlock the intrinsic physical and resolution priors of pretrained text-to-image models for composition tasks without resorting to fine-tuning or brittle inversion techniques. The core methodology combines three components: Manifold-Steered Anchor (MSA) loss to guide latents using pretrained customization adapters for subject fidelity, Degradation-Suppression Guidance (DSG) to steer sampling away from low-quality outputs by manipulating internal attention queries, and Adaptive Background Blending (ABB) for seamless integration. On the DreamEditBench benchmark, the proposed method achieves state-of-the-art performance, with its LoRA variant obtaining a top ImageReward score of 0.5906, surpassing all training-based and training-free baselines. For AI practitioners, this work demonstrates that complex generative capabilities can be elicited from foundation models via inference-time optimization and guidance, providing a computationally efficient alternative to dataset curation and model retraining for specialized applications. |
| CHARM: Control-point-based 3D Anime Hairstyle Auto-Regressive Modeling (Read more on [arXiv](https://arxiv.org/abs/2509.21114) or [HuggingFace](https://huggingface.co/papers/2509.21114))| Yushi Bai, Jingwen Ye, Wang Zhao, Yanning Zhou, Yuze He | This paper presents CHARM, a control-point-based parametric representation and autoregressive transformer framework for generating 3D anime hairstyles. The main objective is to develop a compact, invertible, and structured representation for stylized anime hair to enable scalable, learning-based generation from inputs like point clouds or images. The methodology treats hairstyles as a "hair language" by parameterizing each hair card as a sequence of control points, each defined by five geometric parameters (3D position, width, thickness), and then uses an autoregressive transformer to generate these sequences. CHARM achieves state-of-the-art performance, outperforming other 3D mesh generation methods with a CLIP similarity of 0.9258 and demonstrating over 98% token compression compared to original mesh representations. For AI practitioners, this framework offers a scalable and efficient method for generating editable, high-fidelity 3D anime hair assets, providing a practical solution for automating a labor-intensive component of digital character creation. |
| Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web
  Reconnaissance, Tool Generation, and Task Execution (Read more on [arXiv](https://arxiv.org/abs/2509.21072) or [HuggingFace](https://huggingface.co/papers/2509.21072))| Jinjie Gu, Chenyi Zhuang, Zhiwei Wang, Kaiwen He | This paper presents Recon-Act, a self-evolving multi-agent system that improves browser task automation by using a reconnaissance-action paradigm to dynamically generate tools from execution trajectories. Its primary objective is to reduce excessive trial-and-error in long-horizon web tasks by enabling the system to learn from its own execution failures to generate specialized tools for unfamiliar websites. The methodology is a dual-team architecture where a "Reconnaissance Team" analyzes task trajectories to create "generalized tools" (hints or code), and an "Action Team" executes tasks using these tools, establishing a closed-loop training pipeline currently implemented with human-in-the-loop for analysis and tool management. Recon-Act achieves a new state-of-the-art overall success rate of 36.48% on the VisualWebArena benchmark, outperforming the previous best agent's score of 33.74%. The principal implication for AI practitioners is that architecting agents with a distinct reconnaissance phase to analyze failures and dynamically generate tools offers a potent strategy for enhancing agent adaptability and solvability in complex, information-dense environments. |
| V-GameGym: Visual Game Generation for Code Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.20136) or [HuggingFace](https://huggingface.co/papers/2509.20136))| Shawn Guo, Lingzheng Chai, Renshuai Tao, Jack Yang, Wei Zhang | The paper introduces V-GameGym, a comprehensive benchmark for evaluating the visual game generation capabilities of code large language models beyond simple code execution. The primary objective is to assess code LLMs on multimodal game development tasks by evaluating not only code correctness but also game-specific metrics like playability, visual aesthetics, and dynamic interaction. The methodology involves a clustering-based curation of 2,219 Pygame samples and an automated evaluation pipeline where an LLM-as-Judge assesses generated code, screenshots, and gameplay videos in a sandboxed UI environment. The evaluation of 70 models reveals a significant capability imbalance, with models performing strongly on code generation (most scores over 70) but poorly on visual and video assessments (most scores under 25), leading to a top final score of 45.0 for GPT-5. For AI practitioners, this indicates that while LLMs excel at generating syntactically correct code, their practical application in game development is limited by a critical deficit in generating visually coherent and dynamically playable elements, highlighting a key area for future multimodal model development. |
| Interactive Recommendation Agent with Active User Commands (Read more on [arXiv](https://arxiv.org/abs/2509.21317) or [HuggingFace](https://huggingface.co/papers/2509.21317))| Xueyang Feng, Fei Sun, Xunke Xi, Yujie Luo, TangJiakai5704 | The paper introduces RecBot, a dual-agent framework enabling interactive recommendation through natural language commands within mainstream feeds. The primary objective is to overcome the limitations of passive feedback mechanisms by allowing users to explicitly control recommendation policies in real-time. RecBot employs a Parser Agent to convert user commands into structured preferences and a Planner Agent that orchestrates tool chains for on-the-fly policy adjustment, with the system optimized via simulation-augmented knowledge distillation for deployment. In a three-month online A/B test, RecBot achieved a 1.40% increase in Gross Merchandise Volume (GMV) and a 0.71% reduction in Negative Feedback Frequency compared to the baseline. For AI practitioners, the principal implication is that this dual-agent architecture provides a validated, deployable framework for integrating large language models into recommender systems to enhance user satisfaction and business outcomes through direct, command-based interaction. |
| BESPOKE: Benchmark for Search-Augmented Large Language Model
  Personalization via Diagnostic Feedback (Read more on [arXiv](https://arxiv.org/abs/2509.21106) or [HuggingFace](https://huggingface.co/papers/2509.21106))| Dongha Lee, Kwangwook Seo, Sangam Lee, hyunseo00 | This paper introduces BESPOKE, a realistic benchmark for evaluating personalization in search-augmented LLMs using long-term human histories and diagnostic feedback. The primary objective is to systematically evaluate and diagnose the personalization capabilities of these models by capturing how the same query reflects different intents across users. The methodology involves collecting 2,870 authentic chat and search histories over three weeks from 30 annotators, who then provide queries, detailed information needs, and fine-grained judgments (scores and feedback) on model responses across four criteria. Results show that using a query-aware, selective history profile improves personalization, with the best model achieving an average score of 62.48, and the proposed LLM-based evaluator demonstrates strong human alignment with a 0.853 Pearson correlation. The principal implication for AI practitioners is that effective personalization hinges on sophisticated context construction (query-aware, selective history) and high-quality information retrieval, as both are shown to be critical bottlenecks. |
| Thinking Augmented Pre-training (Read more on [arXiv](https://arxiv.org/abs/2509.20186) or [HuggingFace](https://huggingface.co/papers/2509.20186))| Furu Wei, Li Dong, Shaohan Huang, Nan Yang, Liang Wang | Thinking Augmented Pre-training (TPT) is a data engineering method that improves LLM data efficiency by augmenting text with automatically generated step-by-step thinking trajectories. To improve the learnability of complex tokens, TPT uses an off-the-shelf LLM to generate an explanatory "thought process" for a given document, which is then concatenated with the original text for standard next-token prediction training. The approach achieves a 3x improvement in data efficiency; an 8B model pre-trained with TPT on 100B tokens scored 50.1% on GSM8k, substantially outperforming a vanilla baseline's 19.2% and matching a model trained on 150x more data (15T tokens). The principal implication for AI practitioners is that this offline data transformation can be scalably applied to existing pre-training or mid-training corpora to significantly boost model reasoning performance without needing new source data or altering the training objective. |
| Residual Off-Policy RL for Finetuning Behavior Cloning Policies (Read more on [arXiv](https://arxiv.org/abs/2509.19301) or [HuggingFace](https://huggingface.co/papers/2509.19301))| Pieter Abbeel, Guanya Shi, Rocky Duan, Zhenyu Jiang, Lars Ankile | This paper presents ResFiT, a sample-efficient residual off-policy reinforcement learning (RL) framework to fine-tune pre-trained behavior cloning (BC) policies. The primary objective is to develop a practical method for improving large, action-chunking BC policies directly on high-degree-of-freedom (DoF) robots using online RL with only sparse binary rewards. The methodology involves freezing the pre-trained BC policy and using a highly optimized off-policy RL algorithm to train a lightweight network that learns per-step additive residual corrections to the base policy's actions. Key results demonstrate that on a real-world 29-DoF bimanual humanoid robot, ResFiT boosted the success rate for a package handover task from 23% to 64% with approximately 76 minutes of online interaction. For AI practitioners, this residual approach provides a practical and stable pathway to deploy RL to refine complex visuomotor policies on real-world hardware, as it is agnostic to the base policy architecture and avoids the challenges of directly fine-tuning large models. |
| SD3.5-Flash: Distribution-Guided Distillation of Generative Flows (Read more on [arXiv](https://arxiv.org/abs/2509.21318) or [HuggingFace](https://huggingface.co/papers/2509.21318))| Yi-Zhe Song, Reshinth Adithyan, Jim Scott, Rahim Entezari, Hmrishav | SD3.5-Flash is a few-step distillation framework for rectified flow models that enables high-quality, rapid image generation on consumer devices through distribution-guided training and pipeline optimizations. The main objective is to make computationally prohibitive, high-fidelity generative models efficient enough for practical deployment on accessible consumer hardware like mobile phones and desktop computers. The key methodology involves distilling a multi-step teacher model using two primary innovations: "timestep sharing" to stabilize gradients by leveraging student trajectory samples instead of re-noising endpoints, and "split-timestep fine-tuning" to improve prompt alignment by temporarily expanding model capacity. The primary result is that the 4-step distilled model offers up to an 18x speed-up compared to its teacher model while consistently outperforming existing few-step methods in large-scale user studies on image quality. The principal implication for AI practitioners is the ability to deploy state-of-the-art, high-quality generative AI models directly on resource-constrained edge devices and consumer-grade hardware, reducing reliance on datacenter infrastructure and enabling on-device applications. |
| Quantized Visual Geometry Grounded Transformer (Read more on [arXiv](https://arxiv.org/abs/2509.21302) or [HuggingFace](https://huggingface.co/papers/2509.21302))| Yuqi Li, Chuanguang Yang, Mingqiang Wu, Haotong Qin, Weilun Feng | This paper introduces QuantVGGT, the first post-training quantization (PTQ) framework specifically for billion-scale Visual Geometry Grounded Transformers (VGGTs). The main objective is to quantize VGGTs to low bit-widths for efficient deployment by addressing challenges from heavy-tailed activation distributions and unstable calibration data inherent in multi-view 3D models. The key methodology combines Dual-Smoothed Fine-Grained Quantization (DSFQ), which uses Hadamard rotation and channel smoothing to normalize value distributions, with Noise-Filtered Diverse Sampling (NFDS), which filters statistical outliers and builds a representative calibration set using frame-aware clustering. On the Co3Dv2 camera pose estimation benchmark, the 4-bit QuantVGGT achieves an AUC@30 of 88.2, maintaining over 98% of its full-precision counterpart's accuracy while delivering a 2.5x inference speedup and a 3.7x memory reduction. For AI practitioners, this framework enables the deployment of large-scale 3D vision transformers on resource-constrained hardware by drastically reducing their computational and memory footprint with negligible performance loss. |
| SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and
  Self-Reflective Agent (Read more on [arXiv](https://arxiv.org/abs/2509.20414) or [HuggingFace](https://huggingface.co/papers/2509.20414))| Siyuan Huang, Shujie Zhang, Baoxiong Jia, Yandan Yang | The paper introduces SCENEWEAVER, a reflective agentic framework that unifies diverse 3D scene synthesis methods through tool-based iterative refinement for realistic, instruction-aligned scene generation. The primary objective is to develop a general-purpose 3D environment generation system that addresses the combined requirements of visual realism, physical plausibility, functional diversity, and precise controllability via complex user instructions, which existing single-paradigm methods fail to meet. SCENEWEAVER employs an LLM-based planner within a closed-loop reason-act-reflect cycle; the agent dynamically selects from a standardized suite of synthesis tools, iteratively refines the scene based on self-evaluated physical and semantic feedback, and enforces physical constraints using a physics-aware executor. Primary results demonstrate superior performance in open-vocabulary generation tasks, where SCENEWEAVER achieves an average object count of 36.5 across eight room types while maintaining zero physical errors (collisions and boundary violations), outperforming all baseline methods on these combined metrics. The principal implication for AI practitioners is that a modular, self-reflective agentic framework provides a scalable and extensible paradigm for orchestrating diverse, specialized tools to solve complex, multi-constraint generation tasks, offering a robust method for creating high-fidelity, controllable environments for training embodied agents. |
| Understanding the Thinking Process of Reasoning Models: A Perspective
  from Schoenfeld's Episode Theory (Read more on [arXiv](https://arxiv.org/abs/2509.14662) or [HuggingFace](https://huggingface.co/papers/2509.14662))| Yanbin Fu, Hong Jiao, Chenrui Fan, Nan Zhang, Ming Li | This research applies Schoenfeld's Episode Theory, a cognitive framework for human problem-solving, to analyze and structure the reasoning processes of Large Reasoning Models (LRMs) on mathematical tasks. The primary objective is to develop a principled analytical framework to understand how LRMs organize their thought processes by annotating their reasoning traces with cognitive labels. The key methodology involved manually annotating 3,087 sentences from DeepSeek-R1's solutions to SAT math problems using a hierarchical scheme of seven cognitive categories (e.g., Plan, Implement, Verify), creating the first public benchmark for this task. The primary result shows that an automated annotation method using GPT-4.1 with a detailed guidebook achieves a sentence-level accuracy of 0.805 and a Cohen's kappa of 0.764 on a test subset, demonstrating the feasibility of scalable analysis. For AI practitioners, this work provides a reusable protocol and annotated corpus that enables the fine-grained analysis of machine reasoning, which can be leveraged to build more transparent and controllable AI systems. |
| ScaleDiff: Scaling Difficult Problems for Advanced Mathematical
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.21070) or [HuggingFace](https://huggingface.co/papers/2509.21070))| Yu Li, Xin Gao, Honglin Lin, Zhuoshi Pan, Qizhi Pei | The paper presents ScaleDiff, a pipeline for automatically generating large-scale, difficult mathematical problems to enhance the reasoning capabilities of Large Reasoning Models (LRMs). The objective is to efficiently scale the creation of challenging training data by first identifying difficult problems with a single forward pass using an adaptive thinking model. The core methodology involves training a specialized generator (DiffGen-8B) on these identified problems to create new ones at scale, followed by solution distillation from a teacher model (Qwen3-8B) and filtering to produce the final ScaleDiff-Math dataset. Fine-tuning a Qwen2.5-Math-7B-Instruct model on this dataset yields a 65.9% average accuracy across five benchmarks, representing a relative performance increase of 11.3% over the original dataset. For AI practitioners, the principal implication is that this pipeline provides a cost-effective method to transfer advanced reasoning from a moderately-sized teacher model to a student model, reducing the reliance on larger, more expensive teacher models for data synthesis. |
| Behind RoPE: How Does Causal Mask Encode Positional Information? (Read more on [arXiv](https://arxiv.org/abs/2509.21042) or [HuggingFace](https://huggingface.co/papers/2509.21042))| Yeyun Gong, Lei Ji, Zhenghao Lin, Junu Kim, lx865712528 | This paper proves that the causal mask in Transformer decoders inherently encodes positional information and shows its interaction with RoPE creates non-relative attention patterns in modern LLMs. The research objective is to demonstrate how the causal mask, independent of learnable parameters or explicit positional encodings, induces position-dependent attention patterns favoring nearby tokens, and to analyze its interaction with RoPE. The methodology combines theoretical proof on a simplified, parameter-free Transformer layer with empirical simulations and analysis of attention patterns in a 1.5B parameter model trained without explicit positional encoding, as well as in Llama-3, Phi-4, and Qwen3-8B. The primary results show that the causal mask alone induces attention scores that strictly increase for closer keys and that its interaction with RoPE creates a non-relative bias; in modern LLMs, this bias pattern was observed to have a non-negligible magnitude on a [-1, 1] scale. For AI practitioners, the key implication is that the causal mask is an active source of positional information that biases explicit encodings like RoPE, a joint effect that should be considered when analyzing model behavior, performance, and length generalization. |
| MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for
  Video Temporal Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.21113) or [HuggingFace](https://huggingface.co/papers/2509.21113))| Junyan Zhang, Yibo Yan, Jungang Li, Sicheng Tao, EasonFan | MOSS-ChatV is a reinforcement learning framework introducing a Dynamic Time Warping (DTW)-based process reward to improve the temporal reasoning consistency of Multimodal Large Language Models (MLLMs). The research aims to correct "process inconsistency," where models produce correct answers despite flawed intermediate reasoning, by employing a rule-based Process Reasoning Reward (PRR) within the Group Relative Policy Optimization (GRPO) algorithm to align generated reasoning with annotated reference traces from the new MOSS-Video dataset. MOSS-ChatV achieves 87.2% accuracy on the MOSS-Video test set and improves performance on general benchmarks such as MVBench (67.6%). For AI practitioners, this demonstrates that using reinforcement learning with an efficient, rule-based process supervision reward can significantly enhance a video model's reasoning coherence and performance without requiring a separate, learned reward model. |
| The Unanticipated Asymmetry Between Perceptual Optimization and
  Assessment (Read more on [arXiv](https://arxiv.org/abs/2509.20878) or [HuggingFace](https://huggingface.co/papers/2509.20878))| Du Chen, Siyu Wu, Qi Wang, Jiabei Zhang, TianheWu | This paper reveals a fundamental asymmetry where high-performing Image Quality Assessment (IQA) metrics do not necessarily function as effective optimization objectives for perceptual image generation. The primary objective is to systematically investigate the correlation between a metric's IQA capability and its utility in perceptual optimization, particularly under adversarial training, and to assess the transferability of discriminator-learned features to IQA tasks. The methodology involves using single-image super-resolution with the SwinIR model as a testbed to evaluate diverse DISTS-style perceptual metrics and discriminator architectures under various optimization configurations. The study's results show that metrics with strong IQA scores often fail to yield better optimization outcomes, and features from GAN discriminators transfer poorly for initializing IQA models compared to ImageNet pretraining; specifically, patch-level convolutional discriminators consistently outperform vanilla versions, improving average NR-IQA scores by up to +0.52 points. The principal implication for AI practitioners is that selecting a perceptual loss function based solely on its IQA benchmark performance is unreliable; instead, the discriminator architecture design is more critical, with patch-level convolutional models providing more stable and effective optimization. |
| StyleBench: Evaluating thinking styles in Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.20868) or [HuggingFace](https://huggingface.co/papers/2509.20868))| Javad Lavaei, Costas Spanos, Ming Jin, Shangding Gu, Junyu Guo | The paper introduces StyleBench, a comprehensive benchmark that systematically evaluates five reasoning styles across 15 LLMs (270M to 120B parameters) and five tasks, revealing that optimal style selection is highly contingent on both model scale and task type. The primary objective is to determine how different reasoning strategies (CoT, ToT, AoT, SoT, CoD) perform across diverse tasks and model architectures, and to identify which approaches offer the optimal balance between performance and computational efficiency. The methodology involves evaluating the five reasoning styles on five distinct reasoning tasks—including mathematical, logical, and commonsense reasoning—using 15 open-source models from major architectural families and automatically extracting final answers for comparison against ground truth. The study found no universally optimal style; for instance, Chain-of-Thought (CoT) consistently outperformed others on GSM8K mathematical problems, while search-based methods like Tree-of-Thought (ToT) excelled on open-ended puzzles like Game of 24, but only with large-scale models. Notably, on structured tasks, concise styles like SoT and CoD achieved high accuracy with significantly shorter responses, with one example showing a 94% reduction in length compared to CoT. The principal implication for AI practitioners is that reasoning strategy selection must be tailored to the specific task and available model scale; search-based methods should be used for complex problems with large models, while concise methods offer superior efficiency for well-defined tasks or resource-constrained environments, as models cannot yet learn to autonomously select the optimal style via standard fine-tuning. |
| When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks
  Silently Undermine Validity (Read more on [arXiv](https://arxiv.org/abs/2509.20293) or [HuggingFace](https://huggingface.co/papers/2509.20293))| John P Dickerson, Oussama Elachqar, Astitwa Sarthak Lathe, Chiung-Yi Tseng, Benjamin Feuer | This paper introduces diagnostic metrics to demonstrate that popular LLM-judged benchmarks suffer from severe design failures, such as schema incoherence and factor collapse, which silently undermine their validity. The main objective is to quantify these failure modes by assessing if LLM judges adhere to their rubrics and if the evaluation criteria are meaningfully distinct. The authors propose two novel mechanisms: 'Schematic adherence,' which uses regression to measure how well verdicts are explained by rubric scores, and 'Psychometric validity,' which aggregates internal consistency and discriminant validity signals. Applied to the Arena-Hard Auto benchmark, the analysis revealed severe schema incoherence, with unexplained variance in judgments exceeding 90% for the DeepSeek-R1-32B judge, and significant factor collapse, with inter-criteria correlations often exceeding 0.93. The principal implication for AI practitioners is that rankings from LLM-judged benchmarks should be treated with extreme caution, as common aggregation methods like ELO can mask fundamental invalidity and produce high-confidence leaderboards that are effectively noise, leading to flawed model selection. |
| Discrete Diffusion for Reflective Vision-Language-Action Models in
  Autonomous Driving (Read more on [arXiv](https://arxiv.org/abs/2509.20109) or [HuggingFace](https://huggingface.co/papers/2509.20109))| Hang Zhao, Huimin Wang, Yue Wang, Yinan Zheng, pengxiang | The paper introduces ReflectDrive, a framework that uses discrete diffusion and a reflective inference mechanism with search and inpainting to generate safe and coherent trajectories for autonomous driving. The primary objective is to develop a controllable end-to-end driving system that can enforce hard safety constraints, overcoming the limitations of standard imitation learning models which often violate physical rules. The key methodology involves discretizing the driving space, using a fine-tuned Diffusion Language Model for planning, and applying a two-stage, gradient-free inference process that generates diverse trajectories and then iteratively repairs them by finding safe anchor tokens via local search and using diffusion inpainting. On the NAVSIM closed-loop benchmark, ReflectDrive improves the drivable area compliance (DAC) score by +3.9 points to 99.3 and the overall PDMS score to 91.1 compared to the baseline without reflection. For AI practitioners, this research provides a method for integrating external safety oracles into generative models by leveraging a discrete token space for efficient, gradient-free search-and-repair operations, offering a scalable alternative to computationally expensive guidance or reinforcement learning for enforcing hard constraints. |
| Thinking While Listening: Simple Test Time Scaling For Audio
  Classification (Read more on [arXiv](https://arxiv.org/abs/2509.19676) or [HuggingFace](https://huggingface.co/papers/2509.19676))| Mert Pilanci, Prateek Verma | The paper introduces a test-time scaling framework for audio classification that improves performance by having a frozen LLM reason over sequences of patch-level predictions sampled from a frozen audio model. The main research objective is to devise a method for incorporating reasoning into audio classification pipelines to enable performance scaling at test time without altering the base model or input data. The key methodology involves generating a "reasoning trace" by causally processing audio in patches and sampling multiple category predictions per patch; this trace is then fed into a frozen reasoning model, such as GPT-2 with a retrained embedding matrix, to produce the final classification. Primary results show that on the ESC-50 dataset, using a frozen AST backbone, the method achieved 88.3% top-1 accuracy by sampling 32 times per patch, nearly matching the 88.8% accuracy of a fully fine-tuned AST model. The principal implication for AI practitioners is that the performance of a frozen audio classifier can be significantly enhanced at inference time by dedicating more compute to sample longer reasoning traces and aggregate them with an LLM-based reasoner; notably, a lightweight approach of retraining only the embedding matrix of a small, frozen LLM like GPT-2 is shown to be more effective than zero-shot prompting of much larger models for this task. |
| Blueprints of Trust: AI System Cards for End to End Transparency and
  Governance (Read more on [arXiv](https://arxiv.org/abs/2509.20394) or [HuggingFace](https://huggingface.co/papers/2509.20394))| Roman Zhukov, Florencio Cano Gabarda, Garth Mollett, Emily Fox, Huzaifa Sidhpurwala | This paper introduces the Hazard-Aware System Card (HASC), a dynamic, machine-readable framework for documenting an AI system's architecture, data provenance, and evolving safety and security posture. The primary objective is to create a standardized, living artifact that enhances transparency and accountability by systematically tracking an AI system's identified hazards and remediations throughout its lifecycle. The proposed methodology involves automated generation of the HASC via CI/CD pipelines based on a defined JSON schema and introduces a novel AI Safety Hazard (ASH) identifier (e.g., ASH-2025-0023) to catalog safety flaws, complementing the existing CVE system for security vulnerabilities. While the paper lacks quantitative experimental results, it contextualizes its proposal by citing the projection that the Hugging Face Hub will top 1.7 million models by mid-2025, highlighting the need for scalable governance. For AI practitioners, the HASC provides a concrete, automatable mechanism for creating auditable evidence of system safety and compliance, enabling automated policy enforcement and alignment with standards like ISO/IEC 42001. |
| MI-Fuse: Label Fusion for Unsupervised Domain Adaptation with
  Closed-Source Large-Audio Language Model (Read more on [arXiv](https://arxiv.org/abs/2509.20706) or [HuggingFace](https://huggingface.co/papers/2509.20706))| Hung-yi Lee, dlion168, MonicaHuang | The paper introduces MI-Fuse, a framework for source-free unsupervised domain adaptation in speech emotion recognition using a closed-source Large Audio-Language Model (LALM). The research aims to determine if a student model can be adapted to outperform an API-only LALM on a target domain using only unlabeled audio. The key methodology involves fusing pseudo-labels from the LALM and an auxiliary source-trained classifier, weighting their predictions based on mutual information to mitigate label noise, and stabilizing training with a diversity loss and an exponential moving average teacher. Across six cross-domain transfer settings, MI-Fuse achieved an average unweighted accuracy of 58.38%, outperforming the strongest baseline by 3.9%. For AI practitioners, this work provides a practical method to train specialized, high-performing student models that can surpass general-purpose, closed-source foundation models on a target domain, even when source data is unavailable and the teacher model is a black box. |
