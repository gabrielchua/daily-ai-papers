

## Papers for 2025-09-29

| Title | Authors | Summary |
|-------|---------|---------|
| LongLive: Real-time Interactive Long Video Generation (Read more on [arXiv](https://arxiv.org/abs/2509.22622) or [HuggingFace](https://huggingface.co/papers/2509.22622))|  | LONGLIVE is a frame-level autoregressive framework enabling real-time, interactive generation of long videos with high temporal consistency and prompt adherence. The main objective is to overcome the quality and efficiency challenges of long video generation, specifically enabling real-time interaction and smooth transitions between sequential user prompts while maintaining visual and semantic coherence over extended durations. The methodology combines three key components: a KV-recache mechanism to refresh cached states during prompt switches, a streaming long tuning strategy to align training with long-video inference and mitigate quality degradation, and a short window attention paired with a frame-level attention sink for efficient inference. Primary results demonstrate that LONGLIVE achieves a generation speed of 20.7 FPS on a single NVIDIA H100 GPU, supports videos up to 240 seconds, and achieves a state-of-the-art score of 83.52 on the VBench-Long benchmark, outperforming existing autoregressive and diffusion-based models in both speed and quality. The principal implication for AI practitioners is that this framework provides a viable architecture for building high-performance, interactive video generation tools by showing that autoregressive models, when augmented with specialized cache management and training strategies, can achieve real-time speeds without the computational overhead of diffusion models, making them suitable for dynamic content creation applications. |
| Quantile Advantage Estimation for Entropy-Safe Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.22611) or [HuggingFace](https://huggingface.co/papers/2509.22611))| An Zhang, Jiancan Wu, xiangwang1223, 737443h, junkang0909 | Quantile Advantage Estimation (QAE) is a drop-in replacement for the mean baseline in value-free RL that stabilizes LLM reasoning training by creating a two-regime gate for credit assignment. The paper's objective is to resolve the training instability in Reinforcement Learning with Verifiable Rewards (RLVR), which oscillates between entropy collapse and explosion, by redesigning the advantage estimation baseline. The key methodology replaces the standard mean reward baseline with a group-wise K-quantile baseline, which for binary rewards selectively assigns non-zero advantage to either rare successes on hard queries (where success rate p ≤ 1-K) or residual failures on easy queries (p > 1-K). Primary results demonstrate that QAE provides provable two-sided entropy safety, sparsifies updates by assigning zero advantage to approximately 80% of responses, and improved pass@1 performance on AIME'24 for Qwen3-8B-Base from 39.69% to 48.23% while maintaining comparable pass@16 scores. The principal implication for AI practitioners is that they can significantly stabilize RLVR fine-tuning and improve sample efficiency with a simple, one-line change to the baseline calculation, targeting the core mechanism of credit assignment rather than relying on more complex token-level heuristics. |
| MinerU2.5: A Decoupled Vision-Language Model for Efficient
  High-Resolution Document Parsing (Read more on [arXiv](https://arxiv.org/abs/2509.22186) or [HuggingFace](https://huggingface.co/papers/2509.22186))| SunYuefeng, hotelll, ouyanglinke, wanderkid, starriver030515 | MinerU2.5 is a 1.2B-parameter vision-language model that performs efficient, high-resolution document parsing using a decoupled, coarse-to-fine strategy. The primary objective is to achieve state-of-the-art parsing accuracy for text, tables, and formulas while maintaining high computational efficiency. The methodology involves a two-stage process: first, the model performs rapid layout analysis on a downsampled image, and second, it conducts targeted content recognition on native-resolution crops extracted from the original image based on the detected layout. The model achieves a state-of-the-art overall score of 90.67 on the OmniDocBench benchmark and an inference speed of 2.12 pages/second on an A100 GPU. For AI practitioners, this decoupled architecture provides a computationally efficient design pattern for processing high-resolution documents, enabling the creation of high-quality structured data for applications like Retrieval-Augmented Generation (RAG). |
| EPO: Entropy-regularized Policy Optimization for LLM Agents
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.22576) or [HuggingFace](https://huggingface.co/papers/2509.22576))| Li Yu-Jhe, Wentian Zhao, timecuriosity, ztwang, Iscarrot | The paper introduces Entropy-regularized Policy Optimization (EPO) to address the "exploration-exploitation cascade failure" in training LLM agents on multi-turn, sparse-reward tasks. The objective is to stabilize reinforcement learning by preventing early premature convergence and subsequent late-stage policy collapse. The key methodology involves three components: trajectory-aware entropy regularization, an entropy smoothing regularizer that bounds policy entropy within a moving historical average, and an adaptive weighting schedule to balance exploration and exploitation. EPO achieves up to a 152.1% performance improvement on the ScienceWorld benchmark over a PPO baseline and up to 19.8% on ALFWorld over a GRPO baseline. For AI practitioners, the principal implication is that in long-horizon, sparse-reward LLM agent training, simply adding an entropy bonus is insufficient; they should instead use temporal control mechanisms like EPO's historical smoothing to maintain stable exploration and avoid the identified cascade failure. |
| Variational Reasoning for Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.22637) or [HuggingFace](https://huggingface.co/papers/2509.22637))|  | This paper introduces a variational reasoning framework that improves language model reasoning by treating thinking traces as latent variables optimized via variational inference. The objective is to develop a principled probabilistic training method that maximizes the log-likelihood of generating correct answers, addressing the instability and data cost of existing RL and SFT approaches. The methodology involves optimizing an IWAE-style multi-trace evidence lower bound (ELBO) and training a variational posterior, conditioned on answer hints, using a forward-KL divergence to generate high-quality thinking traces for weighted finetuning. On the Qwen3-4B-Base model, the proposed accuracy-based method achieves a 55.72% average score across five reasoning benchmarks, surpassing the strong Bespoke-Stratos baseline's 51.35%. AI practitioners can implement this framework as a more stable and effective alternative to standard RL finetuning for enhancing the reasoning capabilities of LLMs on complex tasks, as it provides a principled objective and clarifies biases in existing methods like RFT and GRPO. |
| Language Models Can Learn from Verbal Feedback Without Scalar Rewards (Read more on [arXiv](https://arxiv.org/abs/2509.22638) or [HuggingFace](https://huggingface.co/papers/2509.22638))|  | This paper proposes a method for large language models to learn directly from verbal feedback without converting it to scalar rewards. The research objective is to address the information loss, ambiguity, and scale imbalance associated with scalarization in reinforcement learning from human feedback (RLHF). The key methodology is the Feedback-Conditional Policy (FCP), which treats verbal feedback as a conditioning signal and is trained via maximum likelihood on response-feedback pairs, followed by an online bootstrapping phase for refinement. The primary result shows that FCP with online bootstrapping achieves a 38.7% average accuracy on a math benchmark suite, slightly surpassing strong scalar-based baselines like GRPO (38.4%). For AI practitioners, this provides a scalable framework to train models using raw verbal feedback, eliminating the need for designing reward functions, scalar conversion, or data filtering, thereby simplifying the model alignment pipeline. |
| ReviewScore: Misinformed Peer Review Detection with Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2509.21679) or [HuggingFace](https://huggingface.co/papers/2509.21679))|  | This paper introduces REVIEWSCORE, a framework that uses Large Language Models to automatically detect misinformed peer review points, defined as questions already answered in a paper or weaknesses based on incorrect premises. The main objective is to develop and validate an automated method for identifying low-quality peer reviews to improve the integrity of the academic review process in large AI conferences. The key methodology involves an automated argument reconstruction engine that uses a SAT solver and LLM feedback loops to decompose argumentative weaknesses into a set of explicit and implicit premises for granular factuality checking against a human-annotated dataset. The primary result shows that 15.2% of weaknesses and 26.4% of questions in their dataset are misinformed, with the best-performing LLM (`claude-sonnet-3.7`) achieving a moderate human-model agreement F1 score of 0.448 on the REVIEWSCORE detection task. The principal implication for AI practitioners is the potential for integrating this automated system into conference management platforms to flag low-quality reviews, providing direct feedback to reviewers and assisting meta-reviewers in their decisions. |
| CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2509.22647) or [HuggingFace](https://huggingface.co/papers/2509.22647))|  | This paper introduces CapRL, a reinforcement learning framework that trains dense image captioning models by using the ability of a vision-free LLM to answer questions from the caption as an objective reward. The objective is to apply the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to the subjective task of image captioning, overcoming the scalability and memorization issues of Supervised Fine-Tuning (SFT). CapRL's methodology employs a decoupled two-stage pipeline where an LVLM generates a caption, and the reward is the accuracy of a separate, vision-free LLM answering multiple-choice questions about the image based solely on that caption. The primary result shows that within the Prism evaluation framework, the CapRL-3B model achieves an average score of 48.3, matching the performance of the much larger Qwen2.5-VL-72B model and outperforming its baseline by 8.4%. For AI practitioners, this provides a scalable method to generate high-quality, dense image-text data for pre-training LVLMs, enhancing modality alignment without requiring expensive, manually annotated datasets. |
| MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.22281) or [HuggingFace](https://huggingface.co/papers/2509.22281))| Weipeng Zhong, Xudong Xu, Zhen Luo, nfliang, wuzhi-hao | This paper introduces MesaTask, a framework and dataset for generating task-driven 3D tabletop scenes from natural language instructions using a large language model with 3D spatial reasoning. The research objective is to automate the creation of plausible and task-relevant 3D scenes for robotic training, bridging the gap between high-level instructions and specific scene layouts. The methodology centers on a "Spatial Reasoning Chain" that decomposes generation into object inference, spatial interrelation reasoning, and scene graph construction, used to train an LLM via Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) on the new MesaTask-10K dataset of ~10,700 scenes. MesaTask significantly outperforms baselines, achieving a Fréchet Inception Distance (FID) of 40.3, indicating superior realism compared to a GPT-4o baseline score of 74.4. For AI practitioners, this provides a validated framework and a large-scale dataset to automate the generation of diverse and realistic 3D simulation environments, accelerating the development of robotic policies that can interpret and execute complex, language-based commands. |
| No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM
  Reinforcement Learning via Entropy-Guided Advantage Shaping (Read more on [arXiv](https://arxiv.org/abs/2509.21880) or [HuggingFace](https://huggingface.co/papers/2509.21880))|  | The paper introduces RL-ZVP, a novel algorithm for LLM reinforcement learning that extracts useful training signals from zero-variance prompts by using an entropy-guided advantage shaping mechanism. The main objective is to utilize zero-variance prompts—where all model responses share the same reward and are typically discarded by methods like GRPO—to improve the reasoning capabilities and training efficiency of LLMs. The key methodology involves a custom advantage formulation for zero-variance prompts: for all-correct responses, it assigns a positive advantage proportional to token-level entropy, and for all-incorrect responses, it assigns a negative advantage that penalizes low-entropy tokens more severely, while reverting to GRPO for all other prompts. Primary results demonstrate that RL-ZVP significantly outperforms GRPO across six math benchmarks, achieving up to an 8.61 point gain in accuracy (Acc@8) on the AIME25 benchmark and consistently outperforming baselines that filter these prompts. The principal implication for AI practitioners is that they can enhance the data efficiency and final performance of RL fine-tuning for reasoning tasks by implementing the RL-ZVP objective, which salvages previously discarded rollouts to provide a stronger and more stable learning signal. |
| VoiceAssistant-Eval: Benchmarking AI Assistants across Listening,
  Speaking, and Viewing (Read more on [arXiv](https://arxiv.org/abs/2509.22651) or [HuggingFace](https://huggingface.co/papers/2509.22651))|  | This paper introduces VoiceAssistant-Eval, a comprehensive benchmark with 10,497 examples designed to assess AI assistants across integrated listening, speaking, and viewing capabilities. The objective is to address gaps in existing evaluations by creating a framework that tests hands-free interaction, voice personalization, and joint audio-visual understanding. The methodology evaluates 22 models on 13 tasks using a triadic system measuring content quality (via a GPT judge), speech naturalness (UTMOS), and text-speech consistency (modified WER). Key findings reveal a significant disparity between speaking and listening performance, with the 7B Step-Audio-2-mini model's listening accuracy (40.06) more than doubling that of the 32B LLaMA-Omni2 model (16.00). The principal implication for AI practitioners is that progress requires dedicated improvements to audio encoders and multimodal architectures, as simply scaling the LLM component is insufficient for robust performance, evidenced by a 16.3-point accuracy drop on image+audio versus image+text queries. |
| UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon
  Scenarios (Read more on [arXiv](https://arxiv.org/abs/2509.21766) or [HuggingFace](https://huggingface.co/papers/2509.21766))| Zeyu Qin, Haoyu Wang, Xuelin Zhang, Huaisong Zhang, Haotian Luo | This research introduces UltraHorizon, a novel benchmark for evaluating LLM-agent capabilities in ultra-long-horizon, partially observable scenarios where existing benchmarks fall short. Its objective is to systematically measure foundational agent competencies such as sustained reasoning, planning, memory management, and tool use by requiring agents to uncover hidden rules through extended interaction. The methodology utilizes three distinct discovery-oriented environments where trajectories average over 35k tokens and 60 tool calls, with performance evaluated against human baselines. Key results demonstrate a significant performance deficit, with the best LLM agent scoring 14.33 compared to the human baseline of 26.52, and reveal that agent failures stem from "in-context locking" and foundational capability gaps rather than task-intrinsic reasoning difficulty. For AI practitioners, this implies that progress in long-horizon tasks requires developing agent architectures with principled memory integration and robust exploration strategies, as current models lack the inherent capability to utilize extended interaction budgets effectively and simple scaling is insufficient. |
| LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale
  Diffusion Transformer (Read more on [arXiv](https://arxiv.org/abs/2509.22414) or [HuggingFace](https://huggingface.co/papers/2509.22414))|  | LucidFlux is a universal image restoration framework that adapts a large-scale diffusion transformer (Flux.1) to restore high-quality images from degraded inputs without requiring text captions. The main objective is to develop a robust method for restoring images with unknown degradations by effectively conditioning a large generative model while preserving semantic consistency and avoiding the latency and instability of text-based prompts. The key methodology involves a lightweight dual-branch conditioner that processes the degraded input and a lightly restored proxy, a timestep- and layer-adaptive modulation schedule to guide the frozen transformer backbone, and caption-free semantic alignment using SigLIP features extracted from the proxy. LucidFlux achieves state-of-the-art results on multiple benchmarks, attaining a CLIP-IQA+ score of 0.7406 on the RealLQ250 dataset, outperforming prior open-source and commercial methods. The principal implication for AI practitioners is that adapting large diffusion transformers for specialized tasks like image restoration can be more effectively achieved through structured, minimal-overhead conditioning and direct semantic guidance, rather than by adding extensive parameters or relying on external captioning models, offering a practical blueprint for efficient foundation model adaptation. |
| WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level
  Feedback and Step-Level Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.22644) or [HuggingFace](https://huggingface.co/papers/2509.22644))| Zhuofan Zong, Yunqiao Yang, Houxing Ren, Zimu Lu, scikkk | The paper introduces WebGen-Agent, an iterative system for website generation that uses multi-level feedback from a Visual Language Model and a GUI-agent, and a reinforcement learning method, Step-GRPO, to train the agent's reasoning engine. The primary objective is to improve automated website generation by creating an agent that iteratively refines codebases using comprehensive visual and functional feedback, rather than relying solely on code execution verification. The core methodology involves an iterative workflow where a VLM provides scores and suggestions based on website screenshots, and a GUI-agent tests functionality, also providing scores; these step-level scores are then used as a dense reward signal in a step-wise Generalized Reward Policy Optimization (Step-GRPO) process to fine-tune the agent's LLM. The WebGen-Agent workflow increased the accuracy of Claude-3.5-Sonnet on the WebGen-Bench dataset from 26.4% to 51.9%, and the Step-GRPO training method improved the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4%. For AI practitioners, the principal implication is that combining VLM-based visual analysis with GUI-agent functional testing creates a powerful feedback loop that provides dense, reliable reward signals, enabling effective reinforcement learning for complex, visually-dependent code generation tasks and the training of smaller open-source models. |
| SPARK: Synergistic Policy And Reward Co-Evolving Framework (Read more on [arXiv](https://arxiv.org/abs/2509.22624) or [HuggingFace](https://huggingface.co/papers/2509.22624))|  | SPARK is a framework that synergistically co-evolves a large model's policy and reward capabilities by recycling rollouts from verifiable reward-based reinforcement learning. The research objective is to develop an efficient, on-policy RL framework that unifies policy optimization and reward modeling within a single model, eliminating the high costs and potential mismatches associated with separate reward models and human preference data. The key methodology extends Reinforcement Learning with Verifiable Rewards (RLVR) by recycling generated rollouts and their correctness scores to create on-policy data for auxiliary training objectives—pointwise, pairwise, and reflection—which trains the policy model to simultaneously function as its own generative reward model. Primary results show that SPARK-VL-7B achieves an average gain of 9.7% on 7 reasoning benchmarks and 12.1% on 2 reward benchmarks over baselines, demonstrating significant performance improvements. The principal implication for AI practitioners is a more resource-efficient and stable method for model alignment that reduces MLOps complexity by removing the need for a separate reward model training pipeline and human data annotation, while enabling test-time performance scaling through integrated self-reflection. |
| Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in
  Subject-Driven Generation (Read more on [arXiv](https://arxiv.org/abs/2509.21989) or [HuggingFace](https://huggingface.co/papers/2509.21989))| Peter Wonka, Bernard Ghanem, Aleksandar Cvejic, abdo-eldesokey | Mind-the-Glitch introduces a framework for disentangling visual and semantic features from diffusion models to create a new metric, Visual Semantic Matching (VSM), for quantifying and localizing inconsistencies in subject-driven image generation. The main objective is to create a reliable method for evaluating visual consistency that overcomes the limitations of existing global, feature-based metrics by enabling spatial localization of errors. The methodology involves an automated pipeline that generates image pairs with controlled visual inconsistencies for training a dual-branch contrastive architecture, which separates visual and semantic features from a frozen diffusion model backbone. The proposed VSM metric achieved a Pearson correlation of 0.448 with a ground-truth oracle in a controlled evaluation, significantly outperforming CLIP (-0.053), DINO (0.087), and a VLM-based approach (0.072). For AI practitioners, this provides a superior evaluation tool that not only quantifies visual fidelity more accurately than existing metrics but also localizes specific regions of inconsistency, offering actionable insights for model debugging and improvement. |
| See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned
  Aerial Navigation (Read more on [arXiv](https://arxiv.org/abs/2509.22653) or [HuggingFace](https://huggingface.co/papers/2509.22653))| Chih-Hai Su, Yang-Sen Lin, Chih Yao Hu, jayinnn, yuna0x0 | See, Point, Fly (SPF) is a training-free framework that enables universal UAV navigation by repurposing frozen Vision-Language Models (VLMs) to perform 2D spatial grounding for action prediction. The research objective is to develop a zero-shot UAV navigation system that interprets free-form language by framing action prediction not as text generation, but as a 2D spatial grounding task. The key methodology involves prompting a VLM to output a structured JSON containing a 2D waypoint on the current camera image, which is then geometrically unprojected into a 3D displacement vector and executed as low-level UAV control commands in a closed-loop. SPF achieved a 93.9% success rate in the DRL simulation benchmark, outperforming the previous state-of-the-art method by an absolute margin of 63%. For AI practitioners, this work implies that leveraging a VLM's inherent spatial understanding for direct 2D visual grounding offers a more effective and generalizable zero-shot pathway for continuous robot control than methods relying on text-based action generation or predefined skill libraries. |
| Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on
  Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval (Read more on [arXiv](https://arxiv.org/abs/2509.21710) or [HuggingFace](https://huggingface.co/papers/2509.21710))|  | The paper introduces Think-on-Graph 3.0 (ToG-3), a multi-agent framework for adaptive LLM reasoning on heterogeneous graphs in Retrieval-Augmented Generation (RAG). The primary objective is to overcome the limitations of static graph indices in existing Graph-RAG methods by enabling dynamic, query-adaptive graph construction and refinement, particularly for lightweight LLMs. The key methodology is the Multi-Agent Context Evolution and Retrieval (MACER) mechanism, where agents collaboratively engage in an iterative loop of evidence retrieval, sufficiency reflection, and dual-evolution of the query (Evolving Query) and the graph structure (Evolving Sub-Graph). The framework achieves state-of-the-art performance, recording the highest average Exact Match (EM) score of 0.453 and F1 score of 0.312 across deep reasoning benchmarks including HotpotQA, 2WikiMultiHopQA, and Musique. For AI practitioners, the principal implication is that this dual-evolving, multi-agent approach enables the creation of more precise RAG systems that can perform complex multi-hop reasoning even with smaller, locally-deployed models, mitigating the typical performance degradation associated with static graph construction in resource-constrained environments. |
| PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.19894) or [HuggingFace](https://huggingface.co/papers/2509.19894))| Lingpeng Kong, Zhuocheng Gong, Jian Guan, Wei Wu, xl-zhao | The paper introduces PromptCoT 2.0, a framework using an Expectation-Maximization loop to iteratively co-generate rationales and prompts, producing high-quality synthetic data for enhancing LLM reasoning. The primary objective is to develop a scalable method for synthesizing complex and diverse training problems that overcomes the limitations of manual curation and static, heuristic-based generation. The core methodology formulates prompt synthesis as a latent variable model, where rationales mediate between concepts and prompts, and employs an Expectation-Maximization (EM) algorithm to iteratively refine a rationale generation model (E-step) and a prompt generation model (M-step). In a self-play setting, applying PromptCoT 2.0 to a Qwen3-30B model improved AIME 24 accuracy from 87.7% to 92.1%; in supervised fine-tuning, a 7B model trained solely on its synthetic data achieved 73.1% on the same benchmark, drastically up from the baseline 12.8%. For practitioners, this provides a scalable, automated pipeline to generate high-difficulty training corpora that can significantly boost the reasoning capabilities of both frontier and smaller open-source models without relying on expensive human annotation or access to superior teacher models. |
| D-Artemis: A Deliberative Cognitive Framework for Mobile GUI
  Multi-Agents (Read more on [arXiv](https://arxiv.org/abs/2509.21799) or [HuggingFace](https://huggingface.co/papers/2509.21799))| Jinyuan Li, Yuqi Wang, Wenjie Lu, Yibo Feng, Hongze Mi | D-Artemis is a deliberative cognitive framework designed to enhance the reliability and efficiency of mobile GUI agents by emulating a human-like cognitive process. The main objective is to overcome critical challenges in GUI automation, such as data bottlenecks in end-to-end training and the high cost of delayed error detection, by improving the performance of general-purpose Multimodal Large Language Models (MLLMs) without task-specific training. The key methodology involves a three-stage loop for each action: action generation informed by fine-grained app-specific tips, a proactive Pre-execution Alignment stage utilizing a Thought-Action Consistency (TAC) Check module and an Action Correction Agent (ACA) to prevent errors, and a post-execution Status Reflection Agent (SRA) for strategic learning. The framework achieves new state-of-the-art results, including a 75.8% success rate on the AndroidWorld benchmark and 96.8% on ScreenSpot-V2. The principal implication for AI practitioners is that incorporating proactive, deliberative mechanisms like pre-execution verification and correction into agentic frameworks can significantly enhance the performance and generalization of foundational models on complex interactive tasks, providing a more data-efficient path to developing robust autonomous agents. |
| UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models (Read more on [arXiv](https://arxiv.org/abs/2509.21760) or [HuggingFace](https://huggingface.co/papers/2509.21760))| Yuchao Gu, Lan Chen, HelenMao | UniVid is a framework that adapts a single pre-trained video generation model to perform diverse vision tasks through lightweight supervised fine-tuning. The research investigates whether a video generation model, pre-trained solely on natural video data without task-specific annotations, can serve as a universal backbone for a broad range of image and video tasks. The methodology involves fine-tuning a pre-trained video diffusion transformer using Low-Rank Adaptation (LoRA), where tasks are formulated as "visual sentences" (A → A' → B → B') to provide in-context examples. UniVid significantly outperforms the LVM baseline on depth estimation, achieving a root mean square logarithmic error of 0.42 compared to LVM's 1.15, despite being trained on only a small subset of the training data. For AI practitioners, this work suggests that pre-trained video synthesis models can be a highly data-efficient and scalable foundation for building unified vision systems, potentially eliminating the need for costly pre-training on large-scale, multi-source annotated datasets. |
| Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive
  Exploration for Agentic Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.22601) or [HuggingFace](https://huggingface.co/papers/2509.22601))| Gang Li, Zhengbao He, Xiaoyu Tan, Yulei Qin, tedsun | SPEAR is a curriculum-based self-imitation learning recipe that improves reinforcement learning for agentic LLMs by progressively managing policy entropy to balance exploration and exploitation. The primary objective is to schedule a smooth transition from broad skill-level exploration to focused action-level exploitation, guided by the agent's own experiences, to avoid the extremes of policy entropy collapse or runaway divergence during RL training. The key methodology extends the Self-Imitation Learning (SIL) framework with a curriculum that initially uses intrinsic rewards for skill exploration and then progressively increases self-imitation of successful trajectories from a replay buffer; stability is enhanced through advantage recalibration for off-policy updates and covariance-based clipping of high-impact tokens. SPEAR demonstrates significant performance improvements across multiple benchmarks, increasing the success rate of the GRPO baseline on WebShop by up to 20.7% (from 56.8% to 77.5%) and boosting the Dr.BoT baseline on AIME25 by up to 6.1%, with only 10-25% extra theoretical complexity. For AI practitioners, SPEAR offers a plug-and-play framework to stabilize and enhance RL training for agentic LLMs on long-horizon, sparsely-rewarded tasks, providing a structured approach to leverage an agent's past successes for more effective and stable policy optimization without requiring expert demonstrations. |
| Fine-tuning Done Right in Model Editing (Read more on [arXiv](https://arxiv.org/abs/2509.22072) or [HuggingFace](https://huggingface.co/papers/2509.22072))| Du Su, Hongyu Zang, Rui Tang, Fei Sun, Wanli Yang | This paper re-establishes fine-tuning as a leading model editing technique by demonstrating that its previously reported failures stem from flawed depth-first implementations and proposes a simple, effective localized breadth-first approach called LocFT-BF. The research investigates whether fine-tuning is inherently unsuitable for model editing or if its perceived failure is due to its common implementation as a sequential, single-pass, depth-first (DF) pipeline. The methodology involves controlled experiments comparing the DF pipeline with a standard breadth-first (BF) mini-batch pipeline, followed by a systematic analysis of parameter tuning locations across different layers and modules to optimize performance. The primary result is that the proposed LocFT-BF outperforms state-of-the-art methods by an average of 33.72% in editing success rate and is the first method shown to sustain 100K sequential edits and scale to 72B-parameter models. For AI practitioners, the principal implication is that a simple, localized, and properly implemented breadth-first fine-tuning is a highly effective, scalable, and efficient method for model editing, obviating the need for more complex, specialized algorithms. |
| X-Streamer: Unified Human World Modeling with Audiovisual Interaction (Read more on [arXiv](https://arxiv.org/abs/2509.21574) or [HuggingFace](https://huggingface.co/papers/2509.21574))| Guoxian Song, Chenxu Zhang, Zenan Li, You Xie, gutianpei | X-Streamer is an end-to-end framework for generating real-time, infinitely streamable digital humans with unified audiovisual interaction from a single portrait. The primary objective is to develop a unified multimodal human world modeling architecture capable of infinite, real-time text, speech, and video generation while maintaining long-range conversational context and visual consistency. The methodology employs a Thinker-Actor dual-transformer architecture: a frozen pretrained language-speech model (Thinker) performs reasoning, while a chunk-wise autoregressive diffusion model (Actor) translates the Thinker's hidden states into time-aligned, interleaved text, audio, and video streams, stabilized by chunk-wise diffusion forcing and global identity referencing. The system achieves state-of-the-art long-horizon video generation, attaining a Fréchet Video Distance (FVD) of 573.36, and sustains real-time multimodal streaming at 25 fps on two A100 GPUs. For AI practitioners, the principal implication is a validated framework for extending large language-speech models to generate continuous, synchronized video in real-time, enabling the development of persistent and multimodally interactive digital human agents within a single, unified architecture instead of complex modular pipelines. |
| TUN3D: Towards Real-World Scene Understanding from Unposed Images (Read more on [arXiv](https://arxiv.org/abs/2509.21388) or [HuggingFace](https://huggingface.co/papers/2509.21388))| Anna Vorontsova, Alexey Zakharov, Bulat Gabdullin, Nikita Drozdov, Anton Konushin | This paper presents TUN3D, a model for joint 3D layout estimation and object detection that can process multi-view images without ground-truth camera poses or depth supervision. The main objective is to relax the input data requirements for 3D indoor scene understanding, enabling the use of casually captured images from standard cameras instead of requiring depth sensors or pre-computed point clouds. The methodology employs a lightweight sparse-convolutional backbone with two task-specific heads and introduces a novel "2x2D offsets + height" parametric wall representation that simplifies layout estimation by projecting it onto a bird's-eye-view plane. The model achieves state-of-the-art performance, setting a new benchmark for layout estimation on ScanNet with a 66.6 F1 score from ground-truth point clouds, significantly surpassing the prior joint method PQ-Transformer's 54.4 F1 score. The principal implication for AI practitioners is the ability to build 3D indoor scene understanding applications using only visual data from consumer devices, removing the dependency on specialized hardware like depth sensors or trackers. |
| Chasing the Tail: Effective Rubric-based Reward Modeling for Large
  Language Model Post-Training (Read more on [arXiv](https://arxiv.org/abs/2509.21500) or [HuggingFace](https://huggingface.co/papers/2509.21500))|  | This paper introduces a rubric-based reward modeling method to mitigate reward over-optimization in LLM reinforcement fine-tuning (RFT) by improving reward accuracy in the high-reward tail. The primary objective is to develop a workflow for constructing reward models that can reliably distinguish between "great" and "excellent" responses, which the paper's theoretical analysis identifies as the key to effective post-training. The methodology involves an iterative workflow where an LLM proposer refines rubric criteria by analyzing distinguishing features between pairs of high-quality, diverse, off-policy candidate responses. Empirically, using rubrics refined with four "great & diverse" off-policy pairs increased win-rates from 35.9% (SFT) to 39.7% on a generalist domain dataset and improved reward accuracy on high-reward examples from 40.3% to 47.9%. For AI practitioners, the principal implication is that RFT performance is critically dependent on the reward model's ability to make fine-grained distinctions among top-tier outputs, and that iterative rubric refinement using strong, diverse off-policy data is an effective technique to achieve this. |
| RefAM: Attention Magnets for Zero-Shot Referral Segmentation (Read more on [arXiv](https://arxiv.org/abs/2509.22650) or [HuggingFace](https://huggingface.co/papers/2509.22650))| Federico Tombari, Muhammad Ferjad Naeem, Alessio Tonioni, Anna Kukleva, enisimsar | REFAM is a training-free framework that improves zero-shot referring segmentation by using stop words as "attention magnets" to refine cross-attention maps from diffusion transformers. The main objective is to exploit features from pre-trained diffusion transformers for grounding tasks without requiring fine-tuning or architectural modifications. The methodology identifies Global Attention Sinks (GAS), augments referring expressions with auxiliary stop words to absorb surplus background attention, and then filters these "magnets" to produce cleaner grounding maps. REFAM establishes a new state-of-the-art on zero-shot benchmarks, outperforming the previous best method by +2.5 mIoU on the RefCOCOg test set. For AI practitioners, this implies that pre-trained generative diffusion models can be repurposed for high-performance, zero-shot segmentation by directly manipulating their internal attention mechanisms, bypassing the need for task-specific training. |
| WoW: Towards a World omniscient World model Through Embodied Interaction (Read more on [arXiv](https://arxiv.org/abs/2509.22642) or [HuggingFace](https://huggingface.co/papers/2509.22642))| Weishi Mi, Xiaozhu Ju, Chun-Kai Fan, Peidong Jia, Xiaowei Chi | This paper presents WoW, a 14B-parameter generative world model that learns physical intuition from 2 million robot interaction trajectories to generate physically consistent video predictions and translate them into executable actions. The research objective is to validate the hypothesis that authentic physical reasoning in world models must be grounded in large-scale, causally rich interaction data rather than passive video observation. The core methodology is the SOPHIA framework, which uses a Diffusion Transformer to generate future video states and a Vision Language Model agent to critique and iteratively refine these predictions for physical realism, with a Flow-Mask Inverse Dynamics Model (FM-IDM) translating the final imagined video into executable robot actions. On the newly established WoWBench, the model achieves state-of-the-art performance, including 80.16% on physical law adherence and 96.53% on instruction understanding. The principal implication for AI practitioners is that developing physically competent models, particularly for robotics, necessitates training on extensive embodied interaction data, as it is shown to be fundamental for learning causal dynamics and closing the imagination-to-action loop. |
| FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image
  Editing (Read more on [arXiv](https://arxiv.org/abs/2509.22244) or [HuggingFace](https://huggingface.co/papers/2509.22244))| Linghe Kong, Xiaohong Liu, Haotong Qin, Zhiteng Li, Junyi Wu | FlashEdit is a novel framework for real-time, text-guided image editing that decouples speed, structure, and semantics to achieve high-fidelity results with superior background preservation. The primary objective is to overcome the prohibitive latency and quality trade-offs, such as background instability and semantic entanglement, inherent in existing diffusion-based editing methods. The methodology integrates three key innovations: a One-Step Inversion-and-Editing (OSIE) pipeline for speed, a Background Shield (BG-Shield) mechanism that caches background features in self-attention layers to maintain structural integrity, and Sparsified Spatial Cross-Attention (SSCA) which prunes text tokens pre-softmax to ensure precise semantic control. The primary result is a system that performs edits in under 0.2 seconds, achieving a 150.84× speedup over the DDIM+P2P baseline while attaining a state-of-the-art background preservation PSNR of 25.29. For AI practitioners, FlashEdit provides an efficient architecture for building interactive editing applications, demonstrating that the speed-quality trade-off can be resolved through a holistic, multi-level control strategy rather than by tackling latency, structure, and semantics as isolated problems. |
| ERGO: Efficient High-Resolution Visual Understanding for Vision-Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2509.21991) or [HuggingFace](https://huggingface.co/papers/2509.21991))| Ki-Ung Song, Seungmin Yang, Wooksu Shin, Jewon Lee, bokyeong1015 | ERGO introduces an efficient coarse-to-fine reasoning pipeline for high-resolution visual understanding in Vision-Language Models (LVLMs), aiming to mitigate the substantial computational overhead from vision tokens while preserving fine-grained details. The core methodology involves a two-stage approach: initially analyzing a downsampled image to identify task-relevant regions, then cropping and re-encoding these regions at full resolution. This is achieved through reinforcement learning (RL) with a novel Task-driven Contextual Exploration (TCE) reward that combines region-verification and box adjustment components to foster reasoning-driven perception. ERGO achieves superior performance and efficiency, for instance, surpassing Qwen2.5-VL-7B by 4.7 points on the V* benchmark while utilizing only 23% of vision tokens and achieving a 3x inference speedup. This enables AI practitioners to develop LVLMs that robustly identify informative regions from coarse visual cues, leading to more computationally efficient and accurate high-resolution vision-language applications. |
| Where MLLMs Attend and What They Rely On: Explaining Autoregressive
  Token Generation (Read more on [arXiv](https://arxiv.org/abs/2509.22496) or [HuggingFace](https://huggingface.co/papers/2509.22496))| Shiming Liu, Siyuan Liang, Kangwei Liu, Xiaoqing Guo, Ruoyu Chen | EAGLE is a black-box framework designed to explain autoregressive token generation in Multimodal Large Language Models (MLLMs) by attributing outputs to specific visual regions and quantifying modality influence. The research objective is to enhance MLLM interpretability and reliability by understanding how generated tokens depend on visual inputs and to diagnose and mitigate model hallucinations. The methodology involves sparsifying images into superpixels, then optimizing an objective function that unifies insight (sufficiency) and necessity (indispensability) scores via greedy search, and performing modality-aware analysis by tracking token logit changes. Experimentally, EAGLE consistently outperforms state-of-the-art methods, achieving an average of 20.0% higher insertion and 13.4% lower deletion scores for image captioning, and significantly reducing GPU memory usage (e.g., 16.07 GB for LLaVA-1.5 7B compared to 37.25 GB for LLaVA-CAM). This framework offers AI practitioners a faithful and efficient tool for improving decision transparency, diagnosing errors, and enhancing the safety and trustworthiness of MLLMs by identifying critical input regions and disentangling modality reliance. |
| HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion
  Models (Read more on [arXiv](https://arxiv.org/abs/2509.22300) or [HuggingFace](https://huggingface.co/papers/2509.22300))| Romann M. Weber, Farnood Salehi, msadat97 | HiGS is a training-free, plug-and-play sampling method that improves the quality and efficiency of diffusion models by incorporating a momentum-based history of past predictions into each generation step. The objective is to enhance image quality from pretrained diffusion models, particularly when using a low number of function evaluations (NFEs) or low classifier-free guidance (CFG) scales. The methodology computes a guidance direction as the difference between the current model prediction and an exponential moving average of past predictions, which is then refined via a scheduled weight, optional orthogonal projection, and a DCT-based high-pass filter to remove color artifacts. Primary results show that for unguided ImageNet 256x256 generation with a SiT-XL + REPA-E model, HiGS achieves a state-of-the-art FID of 1.61 in only 30 sampling steps, outperforming the baseline FID of 1.83 which requires 250 steps. For AI practitioners, HiGS can be integrated into existing inference pipelines to generate higher-fidelity images significantly faster and with lower guidance scales, without any model retraining or fine-tuning. |
| StateX: Enhancing RNN Recall via Post-training State Expansion (Read more on [arXiv](https://arxiv.org/abs/2509.22630) or [HuggingFace](https://huggingface.co/papers/2509.22630))| Zhiyuan Liu, Xu Han, Zhen Leng Thai, Xingyu Shen, chen-yingfa | StateX is a post-training pipeline that enhances the long-context recall of Recurrent Neural Networks (RNNs) by architecturally expanding their recurrent state size with minimal parameter overhead. The primary objective is to improve the recall capabilities of pre-trained RNNs, such as Gated Linear Attention (GLA) and Mamba2, without the high cost associated with training large-state models from scratch. The methodology involves modifying the architecture of pre-trained models before long-context post-training: for GLA, multiple attention heads are merged into one, and for Mamba2, the key and query projection dimensions are increased, followed by a selective reinitialization of token-mixing parameters. Experiments on 1.3B models show that StateX significantly improves long-context retrieval, increasing the average Needle-in-a-Haystack (NIAH) accuracy up to 64K context from 26.0% to 42.2% for GLA and from 33.2% to 39.2% for Mamba2. For AI practitioners, this provides a cost-effective method to adapt existing pre-trained RNNs for long-context tasks, making them more competitive alternatives to Transformers in scenarios requiring high recall efficiency. |
| X-CoT: Explainable Text-to-Video Retrieval via LLM-based
  Chain-of-Thought Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.21559) or [HuggingFace](https://huggingface.co/papers/2509.21559))| Raghuveer Rao, Sohail Dianat, Majid Rabbani, Jiamian Wang, prasannareddyp | This paper introduces X-CoT, a framework that replaces standard cosine similarity ranking in text-to-video retrieval with an LLM-based Chain-of-Thought (CoT) reasoning process for improved performance and explainability. The primary objective is to interpret retrieval rankings to assess the model and data quality, moving beyond opaque similarity scores. The methodology involves augmenting video datasets with structured text annotations, using an LLM to perform pairwise comparisons on a candidate pool of videos, and aggregating these judgments with the Bradley-Terry model to produce a final, reasoned ranking. X-CoT shows a significant performance boost over embedding models, achieving, for instance, a +5.6% improvement in R@1 for CLIP on the MSVD dataset. For AI practitioners, the principal implication is that X-CoT can be implemented as a plug-and-play component on top of existing retrieval systems to enhance accuracy and provide rationales for debugging and data quality assessment without requiring model retraining. |
| Real-Time Object Detection Meets DINOv3 (Read more on [arXiv](https://arxiv.org/abs/2509.20787) or [HuggingFace](https://huggingface.co/papers/2509.20787))| Xi Shen, Xuanlong Yu, Longfei Liu, Yongjie Hou, Shihua Huang | The paper introduces DEIMv2, a scalable family of real-time object detectors that integrates DINOv3 features to establish new state-of-the-art performance-cost trade-offs. The objective is to adapt the powerful semantic features from the DINOv3 foundation model into an efficient, unified framework for real-time detection across diverse computational budgets. The key methodology involves using DINOv3-pretrained Vision Transformer (ViT) backbones with a novel Spatial Tuning Adapter (STA) for larger models and pruned HGNetv2 backbones for ultra-lightweight variants, all within a DETR-based architecture. Primary results show that DEIMv2-X achieves a state-of-the-art 57.8 AP on COCO with only 50.3M parameters, while DEIMv2-S is the first sub-10M parameter model to surpass 50 AP. The principal implication for AI practitioners is the availability of a highly scalable and efficient object detection framework, providing a single architecture with multiple pre-trained model sizes suitable for deployment on hardware ranging from server-grade GPUs to resource-constrained edge devices. |
| CHURRO: Making History Readable with an Open-Weight Large
  Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition (Read more on [arXiv](https://arxiv.org/abs/2509.19768) or [HuggingFace](https://huggingface.co/papers/2509.19768))|  | The paper introduces CHURRO, an open-weight 3B-parameter Vision-Language Model (VLM) fine-tuned for high-accuracy, low-cost historical text recognition, along with a large-scale dataset, CHURRO-DS. The primary objective is to develop a specialized, cost-effective VLM that can accurately transcribe diverse historical documents, overcoming the limitations of general-purpose models designed for modern text. The methodology involves unifying 155 historical corpora into the CHURRO-DS dataset (99,491 pages) and using it to fine-tune a 3B-parameter Qwen 2.5 VL model. On the CHURRO-DS test set, the resulting CHURRO model achieves a 70.1% normalized Levenshtein similarity on handwritten documents, surpassing the much larger Gemini 2.5 Pro by 6.5% while being 15.5 times more cost-effective. For AI practitioners, this research demonstrates that fine-tuning a smaller open-weight VLM on a high-quality, domain-specific dataset can achieve superior performance and cost-efficiency compared to larger, general-purpose proprietary models for specialized vision-language tasks. |
| Finding 3D Positions of Distant Objects from Noisy Camera Movement and
  Semantic Segmentation Sequences (Read more on [arXiv](https://arxiv.org/abs/2509.20906) or [HuggingFace](https://huggingface.co/papers/2509.20906))| Eija Honkavaara, Arno Solin, Julppe1 | This paper proposes a particle filter-based method for 3D localisation of distant objects from a moving camera using noisy GNSS pose estimates and semantic segmentation sequences. The main objective is to iteratively estimate a target's 3D position and uncertainty in computationally constrained scenarios where traditional 3D reconstruction fails. The methodology employs a bootstrap particle filter that updates a distribution of 3D point hypotheses (particles) by re-weighting them based on their projection's proximity to segmented pixels in the camera frame, with an extension to handle multiple targets by initiating separate filters. Empirical validation using a drone to localise a telecommunication mast approximately 700 metres away achieved a minimum mean Root Mean Square Error (RMSE) of 76.88 metres. The principal implication for AI practitioners is that this lightweight, filter-based approach can be paired with any pre-existing, noisy segmentation model to enable real-time, on-board 3D geolocation for applications like wildfire monitoring without requiring computationally expensive depth estimation or feature-matching techniques. |
| Instruction-Following Evaluation in Function Calling for Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2509.18420) or [HuggingFace](https://huggingface.co/papers/2509.18420))| NikolaiSkripko | The paper introduces IFEval-FC, a benchmark for evaluating large language models' ability to follow precise formatting instructions embedded within JSON schema descriptions during function calling. The objective is to assess LLM reliability in adhering to verifiable format constraints specified in function parameter descriptions, a capability overlooked by existing benchmarks. The methodology involves a dataset of 750 test cases, each containing a function with a specific formatting instruction injected into a parameter's description field, with model outputs evaluated algorithmically via a binary adherence score. The results demonstrate that even state-of-the-art models fail to consistently follow these instructions, with the top-performing model achieving only 79.87% accuracy. The principal implication for AI practitioners is that LLMs in agentic systems are prone to generating syntactically invalid API calls due to poor format instruction adherence, mandating robust output validation and error handling for production deployment. |
