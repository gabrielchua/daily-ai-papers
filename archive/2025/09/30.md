

## Papers for 2025-09-30

| Title | Authors | Summary |
|-------|---------|---------|
| SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable
  Sparse-Linear Attention (Read more on [arXiv](https://arxiv.org/abs/2509.24006) or [HuggingFace](https://huggingface.co/papers/2509.24006))|  | This paper introduces Sparse-Linear Attention (SLA), a fine-tunable hybrid attention mechanism that combines sparse and linear attention to accelerate Diffusion Transformers. The primary objective is to reduce the quadratic computational cost of attention in long-sequence models, like those for video generation, without the quality degradation seen in purely sparse or linear methods. The key methodology involves dynamically classifying attention weight blocks into critical (computed with O(N²) attention), marginal (computed with O(N) linear attention), and negligible (skipped), fusing these operations into a single GPU kernel. The primary result shows that SLA reduces attention computation by 95% and achieves a 2.2x end-to-end speedup on the Wan2.1-1.3B video generation model, maintaining output quality comparable to full attention. The principal implication for AI practitioners is that they can significantly accelerate large-scale generative model inference and training with only a few fine-tuning steps, making high-resolution video generation more computationally tractable. |
| Multiplayer Nash Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2509.23102) or [HuggingFace](https://huggingface.co/papers/2509.23102))|  | The paper introduces Multiplayer Nash Preference Optimization (MNPO), a game-theoretic framework that generalizes two-player Nash learning to an n-player setting for LLM alignment. The primary objective is to develop an alignment method that models complex, non-transitive, and heterogeneous human preferences by framing the optimization problem as a multiplayer game, overcoming the single-opponent bias of existing Nash-based approaches. The core methodology involves each policy competing against a population of opponents to maximize its average preference probability, optimized via an iterative multiplicative weight update rule, with a practical variant (TD-MNPO) that uses a weighted mixture of historical policies as opponents. Empirically, MNPO achieves a win rate of 52.26 on the Arena-Hard benchmark, a 4.23-point improvement over the next-best baseline, INPO. For AI practitioners, MNPO offers a more robust and principled framework for aligning LLMs with complex preference data, leading to superior performance on instruction-following and reasoning tasks compared to standard preference optimization methods. |
| RealUnify: Do Unified Models Truly Benefit from Unification? A
  Comprehensive Benchmark (Read more on [arXiv](https://arxiv.org/abs/2509.24897) or [HuggingFace](https://huggingface.co/papers/2509.24897))| Yuran Wang, Yue Ding, zooblastlbz, THUdyh, DogNeverSleep | The paper introduces RealUnify, a comprehensive benchmark designed to evaluate whether unified multimodal models achieve genuine synergy between their visual understanding and generation capabilities. The main objective is to determine if architectural unification enables synergetic interaction between these constituent functions, a question unaddressed by existing benchmarks that assess them in isolation. A dual-evaluation protocol is used, which combines direct end-to-end assessment with a diagnostic stepwise evaluation that decomposes tasks into distinct understanding and generation phases to identify performance bottlenecks. The primary result from evaluating 12 unified models shows they struggle with synergy, as the best open-source model achieved only 37.5% accuracy on "Understanding Enhances Generation" tasks, far below a 72.7% upper bound established by an oracle combining specialist models. For AI practitioners, this implies that architectural unification alone is insufficient, highlighting the need for new training strategies and inductive biases to unlock the potential of unified modeling for complex tasks. |
| OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation
  and Editing (Read more on [arXiv](https://arxiv.org/abs/2509.24900) or [HuggingFace](https://huggingface.co/papers/2509.24900))| Huanyu Zhang, Chaoyou Fu, Xuehai Bai, Zhihong Chen, DogNeverSleep | This paper introduces OpenGPT-40-Image, an 80k-sample dataset with a hierarchical taxonomy of 51 subtasks designed to advance complex image generation and editing capabilities in multimodal models. The research aims to address the lack of systematic structure and challenging scenarios in existing datasets by creating a comprehensive resource for training models on tasks like scientific imagery and complex instruction following. The methodology involves an automated pipeline leveraging a hierarchical task taxonomy and the GPT-40 model to generate instruction-image pairs with controlled diversity and difficulty. The primary result is that fine-tuning models on this dataset yields significant performance gains, such as an 18.4% improvement for the UniWorld-V1 model on the ImgEdit-Bench editing benchmark. The principal implication for AI practitioners is that this systematically constructed dataset can be used to fine-tune models to better handle complex, multi-step instructions and specialized tasks, improving their robustness and applicability in real-world scenarios. |
| Visual Jigsaw Post-Training Improves MLLMs (Read more on [arXiv](https://arxiv.org/abs/2509.25190) or [HuggingFace](https://huggingface.co/papers/2509.25190))| Lewei Lu, Yushan Zhang, Penghao Wu, luodian, Paranioar | This paper introduces Visual Jigsaw, a self-supervised post-training framework that enhances the vision-centric understanding of Multimodal Large Language Models (MLLMs) by solving visual ordering problems. The research aims to improve an MLLM's visual perception without altering its architecture or output format. The key methodology involves partitioning visual inputs (images, videos, 3D data) into components, shuffling them, and training the MLLM using Reinforcement Learning with Verifiable Reward (RLVR) to reconstruct the correct order. The primary results demonstrate significant improvements across various benchmarks; for instance, the Image Jigsaw method increased the Qwen2.5-VL-7B model's score on the MMVP benchmark by +6.00 points. The principal implication for AI practitioners is that this lightweight, verifiable post-training task offers a practical method to boost the fine-grained perception, spatial reasoning, and temporal understanding of existing text-only output MLLMs without needing additional generative modules or pixel-level reconstruction. |
| SANA-Video: Efficient Video Generation with Block Linear Diffusion
  Transformer (Read more on [arXiv](https://arxiv.org/abs/2509.24695) or [HuggingFace](https://huggingface.co/papers/2509.24695))|  | The paper introduces SANA-Video, an efficient diffusion transformer model for generating high-resolution, minute-length videos with low computational overhead. The objective is to overcome the prohibitive computational cost and slow inference speeds of existing high-quality video generation models. The core methodology integrates a Linear Diffusion Transformer (Linear DiT) using O(N) linear attention instead of O(N^2) vanilla attention, and a Constant-Memory KV Cache for block linear attention, which enables a block-wise autoregressive approach for long video synthesis with fixed memory cost. SANA-Video achieves competitive performance while being 16x faster than comparable models, generating a 5-second 720p video in 36 seconds on an H100 GPU. For AI practitioners, this presents a framework for high-quality video generation with a training cost of only 12 days on 64 H100 GPUs that is deployable on consumer-grade hardware (RTX 5090), lowering the barrier for developing and applying advanced video synthesis. |
| Democratizing AI scientists using ToolUniverse (Read more on [arXiv](https://arxiv.org/abs/2509.23426) or [HuggingFace](https://huggingface.co/papers/2509.23426))|  | TOOLUNIVERSE is an open-source ecosystem designed to build and democratize AI scientists by standardizing how they interact with scientific tools. The primary objective is to create a unified and extensible infrastructure to overcome the limitations of bespoke, rigid AI systems, thereby enabling interoperability and reuse. The methodology centers on an AI-tool interaction protocol that standardizes tool discovery (`Find Tool`) and execution (`Call Tool`), integrating over 600 machine learning models, APIs, and scientific packages into a common framework. In a therapeutic discovery case study for hypercholesterolemia, an AI scientist built using TOOLUNIVERSE identified a potent drug analog (CHEMBL2347006/CHEMBL3970138) with favorable predicted properties, demonstrating the system's ability to automate complex scientific workflows. The principal implication for AI practitioners is that TOOLUNIVERSE provides a standardized, scalable framework to equip language models and agents with diverse, domain-specific tools, significantly reducing the engineering overhead required to build sophisticated AI research assistants. |
| When Does Reasoning Matter? A Controlled Study of Reasoning's
  Contribution to Model Performance (Read more on [arXiv](https://arxiv.org/abs/2509.22193) or [HuggingFace](https://huggingface.co/papers/2509.22193))|  | This paper conducts a large-scale controlled study using synthetic data distillation to systematically evaluate the performance and efficiency trade-offs between Instruction Fine-Tuning (IFT) and reasoning-based training for LLMs. The main objective is to determine the specific conditions—regarding task type, model scale, and computational cost—under which explicit reasoning training provides superior performance compared to standard IFT. The study employs a controlled distillation framework where a single teacher model generates 1.6M paired IFT and reasoning-based training samples to train student models of varying scales (0.5B to 14B), isolating the impact of the supervision format. The primary result is that reasoning-based training unlocks higher performance on open-ended and math tasks, enabling a 3B parameter reasoning model to match the accuracy of a 14B IFT model on several benchmarks. For AI practitioners, the principal implication is that for reasoning-intensive tasks, training models larger than 7B with reasoning data is justified to break performance plateaus, while for cost-sensitive applications or simpler tasks, scaling an IFT model remains the more efficient strategy. |
| GSM8K-V: Can Vision Language Models Solve Grade School Math Word
  Problems in Visual Contexts (Read more on [arXiv](https://arxiv.org/abs/2509.25160) or [HuggingFace](https://huggingface.co/papers/2509.25160))|  | This paper introduces GSM8K-V, a benchmark that transforms text-based math word problems from GSM8K into a purely visual, multi-image comic-style format to evaluate the mathematical reasoning capabilities of Vision Language Models (VLMs). The main research objective is to assess if VLMs can solve grade school math word problems when presented entirely through visual context and to measure the performance gap against their text-based reasoning abilities. The methodology involves a three-stage automated pipeline that decomposes mathematical information from GSM8K, generates multi-scene textual descriptions, and then uses an image generation model to create corresponding visual scenes, which are subsequently refined through human annotation. The primary result is a significant performance drop on the visual benchmark; for instance, the best-performing model, Gemini-2.5-Pro, achieves 95.22% accuracy on text-based GSM8K but only 46.93% on GSM8K-V. The principal implication for AI practitioners is that current VLMs exhibit a critical deficiency in visually grounded mathematical reasoning, indicating that models proficient in textual reasoning do not generalize well to equivalent visual problem formats, thus necessitating the development of more robust multimodal reasoning architectures. |
| EasySteer: A Unified Framework for High-Performance and Extensible LLM
  Steering (Read more on [arXiv](https://arxiv.org/abs/2509.25175) or [HuggingFace](https://huggingface.co/papers/2509.25175))|  | The paper introduces EasySteer, a unified framework for high-performance, extensible large language model (LLM) steering at inference time. The primary objective is to address the computational inefficiency, limited extensibility, and functional restrictions of existing steering frameworks. Methodologically, EasySteer is built on the vLLM inference engine, featuring a modular architecture with a non-intrusive model wrapper, pluggable interfaces for analytical and learning-based vector generation, and fine-grained parameter control mechanisms. The primary result shows that EasySteer achieves a 5.5-11.4× speedup over existing frameworks, reaching 3619.09 tokens/s in long-sequence batch inference compared to 652.63 tokens/s for the pyreft framework. For AI practitioners, EasySteer provides production-ready infrastructure to implement precise, on-the-fly behavioral control over LLMs in high-throughput serving environments without expensive retraining. |
| EditScore: Unlocking Online RL for Image Editing via High-Fidelity
  Reward Modeling (Read more on [arXiv](https://arxiv.org/abs/2509.23909) or [HuggingFace](https://huggingface.co/papers/2509.23909))|  | This paper develops EditScore, a high-fidelity reward model that enables stable online reinforcement learning (RL) for instruction-guided image editing. The main objective is to overcome the lack of a reliable and efficient reward signal, which has hindered the application of online RL to image editing. The methodology involves first creating EditReward-Bench, a benchmark for evaluating editing reward models, and then training EditScore, a series of reward models (7B-72B) on curated data, which leverages a self-ensembling strategy at inference time. The primary result is that the EditScore-72B model with self-ensembling surpasses GPT-5's pairwise accuracy on EditReward-Bench (0.763 vs. 0.755), and its application in RL training improves the OmniGen2 base model's overall score on GEdit-Bench-EN by +0.40. The principal implication for AI practitioners is that they can use the open-source EditScore as a robust reward signal to successfully apply online RL for fine-tuning image editing models, a task where general-purpose VLMs were demonstrated to fail and cause training instability. |
| SparseD: Sparse Attention for Diffusion Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.24014) or [HuggingFace](https://huggingface.co/papers/2509.24014))| Xinchao Wang, Xinyin Ma, Gongfan Fang, adamdad, INV-WZQ | SparseD is a novel sparse attention method that accelerates Diffusion Language Models (DLMs) by leveraging their unique attention characteristics to reduce computational overhead with minimal accuracy loss. The primary research objective is to mitigate the high inference latency in DLMs caused by the quadratic complexity of bidirectional attention, particularly in long-context scenarios. The key methodology involves a three-part strategy: (1) applying full attention for an initial percentage of denoising steps to preserve generation quality, (2) pre-computing head-specific sparse attention patterns only once after this initial phase, and (3) reusing these static patterns for all subsequent denoising steps. The primary result shows that SparseD achieves lossless acceleration, delivering up to a 1.50× speedup over FlashAttention on a model with a 64k context length and 1,024 denoising steps. For AI practitioners, this provides an effective, practical method to deploy DLMs in long-context applications with significantly reduced inference latency without the performance degradation associated with methods designed for autoregressive models or existing DLM caching techniques. |
| Sequential Diffusion Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.24007) or [HuggingFace](https://huggingface.co/papers/2509.24007))|  | The paper introduces Sequential Diffusion Language Models (SDLMs), a method to retrofit pretrained autoregressive language models for faster, dynamic-length parallel decoding. The primary objective is to unify autoregressive and diffusion-based generation to overcome the limitations of fixed-length decoding in diffusion models and the high training costs of existing hybrid approaches. The key methodology involves Next Sequence Prediction (NSP), which generalizes next-token and next-block prediction, implemented via a parallel block training scheme that uses a custom bidirectional attention mask to enable dynamic decoding based on model confidence. The primary result shows that SDLM-32B, trained on only 3.5M samples, achieves a 92.4 score on GSM8K, matching its autoregressive counterpart while achieving up to 2.1x higher throughput than the Qwen-2.5-3B model. For AI practitioners, the principal implication is that existing pretrained autoregressive models can be adapted with minimal fine-tuning to gain significant inference acceleration without a substantial trade-off in performance, offering an efficient alternative to training new models from scratch. |
| Towards Personalized Deep Research: Benchmarks and Evaluations (Read more on [arXiv](https://arxiv.org/abs/2509.25106) or [HuggingFace](https://huggingface.co/papers/2509.25106))|  | This paper introduces the Personalized Deep Research Bench and the PQR Evaluation Framework to measure the ability of Deep Research Agents (DRAs) to tailor outputs to specific user profiles. The primary objective is to systematically evaluate how effectively DRAs adapt complex research, reasoning, and reporting to individual user personas, a dimension neglected by existing evaluation methodologies. The methodology involves a benchmark of 250 queries (50 tasks paired with 25 user profiles) and an LLM-as-a-judge PQR framework that assesses (P) Personalization Alignment, (Q) Content Quality, and (R) Factual Reliability. Experiments reveal a trade-off between capabilities, with open-source systems like OAgents achieving the highest overall personalization score (6.64) but commercial systems demonstrating superior factual accuracy. The principal implication for AI practitioners is that building effective personalized DRAs requires architectures that move beyond simple search tool integration to jointly optimize for user alignment and factual reliability, as current systems show a significant performance gap between these two functions. |
| Random Policy Valuation is Enough for LLM Reasoning with Verifiable
  Rewards (Read more on [arXiv](https://arxiv.org/abs/2509.24981) or [HuggingFace](https://huggingface.co/papers/2509.24981))| Binxing Jiao, Chen Hu, Qingpeng Cai, Yuxiao Ye, Haoran He | i) This paper introduces ROVER, a minimalist Reinforcement Learning (RL) algorithm that improves LLM reasoning by valuing a fixed random policy instead of using complex policy iteration. ii) The objective is to design a simpler, more effective RL with Verifiable Rewards (RLVR) algorithm by exploiting the deterministic, tree-structured Markov Decision Process (MDP) specific to LLM reasoning tasks, thereby avoiding the instability and complexity of standard methods like PPO. iii) ROVER's methodology bypasses generalized policy iteration by first proving that the Q-function of a fixed uniform policy is sufficient for optimal action recovery in this specific MDP structure, and then samples actions from a softmax distribution over these Q-values to balance quality and diversity. iv) The primary results show that ROVER achieves superior performance over strong baselines, yielding a +8.2 improvement on pass@1 for competition-level math tasks and a +17.6% increase in solution diversity. v) The principal implication for AI practitioners is that complex, unstable algorithms like PPO can be replaced with the simpler, more robust, and higher-performing ROVER framework for fine-tuning LLMs on verifiable reasoning tasks, reducing implementation overhead and mitigating diversity collapse. |
| Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach
  for LLM Reasoning in RLVR (Read more on [arXiv](https://arxiv.org/abs/2509.23808) or [HuggingFace](https://huggingface.co/papers/2509.23808))| Cevaaa, MasterVito, AnikiFan, Gambel, Niugan | This paper introduces Velocity-Exploiting Rank-Learning (VERL), a method that challenges the exploration-exploitation trade-off in LLM reasoning by operating on hidden-state dynamics instead of token-level metrics. The research aims to determine if this trade-off is an artifact of measurement and to develop a method that simultaneously enhances both exploration and exploitation in Reinforcement Learning for Verifiable Rewards (RLVR). The key methodology involves quantifying exploration using Effective Rank (ER) of hidden-state matrices and defining exploitation via its novel derivatives: Effective Rank Velocity (ERV) and Effective Rank Acceleration (ERA). The VERL algorithm then uses the theoretically stable ERA as a predictive meta-controller to adaptively shape the RL advantage function, creating a dual-channel incentive structure. The primary result is the empirical demonstration that exploration and exploitation are decoupled at the hidden-state level, and VERL achieves significant performance gains, including up to a 21.4% absolute accuracy improvement on the Gaokao 2024 dataset. For AI practitioners, the principal implication is that they can augment existing RL algorithms like PPO or GRPO with VERL to synergistically improve a model's ability to discover diverse reasoning paths and consolidate correct ones, leading to enhanced generalization on complex reasoning tasks. |
| VideoScore2: Think before You Score in Generative Video Evaluation (Read more on [arXiv](https://arxiv.org/abs/2509.22799) or [HuggingFace](https://huggingface.co/papers/2509.22799))|  | This paper introduces VideoScore2, a multi-dimensional, interpretable framework for evaluating text-to-video generation by providing scores and chain-of-thought rationales. The objective is to develop a human-aligned evaluator that assesses visual quality, text alignment, and physical consistency, overcoming the limitations of single, opaque scores. The methodology involves a two-stage training pipeline on a new 27,168-sample dataset (VIDEOFEEDBACK2): initial supervised fine-tuning for basic reasoning, followed by reinforcement learning with Group Relative Policy Optimization (GRPO) to enhance analytical robustness. VideoScore2 achieves state-of-the-art performance, demonstrating 44.35% accuracy on its in-domain benchmark (a +5.94% improvement over the best baseline) and strong generalization on out-of-domain benchmarks. For AI practitioners, VideoScore2 serves as a more effective reward model for guiding controllable generation through methods like Best-of-N sampling, providing actionable, interpretable feedback for model development. |
| From f(x) and g(x) to f(g(x)): LLMs Learn New Skills in RL by
  Composing Old Ones (Read more on [arXiv](https://arxiv.org/abs/2509.25123) or [HuggingFace](https://huggingface.co/papers/2509.25123))| Hanbin Wang, Ganqu Cui, Yuchen Zhang, Weize Chen, Lifan Yuan | This research demonstrates that Large Language Models can acquire new, generalizable skills by learning to compose existing atomic skills through reinforcement learning when explicitly incentivized with compositional tasks. The main research question is whether reinforcement learning (RL) teaches LLMs genuinely new skills, specifically compositionality, or if it merely activates pre-existing capabilities. The methodology involves a two-stage training protocol on a controlled string transformation task: first, an LLM acquires atomic skills via rejection fine-tuning (RFT), then it is trained on compositional problems using either RL with outcome-based rewards or RFT, without access to the underlying function definitions. The primary result is that RL on Level-2 compositional tasks enables generalization to more complex, unseen problems, with performance on Level-3 tasks improving from near-zero to approximately 30%, whereas RFT on the same data yields negligible improvement. The principal implication for AI practitioners is that developing advanced, generalizable reasoning requires a strategic training approach: first establish a foundation of atomic skills in a base model, then use RL with explicit compositional incentives to teach the model how to combine those skills to solve more complex problems. |
| Euclid's Gift: Enhancing Spatial Perception and Reasoning in
  Vision-Language Models via Geometric Surrogate Tasks (Read more on [arXiv](https://arxiv.org/abs/2509.24473) or [HuggingFace](https://huggingface.co/papers/2509.24473))|  | This paper demonstrates that fine-tuning vision-language models on Euclidean geometry problems, as a surrogate task, significantly enhances their generalizable spatial reasoning capabilities. The research investigates whether training on a curated dataset of geometric problems can instill foundational spatial priors that improve zero-shot performance on diverse, unseen spatial intelligence tasks. The authors constructed `Euclid30K`, a dataset of ~30K geometry problems, and used Group Relative Policy Optimization (GRPO) to fine-tune the Qwen2.5VL and RoboBrain2.0 model families. After training, the `RoboBrain2.0-Euclid-7B` model achieved 49.6% accuracy on VSI-Bench, improving by 6.6 percentage points over its baseline and surpassing the previous state-of-the-art spatial model. The principal implication for AI practitioners is that using compact, principle-driven surrogate datasets can be a more effective strategy for developing transferable, foundational skills in MLLMs than training on larger, task-specific datasets. |
| Critique-Coder: Enhancing Coder Models by Critique Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2509.22824) or [HuggingFace](https://huggingface.co/papers/2509.22824))|  | This paper introduces Critique Reinforcement Learning (CRL), a novel paradigm to enhance code generation models, and a resulting model, CRITIQUE-CODER. The primary objective is to determine if complementing standard reinforcement learning (RL) with an explicit critique-learning signal improves a model's coding and general reasoning abilities. The methodology involves training a model with Group Relative Policy Optimization (GRPO) on a hybrid dataset where 20% of standard RL tasks (generating solutions) are replaced with CRL tasks (judging existing solutions). The key result is that CRITIQUE-CODER-8B achieves 60.8% on LiveCodeBench (v5), outperforming the RL-only baseline, and also demonstrates a +6.1 point average improvement over the base model on the BBEH logic reasoning benchmark. For AI practitioners, the principal implication is that augmenting standard RL fine-tuning with a small proportion of critique-based training can significantly boost not only task-specific performance but also transferable reasoning capabilities in large language models. |
| StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient
  SpeechLLMs (Read more on [arXiv](https://arxiv.org/abs/2509.22220) or [HuggingFace](https://huggingface.co/papers/2509.22220))| Wei Jia, Aiwei Liu, Chuhan Wu, Linhao Zhang, QbethQ | This paper introduces StableToken, a semantic speech tokenizer that achieves state-of-the-art noise robustness for building resilient SpeechLLMs. The research objective is to overcome the fragility of existing tokenizers whose discrete outputs are unstable against meaning-irrelevant acoustic perturbations, which increases the learning burden for downstream models. The key methodology involves a multi-branch quantizer architecture (Voting-LFQ) that processes audio in parallel and merges representations via a differentiable bit-wise majority vote, coupled with a Noise-Aware Consensus Training strategy that forces alignment between clean and perturbed audio views. StableToken sets a new state-of-the-art in token stability, reducing the average Unit Edit Distance (UED) under diverse noise conditions to 10.17%, a relative reduction of over 60% compared to the best supervised baseline. For AI practitioners, this work provides a tokenizer that directly enhances the robustness of SpeechLLMs in noisy, real-world conditions, improving downstream task performance without compromising reconstruction fidelity. |
| VGGT-X: When VGGT Meets Dense Novel View Synthesis (Read more on [arXiv](https://arxiv.org/abs/2509.25191) or [HuggingFace](https://huggingface.co/papers/2509.25191))| Zhaoxiang Zhang, Junran Peng, Zimo Tang, Chuanchen Luo, Yang Liu | This paper presents VGGT-X, a framework for adapting the VGGT 3D foundation model to dense novel view synthesis by addressing its inherent scalability and accuracy limitations. The research aims to resolve the primary obstacles—prohibitive VRAM consumption and noisy initial predictions—that hinder the application of 3D foundation models to dense image sets (1,000+ images) for 3D Gaussian Splatting. The methodology integrates a memory-efficient VGGT implementation (using feature reduction and mixed-precision), an adaptive global alignment strategy for pose refinement using XFeat correspondences, and robust MCMC-3DGS training with joint pose optimization. The framework achieves state-of-the-art results for COLMAP-free NVS, improving pose estimation AUC@30 on the MipNeRF360 dataset from 0.951 to 0.992 and significantly reducing the rendering quality gap compared to COLMAP-initialized methods. For AI practitioners, the principal implication is that with targeted memory optimizations and a robust global alignment post-processing step, large 3D foundation models become a viable and dramatically faster alternative to traditional Structure-from-Motion pipelines for initializing dense 3D reconstruction tasks. |
| BRIDGE - Building Reinforcement-Learning Depth-to-Image Data Generation
  Engine for Monocular Depth Estimation (Read more on [arXiv](https://arxiv.org/abs/2509.25077) or [HuggingFace](https://huggingface.co/papers/2509.25077))|  | BRIDGE is a framework that uses a reinforcement learning-optimized engine to generate a massive (20M+) synthetic RGB-D dataset for training high-performance monocular depth estimation models. The primary objective is to overcome the limitations of data scarcity and quality in monocular depth estimation by creating a large-scale, high-fidelity, and diverse synthetic dataset. The methodology involves an RL-optimized Depth-to-Image (D2I) model that generates 20 million realistic RGB images from source depth maps, followed by a hybrid supervision strategy that combines teacher-generated pseudo-labels with high-precision ground truth depth in similarity-masked regions to train a DINOv2-based MDE model. BRIDGE achieves state-of-the-art zero-shot performance, attaining a δ1 accuracy of 0.982 and an absolute relative error of 0.041 on the NYUv2 dataset, which surpasses Depth Anything V2's 0.979 and 0.045 respectively, while using significantly less training data (20M vs. 62M). For AI practitioners, this RL-based data generation and hybrid supervision approach provides a blueprint for creating massive, high-quality, geometrically consistent training datasets for vision tasks, reducing dependency on costly real-world data collection and improving model performance and training efficiency. |
| Rolling Forcing: Autoregressive Long Video Diffusion in Real Time (Read more on [arXiv](https://arxiv.org/abs/2509.25161) or [HuggingFace](https://huggingface.co/papers/2509.25161))|  | The paper introduces Rolling Forcing, a novel technique for real-time, autoregressive long video generation that significantly reduces error accumulation over extended durations. The objective is to generate high-quality, low-latency, and temporally coherent long video streams while mitigating the severe error accumulation that plagues existing autoregressive methods. The core methodology integrates three components: a rolling-window joint denoising scheme that processes multiple frames simultaneously with progressive noise, an attention sink mechanism that caches initial frames as a global context anchor, and an efficient few-step distillation training algorithm on non-overlapping windows. Extensive experiments demonstrate that Rolling Forcing enables real-time generation at 15.79 FPS on a single GPU with a substantially reduced quality drift score of 0.01, compared to a baseline of 1.66 from Self Forcing. For AI practitioners, this work provides a framework for developing interactive long-form video applications, like neural game engines, that can maintain high temporal consistency over multi-minute sequences. |
| MMPB: It's Time for Multi-Modal Personalization (Read more on [arXiv](https://arxiv.org/abs/2509.22820) or [HuggingFace](https://huggingface.co/papers/2509.22820))|  | This paper introduces MMPB, a comprehensive benchmark for evaluating the personalization capabilities of Vision-Language Models (VLMs) on concept recognition and preference-grounded reasoning. The primary objective is to systematically quantify the ability of VLMs to adapt to individual user contexts, a capability largely unexplored by existing general-purpose VQA benchmarks. The methodology involves creating the MMPB dataset, which contains over 10,000 image-query pairs across 111 personalizable concepts, and using it to evaluate 23 VLMs through a three-stage protocol of concept injection, multi-turn dialogue, and personalized querying. The primary result shows that most VLMs struggle with personalization, with closed-source models achieving an average accuracy of 51.4%, underperforming open-source models (59.9%), and both types exhibiting significant performance drops in multi-turn conversation settings. For AI practitioners, the key implication is that current VLMs are not suitable for personalized applications out-of-the-box, as they show systemic failures in user-centric reasoning and are prone to evasive behaviors due to safety alignments, requiring specialized fine-tuning or architectural modifications. |
| InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long
  Adaptation (Read more on [arXiv](https://arxiv.org/abs/2509.24663) or [HuggingFace](https://huggingface.co/papers/2509.24663))| Yuxuan Li, Chaojun Xiao, Zhou Su, Zihan Zhou, Weilin Zhao | InfLLM-V2 is a dense-sparse switchable attention framework that enables seamless adaptation of language models from short to long sequence processing by reusing existing dense attention parameters. The primary objective is to resolve the architectural mismatch, parameter overhead, and training instability of prior trainable sparse attention methods within the standard pretrain-on-short, finetune-on-long workflow. The methodology involves a parameter-free architectural modification that unifies sparse attention patterns, eliminates gating modules, and uses a hardware-aware, 3-stage block compression process to efficiently select relevant context blocks. The primary result is that InfLLM-V2 is 4x faster than dense attention while retaining 98.1% of the performance on long-context understanding benchmarks and 99.7% on chain-of-thought reasoning. For AI practitioners, this framework provides a practical and efficient method to adapt existing pretrained models for long-context tasks, significantly accelerating inference without substantial performance loss or complex architectural changes. |
| Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference
  Learning (Read more on [arXiv](https://arxiv.org/abs/2509.23285) or [HuggingFace](https://huggingface.co/papers/2509.23285))|  | This paper introduces Tool-Light, a framework utilizing self-evolved preference learning to improve the efficiency and accuracy of Tool-Integrated Reasoning (TIR) in large language models. The main objective is to incentivize LLMs to perform TIR effectively by mitigating suboptimal behaviors like excessive tool usage or overthinking after tool calls. The core methodology is a two-stage training pipeline featuring Supervised Fine-Tuning (SFT) and a multi-round Self-Evolved Direct Preference Optimization (DPO) process, which uses a novel entropy-guided sampling strategy to generate positive-negative reasoning paths. On a suite of 10 reasoning datasets, the Tool-Light trained Qwen2.5-7B model achieved a state-of-the-art average performance of 58.0, outperforming the strong Tool-Star baseline's score of 56.6. For AI practitioners, this work provides a concrete framework to fine-tune agentic models that can use external tools more efficiently and necessarily, reducing redundant computations while improving reasoning accuracy. |
| MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech (Read more on [arXiv](https://arxiv.org/abs/2509.25131) or [HuggingFace](https://huggingface.co/papers/2509.25131))|  | MGM-Omni is a unified Omni LLM designed for omnimodal understanding and personalized, long-horizon speech generation using a dual-track architecture. The objective is to overcome the limitations of cascaded systems in processing and generating long-form audio by addressing challenges in contextual coherence, timbre consistency, and synthesis latency. The model employs a "brain-mouth" design that decouples a multimodal reasoning LLM from a speech synthesis LM and introduces a Chunk-Based Parallel Decoding mechanism to align text and speech token rates. MGM-Omni demonstrates superior long-form audio understanding, achieving a 94% success rate on a needle-in-the-haystack test with audio up to 4,500 seconds, compared to Qwen2.5-Omni's 58%. For AI practitioners, this work provides a data-efficient, end-to-end paradigm for building robust omnimodal systems capable of long-form, personalized speech interaction without the high latency and error accumulation of separate text-to-speech pipelines. |
| HunyuanImage 3.0 Technical Report (Read more on [arXiv](https://arxiv.org/abs/2509.23951) or [HuggingFace](https://huggingface.co/papers/2509.23951))|  | The report introduces HunyuanImage 3.0, an open-source, 80-billion-parameter Mixture-of-Experts (MoE) model that unifies multimodal understanding and generation within a single autoregressive framework. The main objective is to develop a native multimodal model with performance that rivals state-of-the-art closed-source systems by leveraging a pre-trained MoE LLM, meticulous data curation, and a native Chain-of-Thoughts schema. The methodology involves a hybrid architecture combining autoregressive prediction for text with diffusion-based modeling for images, managed by a novel "Generalized Causal Attention" mechanism, and refined through progressive multi-stage pre-training and extensive post-training (SFT, DPO, MixGRPO, SRPO). In human GSB (Good/Same/Bad) evaluations, HunyuanImage 3.0 achieved a relative win rate of 14.10% against the previous best open-source model, HunyuanImage 2.1, and demonstrated comparable quality to leading commercial models. The principal implication for AI practitioners is the public release of a powerful, state-of-the-art foundation model and a detailed technical blueprint, providing a robust open-source alternative for developing advanced, unified multimodal applications. |
| Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs
  at Test Time (Read more on [arXiv](https://arxiv.org/abs/2509.22572) or [HuggingFace](https://huggingface.co/papers/2509.22572))| Yi Yang, Ruijie Quan, Fan Ma, yixuan7878 | This paper introduces Dynamic Experts Search (DES), a test-time scaling strategy that improves the reasoning of Mixture-of-Experts (MoE) LLMs by treating the number of activated experts as a controllable search dimension. The main objective is to investigate if dynamically varying the number of activated experts in MoE models during inference can serve as a new source of solution diversity, enhancing reasoning performance beyond architecture-agnostic, sampling-based test-time scaling methods. The key methodology integrates two components: *Dynamic MoE*, which allows for direct control over the expert count per inference pass, and *Expert Configuration Inheritance*, which maintains a consistent expert count within a single reasoning trajectory while exploring different counts across parallel search paths, guided by an external verifier. Primary results show that on the MATH500 benchmark, DES with the Qwen3-30B-A3B-Instruct model achieved 93.20% accuracy, outperforming baselines such as Best-of-N (92.40%) and BeamSearch (93.00%) at a comparable computational cost. The principal implication for AI practitioners is that the reasoning performance of deployed MoE models can be significantly enhanced at inference time by treating the number of active experts as a tunable search parameter, providing an effective, architecture-aware alternative to simple output sampling for test-time compute scaling. |
| SIRI: Scaling Iterative Reinforcement Learning with Interleaved
  Compression (Read more on [arXiv](https://arxiv.org/abs/2509.25176) or [HuggingFace](https://huggingface.co/papers/2509.25176))|  | The paper introduces SIRI, a reinforcement learning approach to improve reasoning accuracy and token efficiency in Large Reasoning Models (LRMs). The primary objective is to overcome the trade-off between reducing repetitive thinking and maintaining high performance. The key methodology is an iterative training regime that alternates between a compression phase, which shortens the maximum rollout length to force dense reasoning, and an expansion phase, which relaxes the length limit to encourage exploration. After three iterations on DeepSeek-R1-Distill-Qwen-1.5B, the SIRI-low variant improved performance on the AIME24 benchmark by 43.2% while reducing token usage by 46.9%. The principal implication for AI practitioners is that periodically oscillating the output length constraint during RL training is an effective technique for pushing models toward the Pareto frontier of performance and efficiency. |
| Scaling Generalist Data-Analytic Agents (Read more on [arXiv](https://arxiv.org/abs/2509.25084) or [HuggingFace](https://huggingface.co/papers/2509.25084))|  | The paper introduces DATAMIND, a scalable data synthesis and agent training recipe designed to construct high-performance, generalist data-analytic agents from open-source models. The primary objective is to overcome key challenges in open-source agent development, including insufficient data resources, improper training strategies, and unstable code-based multi-turn rollouts. The methodology involves creating the DATAMIND-12K dataset through a fine-grained task taxonomy and self-consistency filtered trajectory sampling, followed by training an agent using a dynamically weighted objective that combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) losses. This approach results in the DATAMIND-14B model achieving a state-of-the-art average score of 71.16% on multiple data analysis benchmarks, outperforming strong proprietary baselines like GPT-5 and DeepSeek-V3.1. For AI practitioners, this work provides a validated, scalable pipeline for building specialized open-source agents that can achieve superior performance, demonstrating that curated data synthesis and a hybrid SFT-RL training strategy can effectively close the performance gap with proprietary systems. |
| From Harm to Help: Turning Reasoning In-Context Demos into Assets for
  Reasoning LMs (Read more on [arXiv](https://arxiv.org/abs/2509.23196) or [HuggingFace](https://huggingface.co/papers/2509.23196))| Nie Zheng, Zihang Fu, Weida Liang, Haonan Wang, tyzhu | This paper introduces Insight-to-Solve (I2S), a test-time framework that converts detrimental few-shot CoT demonstrations into beneficial assets for Reasoning Large Models (RLMs) by decoupling insight extraction from solution generation. The central objective is to understand why high-quality in-context demonstrations often degrade RLM performance and to develop a method that effectively harnesses these demonstrations to improve reasoning. The proposed I2S method is a multi-step procedure that generates a comparison between a demonstration and a target question, extracts abstract reasoning strategies, and then applies these insights to solve the target question independently, with an optional iterative self-refinement step (I2S+). The method consistently improves performance over direct inference, boosting GPT-4.1's accuracy on the AIME'25 benchmark by +14.0%. For AI practitioners, this research provides a concrete prompting framework to mitigate the common failure modes of few-shot CoT—"semantic misguidance" and "strategy transfer failure"—enabling more reliable use of in-context examples for complex reasoning tasks without retraining. |
| Rethinking Large Language Model Distillation: A Constrained Markov
  Decision Process Perspective (Read more on [arXiv](https://arxiv.org/abs/2509.22921) or [HuggingFace](https://huggingface.co/papers/2509.22921))|  | The paper introduces a constrained reinforcement learning framework for LLM distillation that maximizes task rewards while strictly enforcing a divergence threshold from the teacher model. The main objective is to develop a principled method for reward-aware distillation that avoids ad-hoc reward weighting by maximizing task performance subject to a hard constraint on student-teacher policy divergence. The key methodology formulates distillation as a Constrained Markov Decision Process (CMDP) and uses a modified reward function within a policy gradient algorithm, which penalizes trajectories that exceed a predefined cumulative KL-divergence budget, thereby removing the need for state augmentation or teacher access during deployment. Experiments on mathematical reasoning tasks show the method achieves a high Reasoning Win Rate (60.55% for Qwen2.5-1.5B on the Apple/GSM-Symbolic dataset) and constraint satisfaction (96.1%), significantly outperforming Lagrangian relaxation baselines in reasoning quality. For AI practitioners, this provides a more stable and theoretically-grounded approach to distill smaller, reliable models by offering direct control over the trade-off between task performance and teacher fidelity, which is critical for deployment in resource-constrained settings. |
| Taming Masked Diffusion Language Models via Consistency Trajectory
  Reinforcement Learning with Fewer Decoding Step (Read more on [arXiv](https://arxiv.org/abs/2509.23924) or [HuggingFace](https://huggingface.co/papers/2509.23924))|  | This paper introduces decoding and reinforcement learning techniques to enhance the performance and efficiency of Masked Diffusion Language Models (MDLMs). The research objective is to resolve sub-optimal full diffusion decoding and the training-inference inconsistency that arises when applying autoregressive-style reinforcement learning (RL) to non-causal MDLMs. The key methodology involves three components: EOS Early Rejection (EOSER) to prevent premature sequence termination, an Ascending Step-Size (ASS) scheduler to reduce decoding steps to O(log₂L), and Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO) to align the non-causal rollout and optimization trajectories. On the Sudoku task (generation length 256), the proposed CJ-GRPO with EOSER achieved 85.37% accuracy, substantially outperforming the 18.85% of a baseline diffu-GRPO approach. For AI practitioners, this work provides a validated framework to effectively fine-tune and accelerate inference for MDLMs, addressing critical trajectory inconsistencies and making them more viable for complex reasoning tasks. |
| Efficient Multi-turn RL for GUI Agents via Decoupled Training and
  Adaptive Data Curation (Read more on [arXiv](https://arxiv.org/abs/2509.23866) or [HuggingFace](https://huggingface.co/papers/2509.23866))|  | This paper introduces DART, a decoupled reinforcement learning framework with adaptive data curation to efficiently train vision-language model-based GUI agents. The research objective is to resolve the significant training inefficiencies in RL for GUI agents caused by slow, tightly-coupled environment interactions and insufficient high-quality training data. The methodology combines a system architecture with four asynchronous modules (environment cluster, rollout service, data manager, trainer) and a multi-level data curation scheme that includes dynamic sampling, high-entropy step prioritization, and truncated importance sampling. On the OS-World benchmark, the DART-GUI-7B model achieved a 42.13% task success rate, representing a 14.61% absolute improvement over the baseline, while the framework increased training throughput by 1.9x and environment utilization by 5.5x. For AI practitioners, this framework provides a reusable blueprint for scaling RL training for agentic systems by decoupling components to maximize resource utilization and curating data to focus learning on critical decision points. |
| Hyperspherical Latents Improve Continuous-Token Autoregressive
  Generation (Read more on [arXiv](https://arxiv.org/abs/2509.24335) or [HuggingFace](https://huggingface.co/papers/2509.24335))| Hui Xue, guolinke | SphereAR improves continuous-token autoregressive (AR) image generation by constraining latent representations to a fixed-radius hypersphere. The objective is to mitigate the variance collapse that occurs in continuous-token AR models due to heterogeneous variance in VAE latents being amplified by classifier-free guidance (CFG) during decoding. The methodology involves coupling a hyperspherical VAE (S-VAE), which encodes image patches into constant-norm latent tokens, with a causal Transformer whose inputs and outputs are persistently projected back onto the hypersphere to maintain scale-invariance. On ImageNet 256x256 class-conditional generation, the SphereAR-H (943M) model achieves a state-of-the-art FID score of 1.34 for AR models, outperforming larger baselines. The principal implication for AI practitioners is that enforcing scale-invariance in the latent space via hyperspherical constraints is a critical design choice for stabilizing AR decoding and building high-performance continuous-token generative systems. |
| AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced
  Self-Play (Read more on [arXiv](https://arxiv.org/abs/2509.24193) or [HuggingFace](https://huggingface.co/papers/2509.24193))| Yue Yu, Jonathan Wang, Zihan Dong, Yuchen Zhuang, Ran Xu | AceSearcher is a cooperative self-play framework that improves LLM reasoning and multi-hop search by training a single model to act as both a query decomposer and a context-integrating solver. The objective is to enhance the complex reasoning and multi-hop retrieval capabilities of search-augmented LLMs without relying on intermediate supervision or costly inference-time search algorithms. The key methodology is a two-stage training process: first, supervised fine-tuning (SFT) on a diverse mix of QA, decomposition, and reasoning datasets, followed by iterative, preference-based reinforcement fine-tuning (RFT) using Direct Preference Optimization (DPO), where rewards are derived solely from final answer accuracy. Across 10 reasoning-intensive datasets, AceSearcher outperformed state-of-the-art baselines, achieving an average exact match improvement of 7.6%; notably, the 32B parameter version matched the performance of the significantly larger DeepSeek-V3 model on document-level reasoning tasks while using less than 5% of its parameters. The principal implication for AI practitioners is that this self-play and two-stage fine-tuning recipe enables the development of highly capable and parameter-efficient search-augmented LLMs for complex reasoning, reducing reliance on extremely large or proprietary models and eliminating the need for expensive intermediate-step annotations. |
| Pretraining Large Language Models with NVFP4 (Read more on [arXiv](https://arxiv.org/abs/2509.25149) or [HuggingFace](https://huggingface.co/papers/2509.25149))|  | This paper introduces a methodology for stable and accurate large-scale language model (LLM) pretraining using the NVFP4 format. The primary objective was to demonstrate the feasibility of 4-bit floating point training for LLMs, addressing stability and convergence challenges to improve computational efficiency and resource utilization. Key methodological components include preserving numerically sensitive layers in higher precision, applying Random Hadamard Transforms to inputs of weight gradient GEMMs, employing two-dimensional block scaling for weights, and utilizing stochastic rounding for gradients. This approach successfully trained a 12-billion-parameter model on 10 trillion tokens, achieving an MMLU-pro accuracy of 62.58%, closely matching the 62.62% accuracy of an FP8 baseline. For AI practitioners, this work provides a practical path to significantly reduce LLM pretraining computational cost and memory footprint, enabling more efficient development of next-generation models. |
| SCI-Verifier: Scientific Verifier with Thinking (Read more on [arXiv](https://arxiv.org/abs/2509.24285) or [HuggingFace](https://huggingface.co/papers/2509.24285))| Jingqi Ye, Junchi Yao, Fangchen Yu, Chenyu Huang, desimfj | This paper introduces SCI-Verifier, a reasoning-augmented verifier, and SCI-VerifyBench, a cross-disciplinary benchmark, for scientific verification in LLMs. The main objective is to address limitations in existing scientific verification methods by establishing a systematic evaluation framework and developing a robust, reasoning-augmented verifier. The methodology involves constructing SCI-VerifyBench through collecting LLM responses, applying domain-specific equivalence transformations, and combining model/expert annotations, and developing SCI-Verifier using a two-stage post-training pipeline: Supervised Fine-Tuning with filtered reasoning traces and Reinforcement Learning. Experiments on SCI-VerifyBench demonstrate that SCI-Verifier-8B achieves 86.28% total accuracy, outperforming existing open-source models and matching closed-source models like GPT-5 in verification performance on scientific tasks, especially for complex equivalence-based answers. This work provides AI practitioners with a precise evaluation framework and practical guidance, emphasizing the importance of integrating logical reasoning to enhance LLM capabilities and reliability in scientific domains. |
| Alignment through Meta-Weighted Online Sampling: Bridging the Gap
  between Data Generation and Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2509.23371) or [HuggingFace](https://huggingface.co/papers/2509.23371))| Xin Geng, Shiqi Qiao, Biao Liu, Ning Xu, jmyang | MetaAPO is a novel framework that bridges the gap between LLM data generation and preference optimization using a meta-weighted online sampling strategy. The primary objective is to mitigate the distribution mismatch between static offline preference data and the dynamic, evolving policy by adaptively coupling online data generation and model training. MetaAPO utilizes a lightweight two-layer MLP meta-learner to estimate the "alignment gap," guiding targeted online response generation and assigning dynamic sample-wise meta-weights to a hybrid loss function that balances offline and online data contributions. Experiments show MetaAPO consistently outperforms baselines, for example, reducing online annotation requirements by 42% and achieving a 47.48% win rate on AlpacaEval 2 for Llama-3.1-8B, compared to Online DPO's 43.75%. This approach offers AI practitioners a more efficient and robust method for LLM alignment, enabling superior performance while significantly lowering the resource and time costs associated with online data acquisition and model training. |
| WirelessMathLM: Teaching Mathematical Reasoning for LLMs in Wireless
  Communications with Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.23219) or [HuggingFace](https://huggingface.co/papers/2509.23219))| Li Wei, Wenhe Zhang, Yiyang Zhu, Mengbing Liu, XINLI1997 | WirelessMathLM teaches compact LLMs mathematical reasoning for wireless communications using verification-based reinforcement learning. The research objective was to enable LLMs to achieve expert-level performance in specialized wireless mathematics, where current state-of-the-art models struggle. The methodology involved training models (0.5B-7B parameters) with Group Relative Policy Optimization (GRPO) and binary verification rewards on WirelessMathBench-XL, a benchmark of 4,027 problems from 970 papers. The 7B WirelessMathLM achieved 39.5% accuracy on WirelessMathBench-XL, approaching GPT-40 (40.4%) while using approximately 100x fewer parameters than DeepSeek-R1 (671B, 57.4%), and GRPO training dramatically improved performance, for example, doubling the 3B model's accuracy (+103%). This demonstrates that verifiable correctness in technical domains enables efficient and scalable domain specialization for compact LLMs without extensive supervised data or human feedback, with implications for other formally verifiable technical fields. |
| AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety
  Alignment of Large Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2509.24269) or [HuggingFace](https://huggingface.co/papers/2509.24269))|  | AdvChain introduces an adversarial Chain-of-Thought (CoT) tuning framework for robust safety alignment of Large Reasoning Models (LRMs) by teaching dynamic self-correction. The primary objective is to mitigate the "snowball effect," where minor reasoning deviations escalate into harmful compliance or excessive refusal. The key methodology involves constructing an adversarial safety reasoning dataset with Temptation-Correction and Hesitation-Correction samples, then fine-tuning LRMs to enable self-recovery from flawed reasoning. Results show AdvChain significantly enhances robustness; for instance, DeepSeek-R1-7B AdvChain achieved a 4.50% HarmBench Attack Success Rate (ASR), substantially lower than STAR-1's 8.00% and SafeChain's 38.00%, without compromising reasoning capabilities. This work implies that AI practitioners can develop more resilient and practical LRMs by integrating adversarial CoT tuning to instill adaptive error-correction mechanisms. |
| UniVid: The Open-Source Unified Video Model (Read more on [arXiv](https://arxiv.org/abs/2509.24200) or [HuggingFace](https://huggingface.co/papers/2509.24200))| Meng Fang, Biao Wu, Junhui Lin, Jiabin Luo, SteveZeyuZhang | UniVid is an open-source unified video model designed for both understanding and generation tasks. Its main objective is to overcome challenges in maintaining semantic faithfulness during flow-based generation and efficiently extending image-centric MLLMs to video without costly retraining. The model employs a unified architecture coupling a multimodal LLM with a diffusion video decoder via a lightweight adapter, introducing Temperature Modality Alignment for prompt adherence and Pyramid Reflection for efficient temporal reasoning. UniVid achieves state-of-the-art performance, demonstrating a 2.2% improvement on VBench-Long total score and 1.0% and 3.3% accuracy gains on MSVD-QA and ActivityNet-QA, respectively, compared to prior 7B baselines. This unified paradigm provides AI practitioners with a robust and efficient framework for developing integrated video intelligence systems. |
| PARROT: A Benchmark for Evaluating LLMs in Cross-System SQL Translation (Read more on [arXiv](https://arxiv.org/abs/2509.23338) or [HuggingFace](https://huggingface.co/papers/2509.23338))|  | PARROT introduces the first benchmark for evaluating Large Language Models (LLMs) in Cross-System SQL Translation, addressing limitations of existing Text-to-SQL benchmarks. Its primary objective is to evaluate LLMs on adapting SQL queries between diverse database systems like MySQL and ClickHouse, a critical yet underexplored area. The methodology includes curating 598 manually verified query pairs and 5,306 unit-style test cases from 22 production-grade systems, alongside an augmented training pool of 28,003 SQL statements, processed through a comprehensive SQL curation workflow. Initial evaluations show LLMs achieve lower than 38.53% average accuracy, with GPT-4o scoring 58.62% ACCEX (dialect compatibility) and 54.23% ACCRES (result consistency), indicating significant challenges in handling system-specific SQL dialects. This implies AI practitioners must focus on enhancing LLMs' dialect-specific error handling, complex syntax adaptation, and semantic consistency for real-world heterogeneous database environments. |
| MathBode: Frequency-Domain Fingerprints of LLM Mathematical Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.23143) or [HuggingFace](https://huggingface.co/papers/2509.23143))|  | MathBode is a dynamic diagnostic framework using frequency-domain analysis to characterize LLM mathematical reasoning dynamics. The main objective is to move beyond static final-answer accuracy by evaluating amplitude fidelity and timing consistency through frequency-resolved metrics. This is achieved by sinusoidally driving one problem parameter and fitting first-harmonic responses from LLM outputs to derive gain and phase error fingerprints. For Exponential Interest, DeepSeek V3.1 demonstrated high amplitude fidelity with a mean |G-1| of 0.051 at mid-frequencies, while Mixtral 8x7B showed significant distortion with a mean |G-1| of 8.418. This suggests AI practitioners should use frequency-domain diagnostics for LLMs in dynamic or iterative systems, as final-answer accuracy alone is insufficient to predict stability or consistent performance. |
| Local Success Does Not Compose: Benchmarking Large Language Models for
  Compositional Formal Verification (Read more on [arXiv](https://arxiv.org/abs/2509.23061) or [HuggingFace](https://huggingface.co/papers/2509.23061))| Binhang Yuan, Jie Fu, Xingwei Qu, Xu Xu, XINLI1997 | This paper introduces DAFNYCOMP, a benchmark revealing a critical compositional reasoning gap in LLMs for formal verification, showing catastrophic verification failure despite high syntactic accuracy. The main objective is to systematically evaluate LLMs on generating compositional specifications in Dafny for multi-function programs, addressing their lack of compositional reasoning across function boundaries for reliable and verifiable code generation. The methodology involves synthesizing 300 compositional Dafny programs (2-5 chain-based functions with data dependencies) from Python code and requiring LLMs to regenerate missing contract clauses to enable mechanical verification. Results show that LLMs, achieving >58% verification on single-function benchmarks, exhibit a catastrophic 3.69% verification rate on DAFNYCOMP's compositional tasks, a 92% performance gap despite 95.67% syntax correctness. This implies that current LLMs lack robust compositional reasoning for verifiable code generation, necessitating advancements beyond local pattern matching to achieve global contract consistency and inductive reasoning in multi-component systems. |
| ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents (Read more on [arXiv](https://arxiv.org/abs/2509.22830) or [HuggingFace](https://huggingface.co/papers/2509.22830))|  | ChatInject is a novel prompt injection attack that exploits LLM chat templates and multi-turn dialogues to manipulate agent behavior. This research investigates the vulnerability of LLM agents to indirect prompt injection by leveraging their dependence on structured chat templates and susceptibility to contextual manipulation. The methodology involves formatting malicious payloads to mimic native chat templates (ChatInject) and developing a persuasion-driven Multi-turn variant that primes the agent across conversational turns. Experiments show ChatInject significantly increases Attack Success Rates (ASR); for instance, on InjecAgent, ASR improved from 15.13% (default) to 45.90% (ChatInject), with Multi-turn variants reaching 52.33%, demonstrating strong transferability and defense bypass. For AI practitioners, these results highlight critical vulnerabilities in current LLM agent systems, underscoring the inadequacy of existing defenses and the need for more sophisticated security measures against template-based and contextually-primed attacks. |
| UniMIC: Token-Based Multimodal Interactive Coding for Human-AI
  Collaboration (Read more on [arXiv](https://arxiv.org/abs/2509.22570) or [HuggingFace](https://huggingface.co/papers/2509.22570))|  | UniMIC is a unified token-based multimodal interactive coding framework designed for efficient human-AI collaboration. Its objective is to establish an AI-native communication protocol using compact tokenized representations to avoid repeated degradation and latency inherent in pixel-based pipelines. The framework employs modality-specific tokenizers and lightweight Transformer-based entropy models (autoregressive, masked-token, text-conditional) to compress tokens, reducing inter-token redundancy for arithmetic coding. For text-to-image generation, UniMIC achieves 0.0296 bpp with an FID of 80.61 and CLIP-T of 0.315, demonstrating substantial bitrate savings and superior performance compared to baselines like VVC (0.0337 bpp, FID 180.19, CLIP-T 0.286). This establishes a practical paradigm for AI practitioners to enable ultra-low bitrate multimodal communication while preserving semantic fidelity and downstream task performance with Large Multimodal Models. |
| Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and
  Planning (Read more on [arXiv](https://arxiv.org/abs/2509.25052) or [HuggingFace](https://huggingface.co/papers/2509.25052))|  | Cogito, ergo ludo (CEL) is a novel LLM-based agent that learns to master interactive grid-world environments by explicitly reasoning and planning through a continuous interaction-reflection cycle. The agent's primary objective is to build a transparent and improving model of its environment's mechanics and its own strategy from raw interaction, starting from a tabula rasa state. CEL leverages a single Large Language Model (LLM) to function as a Language-based World Model (LWM) for action prediction and a Language-based Value Function (LVF) for state evaluation during episodes, complemented by a post-episode reflection phase that performs Rule Induction to refine environmental dynamics and Strategy and Playbook Summarization for strategic advice. In evaluations across Minesweeper, Frozen Lake, and Sokoban, CEL autonomously discovered rules and developed effective policies, achieving a 54% success rate in Minesweeper, notably surpassing a baseline with ground-truth rules (26%), and a 97% success rate in Frozen Lake within 10 episodes; ablation studies confirmed the criticality of iterative rule induction. This work demonstrates a significant advancement towards more general, interpretable, and auditable AI agents, by enabling explicit, language-based knowledge representation and continuous self-improvement, which can enhance trust and facilitate debugging in complex AI applications. |
| Learning Goal-Oriented Language-Guided Navigation with Self-Improving
  Demonstrations at Scale (Read more on [arXiv](https://arxiv.org/abs/2509.24910) or [HuggingFace](https://huggingface.co/papers/2509.24910))|  | The paper introduces Self-Improving Demonstrations (SID) for goal-oriented language-guided navigation. The primary objective is to address the absence of effective exploration priors in existing methods that rely predominantly on shortest-path trajectories for agent training. SID utilizes an iterative self-improving pipeline where an initial agent generates successful exploration trajectories, which then serve as self-demonstrations to train a more capable agent; this process scales to new environments and integrates VLM-generated captions for language-guided tasks. SID achieves state-of-the-art performance on goal-oriented VLN tasks, notably reaching a 50.9% Success Rate (SR) on SOON's unseen validation splits, exceeding prior leading approaches by 13.9%. This self-improving paradigm offers AI practitioners a scalable solution for developing robust navigation agents, significantly reducing dependence on expensive human annotations for exploration data. |
| REMA: A Unified Reasoning Manifold Framework for Interpreting Large
  Language Model (Read more on [arXiv](https://arxiv.org/abs/2509.22518) or [HuggingFace](https://huggingface.co/papers/2509.22518))| Shuo Zhang, Junrong Yue, Ronghao Chen, Guanzhi Deng, liboaccn | REMA is a novel interpretability framework for analyzing Large Language Model (LLM) reasoning failures via geometric analysis of internal representations. The research objective is to understand how LLMs perform complex reasoning and identify the origins of their failure mechanisms by defining a measurable geometric analysis perspective. The key methodology involves defining the "Reasoning Manifold" as the latent low-dimensional geometric structure of correctly reasoned internal representations, then quantifying reasoning failures as geometric deviations from this manifold using k-nearest neighbor distances and localizing divergence points by layer-wise deviation tracking. Primary results indicate that reasoning states exhibit low-dimensional structures, and error representations consistently show statistically significant geometric deviation from correct reasoning manifolds; for example, a Spearman's rank correlation of p = 0.598 (p < 0.01) was found between Accuracy and Relative Deviation across all model-task pairs. The principal implication for AI practitioners is a unified, model-agnostic tool to quantitatively diagnose where and how severely an LLM's internal states diverge when it makes an error, facilitating targeted debugging and a deeper understanding of black-box model computational processes. |
| BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal
  Decrees and Notifications (Read more on [arXiv](https://arxiv.org/abs/2509.24908) or [HuggingFace](https://huggingface.co/papers/2509.24908))|  | This paper introduces BOE-XSUM, a new dataset for extreme summarization of Spanish legal decrees into clear language, and evaluates generative language models on this task. The primary objective was to determine the extent to which LLMs can produce expert-comparable, concise summaries of complex legal documents. The methodology involved fine-tuning medium-sized LLMs (BERTIN GPT-J 6B, BOLETIN) on the BOE-XSUM dataset and comparing their performance against larger general-purpose models in a zero-shot setting, using metrics like BLEU, ROUGE, METEOR, and BERTScore. Fine-tuned BERTIN GPT-J 6B (32-bit precision) achieved a BERTScore of 41.6%, demonstrating a 24% performance gain over the top zero-shot model, DeepSeek-R1 (33.5% BERTScore). This indicates that AI practitioners can significantly improve extreme summarization of domain-specific legal texts by targeted fine-tuning of smaller LLMs, even outperforming larger zero-shot models. |
| IWR-Bench: Can LVLMs reconstruct interactive webpage from a user
  interaction video? (Read more on [arXiv](https://arxiv.org/abs/2509.24709) or [HuggingFace](https://huggingface.co/papers/2509.24709))| Yunwen Li, Yufan Shen, Minghao Liu, Yang Chen, tricktreat | IWR-Bench is a novel benchmark for evaluating Large Vision-Language Models (LVLMs) in interactive webpage reconstruction from user interaction videos. The main objective is to determine if LVLMs can reconstruct the dynamic and interactive functionalities of a webpage by observing a user interaction video. The key methodology involves 113 curated tasks from real-world websites, providing user interaction videos and static assets, and employing an agent-as-a-judge framework to evaluate functional correctness and visual fidelity. Experimental results on 28 LVLMs indicate that the best model achieved an overall score of 36.35%, with functional correctness (24.39% IFS) significantly trailing visual fidelity (64.25% VFS). This implies that current LVLMs have critical limitations in reasoning about temporal dynamics and synthesizing event-driven logic for truly functional web applications. |
| BPMN Assistant: An LLM-Based Approach to Business Process Modeling (Read more on [arXiv](https://arxiv.org/abs/2509.24592) or [HuggingFace](https://huggingface.co/papers/2509.24592))| Darko Etinger, Nikola Tankovic, jtlicardo | This paper presents BPMN Assistant, an LLM-based tool that uses a specialized JSON representation to generate and edit Business Process Model and Notation (BPMN) diagrams from natural language. The research objective is to determine if this structured JSON intermediate representation is more effective than prompting an LLM to directly manipulate standard BPMN XML. The methodology involves evaluating multiple LLMs on generation and editing tasks, using Graph Edit Distance (GED) to measure generation similarity and a binary success metric to assess editing accuracy. Primary results show that while generation performance was comparable (0.70 similarity for JSON vs. 0.69 for XML), the JSON approach was significantly more reliable for editing tasks, achieving consistently higher success rates and over 2x faster processing (21.46s average latency vs. 46.98s for XML). For AI practitioners, the principal implication is that employing a simplified, structured intermediate representation as an LLM target can dramatically improve the reliability and performance of complex data modification tasks compared to directly manipulating verbose standard formats. |
| Detecting Corpus-Level Knowledge Inconsistencies in Wikipedia with Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.23233) or [HuggingFace](https://huggingface.co/papers/2509.23233))|  | This paper introduces Corpus-Level Inconsistency Detection (CLID) and the agentic LLM-based system CLAIRE for identifying contradictions in Wikipedia. The objective is to formalize CLID, identifying if a fact within a corpus contradicts any other fact in the same corpus, thereby ensuring Wikipedia's accuracy. CLAIRE is an agentic system leveraging LLM reasoning with retrieval, based on the ReAct architecture, incorporating "clarify" and "explain" auxiliary tools, and validated through human-in-the-loop annotation to create WIKICOLLIDE. A user study showed participants identified 64.7% more inconsistencies using CLAIRE; analysis revealed at least 3.3% of English Wikipedia facts contradict other statements, and CLAIRE achieved an AUROC of 75.1% on the WIKICOLLIDE test set. CLID systems, particularly LLM-based agentic approaches like CLAIRE, offer a practical tool for AI practitioners to improve the consistency and reliability of large-scale knowledge corpora, directly benefiting LLM training and RAG systems. |
| RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human
  Mobility (Read more on [arXiv](https://arxiv.org/abs/2509.23115) or [HuggingFace](https://huggingface.co/papers/2509.23115))|  | RHYTHM introduces a computationally efficient framework for human mobility prediction using hierarchical temporal tokenization and frozen Large Language Models (LLMs). The primary objective is to accurately predict human mobility by effectively capturing complex long-range dependencies and multi-scale periodic behaviors inherent in human trajectories. RHYTHM's methodology involves partitioning trajectories into daily segments, encoding them as discrete tokens with hierarchical attention for daily and weekly patterns, and enriching representations with pre-computed prompt embeddings from a frozen LLM. This parameter-efficient adaptation strategy significantly reduces computational overhead, leading to a 2.4% improvement in overall Accuracy@1 and a 24.6% reduction in training time compared to state-of-the-art baselines. This framework provides AI practitioners with a scalable and accessible solution for accurate trajectory prediction, particularly valuable for handling irregular mobility patterns in resource-constrained, real-world environments. |
| Charting a Decade of Computational Linguistics in Italy: The CLiC-it
  Corpus (Read more on [arXiv](https://arxiv.org/abs/2509.19033) or [HuggingFace](https://huggingface.co/papers/2509.19033))| Chiara Alzetta, martasartor, alemiaschi, chiaracf, lucadini | This paper charts a decade of Italian computational linguistics research by analyzing the CLiC-it conference proceedings. The objective was to analyze research trends, collaboration patterns, and thematic evolution within the Italian CL/NLP community from 2014 to 2024. The methodology involved compiling the CLiC-it Corpus from 693 papers using semi-automatic parsing, metadata analysis, network analysis with centrality measures, and BERTopic for topic modeling, including EasyNMT for Italian-to-English translation. Key findings include 2,006 unique authors contributed over ten years, with the 2024 edition having a record 346 authors, and "Lexical and Semantic Resources and Analysis" being the most represented topic (189 papers). This corpus and analysis provide AI practitioners with empirical insights into the evolution of NLP research priorities and collaborative structures, aiding in informed decisions for future AI development, especially regarding neural and conversational technologies. |
| Advancing Reference-free Evaluation of Video Captions with Factual
  Analysis (Read more on [arXiv](https://arxiv.org/abs/2509.16538) or [HuggingFace](https://huggingface.co/papers/2509.16538))| Subarna Tripathi, Tz-Ying Wu, dipta007 | VC-Inspector is a novel reference-free and factually grounded multimodal evaluation framework for video captions, leveraging LLM-generated synthetic data for instruction tuning. The main objective is to develop a reference-free evaluation framework for video captions that relies on factual grounding to accurately assess caption quality without requiring human-annotated ground truth captions. The methodology involves creating a synthetic video caption dataset by using Llama-3.3-70B-Instruct to systematically alter objects and actions in ground truth captions, assigning quality scores based on factual changes, which then instruction-tunes a Qwen2.5-VL model to act as VC-Inspector. On the VATEX-EVAL dataset in a reference-free setting, VC-Inspector-7B achieved a Kendall's correlation (Tb) of 42.58 and Spearman's rank correlation (ρ) of 45.99, outperforming the ViCLIPScore baseline (Tb 30.92, ρ 39.86). This work provides AI practitioners with a scalable, generalizable, and interpretable tool for evaluating video caption factual accuracy without costly human annotations, enabling objective assessment and potential use as a reward model in Reinforcement Learning applications. |
