

## Papers for 2025-10-01

| Title | Authors | Summary |
|-------|---------|---------|
| Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified
  Self-Play (Read more on [arXiv](https://arxiv.org/abs/2509.25541) or [HuggingFace](https://huggingface.co/papers/2509.25541))| Jing Shi, Qinsi Wang, timecuriosity, zhoutianyi, Benjamin-eecs | Vision-Zero is a framework that enables scalable VLM self-improvement through strategic self-play in visual games generated from arbitrary, label-free image pairs. The primary objective is to develop a domain-agnostic training paradigm for VLMs that eliminates dependence on costly, human-annotated data for enhancing reasoning capabilities. The methodology involves a "Who Is the Spy"-style game where VLMs play as both "spy" and "civilian" to generate training data, coupled with a novel algorithm, Iterative Self-Play Policy Optimization (Iterative-SPO), which alternates between self-play and reinforcement learning with verifiable rewards (RLVR) to prevent performance stagnation. The framework achieves state-of-the-art performance across reasoning and vision-centric benchmarks; a Vision-Zero trained Qwen2.5-VL-7B model achieved an average score of 44.1% on a suite of reasoning and math tasks, outperforming the baseline model's 41.1% and other annotation-based methods. The principal implication for AI practitioners is that Vision-Zero provides a highly cost-efficient method to post-train and enhance VLMs using only unlabeled image pairs, thereby avoiding the significant expense and time required for manual data curation and annotation. |
| MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP
  Use (Read more on [arXiv](https://arxiv.org/abs/2509.24002) or [HuggingFace](https://huggingface.co/papers/2509.24002))|  | This paper introduces MCPMark, a benchmark with 127 realistic tasks designed to stress-test LLM agents on complex, multi-step CRUD operations, addressing the limitations of existing shallow MCP benchmarks. The methodology involves tasks across five environments (e.g., GitHub, PostgreSQL) created via a human-AI pipeline, each with a curated initial state and evaluated through programmatic verification in a minimal agent framework. The benchmark reveals significant performance limitations in current models, with the top performer, `gpt-5-medium`, achieving only a 52.56% pass@1 success rate. For AI practitioners, this highlights a critical gap in agent robustness and planning for real-world stateful tasks, indicating that development must shift focus from simple reactive tool use to enhancing execution stability and sophisticated reasoning. |
| The Dragon Hatchling: The Missing Link between the Transformer and
  Models of the Brain (Read more on [arXiv](https://arxiv.org/abs/2509.26507) or [HuggingFace](https://huggingface.co/papers/2509.26507))|  | The paper introduces Brain-inspired Dragon Hatchling (BDH), a novel, biologically-plausible state-space language model architecture based on local graph dynamics that achieves Transformer-competitive performance.  The main research question is how to develop a new LLM architecture that connects the macro-level function of Transformers with micro-level, biologically-inspired neuronal dynamics to achieve strong performance, inherent interpretability, and a theoretical foundation for reasoning over time.  The key methodology is the proposal of BDH, formulated as an edge-reweighting process on a graph of `n` neurons, with a practical GPU-friendly variant (BDH-GPU) that uses low-rank matrix factorizations, a 'ReLU-lowrank' feed-forward block, and a linear attention mechanism operating in a high-dimensional (`n`), positive activation space.  The primary results show that BDH-GPU exhibits scaling laws comparable to Transformers, empirically matching the performance of a GPT-2 style baseline (GPTXL) on language and translation tasks across scales from 10M to 1B parameters; at the 1B scale, a BDH-GPU variant achieves a validation loss of approximately 0.37, on par with the baseline. The architecture's learned parameters form modular, scale-free graphs, and it exhibits sparse positive activations (~5% density) and monosemantic synapses.  The principal implication for AI practitioners is that the BDH-GPU architecture offers a competitive alternative to the Transformer that is designed for interpretability and composability. Its most impactful finding is achieving this performance with a model whose state and parameters have a clear, local, graph-based interpretation, enabling novel capabilities like direct model merging by concatenating parameter tensors, which could simplify the creation of larger, specialized models from smaller components. |
| TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.25760) or [HuggingFace](https://huggingface.co/papers/2509.25760))|  | This paper presents TruthRL, a reinforcement learning framework that uses a ternary reward system to improve LLM truthfulness by incentivizing correct answers and abstention over hallucination. The primary objective is to develop a training method that directly optimizes a model's ability to maximize correct answers, minimize hallucinations, and appropriately abstain when uncertain, moving beyond simple accuracy maximization. The key methodology is an RL framework, implemented with GRPO, that utilizes a ternary reward signal: +1 for correct answers, 0 for abstentions, and -1 for hallucinations. Across four knowledge-intensive benchmarks, this approach reduces hallucinations by 28.9% and improves truthfulness by 21.1% compared to vanilla RL. The principal implication for AI practitioners is that explicitly designing learning objectives and reward structures for truthfulness—particularly by neutrally treating abstention—is a more effective strategy for developing reliable and less hallucinatory models than solely optimizing for accuracy. |
| OceanGym: A Benchmark Environment for Underwater Embodied Agents (Read more on [arXiv](https://arxiv.org/abs/2509.26536) or [HuggingFace](https://huggingface.co/papers/2509.26536))|  | OceanGym is a benchmark environment for evaluating Multi-modal Large Language Model (MLLM)-driven embodied agents on perception and decision-making tasks in simulated underwater settings. The primary objective is to assess the capabilities and limitations of MLLM agents in challenging underwater environments characterized by low visibility, dynamic conditions, and reliance on optical and sonar data. The methodology involves a high-fidelity simulation environment built in Unreal Engine 5.3 with eight realistic task domains and a unified agent framework where an MLLM processes language instructions, multi-view sensory inputs, and a sliding-window memory to control an Autonomous Underwater Vehicle (AUV) based on a POMDP formulation. Results show a substantial performance gap between AI and humans; in deep-water decision tasks, the best MLLM-driven agent (GPT-4o-mini) achieved an average score of 14.8%, while human experts scored 69.6%. The principal implication for AI practitioners is that current MLLM agents lack the robustness for real-world underwater deployment, indicating a critical need for advancing multi-modal fusion (particularly for sonar data), memory retention, and long-horizon planning under extreme perceptual uncertainty. |
| DC-VideoGen: Efficient Video Generation with Deep Compression Video
  Autoencoder (Read more on [arXiv](https://arxiv.org/abs/2509.25182) or [HuggingFace](https://huggingface.co/papers/2509.25182))|  | DC-VideoGen is a post-training framework that accelerates video diffusion models by adapting them to a new, highly compressed latent space. The main objective is to reduce the high computational costs of training and inference for large-scale video generation models while maintaining or improving output quality. The methodology combines a Deep Compression Video Autoencoder (DC-AE-V) using a novel chunk-causal temporal design for 32×/64× spatial compression, with an efficient adaptation strategy (AE-Adapt-V) that aligns the model's embedding space before lightweight LoRA fine-tuning. The framework achieves up to a 14.8× reduction in inference latency on 2160×3840 resolution video generation compared to the Wan-2.1-T2V-1.3B base model, while enabling generation at this resolution on a single NVIDIA H100 GPU. The principal implication for AI practitioners is the ability to make existing, computationally expensive video diffusion models significantly more efficient for deployment and further development with minimal, low-cost fine-tuning, thereby increasing accessibility to high-fidelity video generation. |
| Who's Your Judge? On the Detectability of LLM-Generated Judgments (Read more on [arXiv](https://arxiv.org/abs/2509.25154) or [HuggingFace](https://huggingface.co/papers/2509.25154))|  | This paper introduces the task of detecting LLM-generated judgments and proposes J-Detector, a lightweight detector using explicit features to distinguish them from human judgments. The main objective is to formalize and systematically investigate the detectability of LLM judgments based solely on candidate content and numerical scores, a scenario where textual feedback is unavailable. The key methodology involves J-Detector, a neural model augmented with extracted linguistic features (e.g., length, complexity) and LLM-enhanced features that capture systematic biases in LLM judges. The primary result shows that J-Detector significantly outperforms baselines, achieving an average F1 score of 87.7% across four datasets, compared to 68.8% for RoBERTa-based detectors, and demonstrates that detectability is influenced by group size, judgment dimensions, and rating scale. The principal implication for AI practitioners is that the inherent, systematic biases of LLM judges can be exploited for detection, providing a method to audit and ensure the fairness of automated evaluation systems. |
| Learning to See Before Seeing: Demystifying LLM Visual Priors from
  Language Pre-training (Read more on [arXiv](https://arxiv.org/abs/2509.26625) or [HuggingFace](https://huggingface.co/papers/2509.26625))| Koustuv Sinha, Yufan Ren, David Fan, Shengbang Tong, Junlin Han | This paper systematically investigates how Large Language Models (LLMs) acquire visual priors from text-only pre-training and proposes a data-centric recipe to enhance these capabilities. The main objective is to deconstruct the origin and structure of emergent visual priors in LLMs, determining which types of language data cultivate specific visual abilities. The methodology involves over 100 controlled experiments analyzing MLLM performance after varying the LLM pre-training data composition, model scale, and data scale across 16 VQA benchmarks. The primary result is that visual priors decompose into a reasoning prior, which scales progressively with the proportion of reasoning-centric data up to a 75% ratio in the pre-training mix, and a perception prior, which emerges more diffusely from broad corpora. The principal implication for AI practitioners is that they can build more capable MLLMs by deliberately composing the LLM pre-training corpus to be heavily skewed towards reasoning-centric text to cultivate a transferable, modality-agnostic visual reasoning foundation. |
| Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token
  Pruning for Efficient Supervised Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2509.23873) or [HuggingFace](https://huggingface.co/papers/2509.23873))| Yue Min, Cong Wang, JiajunZhang, Jessamine, Steven-Shaobo | This paper introduces Q-Tuning, a unified framework for joint sample and token pruning that enhances the efficiency and performance of supervised fine-tuning for large language models. The objective is to develop a coordinated strategy that jointly optimizes both sample selection and token retention to overcome the limitations of fragmented, single-dimension pruning methods. The core methodology, Quadrant-based Tuning (Q-Tuning), uses an "Error-Uncertainty (EU) Plane" to categorize training instances by model perplexity and entropy, enabling a two-stage process that first prunes entire samples classified as harmful noise or redundant knowledge, and then applies an asymmetric token-pruning policy exclusively to high-error, high-confidence samples. On the GSM8K benchmark, Q-Tuning with LLaMA3-8B achieved an accuracy of 48.07 using only 35% of the training data, significantly outperforming the 42.05 score from training on the full dataset. The principal implication for AI practitioners is that this method provides a scalable blueprint to reduce SFT computational costs while simultaneously improving model performance, making high-quality alignment more accessible under budget constraints. |
| Thinking Sparks!: Emergent Attention Heads in Reasoning Models During
  Post Training (Read more on [arXiv](https://arxiv.org/abs/2509.25758) or [HuggingFace](https://huggingface.co/papers/2509.25758))|  | This paper uses circuit analysis to demonstrate that post-training for complex reasoning sparks the emergence of novel, functionally specialized attention heads. The primary objective is to mechanistically analyze how different post-training regimes—distillation, Supervised Fine-Tuning (SFT), and Group Relative Policy Optimization (GRPO)—alter a model's internal architecture to enhance reasoning. The methodology involves using edge attribution patching with integrated gradients (EAP-IG) to map computational circuits in Qwen-family models before and after post-training, followed by causal validation via head ablation studies. The primary results reveal distinct architectural changes: distillation and SFT foster a cumulative addition of many stable reasoning heads, while GRPO performs a dynamic search, activating and pruning a smaller, more targeted set of heads that correlate with reward signals; for instance, ablating the emergent reasoning heads in the DeepSeek-R1-Distill-Qwen-1.5B model caused its AIME'24 pass@1 score to drop from 30.0 to 26.6. The principal implication for AI practitioners is that the choice of post-training method creates a direct trade-off between installing powerful, broad reasoning circuits that may "overthink" simple tasks (SFT/distillation) and performing targeted, efficient optimization that may be less general (GRPO), requiring careful selection of training policy to balance reasoning capability with execution reliability. |
| VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in
  Real-world Applications (Read more on [arXiv](https://arxiv.org/abs/2509.26490) or [HuggingFace](https://huggingface.co/papers/2509.26490))|  | The paper introduces VitaBench, a benchmark for evaluating LLM agents on 400 complex, interactive tasks grounded in real-world life-serving applications using 66 tools. The primary objective is to systematically measure agentic task complexity across three dimensions—reasoning, tool use, and interaction—to better reflect the challenges of practical deployment. The methodology involves a simulation environment with graph-interconnected tools, tasks derived from real user requests, and a novel rubric-based sliding window evaluator for assessing long-horizon, multi-path trajectories. The evaluation demonstrates that even top-performing models achieve only a 30.0% Avg@4 success rate on cross-scenario tasks, a sharp decline from over 50% in single-scenario settings. For AI practitioners, this significant performance degradation underscores that current agents have fundamental deficiencies in cross-domain reasoning and tool composition, identifying these as critical limitations to address for reliable real-world applications. |
| dParallel: Learnable Parallel Decoding for dLLMs (Read more on [arXiv](https://arxiv.org/abs/2509.26488) or [HuggingFace](https://huggingface.co/papers/2509.26488))|  | dParallel introduces a certainty-forcing distillation method to significantly accelerate inference in diffusion large language models (dLLMs) by enabling highly parallel token decoding. The objective is to overcome the primary bottleneck of "sequential certainty convergence," where dLLMs achieve high confidence for token predictions in a slow, left-to-right manner, which inhibits their inherent parallelism. The key methodology is certainty-forcing distillation, a novel self-distillation training strategy that combines a consistency loss to maintain the original generation trajectory with a certainty loss that minimizes the predictive entropy for correctly predicted tokens, thereby training the model to become certain about many tokens in parallel. The primary result demonstrates that when applied to LLaDA-8B-Instruct on the GSM8K benchmark, dParallel reduces the number of decoding steps from 256 to 30, achieving an 8.5x inference speedup without performance degradation. For AI practitioners, this research provides an efficient, LoRA-based fine-tuning technique to drastically reduce the latency of existing dLLMs, making them more practical for deployment in latency-sensitive applications that require fast text generation. |
| IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance (Read more on [arXiv](https://arxiv.org/abs/2509.26231) or [HuggingFace](https://huggingface.co/papers/2509.26231))|  | This paper proposes Implicit Multimodal Guidance (IMG), a re-generation framework that uses a multimodal large language model (MLLM) and a novel adapter to correct misalignments in diffusion-generated images without requiring model finetuning or explicit editing. The primary objective is to improve the alignment between text prompts and generated images by identifying and correcting conceptual errors, such as missing objects or incorrect attributes, that occur in state-of-the-art diffusion models. The methodology involves an MLLM identifying discrepancies between an initial image and its prompt, an "Implicit Aligner" network using the MLLM's guidance to refine the image's conditioning features, and then re-generating a new image from these corrected features using an "Iteratively Updated Preference Objective" for training. Evaluations show that when applied to SDXL, IMG achieves an average Human Preference Score (HPS) win rate of 87.2% against the base model on the Human Preference Datasets (HPD) benchmark. For AI practitioners, IMG provides a flexible, plug-and-play adapter to enhance the prompt adherence and quality of existing pre-trained diffusion models, offering a practical alternative to full model retraining or complex post-generation editing pipelines. |
| MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation (Read more on [arXiv](https://arxiv.org/abs/2509.26391) or [HuggingFace](https://huggingface.co/papers/2509.26391))| Limin Wang, Gangshan Wu, Yilu Wu, wangsssssss, flateon | MotionRAG is a retrieval-augmented framework that improves motion realism in image-to-video generation by transferring motion priors from reference videos. The objective is to overcome the difficulty of modeling complex, physically plausible motion in diffusion-based video synthesis by leveraging external motion examples. The methodology involves a three-stage process: text-based retrieval of relevant videos, a Context-Aware Motion Adaptation (CAMA) module using a causal transformer to adapt motion features via in-context learning, and a motion-adapter to inject these features into a pretrained video diffusion model. On the OpenVid-1K dataset, MotionRAG improved the Action Score of the CogVideoX model from 59.9 to 65.8, demonstrating enhanced motion quality with negligible computational overhead. For AI practitioners, this provides a modular, plug-and-play method to enhance motion fidelity in existing video generation models and enables zero-shot adaptation to new domains by simply curating a relevant retrieval database without retraining. |
| Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and
  Multi-Scale Global-Local Attention (Read more on [arXiv](https://arxiv.org/abs/2509.23610) or [HuggingFace](https://huggingface.co/papers/2509.23610))|  | The paper introduces Dolphin, an efficient audio-visual speech separation (AVSS) model that achieves state-of-the-art performance with significantly reduced computational cost. The objective is to resolve the inherent trade-off between separation quality and computational overhead in AVSS, particularly the high cost of large-scale visual encoders. The methodology features two key innovations: DP-LipCoder, a lightweight dual-path video encoder that transforms lip motion into discrete, audio-aligned semantic tokens using vector quantization and knowledge distillation, and a single-iteration audio separator incorporating a global-local attention (GLA) block to efficiently model multi-scale dependencies. On benchmark datasets, Dolphin surpasses the previous state-of-the-art model in separation quality while achieving over a 2.4x reduction in MACs and over 6x faster GPU inference speed. The principal implication for AI practitioners is that Dolphin offers a practical and deployable architecture for high-performance AVSS in resource-constrained environments, demonstrating that efficient, discrete visual representations can replace computationally expensive visual backbones without sacrificing quality. |
| Mem-α: Learning Memory Construction via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.25911) or [HuggingFace](https://huggingface.co/papers/2509.25911))| Yuzhen Mao, Ryuichi Takanobu, Yu Wang, ai-hyz, zkadelzq | Mem-α is a reinforcement learning framework for training large language model agents to dynamically construct and manage a multi-component external memory. The primary objective is to determine if reinforcement learning can train an LLM agent to learn optimal policies for managing a complex memory system—what to store, how to structure it, and when to update—by optimizing directly for downstream task performance. The methodology involves formulating memory construction as a sequential decision-making problem trained with Group Relative Policy Optimization (GRPO), where an agent interacts with a three-part memory (core, episodic, semantic) and receives a composite reward signal based on question-answering accuracy, tool call success, memory compression, and content validity. The Mem-α agent, built on Qwen3-4B, achieved a 0.642 average performance score across validation tasks, significantly outperforming the base model with the same memory architecture (0.389) and standard baselines like Long-Context (0.588), while also generalizing to sequences over 13x its training length. The principal implication for AI practitioners is that directly optimizing memory management policies via reinforcement learning can substantially improve the long-context reasoning and information retention capabilities of smaller models, providing a more robust alternative to relying on manually engineered prompting or simple retrieval-augmented generation heuristics. |
| Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal
  LLMs (Read more on [arXiv](https://arxiv.org/abs/2509.22646) or [HuggingFace](https://huggingface.co/papers/2509.22646))|  | This paper introduces DEEPTRACEREWARD, a benchmark for identifying human-perceived artifacts in AI-generated videos with spatiotemporal grounding, and trains a reward model that significantly outperforms SOTA MLLMs on this task. The research aims to determine if MLLMs can detect and explain deepfake traces like humans do, and to create a dataset to train models for this capability. The methodology involves creating a dataset of 4.3K expert annotations on 3.3K videos, where each annotation contains a natural language explanation, bounding boxes, and timestamps for a visual artifact, which is then used to supervised fine-tune a 7B multimodal language model. The primary result is that the fine-tuned model achieves a 70.2% overall score, surpassing GPT-5 by 34.7%, and demonstrates a clear difficulty gradient where binary classification (99.4% accuracy) is substantially easier than fine-grained spatial or temporal localization of artifacts. For AI practitioners, this work provides a concrete framework and a reward model for evaluating and improving video generation systems by targeting specific, human-noticeable visual failures, moving beyond holistic quality metrics to enable more trustworthy and realistic video synthesis. |
| DeepScientist: Advancing Frontier-Pushing Scientific Findings
  Progressively (Read more on [arXiv](https://arxiv.org/abs/2509.26603) or [HuggingFace](https://huggingface.co/papers/2509.26603))|  | DeepScientist is an autonomous AI system that formalizes scientific discovery as a Bayesian Optimization problem to progressively generate novel, SOTA-surpassing methods. The primary objective is to develop a fully autonomous, goal-oriented system capable of conducting scientific discovery over long timelines to produce findings that surpass human-designed state-of-the-art methods on frontier AI tasks. The system uses a hierarchical "hypothesize, verify, and analyze" loop, leveraging a Bayesian Optimization framework with a surrogate model and an Upper Confidence Bound (UCB) acquisition function to intelligently select hypotheses from a cumulative "Findings Memory" for experimental validation. The system autonomously developed novel methods that surpassed human SOTA on three separate tasks, including improving accuracy on an Agent Failure Attribution benchmark by 183.7%; however, the overall success rate from implemented ideas to validated progress was only 1-3%. The principal implication for AI practitioners is that while autonomous systems can vastly accelerate the trial-and-error process of innovation, the primary bottleneck shifts from ideation to efficient, scalable validation and filtering due to the exceptionally low success rate of AI-generated hypotheses. |
| Attention as a Compass: Efficient Exploration for Process-Supervised RL
  in Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2509.26628) or [HuggingFace](https://huggingface.co/papers/2509.26628))|  | AttnRL is a Process-Supervised Reinforcement Learning framework that uses model attention scores to guide exploration, improving the performance and training efficiency of reasoning models. The primary objective is to overcome the exploration and training inefficiencies of existing Process-Supervised RL (PSRL) methods by developing a system that intelligently selects branching points and adapts sampling strategies. The core methodology involves Attention-based Tree Branching (ATB), which uses a Forward Context Influence (FCI) score to identify critical reasoning steps for exploration, combined with an adaptive sampling mechanism and a one-step off-policy training pipeline to reduce redundant generation. On six mathematical reasoning benchmarks, AttnRL improved a 7B parameter base model's average score from 66.0 to 68.7, achieving this result in 500 training steps versus the 800 steps required by baselines. The principal implication for AI practitioners is that internal model mechanics like attention can serve as a highly efficient, low-cost heuristic for guiding RL exploration, while the one-step off-policy design offers a practical method to significantly reduce the computational overhead and wall-clock time of PSRL training cycles. |
| DA^2: Depth Anything in Any Direction (Read more on [arXiv](https://arxiv.org/abs/2509.26618) or [HuggingFace](https://huggingface.co/papers/2509.26618))|  | DA² is an end-to-end, zero-shot panoramic depth estimation model that combines a massive curated dataset with a distortion-aware Vision Transformer architecture. The objective is to develop a highly accurate and generalizable panoramic depth estimator that overcomes the limitations of data scarcity and spherical distortions inherent in 360° images. The methodology involves a data curation engine that converts over 543K perspective RGB-depth pairs into panoramic samples and a novel SphereViT model that uses cross-attention to explicitly incorporate spherical coordinates, making the features distortion-aware. DA² achieves state-of-the-art zero-shot performance across multiple benchmarks, demonstrating an average 38% improvement in Absolute Relative Error (AbsRel) over the strongest prior zero-shot baseline. For AI practitioners, this work provides a powerful, efficient model and a large-scale dataset for generating geometrically consistent 3D reconstructions from single panoramic images, directly benefiting AR/VR, robotics simulation, and 3D content creation. |
| OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost
  Always! (Read more on [arXiv](https://arxiv.org/abs/2509.26495) or [HuggingFace](https://huggingface.co/papers/2509.26495))|  | This paper introduces OFFTOPICEVAL, a benchmark to evaluate the "operational safety" of LLMs, defined as their ability to adhere to a specific purpose by accepting in-domain and refusing out-of-domain queries. The primary objective is to quantify the capability of LLMs to reject out-of-domain (OOD) queries, especially when they are adversarially transformed to appear in-domain. The methodology involves testing 20 open-weight models across 21 purpose-specific agent roles using in-domain (ID), direct OOD, and "adaptive OOD" queries, which are generated via a "prompt laundering" transformation. Results show a catastrophic failure in operational safety against adaptive attacks; for instance, the Llama-3.3 (70B) model's OOD refusal rate plummets from 69.73% on direct queries to just 4.21% on adaptive queries. The principal implication for AI practitioners is that system prompts are insufficient to guarantee that a purpose-specific agent remains on-topic, making them unsafe for deployment against even simple adversarial inputs; prompt-based mitigation strategies like the proposed "P-ground" and "Q-ground" are shown to be a necessary first step, improving refusal rates by up to 41%. |
| A Cartography of Open Collaboration in Open Source AI: Mapping
  Practices, Motivations, and Governance in 14 Open Large Language Model
  Projects (Read more on [arXiv](https://arxiv.org/abs/2509.25397) or [HuggingFace](https://huggingface.co/papers/2509.25397))| Jennifer Ding, Cailean Osborne, Johan Linåker, burtenshaw | This paper presents a qualitative cartography of open collaboration practices, motivations, and governance structures across the lifecycle of 14 open large language model projects. The study's objective is to understand how open LLM projects are initiated, organized, and governed by mapping where collaboration occurs, what motivates developers, and how these efforts are coordinated. The methodology consists of an exploratory analysis based on semi-structured interviews with 17 developers from 14 projects spanning grassroots initiatives, research institutes, startups, and major technology companies. Results show that open collaboration extends beyond the model itself to datasets, benchmarks, and frameworks, and the analysis identified five distinct organizational governance models ranging from single-company projects to non-profit-sponsored grassroots initiatives. The principal implication for AI practitioners is that collaboration opportunities are highly dependent on the project's lifecycle stage; broad community engagement is most feasible post-release through derivative development and feedback, whereas pre-release collaboration is typically limited to strategic, resource-intensive partnerships or specialized contributions to artifacts like evaluation frameworks. |
| Regression Language Models for Code (Read more on [arXiv](https://arxiv.org/abs/2509.26476) or [HuggingFace](https://huggingface.co/papers/2509.26476))|  | This paper introduces Regression Language Models (RLMs), a unified text-to-text framework for predicting numeric metrics like latency, memory, and accuracy directly from diverse code representations without domain-specific feature engineering. The primary objective is to determine if a single, pretrained encoder-decoder language model can effectively perform regression on a wide variety of code-based inputs—from high-level languages to neural network intermediate representations (IR)—and outperform specialized methods. The methodology treats regression as a next-token prediction task using a T5Gemma-initialized model, where numeric target values are represented using a custom, normalization-free, digit-by-digit tokenization scheme and predicted autoregressively. A single 300M parameter RLM achieves a mean Kendall-Tau of 0.46 on five neural architecture search (NAS) benchmarks, outperforming previous state-of-the-art graph neural network models. For AI practitioners, this means a single, unified RLM can be used to predict performance metrics for diverse computational graphs and source code directly from their text representations, significantly simplifying the performance modeling pipeline by eliminating the need for manual feature engineering or specialized graph-based architectures. |
| InfoAgent: Advancing Autonomous Information-Seeking Agents (Read more on [arXiv](https://arxiv.org/abs/2509.25189) or [HuggingFace](https://huggingface.co/papers/2509.25189))|  | This paper introduces InfoAgent, an open-source 14B parameter deep research agent that utilizes a novel data synthesis pipeline and a two-stage training process to solve complex, long-horizon information-seeking tasks. The primary objective is to advance autonomous agent capabilities by developing a methodology for creating challenging, multi-hop training data and establishing an efficient, self-hosted interactive web environment. The core methodology consists of a data pipeline that builds entity trees from Wikipedia, applies sub-tree sampling with entity fuzzification to systematically increase question difficulty, and then uses a two-stage training recipe of cold-start supervised fine-tuning followed by reinforcement learning. The resulting InfoAgent achieves 15.3% accuracy on the BrowseComp benchmark, outperforming larger open-source models such as WebSailor-72B and DeepDive-32B. For AI practitioners, this research provides a concrete framework emphasizing that sophisticated data synthesis strategies forcing long-horizon reasoning and a high-quality, custom tool infrastructure are critical components for building high-performing, open-source information-seeking agents. |
| Humanline: Online Alignment as Perceptual Loss (Read more on [arXiv](https://arxiv.org/abs/2509.24207) or [HuggingFace](https://huggingface.co/papers/2509.24207))|  | This paper introduces "humanline," a design pattern derived from prospect theory that incorporates human perceptual biases into alignment objectives to close the performance gap between online and offline methods. The core objective is to understand why online alignment outperforms offline alignment and to replicate its benefits without expensive on-policy data collection. The methodology involves creating humanline variants of DPO, KTO, and GRPO by applying two changes: periodic syncing of the reference model with a previous policy version and asymmetric upstream clipping of token-wise likelihood ratios. The primary result shows that humanline variants trained with offline data can match their online counterparts' performance, with offline+humanline GRPO achieving a 1.6x higher winrate than standard offline GRPO on an instruction-following task. For AI practitioners, this implies that state-of-the-art alignment can be achieved more cheaply and quickly by applying the humanline pattern to existing offline datasets, avoiding the cost and instability of online on-policy training. |
| Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents (Read more on [arXiv](https://arxiv.org/abs/2509.26539) or [HuggingFace](https://huggingface.co/papers/2509.26539))|  | Ferret-UI Lite is a 3B parameter end-to-end multimodal model developed for on-device Graphic User Interface (GUI) agent tasks. The primary objective is to investigate strategies for building compact and efficient GUI agents capable of operating across diverse platforms like mobile, web, and desktop. The methodology involves a two-stage training strategy: supervised fine-tuning (SFT) on a curated mixture of real and synthetic GUI data, followed by reinforcement learning with verifiable rewards (RLVR) to refine both grounding and navigation performance, augmented by inference-time chain-of-thought reasoning and a zoom-in visual tool. Ferret-UI Lite demonstrates competitive GUI grounding performance, achieving 53.3% accuracy on the ScreenSpot-Pro benchmark, but its multi-step navigation capabilities are limited, with a 28.0% success rate on the AndroidWorld benchmark. The principal implication for AI practitioners is that while small, on-device GUI agents can achieve strong grounding accuracy through data curation and RL, their capacity for robust, long-horizon reasoning in complex navigation tasks remains a significant challenge, indicating a direct trade-off between model efficiency and advanced agentic capabilities. |
| More Thought, Less Accuracy? On the Dual Nature of Reasoning in
  Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.25848) or [HuggingFace](https://huggingface.co/papers/2509.25848))| Fabian Waschkowski, Mengqi He, Zhaoyuan Yang, Shu Zou, Xinyu Tian | This research identifies that prolonged reasoning in Vision-Language Models (VLMs) can impair perceptual accuracy due to "visual forgetting" and proposes Vision-Anchored Policy Optimization (VAPO) to enforce visual grounding and improve performance. The main objective is to investigate the dual nature of multimodal reasoning, where extended thought processes can degrade perceptual grounding, and to develop a method to counteract this effect. The key methodology is VISION-ANCHORED POLICY OPTIMIZATION (VAPO), a policy gradient algorithm that inserts "visual anchors"—verifiable claims about the image—into the reasoning process and uses the model's judgment on these claims to generate a perception reward for training. The primary result is that the proposed VAPO-Thinker-7B model achieves new state-of-the-art performance, improving upon the previous best result by 3.2% (from 59.9% to 63.1%) on average across general-purpose benchmarks. The principal implication for AI practitioners is that simply encouraging longer reasoning chains in VLMs is insufficient and can be detrimental; it is critical to implement mechanisms that explicitly reinforce the model's connection to visual input throughout the entire reasoning process to prevent performance degradation on vision-intensive tasks. |
| Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with
  LLMs (Read more on [arXiv](https://arxiv.org/abs/2509.23166) or [HuggingFace](https://huggingface.co/papers/2509.23166))| Yao Shu, Fei Yu, Ying He, Hong Wang, Chenxing Wei | This paper presents T²PAM, a test-time policy adaptation paradigm, and its algorithm ROSA, which enables Large Language Models to perform efficient in-conversation self-correction using real-time user feedback. The primary objective is to address performance degradation in multi-turn interactions by dynamically updating the model's policy to align with user preferences during inference, thereby avoiding costly offline retraining. ROSA operationalizes this by formulating an RLHF objective, analytically deriving a closed-form optimal policy from user feedback, and then guiding model parameters toward this target in a single, efficient update step using linearized optimization with the Conjugate Gradient method. Extensive experiments show significant improvements; for instance, applying ROSA with a model-based reward to the Qwen3-0.6B model on the MATH dataset increased its final accuracy from a baseline of 25.00% to 52.20%, an absolute improvement of +27.20%. The principal implication for AI practitioners is that ROSA offers a lightweight, practical method to enhance the performance and adaptability of conversational agents on complex tasks by enabling them to learn and correct errors directly from live user interactions with minimal computational overhead. |
| Benefits and Pitfalls of Reinforcement Learning for Language Model
  Planning: A Theoretical Perspective (Read more on [arXiv](https://arxiv.org/abs/2509.22613) or [HuggingFace](https://huggingface.co/papers/2509.22613))|  | This paper theoretically analyzes RL methods for LLM planning, demonstrating that while Policy Gradient (PG) generalizes better than Supervised Fine-Tuning (SFT) through exploration, it suffers from diversity collapse, an issue mitigated by Q-learning with process-based rewards. The main objective is to establish a theoretical basis for the effectiveness and limitations of RL (PG and Q-learning) over SFT in planning, abstracted as a graph-pathfinding problem. The methodology involves a theoretical analysis of the learning dynamics and stable points for each training paradigm within a tractable graph-based framework using a simplified Transformer model. Primary results show that while PG can achieve 100% training accuracy, its output diversity continuously declines, whereas Q-learning with process-level rewards converges to a diversity-preserving solution that captures the correct graph structure, avoiding the reward hacking seen with outcome-only rewards. The principal implication for AI practitioners is that using PG for planning may inadvertently reduce solution diversity and harm generalization; employing Q-learning with process-based rewards is a more robust alternative that maintains diversity and enables more efficient off-policy training. |
| TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics (Read more on [arXiv](https://arxiv.org/abs/2509.26329) or [HuggingFace](https://huggingface.co/papers/2509.26329))| Szu-Chi Chen, Yueh-Hsuan Huang, Jia-Kai Dong, Yu-Hua Chen, Yi-Cheng Lin | This paper presents TAU, a benchmark for evaluating Large Audio-Language Models' (LALMs) understanding of culturally specific, non-semantic Taiwanese "soundmarks". The primary objective is to assess if current models can generalize to localized audio cues that are independent of lexical content and require cultural exposure to recognize. The benchmark was constructed using a human-in-the-loop pipeline involving curated concept collection, LLM-assisted generation of 1,794 multiple-choice questions for 702 audio clips, and an automated filtering process using ASR to ensure questions are not solvable by transcript alone. Experiments show that the best-performing model, Gemini 2.5 Pro, achieves a maximum accuracy of 73.9%, which is significantly lower than the human topline of 84.0%, revealing a substantial performance gap on localized audio tasks. The principal implication for AI practitioners is that models trained on globally-sourced data exhibit significant cultural blind spots, highlighting the critical need to incorporate localized datasets and evaluation methods to build more equitable and robust multimodal systems, as prompt engineering alone proved insufficient to close this gap. |
| EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series
  Forecasting (Read more on [arXiv](https://arxiv.org/abs/2509.26157) or [HuggingFace](https://huggingface.co/papers/2509.26157))|  | The paper introduces EntroPE, a time series forecasting framework that uses conditional entropy to dynamically create variable-length patches aligned with the data's temporal structure. The objective is to overcome the limitations of fixed-length, temporally-agnostic patching in transformers, which can fragment coherent patterns and cause train-inference distribution shifts. The methodology employs an Entropy-based Dynamic Patcher (EDP) with a lightweight causal transformer to identify transition points for patch boundaries, and an Adaptive Patch Encoder (APE) that uses cross-attention to create fixed-size representations from these variable-length patches. Experiments show EntroPE improves accuracy over baselines, achieving an approximate 20% accuracy gain on the ETTh1 benchmark relative to PatchTST. The principal implication for AI practitioners is that incorporating information-theoretic criteria into the input tokenization process is a practical method to improve model performance and efficiency in time series forecasting by respecting the data's intrinsic temporal dynamics. |
| VisualOverload: Probing Visual Understanding of VLMs in Really Dense
  Scenes (Read more on [arXiv](https://arxiv.org/abs/2509.25339) or [HuggingFace](https://huggingface.co/papers/2509.25339))| Muhammad Huzaifa, Soumya Jahagirdar, M. Jehanzeb Mirza, Wei Lin, Paul Gavrikov | This paper introduces VisualOverload, a VQA benchmark designed to test the fine-grained perception capabilities of Vision-Language Models (VLMs) in visually dense scenes. The primary objective is to assess whether state-of-the-art VLMs can perform fundamental, knowledge-free vision tasks in complex, "overloaded" environments, where existing benchmarks might overestimate their capabilities. The methodology involves creating a new dataset of 2,720 manually annotated question-answer pairs based on 150 high-resolution, public-domain paintings, covering six categories: activity, attribute, counting, OCR, reasoning, and scene classification, and then evaluating 37 different VLMs. The primary result is that even the best-performing model (`o3`) achieves only 69.5% overall accuracy and a mere 19.6% accuracy on the hardest question split, revealing significant failures in counting, OCR, and maintaining logical consistency. The principal implication for AI practitioners is that current VLMs are unreliable for applications requiring detailed perception in visually complex settings, as the vision encoder acts as a significant information bottleneck, limiting performance on fine-grained tasks. |
| jina-reranker-v3: Last but Not Late Interaction for Document Reranking (Read more on [arXiv](https://arxiv.org/abs/2509.25085) or [HuggingFace](https://huggingface.co/papers/2509.25085))|  | jina-reranker-v3 is a 0.6B parameter multilingual document reranker introducing a novel "last but not late interaction" architecture for efficient and effective document ranking. The research aims to bridge the efficiency-effectiveness tradeoff in neural document reranking by enabling rich cross-document interactions during encoding while maintaining competitive performance and efficiency. This is achieved through a novel architecture based on Qwen3-0.6B, employing causal self-attention within a shared context window to process queries and multiple documents simultaneously, followed by contextual embedding extraction from special tokens and a lightweight MLP projector. jina-reranker-v3 achieves state-of-the-art 61.94 nDCG@10 on the BEIR benchmark, outperforming the 1.5B parameter mxbai-rerank-large-v2 (61.44 nDCG@10) with 2.5 times fewer parameters. AI practitioners can leverage jina-reranker-v3 for high-performance, parameter-efficient document reranking in diverse domains, including complex retrieval tasks and multilingual scenarios, without incurring the computational costs of larger generative models. |
| d^2Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching (Read more on [arXiv](https://arxiv.org/abs/2509.23094) or [HuggingFace](https://huggingface.co/papers/2509.23094))| Jiarui Wang, Jiale Fu, Xiangzhong Luo, Yue Cai, Yuchu Jiang | d²Cache accelerates diffusion-based LLMs by introducing a training-free approximate Key-Value (KV) cache framework. The main objective is to overcome the inference efficiency challenges of dLLMs, which cannot directly benefit from standard KV caching due to bidirectional attention. d²Cache employs a two-stage fine-grained token selection strategy, using certainty prior for masked tokens and attention rollout for remaining tokens, to adaptively update only necessary KV states while caching others for reuse. Experiments demonstrate that d²Cache achieves an average 3.2x–4.0x inference speedup over Vanilla dLLMs and improves inference throughput on Dream-Inst/GSM8K by 4.7x (from 2.62 to 12.25 tokens/second) without sacrificing generation quality. This allows AI practitioners to significantly enhance dLLM inference efficiency and generation reliability, making these models more practical for various language tasks. |
| TTT3R: 3D Reconstruction as Test-Time Training (Read more on [arXiv](https://arxiv.org/abs/2509.26645) or [HuggingFace](https://huggingface.co/papers/2509.26645))| Anpei Chen, Andreas Geiger, Yuliang Xiu, Yue Chen, rover-xingyu | TTT3R enhances 3D reconstruction models by integrating a Test-Time Training perspective into RNN-based architectures. The research aims to overcome length generalization limitations and state forgetting in online 3D reconstruction models. This is achieved by reformulating state updates as an online learning process, where a confidence-guided, closed-form learning rate derived from alignment confidence between memory and observations is used for memory updates. TTT3R achieves a 2x improvement in global pose estimation over baselines while operating at 20 FPS with only 6 GB of GPU memory for thousands of images. This training-free, plug-and-play intervention offers AI practitioners a more scalable, efficient, and robust solution for real-time 3D reconstruction without additional fine-tuning or parameters. |
| Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics
  Research Benchmark (Read more on [arXiv](https://arxiv.org/abs/2509.26574) or [HuggingFace](https://huggingface.co/papers/2509.26574))| Penghao Zhu, Tianci Zhou, Xiaocheng Yang, Minyang Tian, Minhui Zhu | This paper introduces CritPt, a benchmark of 71 unpublished, research-level physics challenges designed to test the complex reasoning capabilities of Large Language Models (LLMs). The research objective is to determine if current LLMs can effectively solve unseen, open-ended problems characteristic of frontier physics research. The methodology consists of evaluating LLMs on problems created by over 50 physicists, using a two-step generation protocol and a physics-informed automated grading pipeline that verifies numerical, symbolic, and code-based answers. The primary finding is that the best-performing base model, GPT-5 (high), achieves only 4.0% average accuracy on full challenges, which increases to 11.7% when augmented with a code interpreter and web search. The principal implication for AI practitioners is that a significant gap exists between current model capabilities and the rigorous demands of scientific research, highlighting the need for developing more robust and scientifically grounded reasoning systems. |
| Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced
  Performance Gap (Read more on [arXiv](https://arxiv.org/abs/2509.26542) or [HuggingFace](https://huggingface.co/papers/2509.26542))| Hengfan Zhang, Yudong Liu, Qinsi Wang, Zhengmian Hu, linyueqian | This paper introduces the VERA benchmark to systematically quantify the "Voice Reasoning Gap" (VRG), the performance degradation observed when language models reason through a voice interface versus a text interface. The primary objective is to diagnose this modality-induced gap, hypothesizing it stems from an architectural conflict between low-latency audio streaming and the iterative computation required for complex reasoning. The methodology involves creating the VERA benchmark with 2,931 voice-adapted tasks across five reasoning domains and evaluating 12 diverse voice systems against text-only baselines, measuring both accuracy and first-response latency. A primary result shows a 68.7-point accuracy drop on competition mathematics, with a leading text model (GPT-5) achieving 74.8% accuracy while its voice counterpart (GPT-realtime) scores only 6.1%. The principal implication for AI practitioners is that overcoming the VRG requires fundamental architectural shifts away from monolithic streaming models toward systems that explicitly decouple the reasoning process from real-time speech narration. |
| LayerD: Decomposing Raster Graphic Designs into Layers (Read more on [arXiv](https://arxiv.org/abs/2509.25134) or [HuggingFace](https://huggingface.co/papers/2509.25134))| Kota Yamaguchi, Naoto Inoue, Kang-Jun Liu, Tomoyuki Suzuki | The paper presents LayerD, a framework that automatically decomposes single raster graphic designs into a sequence of editable layers. The primary objective is to reverse the image composition process by iteratively applying top-layer matting to extract the unoccluded foreground, followed by background completion using an inpainting model, and a novel palette-based refinement step to improve quality. On the Crello dataset, LayerD significantly outperforms baselines, achieving a higher Alpha soft IoU (~0.83 vs. <0.75 for VLM/YOLO baselines) and lower RGB L1 error with zero allowed edits. For AI practitioners, this work provides a complete pipeline and models for reverse-engineering flat graphic assets, enabling layer-based editing capabilities in creative tools for images that have lost their original layer structure. |
| Who invented deep residual learning? (Read more on [arXiv](https://arxiv.org/abs/2509.24732) or [HuggingFace](https://huggingface.co/papers/2509.24732))| Juergen Schmidhuber | This paper argues that the foundational principle of deep residual learning—residual connections with a weight of 1.0 to ensure constant error flow—was introduced in 1991 for Recurrent Neural Networks (RNNs). The primary objective is to document the historical evolution of residual connections and attribute their invention to Sepp Hochreiter's 1991 diploma thesis, which mathematically derived them to solve the vanishing gradient problem. The methodology employed is a historical review of publications, tracing the concept's lineage from 1991 RNNs, through 1997 LSTMs and their gated 1999 variants, to their adaptation in feedforward architectures like Highway Networks (May 2015) and ResNets (Dec 2015). The primary result is the presented timeline, which establishes ResNet as an open-gated variant of the earlier Highway Network and a feedforward version of the 1997 LSTM. A key quantitative finding illustrates that a connection weight of 0.99 reduces a backpropagated error signal over 100 steps to ~37% of its original magnitude, whereas a weight of 0.9 reduces it to ~0.0027%, demonstrating the necessity of a weight of exactly 1.0 to prevent vanishing gradients. The principal implication for AI practitioners is that the core mechanism enabling modern very deep networks is the constant error flow via identity connections, a principle first developed for RNNs to solve the vanishing gradient problem, not a concept unique to recent feedforward architectures. |
| Knowledge Homophily in Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.23773) or [HuggingFace](https://huggingface.co/papers/2509.23773))| Nedim Lipka, Mahantesh Halappanavar, Zhisheng Qi, Utkarsh Sahu, Franck-Dernoncourt | This research demonstrates that Large Language Models exhibit "knowledge homophily," where the model's knowledge level about topologically close entities in a knowledge graph is similar, and leverages this structural pattern to improve knowledge-intensive tasks. The main objective is to empirically discover this knowledge homophily pattern and apply it to efficiently identify and address knowledge gaps in LLMs. The key methodology involves first computing entity-level "knowledgeability" scores by prompting an LLM on knowledge graph triplets, and then training a Graph Neural Network (GNN) to predict these scores for all entities by leveraging their local graph neighborhood. The primary results show that the GNN-guided approach for knowledge injection improves generalization gain from a random baseline of 60.5% to 67.7%, and the homophily-aware knowledge retrieval for question-answering improves 2-hop QA accuracy by an average of 4.57% over a semantic search baseline. The principal implication for AI practitioners is that an LLM's knowledge gaps can be predicted from a small sample of probed entities, enabling more efficient active labeling for fine-tuning and the development of smarter context retrieval systems that prioritize less-known, more informative facts for RAG. |
| BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source
  Software (Read more on [arXiv](https://arxiv.org/abs/2509.25248) or [HuggingFace](https://huggingface.co/papers/2509.25248))|  | This paper introduces BUILD-BENCH, a representative benchmark for compiling real-world open-source software, and presents OSS-BUILD-AGENT, an agentic system designed for this task. The main objective is to create a more challenging and generalizable benchmark for automated C/C++ open-source software compilation and to evaluate the performance of LLM agents against rule-based and existing agentic methods. The key methodology involves creating the BUILD-BENCH dataset by sampling 148 less-popular GitHub repositories to better represent in-the-wild challenges and developing OSS-BUILD-AGENT, a multi-agent system with an LLM-assisted instruction retrieval module and an iterative error resolution loop. The primary result shows that the proposed OSS-BUILD-AGENT, using LLM-assisted retrieval with the Claude 3.7-Sonnet model, achieved a 66.4% strict success rate on BUILD-BENCH, substantially outperforming prior methods. The principal implication for AI practitioners is that iterative agentic frameworks with explicit instruction retrieval and multi-turn error resolution loops are significantly more effective for automating complex software builds than single-turn prompts or rule-based systems, and their performance scales with the intelligence of the underlying LLM. |
| Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional
  Video Generation (Read more on [arXiv](https://arxiv.org/abs/2509.26555) or [HuggingFace](https://huggingface.co/papers/2509.26555))|  | This work introduces Stable Cinemetrics (SCINE), a framework using four hierarchical taxonomies—Setup, Event, Lighting, and Camera—to evaluate text-to-video models against professional filmmaking standards. The main objective is to assess the readiness of current generative models for professional use by measuring their adherence to 76 fine-grained cinematic control nodes. The methodology involves a large-scale human study where 80+ film professionals evaluated over 20K videos from 10+ models, and the training of a vision-language model (VLM) for automated evaluation. Primary results reveal that even the strongest models exhibit significant gaps, particularly in Event and Camera-related controls, while the trained VLM achieves a 72.36% accuracy in alignment with expert annotations. The principal implication for AI practitioners is that SCINE provides a granular, structured benchmark to diagnose specific model failures in cinematic control, guiding development beyond general prompt fidelity toward professional-grade video synthesis. |
| ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency
  Estimation (Read more on [arXiv](https://arxiv.org/abs/2509.26278) or [HuggingFace](https://huggingface.co/papers/2509.26278))| Antonio Liotta, Jacopo Staiano, Edoardo Bianchi | ProfVLM is a lightweight, multi-view video-language model that jointly estimates skill proficiency and generates expert-like textual feedback by reformulating the task as conditional text generation. The objective is to develop a computationally efficient and explainable framework for multi-view skill assessment that unifies proficiency classification and natural language feedback generation into a single generative task. The model employs a frozen TimeSformer visual encoder, a novel AttentiveGatedProjector module for multi-view feature fusion via attention and learnable gating, and a lightweight language model (SmolLMv2-135M) fine-tuned with LoRA to generate the final output. ProfVLM achieves state-of-the-art 48.2% top-1 accuracy on the EgoExo4D benchmark while using 20x fewer trainable parameters (5.3M vs. 121M) and reducing training time by 60% compared to baselines. The principal implication for AI practitioners is that framing video analysis tasks as conditional language generation can create more efficient, parameter-light, and interpretable models capable of providing richer, human-like feedback than traditional classification-only architectures. |
| MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial
  Purification (Read more on [arXiv](https://arxiv.org/abs/2509.25082) or [HuggingFace](https://huggingface.co/papers/2509.25082))| Zhiming Luo, Carl Yang, Kejia Zhang, Junwei Wu, Xiaoyi Huang | The paper introduces MANI-Pure, a diffusion-based adversarial purification framework that uses magnitude-adaptive noise injection to selectively target perturbations in the frequency domain. The objective is to improve upon existing purification defenses by replacing uniform noise injection with a method that adapts to the empirically observed non-uniform distribution of adversarial perturbations, thereby enhancing robustness while preserving clean image fidelity. The core methodology involves a forward process (MANI) that computes frequency-band-wise weights from an input's magnitude spectrum to generate heterogeneous noise targeting vulnerable low-magnitude frequencies, and a reverse process (FreqPure) that preserves low-frequency content from the original input. On CIFAR-10 against a ViT-L/14 classifier, MANI-Pure achieves 92.19% robust accuracy under an AutoAttack (l-infinity) threat model while maintaining a standard accuracy of 94.14%, narrowing the clean accuracy gap to the original classifier to 0.59%. The principal implication for AI practitioners is that MANI can be used as an efficient, plug-and-play module to significantly boost the adversarial robustness of various off-the-shelf models at inference time without requiring retraining. |
| Specialization after Generalization: Towards Understanding Test-Time
  Training in Foundation Models (Read more on [arXiv](https://arxiv.org/abs/2509.24510) or [HuggingFace](https://huggingface.co/papers/2509.24510))|  | This paper explains Test-Time Training (TTT) as a "specialization after generalization" mechanism, investigating why it improves performance on in-distribution data by allowing globally underparameterized models to locally reallocate capacity to task-relevant concepts. The work uses the Linear Representation Hypothesis (LRH) and trains a sparse autoencoder on ImageNet to show local data neighborhoods are sparsely supported, complemented by scaling studies comparing TTT against globally trained models across vision and language tasks. The key finding is that TTT's performance advantage over global training is largest for smaller models and diminishes as models become overparameterized; for instance, a sparsely supported TTT model using just ~40 active concepts achieved 72.64% accuracy on ImageNet, matching a dense TTT model and indicating an implicit bias towards sparse solutions. For AI practitioners, this implies TTT is a highly effective strategy for boosting performance on smaller, underparameterized models, but its relative benefit decreases for very large models that have sufficient capacity for global generalization. |
| Estimating Time Series Foundation Model Transferability via In-Context
  Learning (Read more on [arXiv](https://arxiv.org/abs/2509.23695) or [HuggingFace](https://huggingface.co/papers/2509.23695))| Jun Qi, Chao-Han Huck Yang, Chengqi Zhang, Ming Jin, Qingren | This paper proposes TIMETIC, a framework for estimating the transferability of Time Series Foundation Models (TSFMs) by reformulating model selection as an in-context learning problem. The objective is to efficiently predict the fine-tuning performance of a given TSFM on a target dataset without incurring the computational cost of actual fine-tuning. The methodology characterizes datasets via statistical features and models via a novel entropy profile across layers, structuring this information into a context table that prompts a tabular foundation model to predict performance for new model-data pairs. The framework achieves a mean Spearman rank correlation of approximately 0.6 between its estimated scores and actual fine-tuned performance, representing a 30% improvement over using zero-shot performance as a ranking proxy. For AI practitioners, TIMETIC offers a computationally efficient method to pre-select the most promising TSFM for a specific downstream forecasting task, significantly reducing the resources required for model selection. |
