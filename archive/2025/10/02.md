

## Papers for 2025-10-02

| Title | Authors | Summary |
|-------|---------|---------|
| DeepSearch: Overcome the Bottleneck of Reinforcement Learning with
  Verifiable Rewards via Monte Carlo Tree Search (Read more on [arXiv](https://arxiv.org/abs/2509.25454) or [HuggingFace](https://huggingface.co/papers/2509.25454))|  | The paper introduces DeepSearch, a framework integrating Monte Carlo Tree Search (MCTS) into the Reinforcement Learning with Verifiable Rewards (RLVR) training loop to improve language model reasoning. The objective is to overcome the performance plateaus in current RLVR training caused by insufficient exploration by embedding a systematic search process directly into model training. The methodology uses MCTS with three key innovations: a global frontier selection strategy to prioritize promising nodes, entropy-based guidance to identify confident paths for supervision, and an adaptive replay buffer with solution caching for efficiency. The primary result shows that the DeepSearch 1.5B model achieves a new state-of-the-art 62.95% average accuracy on mathematical reasoning benchmarks, a 1.25 percentage point improvement over the previous best, while using 5.7x fewer GPU hours than extended training methods. The principal implication for AI practitioners is that integrating structured search into the training phase provides a more computationally efficient path to improving reasoning performance than brute-force scaling of training steps. |
| GEM: A Gym for Agentic LLMs (Read more on [arXiv](https://arxiv.org/abs/2510.01051) or [HuggingFace](https://huggingface.co/papers/2510.01051))|  | This paper introduces GEM (General Experience Maker), an open-source, standardized environment framework for training and evaluating agentic large language models (LLMs). The primary objective is to provide a unified infrastructure to accelerate research in agentic LLMs by standardizing the agent-environment interface for diverse, multi-turn, long-horizon tasks, including those requiring tool use. The authors developed the GEM framework, which features a standardized API, a suite of environments across five categories, and tool integration, and they propose a baseline multi-turn algorithm, REINFORCE with Return Batch Normalization (ReBN). The paper demonstrates through extensive benchmarking that ReBN is effective across 24 environments and that combining RL with tool integration substantially boosts performance; for example, a Qwen3-4B model's average score on math benchmarks increased from 35.3% to 49.8% when trained with a Python tool. The principal implication for AI practitioners is that GEM offers a decoupled and standardized library for developing and benchmarking agentic LLMs, simplifying the creation of complex interactive scenarios and enabling apples-to-apples comparisons of different RL algorithms. |
| VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified
  Rewards in World Simulators (Read more on [arXiv](https://arxiv.org/abs/2510.00406) or [HuggingFace](https://huggingface.co/papers/2510.00406))| Zirui Ge, Yihao Wang, Runze Suo, Pengxiang Ding, Hengtao Li | This paper introduces VLA-RFT, a framework for reinforcement fine-tuning Vision-Language-Action (VLA) models using a learned, data-driven world model as a simulator. The main objective is to overcome the limitations of imitation learning, such as compounding errors and poor robustness, without requiring costly real-world interactions. The methodology involves training a world model to predict future visual observations from actions, which then facilitates policy rollouts to generate dense, trajectory-level rewards by comparing predicted visual trajectories to expert references; these rewards are used to optimize the VLA policy with Generalized Reinforcement Policy Optimization (GRPO). With fewer than 400 fine-tuning steps, VLA-RFT increased the average success rate on the LIBERO benchmark to 91.1%, surpassing a strong supervised baseline (86.6%) that required 150K iterations. The principal implication for AI practitioners is that this world-model-based approach provides a highly sample-efficient method to significantly improve the performance and robustness of pre-trained VLA models, drastically reducing the need for real-world interaction or extensive supervised fine-tuning. |
| Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget
  Allocation (Read more on [arXiv](https://arxiv.org/abs/2509.25849) or [HuggingFace](https://huggingface.co/papers/2509.25849))|  | This paper introduces Knapsack RL, a framework that optimizes the allocation of computational exploration budgets for Reinforcement Learning (RL) with Large Language Models (LLMs). The research objective is to determine the optimal distribution of a fixed total exploration budget across heterogeneous training tasks to maximize learning, overcoming the inefficiencies of uniform allocation. The methodology formulates this as a classical knapsack problem, where each task is assigned a "value" based on its current success rate and the probability of yielding a non-zero gradient, allowing for dynamic allocation of computational rollouts. The primary result is that this approach increases the effective gradient ratio by 20-40% over the baseline GRPO algorithm and achieves comparable performance with approximately half the computational resources. The principal implication for AI practitioners is that they can significantly improve the efficiency and final performance of RL-based LLM fine-tuning without increasing computational costs, effectively providing a "free lunch" by reallocating existing resources more intelligently. |
| Code2Video: A Code-centric Paradigm for Educational Video Generation (Read more on [arXiv](https://arxiv.org/abs/2510.01174) or [HuggingFace](https://huggingface.co/papers/2510.01174))|  | The paper introduces Code2Video, a code-centric, multi-agent paradigm that generates educational videos by structuring lecture content into executable Python code for a rendering engine. The research objective is to create a controllable, interpretable, and scalable method for producing temporally coherent and spatially precise educational videos, overcoming the structural limitations of pixel-space synthesis models. The methodology utilizes a tri-agent framework where a Planner creates a storyboard, a Coder translates it into parallelized and debugged Manim code, and a Critic refines spatial layouts using a novel visual anchor prompt and VLM feedback. On the newly introduced MMMC benchmark, Code2Video demonstrates a 40% improvement on the TeachQuiz knowledge transfer metric over direct code generation baselines, producing videos comparable to human-crafted tutorials. The principal implication for AI practitioners is that using executable code as an intermediate representation within a multi-agent system offers a robust and scalable solution for complex generative tasks requiring high structural fidelity and control, significantly outperforming direct pixel-level generation methods. |
| ACON: Optimizing Context Compression for Long-horizon LLM Agents (Read more on [arXiv](https://arxiv.org/abs/2510.00615) or [HuggingFace](https://huggingface.co/papers/2510.00615))|  | The paper introduces Agent Context Optimization (ACON), a framework that uses an LLM to iteratively refine natural language guidelines for compressing the extensive interaction histories and observations of long-horizon LLM agents.  The primary objective is to develop a systematic and adaptive method for context compression that reduces computational costs and memory usage in long-horizon tasks, while preserving or even improving the agent's task-completion performance.  The key methodology is a gradient-free, failure-driven guideline optimization process: an "optimizer" LLM analyzes trajectories where an agent with full context succeeds but fails with compressed context, generating natural language feedback to refine the compression instructions. This process has two stages: utility maximization (UT) to improve task success, and compression maximization (CO) to increase conciseness. The resulting optimized compressor can then be distilled into a smaller model.  Experiments show that ACON reduces memory usage by 26-54% across benchmarks; specifically, on AppWorld, it reduced peak input tokens by 26% for a gpt-4.1 agent while maintaining task accuracy (56.5% with ACON vs. 56.0% without). For smaller agent models, ACON improved performance by up to 46%.  The principal implication for AI practitioners is that ACON provides a model-agnostic and deployment-friendly framework to significantly lower the operational costs and latency of LLM agents. By distilling the optimized compression logic into smaller models, engineers can create more efficient and scalable agentic systems without relying exclusively on large, expensive models for context management. |
| PIPer: On-Device Environment Setup via Online Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.25455) or [HuggingFace](https://huggingface.co/papers/2509.25455))|  | The paper introduces PIPER, a method for training on-device models to automate software environment setup using a two-stage fine-tuning process. The research objective is to create a specialized, small model that can overcome the limitations of general-purpose LLMs and perform comparably to larger models on this task. The key methodology combines Supervised Fine-Tuning (SFT) on successful scripts generated by a larger model, followed by Reinforcement Learning with Verifiable Rewards (RLVR) using an execution-free, LLM-as-a-Judge proxy for reward generation. On the EnvBench-Python benchmark, the resulting 8B parameter model achieves a pass@5 score of 27, performing on par with the much larger Qwen3-32B and GPT-40 models. For AI practitioners, this work demonstrates that combining SFT with proxy-reward RLVR enables the development of high-performing, cost-effective on-device models for complex software engineering tasks, reducing dependency on larger, API-gated systems. |
| Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals
  Long-Range Dependency Pitfalls (Read more on [arXiv](https://arxiv.org/abs/2510.00184) or [HuggingFace](https://huggingface.co/papers/2510.00184))| Stuart Shieber, Chenhao Tan, Itamar Pres, Xiaoyan Bai, yuntian-deng | This paper reverse-engineers a Transformer that learns multi-digit multiplication to show standard models fail due to an inability to learn long-range dependencies with an auto-regressive loss. The primary objective is to determine why standard fine-tuned (SFT) Transformers fail at multiplication by analyzing the mechanisms of a successful model trained with implicit chain-of-thought (ICoT). The key methodology involves reverse-engineering a 2-layer ICoT model using logit attributions, linear probes on hidden states to decode intermediate sums, and PCA to analyze the geometry of digit representations. The ICoT model learns to form a directed acyclic graph with its attention heads to compute and cache partial products, while the SFT model fails to learn these long-range dependencies; adding an auxiliary loss to predict the running sum enables a standard model to achieve 99% accuracy, up from less than 1% for the SFT model. The principal implication for AI practitioners is that for algorithmic tasks requiring complex long-range dependencies, standard auto-regressive fine-tuning is insufficient, and incorporating task-specific inductive biases, such as auxiliary losses on intermediate computational steps, is critical to escape local optima and achieve high performance. |
| BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model
  Responses (Read more on [arXiv](https://arxiv.org/abs/2510.00232) or [HuggingFace](https://huggingface.co/papers/2510.00232))| Julian McAuley, Ruizhe Chen, Churan Zhi, Xunzhi He, XinXuNLPer | This paper introduces BIASFREEBENCH, a benchmark for systematically evaluating bias mitigation techniques in LLM responses. The objective is to create a unified framework for comparing prompting- and training-based debiasing methods by assessing their impact on generated text, rather than on internal model probabilities. The methodology involves evaluating eight techniques (e.g., Self-Reflection, DPO) on seven LLMs using reorganized BBQ and FairMT-Bench datasets and a new metric, the Bias-Free Score (BFS), which measures the proportion of fair, safe, and anti-stereotypical responses. The primary result is that prompting-based methods generally outperform training-based methods; for instance, on the BBQ dataset, Chain-of-Thought (CoT) prompting increased the BFS of Llama-3.1 from 52.41% to 82.82%, whereas SFT training reduced it to 52.11%. The principal implication for AI practitioners is that implementing carefully designed prompts is often a more effective and computationally efficient strategy for mitigating response-level bias than undertaking complex and potentially capability-degrading model fine-tuning. |
| Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel
  Execution (Read more on [arXiv](https://arxiv.org/abs/2509.25301) or [HuggingFace](https://huggingface.co/papers/2509.25301))|  | This paper introduces FLASH-SEARCHER, a novel agent framework that accelerates complex reasoning tasks by reformulating sequential execution into a parallel, Directed Acyclic Graph (DAG)-based workflow. The primary objective is to overcome the inefficiency and high latency of traditional, sequential tool-augmented agent frameworks by developing a paradigm that enables concurrent execution of independent reasoning paths. The core methodology involves decomposing a complex task into subtasks with explicit dependencies, represented as a DAG, which allows for parallel inferential execution and tool orchestration, while adaptive progress tracking dynamically optimizes the graph based on intermediate results. Primary results demonstrate that the FLASH-SEARCHER framework achieves 67.7% accuracy on the BrowseComp benchmark while reducing agent execution steps by up to 35% compared to sequential approaches, improving both effectiveness and efficiency. The principal implication for AI practitioners is that adopting a DAG-based parallel execution architecture provides a direct method to reduce latency and computational cost in tool-intensive agent applications, offering a scalable alternative to linear reasoning chains. |
| BroRL: Scaling Reinforcement Learning via Broadened Exploration (Read more on [arXiv](https://arxiv.org/abs/2510.01180) or [HuggingFace](https://huggingface.co/papers/2510.01180))|  | BroRL scales reinforcement learning for large language models by increasing the number of rollouts per prompt, demonstrating a more efficient scaling axis than simply increasing training steps. The main objective is to determine if scaling the number of rollouts (N) in Reinforcement Learning with Verifiable Rewards (RLVR) can overcome the performance saturation seen in step-scaling methods. The methodology is based on a mass balance analysis which proves that a larger N minimizes a negative "unsampled coupling" term in the policy update; this is implemented by increasing N from 16 to 512 within a PPO-based framework while scaling the learning rate. BroRL successfully revives a saturated model, improving its Math score to 63.03 where the step-scaling ProRL baseline degrades to 62.02, while also nearly doubling generation throughput from 36.5 to 72.4 samples/s. The principal implication for AI practitioners is that scaling rollout size is a critical, computationally efficient method to overcome performance plateaus in RL training and improve hardware utilization by shifting the sample generation bottleneck from memory-bound to compute-bound. |
| Beyond Log Likelihood: Probability-Based Objectives for Supervised
  Fine-Tuning across the Model Capability Continuum (Read more on [arXiv](https://arxiv.org/abs/2510.00526) or [HuggingFace](https://huggingface.co/papers/2510.00526))| Hanghang Tong, Heng Ji, Xiusi Chen, Ruizhong Qiu, Gaotang Li | This paper demonstrates that the optimal supervised fine-tuning (SFT) objective for language models is not universally negative log likelihood (NLL), but depends on the base model's prior capability for a given task, a concept the authors frame as the "model-capability continuum". The objective is to characterize when NLL is suboptimal for SFT and identify which alternative probability-based objectives are more effective depending on the alignment between the base model's priors and the fine-tuning task. The study introduces a continuum from model-strong (MS) domains (e.g., math), where the model has strong priors, to model-weak (MW) domains (e.g., novel puzzles), and empirically evaluates objectives like `-log p` (NLL) and `-p` across seven model backbones and three domains. The primary result shows that in MS settings, prior-leaning objectives that downweight low-probability tokens consistently outperform NLL, with a `-p` objective on Qwen2.5-Math-7B achieving 36.51% average accuracy versus 22.67% for NLL, while NLL dominates in MW settings. For AI practitioners, the principal implication is to select the SFT objective adaptively: use prior-leaning objectives like `-p` or thresholded NLL when fine-tuning on tasks where the base model already has strong knowledge, and retain the standard NLL for tasks novel to the model. |
| On Predictability of Reinforcement Learning Dynamics for Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2510.00553) or [HuggingFace](https://huggingface.co/papers/2510.00553))| Yuqing Huang, Zijun Yao, Ding Cao, Yuchen Cai, xx18 | This research finds that reinforcement learning-induced parameter updates in large language models are dominated by a single, linearly evolving Rank-1 subspace, enabling predictable training acceleration. The main objective was to determine if RL-guided parameter updates follow consistent patterns and how these patterns create reasoning capabilities. The authors primarily used Singular Value Decomposition (SVD) to analyze the parameter update matrix (ΔW) and Partial Least Squares (PLS) regression to model the temporal evolution of its dominant Rank-1 component. Key results show that this Rank-1 subspace alone recovers over 99% of reasoning performance gains and evolves with high linearity (average R² > 0.91), which allowed the proposed AlphaRL framework to achieve up to a 2.5x training speedup while retaining over 96% of final performance. The principal implication for AI practitioners is the ability to significantly reduce the computational cost of RL training by extrapolating the final parameter update from a short early training window, without needing additional modules or hyperparameter tuning. |
| GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness (Read more on [arXiv](https://arxiv.org/abs/2510.00536) or [HuggingFace](https://huggingface.co/papers/2510.00536))| Chien-Sheng Wu, Caiming Xiong, Yutong Dai, Haoyi Qiu, Kung-Hsiang Huang | GUI-KV is a no-retraining KV cache compression method that leverages the unique spatio-temporal redundancies in GUI agent workloads to improve inference efficiency and accuracy. The objective is to develop an efficient, plug-and-play KV cache compression technique for VLM-based GUI agents that process long sequences of high-resolution screenshots without requiring model retraining. The method combines spatial saliency guidance, which uses the L2 norm of hidden states to identify important visual tokens, with temporal redundancy scoring, which uses QR decomposition to prune key vectors from past screenshots that are already represented in the current screenshot's key subspace. On the AgentNetBench benchmark in a 5-screenshot setting, GUI-KV reduces decoding FLOPs by 38.9% while simultaneously increasing step accuracy by 4.1% compared to a full-cache baseline. For AI practitioners, this implies that VLM-based GUI agents can be deployed with significantly lower computational cost and memory usage, potentially enabling better performance on resource-constrained hardware by mitigating long-context distraction. |
| Training Vision-Language Process Reward Models for Test-Time Scaling in
  Multimodal Reasoning: Key Insights and Lessons Learned (Read more on [arXiv](https://arxiv.org/abs/2509.23250) or [HuggingFace](https://huggingface.co/papers/2509.23250))|  | This paper investigates the design, training, and evaluation of Vision-Language Process Reward Models (VL-PRMs) to improve multimodal reasoning through test-time scaling. The main objective is to elucidate the VL-PRM design space by systematically exploring diverse strategies for dataset construction, training with perception-focused supervision, and test-time scaling. The methodology involves creating the VL-PRM300K dataset using a hybrid framework combining Monte Carlo Tree Search with judgments from a strong VLM (o4-mini), which is then used to fine-tune Qwen-VL-based PRMs. Key results show that VL-PRMs used as Outcome Reward Models (ORMs) for scoring complete solutions outperform per-step guided search, and that scaling can unlock latent reasoning abilities, improving a Gemma3-12B model's performance on PuzzleVQA by 12.7%. The principal implication for AI practitioners is that integrating a VL-PRM for one-shot solution verification at inference time is a highly effective and computationally cheaper strategy than per-step guidance to significantly boost the performance of large VLMs on complex reasoning tasks. |
| Infusing Theory of Mind into Socially Intelligent LLM Agents (Read more on [arXiv](https://arxiv.org/abs/2509.22887) or [HuggingFace](https://huggingface.co/papers/2509.22887))|  | This paper introduces ToMAgent (TOMA), a dialogue agent that integrates explicit Theory of Mind (ToM) modeling with dialogue lookahead to improve goal-oriented social reasoning in LLMs. The main research objective is to determine how to effectively equip LLMs with Theory of Mind abilities to improve their social reasoning and goal achievement in interactive dialogues. The key methodology involves a lookahead training framework that generates candidate mental state hypotheses and utterances, simulates conversation outcomes to score goal achievement, and then fine-tunes an LLM on the mental state-utterance pairs from the most successful simulated trajectories. Primary results on the Sotopia benchmark show that TOMA achieves a social score improvement of up to 18.9% over the base model variant and exhibits more strategic, long-horizon reasoning, while also demonstrating competitive performance against a GPT-5-nano baseline. The principal implication for AI practitioners is that explicitly training models to generate and leverage internal mental state representations, guided by future goal achievement, is a highly effective strategy for developing more socially intelligent and successful goal-oriented agents, moving beyond simple reactive utterance generation. |
| Making, not Taking, the Best of N (Read more on [arXiv](https://arxiv.org/abs/2510.00931) or [HuggingFace](https://huggingface.co/papers/2510.00931))|  | This paper proposes Fusion-of-N (FUSION), a synthesis-based method that uses an LLM to generate a single superior response from multiple candidate outputs, outperforming traditional Best-of-N (BON) selection. The primary objective is to investigate whether synthesizing information from N candidate generations is a more effective aggregation strategy than selecting the single best candidate, particularly for test-time scaling and synthetic data generation. The core methodology involves using a general LLM as a "fusor" to analyze a pool of N candidate generations and synthesize a new, final answer that combines their strengths, which is then benchmarked against BON across 11 languages and multiple tasks. Results demonstrate that FUSION consistently outperforms BON; for instance, in test-time scaling on the mArena-v2 benchmark, FUSION improves the win-rate by +3.8% against GEMINI2.5-PRO compared to BON. The principal implication for AI practitioners is to shift from a "winner-takes-all" selection paradigm to a collaborative synthesis approach, using a capable LLM to fuse multiple generated samples to produce a higher-quality final output, thereby making more efficient use of generated candidates and unlocking superior performance. |
| CurES: From Gradient Analysis to Efficient Curriculum Learning for
  Reasoning LLMs (Read more on [arXiv](https://arxiv.org/abs/2510.01037) or [HuggingFace](https://huggingface.co/papers/2510.01037))| Hengyi Cai, Erxue Min, Bokai Ji, Zexu Sun, Yongcheng Zeng | This paper presents CurES, a curriculum learning framework that enhances the training efficiency of reasoning LLMs by dynamically allocating computational resources based on gradient analysis. The research aims to improve training efficiency by theoretically linking gradient optimization to two key factors: the sampling distribution of prompts and the allocation of rollout quantities. CurES employs Bayesian posterior estimation to assess prompt difficulty based on the model's answering accuracy, then adaptively reallocates prompt sampling probabilities and rollout quantities to focus computation on moderately difficult examples. Experiments demonstrate that CurES outperforms the Group Relative Policy Optimization (GRPO) baseline by +4.82 points on a 7B model and converges up to 5.5x faster. The principal implication for AI practitioners is a more computationally efficient training strategy for reasoning models, which reduces resource waste by moving beyond uniform data sampling to a dynamic curriculum that prioritizes the most informative training instances. |
| In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.00777) or [HuggingFace](https://huggingface.co/papers/2510.00777))| Chaehyeon Chung, Seunghyuk Cho, Saemi Moon, Minjong Lee, Youngbin Choi | This paper introduces in-place feedback, an interaction paradigm where users directly edit an LLM's response to guide multi-turn reasoning, demonstrating superior performance and token efficiency over conventional feedback methods. The primary objective is to determine if direct state repair via in-place editing is a more effective mechanism for error correction in complex reasoning tasks compared to traditional conversational feedback. The study empirically compares in-place feedback against standard multi-turn feedback on reasoning benchmarks (MATH-hard, MMLU-pro, GPQA) and uses controlled experiments on the ZebraLogic dataset with automated feedback agents to measure turn-level dynamics. The primary result is that in-place feedback consistently achieves higher task accuracy and reduces aggregate token usage by 79.1% relative to multi-turn feedback. For AI practitioners, this implies that implementing in-place editing interfaces in collaborative AI applications offers a more direct and efficient method for correcting model errors, mitigating common failure modes like feedback disregard and error propagation seen in conversational refinement. |
| JoyAgent-JDGenie: Technical Report on the GAIA (Read more on [arXiv](https://arxiv.org/abs/2510.00510) or [HuggingFace](https://huggingface.co/papers/2510.00510))|  | This paper presents JoyAgent-JDGenie, a generalist agent architecture designed to enhance robustness by systematically integrating a multi-agent framework, hierarchical memory, and a refined tool suite. The primary objective is to create a unified framework that overcomes the limitations of isolated component improvements in existing agent systems. The key methodology combines a heterogeneous ensemble of Plan-Execute and ReAct agents coordinated by a critic model, a three-layer memory system (working, semantic, procedural), and a tool suite focused on search, code execution, and multimodal parsing. The framework achieves a 75.2 Pass@1 score on the GAIA validation set and 67.1 Pass@1 on the test set, surpassing contemporary open-source baselines. The principal implication for AI practitioners is that system-level integration of complementary agent architectures and memory systems is more effective for building robust generalist agents than optimizing individual components, with the fusion of different reasoning patterns proving critical for performance gains. |
| An Empirical Study of Testing Practices in Open Source AI Agent
  Frameworks and Agentic Applications (Read more on [arXiv](https://arxiv.org/abs/2509.19185) or [HuggingFace](https://huggingface.co/papers/2509.19185))| Bram Adams, Gopi Krishnan Rajbahadur, Emad Fallahzadeh, Mohammed Mehedi Hasan, hao-li | This large-scale empirical study analyzes unit testing practices in 39 open-source AI agent frameworks and 439 agentic applications to establish a quality assurance baseline. The primary objective is to identify common testing patterns and map their distribution across canonical agent architectural components. Employing repository mining and qualitative card-sorting on test functions, the study reveals a fundamental inversion of testing effort: deterministic infrastructure like `Resource Artifacts` (tools) and `Coordination Artifacts` (workflows) consumes over 70% of testing, while the non-deterministic, FM-based `Plan Body` receives less than 5% and the critical `Trigger` component (prompts) is tested in only about 1% of cases. Although practitioners adapt traditional patterns like membership testing to manage uncertainty, novel agent-specific methods like `DeepEval` see negligible adoption (~1%). The principal implication for AI practitioners is the urgent need to establish systematic prompt regression testing suites to mitigate risks from model evolution and performance decay. |
| Eliciting Secret Knowledge from Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.01070) or [HuggingFace](https://huggingface.co/papers/2510.01070))| Neel Nanda, Senthooran Rajamanoharan, Rowan Wang, Emil Ryd, Bartosz Cywiński | This research establishes a benchmark for secret elicitation by training LLMs to hide knowledge and then evaluating black-box and white-box techniques to uncover it. The primary objective is to discover and evaluate methods for eliciting knowledge that an LLM possesses and applies but is explicitly trained to deny knowing. The methodology involves fine-tuning three LLM families (Taboo, Secret Side Constraint, User Gender) to conceal specific secrets and then using an automated auditing framework where a weaker "auditor" LLM evaluates various elicitation techniques, including prefill attacks, logit lens, and Sparse Autoencoders (SAEs). The primary result is that black-box prefill attacks are highly effective, enabling the auditor to achieve over 90% success rate in uncovering secrets in the Secret Side Constraint and User Gender settings, significantly outperforming simple baselines. For AI practitioners, this implies that even without internal model access, simple and practical black-box techniques like prefill attacks can be highly effective for auditing models and probing for concealed knowledge or unintended behaviors. |
| ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced
  Wasserstein Distance for Variance Reduction (Read more on [arXiv](https://arxiv.org/abs/2510.01061) or [HuggingFace](https://huggingface.co/papers/2510.01061))|  | ReSWD is an unbiased, variance-reduced estimator for the Sliced Wasserstein Distance that integrates Weighted Reservoir Sampling to reuse informative projection directions. The primary objective is to mitigate the high variance of standard Monte Carlo-based SWD estimators, which leads to noisy gradients and slow convergence in optimization tasks. The core methodology, Reservoir SWD (ReSWD), maintains a persistent reservoir of high-contribution projection directions across optimization steps, using their 1D Wasserstein cost as weights in a reservoir sampling scheme to focus computational effort while remaining unbiased. Experiments show that on 1D distribution matching tasks, ReSWD achieves a lower final mean Wasserstein distance (0.622 x 10⁻³) than standard SWD (0.733 x 10⁻³) and other variance reduction methods. For AI practitioners, ReSWD serves as a more efficient drop-in replacement for SWD-based loss functions in applications like generative model guidance and color correction, enabling more stable training and faster convergence. |
| VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained
  Perception in VLMs (Read more on [arXiv](https://arxiv.org/abs/2509.25916) or [HuggingFace](https://huggingface.co/papers/2509.25916))|  | VLM-FO1 is a plug-and-play framework that endows pre-trained Vision-Language Models (VLMs) with fine-grained perception by reframing object localization from a coordinate generation problem into a feature retrieval task. The research objective is to bridge the gap between the high-level reasoning of VLMs and the precise spatial localization required for perception-centric tasks. The methodology involves a Hybrid Fine-grained Region Encoder (HFRE) with a Dual-Vision Encoder that converts region proposals into distinct "region tokens" and a two-stage, decoupled training process that preserves the base VLM's capabilities. The lightweight VLM-FO1-3B model achieves state-of-the-art performance, attaining 44.4 mAP on the COCO object detection benchmark, which is a significant improvement over baseline VLMs. For AI practitioners, the principal implication is the ability to enhance existing, pre-trained VLMs with superior perception capabilities via a modular component, avoiding costly full model retraining and without degrading the original model's general visual understanding. |
| Boolean Satisfiability via Imitation Learning (Read more on [arXiv](https://arxiv.org/abs/2509.25411) or [HuggingFace](https://huggingface.co/papers/2509.25411))| Xiangyu Xu, Jun Chen, Yuanhao Yu, Huan Liu, Zewei Zhang | ImitSAT is a branching policy for CDCL solvers that uses imitation learning on expert-derived decision sequences to reduce solver runtime. The primary objective is to create a learning-based branching policy that improves upon traditional heuristics and reinforcement learning methods by directly imitating high-quality decision sequences extracted from expert solver runs. The key methodology involves first distilling raw, noisy CDCL solver trails into compact, nearly conflict-free "KeyTraces" that contain only surviving decisions, and then training an autoregressive Transformer model via behavior cloning to predict the next branching decision based on the problem instance and the current KeyTrace prefix. On structured SAT instances from the PRET family, ImitSAT reduces the median propagation count by 58% (MRPP of 0.42) relative to a standard CDCL solver, demonstrating strong generalization from its training on random 3-SAT. For AI practitioners, the principal implication is that distilling expert trajectories from search-based solvers into clean, sequential decision paths provides a highly effective data source for imitation learning, enabling sequence models to replace complex heuristics in combinatorial optimization problems. |
| Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic
  Architectures (Read more on [arXiv](https://arxiv.org/abs/2509.25045) or [HuggingFace](https://huggingface.co/papers/2509.25045))| Andrea Passerini, Jacopo Staiano, Bruno Lepri, Carlo Nicolini, Marco Bronzini | The paper introduces the Hyperdimensional Probe, a novel method using Vector Symbolic Architectures (VSAs) to decode interpretable concepts from the residual stream of Large Language Models. The primary objective is to create a decoding paradigm that overcomes the limitations of methods like Direct Logit Attribution (DLA) and Sparse Autoencoders (SAEs) by projecting internal LLM representations into a structured, human-readable symbolic space. The methodology involves compressing an LLM's late-layer embeddings via k-means clustering and sum pooling, then training a shallow neural network to map these compressed representations to pre-defined VSA hypervectors, which are then queried using hypervector algebra to extract specific concepts. In controlled analogy-completion tasks, the probe achieved an average concept retrieval precision of 83% (`probing@1`), demonstrating a robust ability to identify the correct target concept even when the LLMs' own next-token prediction accuracy was low (average 31% `next-token@1`). For AI practitioners, this work provides a computationally efficient tool for mechanistic interpretability that can diagnose model failures by revealing when a model correctly represents information internally but fails to articulate it, offering more granular debugging capabilities than token-based analysis. |
