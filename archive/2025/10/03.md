

## Papers for 2025-10-03

| Title | Authors | Summary |
|-------|---------|---------|
| LongCodeZip: Compress Long Context for Code Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.00446) or [HuggingFace](https://huggingface.co/papers/2510.00446))|  | LongCodeZip is a training-free framework designed to compress long code contexts for Large Language Models using a dual-stage, perplexity-based strategy. The objective is to develop a code-specific context compression method that preserves structural and semantic information critical for programming tasks, overcoming the limitations of generic text compressors. The methodology consists of a coarse-grained stage that ranks and selects function-level chunks using conditional perplexity, followed by a fine-grained stage that segments retained functions into semantic blocks and selects an optimal subset using a 0/1 knapsack algorithm. Evaluations show the framework achieves up to a 5.6x compression ratio on tasks like code completion while maintaining performance comparable to models using the full, uncompressed context. For AI practitioners, this model-agnostic, plug-and-play tool enables the use of LLMs on large codebases with significantly reduced API costs and latency, even when using a small 0.5B parameter model for the compression step. |
| Self-Forcing++: Towards Minute-Scale High-Quality Video Generation (Read more on [arXiv](https://arxiv.org/abs/2510.02283) or [HuggingFace](https://huggingface.co/papers/2510.02283))|  | Self-Forcing++ is a method that extends autoregressive video generation to minute-scale duration by mitigating error accumulation. The objective is to overcome the quality degradation that occurs when student models generate videos longer than the short horizon of their bidirectional teacher models, without requiring long-video datasets for retraining. The key methodology involves having the student model generate long, error-accumulated rollouts and then using the short-horizon teacher to provide corrective guidance on sampled segments of these rollouts via extended distribution-matching distillation and a rolling KV cache. This approach achieves generation of videos up to 4 minutes and 15 seconds, a 50x improvement over the baseline, and on 100-second videos, it attains a dynamic degree of 54.12, outperforming the Self-Forcing baseline by 104.9%. For AI practitioners, this presents a framework to significantly extend the temporal capabilities of autoregressive models by using a short-horizon teacher for self-correction on extrapolated outputs, thereby circumventing the need for extensive long-duration training data. |
| StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided
  Illusions (Read more on [arXiv](https://arxiv.org/abs/2510.02314) or [HuggingFace](https://huggingface.co/papers/2510.02314))|  | This paper introduces StealthAttack, a robust data poisoning method for 3D Gaussian Splatting that embeds viewpoint-dependent illusions by strategically placing poison points in low-density regions and disrupting multi-view consistency.  The primary objective is to develop an effective and stealthy data poisoning attack against 3D Gaussian Splatting (3DGS) that can inject a visible illusory object into a specific target viewpoint while minimally affecting the rendering quality of all other non-target viewpoints.  The methodology combines a "Density-Guided Point Cloud Attack," which uses Kernel Density Estimation (KDE) to identify and place poison Gaussian points in low-density scene regions, and a "View Consistency Disruption Attack," which applies scheduled adaptive Gaussian noise to innocent training views to weaken 3DGS's multi-view consistency property.  The proposed method significantly outperforms baseline attacks, achieving a V-ILLUSORY PSNR of 27.04 dB on the poisoned view for the Mip-NeRF 360 dataset, compared to 17.60 dB from the best-performing baseline, while maintaining high fidelity (27.76 dB PSNR) on innocent views.  For AI practitioners, this work reveals a critical vulnerability in 3DGS models: their foundational multi-view consistency can be subverted to embed malicious content, necessitating robust data validation and model verification pipelines before deploying 3DGS in security-sensitive applications. |
| ExGRPO: Learning to Reason from Experience (Read more on [arXiv](https://arxiv.org/abs/2510.02245) or [HuggingFace](https://huggingface.co/papers/2510.02245))| Dongrui Liu, Xiaoye Qu, Zhi Wang, Yafu Li, Runzhe Zhan | ExGRPO is a framework that enhances large language model reasoning by systematically managing and replaying valuable experiences within Reinforcement Learning from Verifiable Rewards (RLVR). The research investigates what makes a reasoning experience valuable and how to exploit it to overcome the sample inefficiency of on-policy RLVR. The key methodology involves maintaining a replay buffer, partitioning successful trajectories into buckets based on correctness, prioritizing medium-difficulty questions, and selecting the lowest-entropy trajectories for replay with a mixed-policy objective. ExGRPO achieves an average performance gain of +3.5 points on in-distribution mathematical benchmarks and +7.6 points on out-of-distribution benchmarks over on-policy RLVR, while also stabilizing training for weaker models where on-policy methods collapse. For AI practitioners, this demonstrates that principled experience management based on rollout correctness and entropy is a crucial technique for improving the sample efficiency and stability of RL fine-tuning for large reasoning models. |
| StockBench: Can LLM Agents Trade Stocks Profitably In Real-world
  Markets? (Read more on [arXiv](https://arxiv.org/abs/2510.02209) or [HuggingFace](https://huggingface.co/papers/2510.02209))| Jianing Yu, Jin Ye, Yantao Liu, Zijun Yao, Yanxu Chen | This paper introduces STOCKBENCH, a contamination-free benchmark designed to evaluate the profitability and risk management of LLM agents in realistic stock trading simulations. The objective is to assess if LLM agents can make sequential, profitable trading decisions in a dynamic, multi-month market environment using real-world data streams. The methodology consists of a back-trading workflow where agents receive daily prices, fundamentals, and news for 20 DJIA stocks and must issue buy, sell, or hold commands, with performance evaluated by cumulative return, maximum drawdown, and Sortino ratio. The primary result is that while most LLM agents fail to outperform a simple buy-and-hold baseline, some models like Qwen3-235B-Think achieved higher returns (2.5% vs 0.4% baseline), but excelling at reasoning tasks does not guarantee superior trading performance. The principal implication for AI practitioners is that an LLM's performance on static knowledge benchmarks does not translate to effective decision-making in dynamic, high-stakes environments, necessitating agent-specific evaluation frameworks that test sequential action and adaptation. |
| Interactive Training: Feedback-Driven Neural Network Optimization (Read more on [arXiv](https://arxiv.org/abs/2510.02297) or [HuggingFace](https://huggingface.co/papers/2510.02297))|  | This paper introduces Interactive Training, a framework for real-time, feedback-driven intervention in neural network training by humans or AI agents. The primary objective is to create and validate a framework that shifts neural network optimization from a static, predefined process to a dynamic one, allowing for mid-training adjustments to improve stability and performance. The methodology involves a three-part system: a FastAPI control server to manage commands, an `Interactive Trainer` built on Hugging Face's `Trainer` class that applies interventions via callbacks, and a React-based frontend for visualization and user input. In a GPT-2 finetuning experiment on Wikitext-2, human-in-the-loop interactive training achieved a lower final validation loss (approx. 4.5) compared to a static learning rate schedule (approx. 5.0), and an LLM-based agent successfully stabilized a training run initiated with an excessively high learning rate that otherwise failed to converge. The principal implication for AI practitioners is the ability to actively debug, steer, and adapt training runs in real-time, reducing compute cycles wasted on failed experiments and enabling continuous model improvement based on live feedback or observed instabilities. |
| VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.01444) or [HuggingFace](https://huggingface.co/papers/2510.01444))|  | The paper introduces VOGUE, a method that improves multimodal reasoning in MLLMs by guiding reinforcement learning exploration using the uncertainty derived from visual input perturbations. The primary objective is to address the exploration problem in multimodal RLVR by leveraging the inherent uncertainty of visual inputs to build more robust reasoning policies. VOGUE employs a dual-branch architecture that processes both an original and a stochastically augmented image; it quantifies visual uncertainty as the symmetric KL divergence between the two resulting policy distributions, using this signal to shape the learning objective with an uncertainty-proportional advantage bonus and an annealed sampling schedule. Implemented on Qwen2.5-VL-7B, VOGUE increased pass@1 accuracy over the GRPO baseline by an average of 2.6% across three visual math benchmarks and 3.7% across three general-domain reasoning benchmarks, while also improving pass@4 performance. For AI practitioners, the principal implication is that MLLM robustness and reasoning can be enhanced by incorporating input-space exploration—specifically by perturbing visual inputs to quantify model uncertainty and using this signal to directly guide the RL fine-tuning process—rather than relying solely on output-space exploration strategies. |
| The Rogue Scalpel: Activation Steering Compromises LLM Safety (Read more on [arXiv](https://arxiv.org/abs/2509.22067) or [HuggingFace](https://huggingface.co/papers/2509.22067))| Ivan Oseledets, Oleg Y. Rogov, Alexey Dontsov, Andrey Galichin, Anton Korznikov | This paper demonstrates that activation steering systematically compromises LLM safety, showing that adding even random or benign vectors to a model's hidden states can induce compliance with harmful requests. The research objective was to systematically quantify the safety vulnerabilities introduced by activation steering, a technique often framed as a precise alternative to fine-tuning. Using the JailbreakBench dataset and an LLM-as-judge, the authors applied steering vectors from random distributions and sparse autoencoders (SAEs) to the residual streams of models from the Llama3, Qwen2.5, and Falcon families. Key results show that random steering alone can increase harmful compliance from 0% to 2-27%, and a universal attack vector, created by averaging just 20 vectors that jailbreak a single prompt, increases compliance on unseen prompts by an average of 4x. The principal implication for AI practitioners is that activation steering presents a critical, exploitable vulnerability; systems exposing this capability, even for seemingly benign control, are susceptible to black-box attacks that can reliably bypass alignment safeguards. |
| CLUE: Non-parametric Verification from Experience via Hidden-State
  Clustering (Read more on [arXiv](https://arxiv.org/abs/2510.01591) or [HuggingFace](https://huggingface.co/papers/2510.01591))| Dian Yu, Linfeng Song, Yujun Zhou, Ruosen Li, Zhenwen Liang | This paper introduces CLUE, a non-parametric verifier that classifies the correctness of LLM outputs by clustering hidden-state activation trajectories from past experience. The primary objective is to determine if an LLM's internal hidden-state trajectory contains a geometrically separable signal for correctness that can be leveraged by a simple, training-free verifier to outperform text-level and confidence-based methods. CLUE operates by first computing "success" and "failure" centroids from the mean activation-deltas (the difference in hidden states before and after the reasoning block) of a labeled experience set; new solutions are then classified based on their layer-averaged Euclidean distance to these two centroids. Empirically, CLUE outperforms LLM-as-a-judge and confidence-based baselines; on the AIME 24 benchmark with a 1.5B model, CLUE improves reasoning accuracy from 56.7% (majority@64) to 70.0% (top-maj@16 reranking). The principal implication for AI practitioners is that an LLM's internal reasoning geometry, particularly after RL-based fine-tuning, provides a robust, low-cost signal for verification, enabling the creation of lightweight, training-free verifiers that can significantly improve reasoning accuracy by reranking candidate solutions without expensive external judges. |
| RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via
  Multi-Stage Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2510.02240) or [HuggingFace](https://huggingface.co/papers/2510.02240))|  | The paper introduces REWARDMAP, a multi-stage reinforcement learning framework to enhance fine-grained visual reasoning in MLLMs by addressing sparse reward challenges in tasks like transit map navigation. The main research objective is to develop a training strategy that improves MLLM capabilities in both detailed visual understanding and complex spatial reasoning, where standard methods fail. The key methodology combines a new dataset, REASONMAP-PLUS, for dense reward signals with a multi-stage RL curriculum that progresses from simple to complex tasks, utilizing a difficulty-aware reward design that grants partial credit for correct reasoning steps. Models trained with REWARDMAP achieve an average performance improvement of 3.47% across six different visual reasoning benchmarks, demonstrating enhanced generalization. The principal implication for AI practitioners is that a curriculum-based RL approach with a granular, shaped reward signal is a more effective strategy than standard SFT or basic RL for fine-tuning MLLMs on specialized, multi-step visual tasks with inherently sparse supervision. |
| RLP: Reinforcement as a Pretraining Objective (Read more on [arXiv](https://arxiv.org/abs/2510.01265) or [HuggingFace](https://huggingface.co/papers/2510.01265))|  | This paper introduces Reinforcement Learning Pre-training (RLP) to investigate if incorporating RL during pretraining is a more optimal method for developing reasoning in language models than reserving it for post-training. The key methodology treats chain-of-thought generation as an action and computes a dense, verifier-free reward based on the information gain—the log-likelihood increase for the next token when conditioned on the generated thought—relative to a no-think Exponential Moving Average (EMA) baseline. The primary result shows that pretraining the Qwen3-1.7B-BASE model with RLP lifts its average performance across an eight-benchmark math-and-science suite by 19% over the baseline. The principal implication for AI practitioners is that RLP offers a scalable, domain-agnostic pretraining objective that can instill robust reasoning abilities using general-purpose corpora, with gains that compound during subsequent alignment stages. |
| DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag
  Editing (Read more on [arXiv](https://arxiv.org/abs/2510.02253) or [HuggingFace](https://huggingface.co/papers/2510.02253))| Zhuming Lian, Shaocong Zhang, Shuli Leng, Shilin Lu, Zihan Zhou | DragFlow is the first framework to enable high-fidelity, region-based drag editing on Diffusion Transformer (DiT) models by replacing point-wise supervision with an affine transformation-based regional approach. The objective is to effectively harness the strong generative priors of modern DiT models for drag-based image editing, overcoming the limitations of previous point-based methods which perform poorly on DiT feature representations. The key methodology introduces region-level motion supervision using affine transformations, enforces background preservation with gradient mask-based hard constraints, and enhances subject consistency in CFG-distilled models via a pretrained personalization adapter. On the newly curated ReD Bench, DragFlow achieves state-of-the-art spatial accuracy, outperforming baselines with a Mean Distance (MD1) of 19.46, while demonstrating superior feature preservation. For AI practitioners, the principal implication is that robust drag-style editing can be applied to powerful DiT-based models by adopting region-level supervision, which is better suited for the fine-grained feature maps of transformers than traditional point-level techniques, enabling more controllable and higher-quality image manipulation. |
| The Unreasonable Effectiveness of Scaling Agents for Computer Use (Read more on [arXiv](https://arxiv.org/abs/2510.02250) or [HuggingFace](https://huggingface.co/papers/2510.02250))|  | The paper introduces Behavior Best-of-N (bBoN), a method that significantly improves computer-use agent (CUA) performance by generating multiple complete trajectories and selecting the best one using structured textual summaries. The objective is to mitigate the unreliability and high variance of CUAs on long-horizon tasks by developing an effective "wide scaling" framework that leverages multiple agent rollouts. The bBoN methodology first employs a Vision-Language Model (VLM) to convert each raw trajectory into a "behavior narrative"—a concise sequence of action-effect facts—and then a separate VLM-based judge performs a comparative evaluation on these narratives to select the optimal trajectory. On the OSWorld benchmark, bBoN establishes a new state-of-the-art success rate of 69.9% at 100 steps, a 10% absolute improvement over the previous best of 59.9% and approaching the 72% human-level performance benchmark. The principal implication for AI practitioners is that they can enhance agentic system robustness by parallelizing entire task rollouts and implementing a comparative selection mechanism over structured trajectory representations, rather than relying on single agent executions or step-wise decision scaling. |
| Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation (Read more on [arXiv](https://arxiv.org/abs/2510.01284) or [HuggingFace](https://huggingface.co/papers/2510.01284))|  | The paper introduces OVI, a unified generative model that produces synchronized audio and video in a single pass using a symmetric twin-backbone diffusion transformer architecture. The objective is to create an end-to-end system for joint audio-video generation that eliminates the need for separate pipelines or post-hoc alignment. The methodology involves coupling two architecturally identical Diffusion Transformers (DiTs) via blockwise, bidirectional cross-attention and aligning their different temporal resolutions using scaled Rotary Positional Embeddings (RoPE). In pairwise human preference studies, OVI was preferred for audio-visual synchronization over the JavisDiT baseline in 79.3% of comparisons. For AI practitioners, this work provides a scalable framework demonstrating that architectural symmetry and deep cross-modal fusion can achieve inherent synchronization, offering a robust template for developing simpler and more coherent multimodal generative systems. |
| Learning to Reason for Hallucination Span Detection (Read more on [arXiv](https://arxiv.org/abs/2510.02173) or [HuggingFace](https://huggingface.co/papers/2510.02173))| Hadi Pouransari, Kundan Krishna, Hema Swetha Koppula, Ting-Yao Hu, Hsuan Su | This paper introduces RL4HS, a reinforcement learning framework using span-level rewards to train large language models to reason and detect specific hallucinated spans in text. The primary research objective is to determine if a learned, task-specific reasoning process is more effective for hallucination span detection than prompting general-purpose reasoning models or standard fine-tuning. The key methodology involves using Group Relative Policy Optimization (GRPO) with a span-F1 reward function and introducing Class-Aware Policy Optimization (CAPO) to address reward imbalance between hallucination and non-hallucination classes. On the RAGTruth benchmark, the proposed RL4HS-14B model achieves an average span-F1 score of 58.3, outperforming both supervised fine-tuning (55.4) and pretrained reasoning models. The principal implication for AI practitioners is that for complex, multi-step NLP tasks like hallucination span detection, reinforcement learning with fine-grained, task-specific rewards is a more effective alignment strategy than prompting general reasoning models or using supervised fine-tuning alone. |
| TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP
  Environments (Read more on [arXiv](https://arxiv.org/abs/2510.01179) or [HuggingFace](https://huggingface.co/papers/2510.01179))|  | TOUCAN introduces a 1.5 million-sample dataset of tool-agentic trajectories synthesized from real-world Model Context Protocol (MCP) environments to train capable LLM agents. The objective is to address the scarcity of high-quality, large-scale, and permissively licensed tool-agentic data by creating a pipeline to generate realistic training trajectories involving authentic tool execution from a broad set of MCPs. The methodology is a five-stage pipeline that onboards and filters 495 real-world MCP servers, synthesizes tasks using five LLMs, applies model-based quality filtering, generates trajectories with three teacher models performing real tool execution via agentic frameworks, and conducts rule-based and model-based post-filtering. Models fine-tuned on TOUCAN show significant performance gains; a fine-tuned Qwen2.5-32B model improved its overall score on the BFCL V3 benchmark by 8.72% (from 61.73% to 70.45%), surpassing larger closed-source models. For AI practitioners, TOUCAN provides a large-scale, permissively licensed dataset for supervised fine-tuning to substantially improve the tool-calling and agentic reasoning of open-source LLMs, enabling the creation of more robust agents. |
| F2LLM Technical Report: Matching SOTA Embedding Performance with 6
  Million Open-Source Data (Read more on [arXiv](https://arxiv.org/abs/2510.02294) or [HuggingFace](https://huggingface.co/papers/2510.02294))|  | F2LLM is a suite of text embedding models that matches state-of-the-art performance using only 6 million curated, open-source, non-synthetic data tuples. The primary objective was to develop a high-performing embedding model that is reproducible and budget-friendly, avoiding the massive pretraining, complex pipelines, and costly synthetic data used by previous top models. The methodology involves directly fine-tuning Qwen3 foundation models in a single stage with a contrastive loss objective on a unified dataset of query-document-negative tuples compiled from various open-source datasets. The F2LLM-4B model achieves an average score of 73.67 on the MTEB leaderboard, ranking 7th overall and 2nd among models of its size, while F2LLM-1.7B ranks 1st in the 1B-2B size range. For AI practitioners, this research provides a fully open-source (models, data, code) and cost-effective blueprint for creating powerful embedding models, demonstrating that meticulous data curation can be a viable alternative to massive-scale pretraining and synthetic data generation. |
| Go with Your Gut: Scaling Confidence for Autoregressive Image Generation (Read more on [arXiv](https://arxiv.org/abs/2509.26376) or [HuggingFace](https://huggingface.co/papers/2509.26376))| Disen Lan, Rongjin Guo, Wen-Jie Shu, Xianfeng Wu, Harold Haodong Chen | ScalingAR is a test-time scaling framework that improves autoregressive image generation by dynamically pruning low-quality generation paths and adjusting guidance strength using a novel confidence score derived from token entropy. The main objective is to develop an efficient test-time scaling strategy for next-token prediction (NTP) based image generation that avoids the need for partial decoding or external reward models. The key methodology involves a Dual-Channel Confidence Profile that fuses an Intrinsic Channel (token-level uncertainty and worst-block spatial stability) with a Conditional Channel (text utilization strength measured by KL divergence) to guide policies for adaptive termination and dynamic scheduling of Classifier-Free Guidance. Experiments show ScalingAR improves the LlamaGen base model by 15.2% on the TIIF-Bench benchmark and reduces visual token consumption by 62.0% compared to classic scaling baselines like Best-of-N, while achieving higher quality. The principal implication for AI practitioners is that they can use this inference-time technique to enhance the performance and efficiency of pre-trained autoregressive image generators without any model retraining, by leveraging intrinsic model signals to guide the sampling process. |
| Visual Multi-Agent System: Mitigating Hallucination Snowballing via
  Visual Flow (Read more on [arXiv](https://arxiv.org/abs/2509.21789) or [HuggingFace](https://huggingface.co/papers/2509.21789))| Zhangquan Chen, Yongbo He, Guibin Zhang, Chengming Xu, Xinlei Yu | This paper introduces Visual Flow (ViF), a lightweight method to mitigate the compounding of visual errors, termed "hallucination snowballing," in Visual Language Model (VLM) based Multi-Agent Systems (MAS). The research objective is to diagnose this phenomenon, which is attributed to diminishing visual attention across agent turns, and to develop a mitigation strategy that preserves visual fidelity. The key methodology involves identifying a critical subset of vision tokens with a unimodal attention peak, which best preserve visual evidence, and directly relaying them as an auxiliary "Visual Flow" between agents, augmented by an attention reallocation mechanism. Experiments demonstrate that ViF consistently reduces hallucination snowballing, achieving a 39.8% average reduction in the Hallucination Snowballing (HS) score for the LLaVA-NeXT-7B model within a circular MAS structure. For AI practitioners, the principal implication is that robust VLM-based MAS design must supplement inter-agent textual communication with a direct visual information relay to prevent the propagation and amplification of initial perception errors. |
| VideoNSA: Native Sparse Attention Scales Video Understanding (Read more on [arXiv](https://arxiv.org/abs/2510.02295) or [HuggingFace](https://huggingface.co/papers/2510.02295))| Xiaojun Shan, Ethan Armand, Shusheng Yang, Wenhao Chai, Enxin Song | VideoNSA introduces a hardware-aware, learnable sparse attention mechanism for video-language models to efficiently process ultra-long video contexts. The research objective is to overcome the context length limitations of multimodal models for video understanding by developing a scalable attention mechanism that maintains performance on complex reasoning tasks. The method adapts the Qwen2.5-VL model with a hybrid attention strategy, applying standard grouped-query attention to text and Native Sparse Attention (NSA) to video tokens, which dynamically combines token compression, selection, and sliding window branches via learnable gates. VideoNSA demonstrates improved performance across long-video benchmarks, achieving leading results on tasks like temporal reasoning while using only 3.6% of the full attention budget for a 128K token context. For AI practitioners, this hybrid sparse attention framework provides a scalable method for building video foundation models capable of processing significantly longer contexts, such as hour-long videos, without prohibitive computational costs. |
| Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and
  Reasoning in Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.01304) or [HuggingFace](https://huggingface.co/papers/2510.01304))| Yukun Qi, Xikun Bao, Shiting Huang, Wenxuan Huang, Yu Zeng | The paper introduces AGILE, an agentic reinforcement learning framework that uses interactive jigsaw puzzle solving to enhance the visual perception and reasoning of Vision-Language Models. The objective is to overcome the limitations of current VLMs in core reasoning and the scarcity of high-quality multimodal training data by using a scalable proxy task. The methodology formulates jigsaw solving as an interactive process where the VLM generates Python code to perform actions (e.g., Swap, Crop, Zoom), receives visual feedback from an environment, and is trained via a cold-start phase with expert trajectories followed by reinforcement learning. Primary results demonstrate that AGILE increases accuracy on 2x2 jigsaw tasks from 9.5% to 82.8% and achieves an average performance improvement of 3.1% across nine general vision benchmarks. The principal implication for AI practitioners is that programmatically generated, interactive proxy tasks can serve as a scalable and efficient alternative to scarce, human-annotated data for improving the fundamental reasoning and generalization capabilities of VLMs. |
| Automated Structured Radiology Report Generation with Rich Clinical
  Context (Read more on [arXiv](https://arxiv.org/abs/2510.00428) or [HuggingFace](https://huggingface.co/papers/2510.00428))| Won Hwa Kim, Dongseop Kim, Juho Jung, Dong Bok Lee, Seongjae Kang | This paper introduces Contextualized Structured Radiology Report Generation (C-SRRG), a framework that incorporates rich clinical context to improve the accuracy and mitigate temporal hallucinations in automated reports. The research objective is to enhance SRRG systems by enabling them to utilize clinical information—multi-view images, indications, techniques, and prior studies—mirroring the diagnostic workflow of radiologists. The methodology involves curating a new C-SRRG dataset by integrating this clinical context and then fine-tuning state-of-the-art medical multimodal large language models (MLLMs) on this data. Incorporating clinical context significantly improves performance, reducing temporal hallucinations by up to 18.0% for impression generation and increasing the F1-SRR-BERT score by up to +7.1 for the Lingshu-7B model. The principal implication for AI practitioners is that explicitly conditioning models on comprehensive, domain-specific context is a critical strategy for improving factual accuracy and reducing hallucinations, with this effect becoming more pronounced as model scale increases. |
| Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject
  Fidelity (Read more on [arXiv](https://arxiv.org/abs/2510.02315) or [HuggingFace](https://huggingface.co/papers/2510.02315))| Thomas Hofmann, Enis Simsar, Eric Tillmann Bill | This paper introduces a stochastic optimal control (SOC) framework for flow matching (FM) models to improve multi-subject fidelity in text-to-image generation. The main objective is to develop a principled, optimizable objective for steering sampling dynamics to mitigate attribute leakage, identity entanglement, and subject omissions. The key methodology formulates subject disentanglement as a control problem over a trained FM sampler, yielding two algorithms: a training-free test-time controller and a lightweight fine-tuning rule called Adjoint Matching. The primary result is that the proposed FOCUS method achieves state-of-the-art multi-subject fidelity, attaining a composite improvement score of 5.9174 on Stable Diffusion 3.5 when fine-tuned, significantly outperforming prior attention-based heuristics. For AI practitioners, this research provides two architecture-agnostic, principled algorithms to systematically improve the compositional abilities of modern T2I models, either through a fast test-time intervention or a lightweight fine-tuning process that generalizes from limited data. |
| Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming
  Attacks (Read more on [arXiv](https://arxiv.org/abs/2510.02286) or [HuggingFace](https://huggingface.co/papers/2510.02286))| Alan Ritter, Miguel Ballesteros, Roshan Sridhar, Afshin Oroojlooy, Ruohao Guo | This paper introduces DIALTREE-RPO, a reinforcement learning framework for discovering multi-turn red-teaming attacks against large language models. The primary objective is to autonomously discover diverse and effective multi-turn attack strategies by framing the red-teaming dialogue as a sequential decision-making problem. The proposed methodology, DIALTREE-RPO, is an on-policy RL framework that integrates dialogue tree rollout with pruning for structured exploration and an adaptive masking technique to stabilize training. The approach achieved an average Attack Success Rate (ASR) of 85.3% across 10 target LLMs, outperforming previous state-of-the-art methods by more than 25.9%. For AI practitioners, this research demonstrates that current LLMs are significantly more vulnerable to strategic, multi-turn conversational attacks than to single-turn attacks, highlighting the necessity for developing more robust, context-aware safety mechanisms. |
| A Rigorous Benchmark with Multidimensional Evaluation for Deep Research
  Agents: From Answers to Reports (Read more on [arXiv](https://arxiv.org/abs/2510.02190) or [HuggingFace](https://huggingface.co/papers/2510.02190))| Tianle Gu, Yi Lu, Yuxuan Zhang, Yixu Wang, Yang Yao | This research introduces Rigorous Bench, a benchmark with a multidimensional evaluation framework to assess Deep Research Agents (DRAs) that generate long-form reports.  The main objective is to develop a benchmark tailored for report-style outputs from DRAs, enabling a comprehensive assessment of integrated capabilities like task decomposition, cross-source retrieval, and structured synthesis, which existing short-text benchmarks cannot evaluate.  The key methodology involves the "Rigorous Bench" dataset, comprising 214 expert-curated queries with reference bundles containing specific rubrics, trustworthy source links (TSLs), and focus keywords. Evaluation is performed using an `IntegratedScore`, which is a multiplicative product of three metrics: Semantic Quality, Topical Focus (as `1 - SemanticDrift`), and Retrieval Trustworthiness.  The primary results from evaluating 13 models show that DRAs consistently outperform web-search-tool-augmented models, with `Qwen-deep-research` achieving the highest `IntegratedScore` of 34.6480, although it did not lead in every individual sub-metric.  The principal implication for AI practitioners is that they can use this benchmark and framework to conduct granular capability assessments of their agent systems, moving beyond content quality to systematically optimize for retrieval trustworthiness and thematic consistency, thereby guiding the development of more robust and reliable DRAs. |
| ModernVBERT: Towards Smaller Visual Document Retrievers (Read more on [arXiv](https://arxiv.org/abs/2510.01149) or [HuggingFace](https://huggingface.co/papers/2510.01149))|  | This research introduces ModernVBERT, a compact 250M-parameter vision-language encoder that establishes a leading performance-size tradeoff for visual document retrieval. The paper's objective is to systematically identify which design choices—including attention mechanisms, modality alignment, and contrastive training regimes—best enhance the performance of modern visual document retrievers. Through controlled experiments, the study employs a two-stage training process: first, aligning a pretrained bidirectional text encoder with a vision encoder using a Masked Language Modeling (MLM) objective, followed by contrastive post-training with InfoNCE loss. The primary result shows that using a native bidirectional attention mechanism with multi-vector late interaction significantly boosts performance, outperforming an equivalent causal decoder model by +10.6 nDCG@5 on the ViDoRe benchmark. The principal implication for AI practitioners is that for building efficient and powerful document retrieval systems, it is more effective to use purpose-built bidirectional encoders rather than repurposing larger, causal generative VLMs, as this allows for superior performance with late interaction at a fraction of the computational cost. |
| Transformers Discover Molecular Structure Without Graph Priors (Read more on [arXiv](https://arxiv.org/abs/2510.02259) or [HuggingFace](https://huggingface.co/papers/2510.02259))|  | A standard Transformer architecture can learn molecular structure and predict energies and forces competitively with specialized Graph Neural Networks (GNNs) without any explicit graph-based priors. The main objective was to investigate whether an unmodified Transformer, trained directly on Cartesian coordinates, can approximate molecular energies and forces without predefined graphs or physical inductive biases. The methodology involved training a LLaMA2-based Transformer on the OMol25 dataset using a two-stage procedure: autoregressive pre-training on discretized molecular sequences, followed by fine-tuning with a bi-directional attention mask to regress continuous energy and force values. The primary result is that under a matched compute budget, the 1B parameter Transformer achieved a force Mean Absolute Error of 18.35 meV/Å, comparable to the 13.01 meV/Å of a state-of-the-art equivariant GNN, while being faster in wall-clock time. The principal implication for AI practitioners is that for complex scientific domains like molecular modeling, general-purpose, scalable architectures like Transformers can be a viable alternative to highly specialized models, potentially simplifying development and leveraging mature hardware/software ecosystems without the need to hard-code domain-specific inductive biases. |
| Rethinking the shape convention of an MLP (Read more on [arXiv](https://arxiv.org/abs/2510.01796) or [HuggingFace](https://huggingface.co/papers/2510.01796))|  | This paper proposes an inverted "wide-narrow-wide" (Hourglass) MLP architecture that places skip connections in a higher-dimensional space, demonstrating superior parameter efficiency over conventional designs. The main objective is to test the hypothesis that performing incremental residual updates in an expanded-dimensional space is more effective than in the narrower input/output space of conventional MLPs. The methodology involves comparing the proposed Hourglass MLP against conventional "narrow-wide-narrow" MLPs on generative image tasks, systematically searching architectural parameters to characterize and compare their performance-parameter Pareto frontiers. Results show that Hourglass architectures consistently achieve superior Pareto frontiers; for example, on ImageNet-32 denoising, an Hourglass model reaches 22.31 dB PSNR with 66M parameters, while a conventional model requires 75M parameters for the same score. The study also finds that an initial fixed random projection to the expanded space yields performance comparable to a fully trained projection. The principal implication for AI practitioners is that in residual architectures, inverting the standard MLP shape to "wide-narrow-wide" can yield more parameter-efficient models, and the necessary input up-projection can be a fixed random matrix, which saves parameters and potentially reduces memory bandwidth. |
| VLA-R1: Enhancing Reasoning in Vision-Language-Action Models (Read more on [arXiv](https://arxiv.org/abs/2510.01623) or [HuggingFace](https://huggingface.co/papers/2510.01623))| Dapeng Zhang, Xiaofeng Wang, Boyuan Wang, Zeyu Zhang, Angen Ye | i) VLA-R1 is a reasoning-enhanced vision-language-action model that improves robotic manipulation by integrating Chain-of-Thought (CoT) supervision with Reinforcement Learning from Verifiable Rewards (RLVR). ii) The primary objective is to bridge the gap between reasoning and execution in VLA models by addressing their lack of explicit step-by-step reasoning and systematic post-training reinforcement. iii) The key methodology involves a two-stage training process: first, Supervised Fine-Tuning (SFT) on the newly created VLA-CoT-13K dataset, followed by post-training with Group Relative Policy Optimization (GRPO) using verifiable rewards for region alignment (GIoU), trajectory consistency (Fréchet distance), and output formatting. iv) VLA-R1 achieves state-of-the-art results, including a 36.51 Intersection over Union (IoU) on the in-domain affordance benchmark, which represents a 17.78% improvement over the strongest baseline. v) The principal implication for AI practitioners is that combining data-level explicit reasoning supervision (CoT) with optimization-level reinforcement learning using geometrically-grounded, verifiable rewards is an effective strategy for building more robust, accurate, and generalizable embodied AI systems. |
| VIRTUE: Visual-Interactive Text-Image Universal Embedder (Read more on [arXiv](https://arxiv.org/abs/2510.00523) or [HuggingFace](https://huggingface.co/papers/2510.00523))| Yuki Mitsufuji, Shusuke Takahashi, Qiyu Wu, Kazuya Tateishi, Wei-Yao Wang | This paper introduces VIRTUE, a visual-interactive universal text-image embedder that integrates a segmentation model with a Vision-Language Model (VLM) to process both textual and visual interaction prompts. The research aims to develop and evaluate an embedding model that can incorporate explicit visual signals like bounding boxes to perform fine-grained, entity-aware retrieval while maintaining global scene context. The methodology combines a pretrained SAM2 segmentation model to generate entity-level embeddings from visual prompts with a Qwen2-VL model that processes these alongside global image and text embeddings for contrastive learning. VIRTUE demonstrates state-of-the-art performance, achieving improvements of 15.2%–20.3% on the new visual-interactive SCaR benchmark and 3.1%–8.5% on the MMEB universal embedding benchmark. The primary implication for AI practitioners is that this architecture provides a generic framework for building embedding systems that support direct user interaction with image regions, enabling more controllable, accurate retrieval and on-the-fly correction of model predictions at inference. |
| Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm:
  Demystifying Some Myths About GRPO and Its Friends (Read more on [arXiv](https://arxiv.org/abs/2509.24203) or [HuggingFace](https://huggingface.co/papers/2509.24203))| Wenhao Zhang, Yushuo Chen, Yuchang Sun, Yanxi Chen, Chaorui Yao | This paper presents a first-principles derivation showing that group-relative REINFORCE is an inherently off-policy algorithm. The primary objective is to reinterpret group-relative REINFORCE without on-policy data assumptions, thereby providing a theoretical foundation for its use in off-policy settings and demystifying the mechanisms of related algorithms like GRPO. The key methodology involves deriving the group-relative REINFORCE update rule as a single gradient step on a surrogate loss function, which itself is designed to enforce consistency conditions from an underlying KL-regularized objective, and validating insights through empirical studies on LLM reasoning tasks. The primary results demonstrate that for GRPO-style algorithms in off-policy settings, clipping is a more critical mechanism for stability than importance sampling; for instance, on the GSM8k task with `sync_interval = 20`, a clipping-only variant (REC-OneSide-NoIS) with an enlarged clipping range of (0.6, 2.0) accelerated training without sacrificing stability, whereas vanilla REINFORCE collapsed. The principal implication for AI practitioners is that they can adapt REINFORCE-style algorithms for off-policy LLM training by focusing on regularization techniques like aggressive clipping—even with ranges far beyond conventional values—and employing justified data-weighting heuristics, often without needing importance sampling. |
| SKYLENAGE Technical Report: Mathematical Reasoning and
  Contest-Innovation Benchmarks for Multi-Level Math Evaluation (Read more on [arXiv](https://arxiv.org/abs/2510.01241) or [HuggingFace](https://huggingface.co/papers/2510.01241))| Weiqi Zhai, Linlin Miao, Boyu Yang, Ze Xu, Hu Wei | This paper introduces two complementary mathematical reasoning benchmarks, SKYLENAGE-ReasoningMATH and SKYLENAGE-MATH, to provide high-difficulty, fine-grained evaluation for frontier large language models. The primary objective is to overcome the ceiling effects and ability masking of existing benchmarks by creating testbeds that diagnose structural reasoning ability across multiple academic levels and subjects. The methodology involves evaluating 15 contemporary LLMs on a 100-item structure-aware diagnostic set and a 150-item contest-style suite spanning high school to doctoral levels, using a unified Chain-of-Thought protocol. The primary results show clear model separation, with the top-performing model achieving 44% accuracy on the contest suite compared to the runner-up's 37%, and hardest-quintile analysis on the reasoning set revealing significant robustness gaps obscured by overall scores. The principal implication for AI practitioners is that single-score leaderboards are insufficient; using structured, multi-level benchmarks is critical for identifying specific model weaknesses in areas like high-difficulty reasoning and numeric density robustness, enabling targeted development and informed model selection. |
| Parallel Scaling Law: Unveiling Reasoning Generalization through A
  Cross-Linguistic Perspective (Read more on [arXiv](https://arxiv.org/abs/2510.02272) or [HuggingFace](https://huggingface.co/papers/2510.02272))|  | This study investigates the cross-lingual generalization of reasoning capabilities in Large Reasoning Models (LRMs) trained with English-centric Reinforcement Post-Training (RPT). The main objective is to quantify how effectively reasoning skills transfer from English to other languages and to identify training strategies that improve this cross-lingual generalization. The methodology combines observational studies on open-source LRMs, controlled interventional studies on factors like initial model type and size, and a parallel training study using the Group Rollout Policy Optimization (GRPO) algorithm. Primary results reveal a "First-Parallel Leap," where transitioning from monolingual to bilingual parallel training causes a disproportionate jump in the Multilingual Transferability Index (MTI) from 1.16 to 2.50, and establishes a "Parallel Scaling Law" where transferability scales as a power-law with the number of parallel languages (`f(X) = 2.00 * X^0.29`). The principal implication for AI practitioners is that monolingual RPT is insufficient; incorporating even a single parallel language during training is a highly effective strategy to mitigate overfitting to English-specific patterns and develop more robust, language-agnostic reasoning systems. |
| Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness (Read more on [arXiv](https://arxiv.org/abs/2510.01670) or [HuggingFace](https://huggingface.co/papers/2510.01670))|  | This paper introduces Blind Goal-Directedness (BGD), a phenomenon where Computer-Use Agents (CUAs) pursue goals irrespective of safety or context, and presents the BLIND-ACT benchmark to measure this risk in frontier models. The primary objective is to systematically characterize and evaluate BGD in state-of-the-art CUAs across three prevalent patterns: lack of contextual reasoning, assumptions under ambiguity, and contradictory or infeasible goals. The methodology involves the development of BLIND-ACT, a 90-task benchmark built on OSWorld, and the use of an LLM-based judge to evaluate agent trajectories for BGD intentions and completion, which achieved 93.75% agreement with human annotations. The study found that nine evaluated frontier models exhibited a high average BGD rate of 80.8%, demonstrating that this is a widespread issue, and showed that prompting-based interventions only partially mitigate the risk. The principal implication for AI practitioners is that current CUAs possess a fundamental alignment flaw, making them unsafe for deployment; engineers must implement stronger, trajectory-level safeguards beyond simple prompting to ensure reliable and safe agent behavior. |
| Generalized Parallel Scaling with Interdependent Generations (Read more on [arXiv](https://arxiv.org/abs/2510.01143) or [HuggingFace](https://huggingface.co/papers/2510.01143))| Mrinal Kumar, Yun He, Eryk Helenowski, David Brandfonbrener, Harry Dong | This paper introduces Bridge, a low-cost architectural addition for LLMs that facilitates information sharing between parallel generations from a single prompt to improve response set quality. The main objective is to overcome the limitations of independent parallel sampling by enabling interdependent generations, thereby allowing all N parallel responses to leverage the total available compute and information. The key methodology involves adding "Bridge" blocks, which are small attention mechanisms that operate across the batch dimension of the hidden state tensor (`B x S x D`) at each timestep, allowing tokens from different sequences to interact. Bridge improves the relative mean accuracy gains from reinforcement learning with verifiable rewards (RLVR) by up to 50% on the DS-Qwen-7B model across 7 math benchmarks, with only a 2.8%-5.1% increase in parameters. The principal implication for AI practitioners is that Bridge offers a more effective parallel scaling method for tasks like best-of-N selection, synthetic data generation, and RL fine-tuning, improving both individual response accuracy and the overall quality of the generated set without significant architectural changes or post-processing heuristics. |
| RLAD: Training LLMs to Discover Abstractions for Solving Reasoning
  Problems (Read more on [arXiv](https://arxiv.org/abs/2510.02263) or [HuggingFace](https://huggingface.co/papers/2510.02263))| Ruslan Salakhutdinov, Amrith Setlur, Yoonho Lee, Anikait Singh, Yuxiao Qu | The paper introduces RLAD, a two-player reinforcement learning framework that jointly trains an abstraction generator and a solution generator to improve LLM reasoning by discovering and utilizing high-level procedural knowledge. The primary objective is to train LLMs to discover and leverage concise, reusable "reasoning abstractions" to guide exploration and enhance performance on complex reasoning problems. The key methodology is a cooperative two-player RL paradigm where an "abstraction generator" is rewarded based on the performance improvement of an "abstraction-conditioned solution generator," which is in turn rewarded for correctly solving the problem using the provided abstraction. On the AIME 2025 benchmark, the RLAD-trained model achieved 48.33% accuracy using the best of four proposed abstractions, outperforming a DAPO baseline which scored 39.79%. The principal implication for AI practitioners is that for complex reasoning tasks, allocating test-time compute to generate multiple diverse strategic abstractions (high-level plans) before generating solutions is more effective for improving performance than simply sampling a larger number of solution attempts. |
| MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment
  Abilities in MLLMs (Read more on [arXiv](https://arxiv.org/abs/2510.01691) or [HuggingFace](https://huggingface.co/papers/2510.01691))| Junzhi Ning, Chenglong Ma, Wanying Qu, Jinjie Wei, Jiyao Liu | This paper introduces MedQ-Bench, a comprehensive benchmark for evaluating the medical image quality assessment capabilities of Multimodal Large Language Models (MLLMs). The research objective is to systematically assess the perceptual and reasoning abilities of MLLMs in medical IQA by mirroring the clinical workflow of first perceiving quality attributes and then forming a judgment. The methodology consists of constructing the MedQ-Bench dataset, which includes 3,308 samples across 5 modalities and 40+ quality attributes, and evaluating 14 MLLMs using a perception-reasoning paradigm with a multi-dimensional judging protocol. The primary results reveal a significant human-AI performance gap, with the top-performing model (GPT-5) achieving 68.97% accuracy on perception tasks, substantially underperforming human experts (82.50%). For AI practitioners, the principal implication is that improving MLLMs for clinical applications requires a foundational focus on enhancing low-level visual perception and reasoning, as this is identified as the main bottleneck over instruction-following abilities. |
| TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis (Read more on [arXiv](https://arxiv.org/abs/2510.01538) or [HuggingFace](https://huggingface.co/papers/2510.01538))| Yuting He, Yiwei Xu, Jiaqi Wei, Xiang Zhang, Haokun Zhao | This paper introduces TimeSeriesScientist (TSci), a general-purpose, LLM-driven multi-agent framework designed to automate the end-to-end univariate time series forecasting pipeline. The primary objective is to create a domain-agnostic system that minimizes human intervention in the labor-intensive preprocessing, validation, and ensembling stages of forecasting. The methodology employs four specialized agents (Curator, Planner, Forecaster, Reporter) that collaboratively perform diagnostics, model selection, ensembling, and report generation through LLM reasoning and tool use. Empirical results on eight benchmarks show that TSci reduces forecast error by an average of 38.2% compared to LLM-based baselines. The principal implication for AI practitioners is that this agentic framework provides a practical, interpretable, and extensible "white-box" system that automates the complex forecasting workflow, significantly reducing the manual effort required to build and deploy reliable forecasting models. |
| Spectral Scaling Laws in Language Models: How Effectively Do
  Feed-Forward Networks Use Their Latent Space? (Read more on [arXiv](https://arxiv.org/abs/2510.00537) or [HuggingFace](https://huggingface.co/papers/2510.00537))|  | This paper introduces spectral scaling laws for Feed-Forward Networks (FFNs) in LLMs, revealing an asymmetric relationship between FFN width and effective latent space utilization. The research aims to quantify how effectively increasing FFN width expands the usable latent space in LLMs, moving beyond performance-based scaling laws to analyze internal representational efficiency. The authors analyze the eigenspectrum of FFN post-activation covariance matrices using a diagnostic suite including Hard Rank (participation ratio) and Soft Rank (Shannon Rank) across LLaMA, GPT-2, and nGPT models with varying FFN widths. The study identifies an "Asymmetric Spectral Scaling Law": soft rank scales almost linearly with FFN width (e.g., exponent β=1.06 for LLaMA-130M), while hard rank grows sublinearly (β=0.60), indicating that increased width predominantly adds low-energy tail directions while the dominant-mode subspace saturates early. This recasts FFN width selection as a spectral utilization trade-off, providing AI practitioners a principled method (e.g., monitoring effective dimension) to guide architectural choices and avoid allocating parameters to under-utilized latent dimensions. |
| FrameThinker: Learning to Think with Long Videos via Multi-Turn Frame
  Spotlighting (Read more on [arXiv](https://arxiv.org/abs/2509.24304) or [HuggingFace](https://huggingface.co/papers/2509.24304))| Daizong Liu, Siyuan Huang, Yafu Li, Xiaoye Qu, Zefeng He | FrameThinker is a novel framework that enables Large Vision-Language Models (LVLMs) to perform active, iterative reasoning on long videos by dynamically selecting relevant frames for analysis. The primary objective is to overcome the inefficiency and performance limitations of traditional uniform frame sampling by teaching models to strategically interrogate video content. The methodology uses a two-phase training process: Supervised Fine-Tuning (SFT) to learn basic action syntax, followed by Reinforcement Learning (RL) with a Cognitive Consistency Verification (CCV) module to optimize the decision-making policy for frame selection. The 7B FrameThinker model achieves a new state-of-the-art 76.1% accuracy on the LongVideo-Reason benchmark using an average of only 20.6 frames, outperforming a competitive baseline that uses 512 frames. For AI practitioners, this research provides a paradigm for building more computationally efficient and accurate long video analysis systems by shifting from passive, dense processing to active, sparse frame selection guided by the model's own reasoning process. |
| AReUReDi: Annealed Rectified Updates for Refining Discrete Flows with
  Multi-Objective Guidance (Read more on [arXiv](https://arxiv.org/abs/2510.00352) or [HuggingFace](https://huggingface.co/papers/2510.00352))| Pranam Chatterjee, Yinuo Zhang, Tong Chen | AReUReDi is a discrete optimization algorithm that extends rectified discrete flows with multi-objective guidance to generate Pareto-optimal biological sequences. The research objective is to develop a sequence-based generative framework with theoretical guarantees for multi-objective Pareto optimality to design biomolecules satisfying multiple conflicting properties. The methodology integrates annealed Tchebycheff scalarization to unify objectives, locally balanced proposals to blend guidance with a generative prior, and Metropolis-Hastings updates to steer sampling toward the Pareto front. In designing peptide binders for the PPP5 target, AReUReDi achieved a half-life of 38.28 hours, substantially outperforming the next-best evolutionary algorithm which scored 2.90 hours. The principal implication for AI practitioners is that AReUReDi provides a general, theoretically-grounded framework for guiding pre-trained discrete generative models to produce outputs that are co-optimized for multiple, user-defined objectives. |
| SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking
  for Training-free Zero-Shot Composed Image Retrieval (Read more on [arXiv](https://arxiv.org/abs/2509.26330) or [HuggingFace](https://huggingface.co/papers/2509.26330))| Huei-Fang Yang, Yu-Yen Lin, Ren-Di Wu | SQUARE is a novel, two-stage, training-free framework that enhances zero-shot composed image retrieval (ZS-CIR) by using Multimodal Large Language Models (MLLMs) for both query enrichment and candidate reranking. The main objective is to improve the accuracy of ZS-CIR systems without requiring task-specific training or labeled data by more effectively capturing complex user intent expressed through a reference image and modification text. The methodology involves a two-stage process: 1) Semantic Query-Augmented Fusion (SQAF), where an MLLM generates a caption of the target image to enrich the initial VLM-based query embedding, and 2) Efficient Batch Reranking (EBR), where top candidates are presented as a grid to an MLLM for joint, single-pass reranking. The framework demonstrates state-of-the-art performance, achieving a mAP@50 of 38.82% on the CIRCO benchmark with a ViT-G/14 backbone, outperforming previous methods. The EBR stage alone improves mAP@5 on CIRCO from 30.89% to 35.61%. The principal implication for AI practitioners is that MLLMs can be deployed as powerful, zero-shot, modular components for complex retrieval tasks, enabling efficient batch-wise reranking that significantly improves accuracy over embedding-only methods without the need for model fine-tuning. |
| IoT-MCP: Bridging LLMs and IoT Systems Through Model Context Protocol (Read more on [arXiv](https://arxiv.org/abs/2510.01260) or [HuggingFace](https://huggingface.co/papers/2510.01260))| Yiming Li, Yiyi Lu, Mingchen Ma, Guanliang Lyu, Ningyuan Yang | The paper presents IoT-MCP, a decoupled framework for bridging LLMs and IoT systems via edge-deployed servers implementing the Model Context Protocol. The primary objective is to develop a robust, low-latency framework that standardizes communication between LLMs and heterogeneous IoT devices, overcoming challenges of hardware diversity and resource constraints. The methodology involves a decoupled three-domain architecture (Local Host, Connection Server, IoT Devices) to separate LLM interaction from device management, and the development of IoT-MCP Bench, a new benchmark with 1,254 tasks for systematic evaluation. The framework achieved a 100% success rate on basic tool execution tasks across 22 sensor types, with a 205ms average end-to-end response time and a 74KB peak memory footprint on microcontrollers. For AI engineers, this provides an open-source, validated framework and a standardized evaluation methodology for integrating LLM-based natural language interfaces with resource-constrained IoT ecosystems, demonstrating a practical path for deploying agentic AI in physical environments. |
