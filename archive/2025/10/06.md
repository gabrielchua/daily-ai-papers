

## Papers for 2025-10-06

| Title | Authors | Summary |
|-------|---------|---------|
| Apriel-1.5-15b-Thinker (Read more on [arXiv](https://arxiv.org/abs/2510.01141) or [HuggingFace](https://huggingface.co/papers/2510.01141))|  | The paper presents Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights multimodal model designed to achieve frontier-level performance through training strategy rather than parameter scale. The research objective is to demonstrate that a compact, open model can attain advanced reasoning capabilities while remaining economical to train and deploy on a single GPU. The core methodology is a three-stage "mid-training" pipeline starting from Pixtral-12B: (1) depth upscaling the architecture, (2) staged continual pre-training with targeted synthetic data for visual reasoning, and (3) high-signal, text-only supervised fine-tuning with explicit reasoning traces, without reinforcement learning. The model achieves a score of 52 on the Artificial Analysis Intelligence Index, matching the performance of DeepSeek-R1-0528 despite using significantly fewer computational resources. The principal implication for AI practitioners is that strategic, data-centric mid-training can close the capability gap with massive-scale models, enabling the development of high-performance, resource-efficient systems for constrained deployment environments. |
| Efficient Multi-modal Large Language Models via Progressive Consistency
  Distillation (Read more on [arXiv](https://arxiv.org/abs/2510.00515) or [HuggingFace](https://huggingface.co/papers/2510.00515))|  | This paper introduces EPIC, a progressive consistency distillation framework to efficiently train multi-modal large language models (MLLMs) with compressed visual tokens. The primary objective is to overcome the increased learning difficulty caused by the feature space perturbations from aggressive token compression during training. The key methodology involves a shared-weight teacher-student model that progressively increases the token compression ratio and shifts the compression layer from deep to shallow throughout training, following an easy-to-hard curriculum guided by KL-divergence loss. Experimental results show that an MLLM trained with EPIC achieves performance comparable to the vanilla LLaVA-v1.5 model on 10 benchmarks (61.4% average accuracy) while using only 192 visual tokens instead of 576, a 66.7% reduction. For AI practitioners, this framework offers a method to significantly reduce the computational cost and memory footprint of MLLMs at inference without modifying the model architecture, making them more suitable for deployment on resource-constrained hardware. |
| Compose Your Policies! Improving Diffusion-based or Flow-based Robot
  Policies via Test-time Distribution-level Composition (Read more on [arXiv](https://arxiv.org/abs/2510.01068) or [HuggingFace](https://huggingface.co/papers/2510.01068))|  | This research introduces General Policy Composition (GPC), a training-free framework that improves diffusion- and flow-based robot policies by combining the distributional scores of multiple pre-trained models at test-time. The main objective is to create a superior policy that exceeds the performance of its individual parent policies without requiring additional training. The key methodology involves forming a convex combination of the learned score functions from heterogeneous pre-trained policies and using a test-time search to identify optimal weighting for specific tasks, a process supported by a theoretical proof of single-step functional improvement. Experiments show consistent performance gains across multiple benchmarks, such as an average success rate increase of up to +7.55% on Robomimic and PushT tasks when composing a VLA and a VA policy. For AI practitioners, GPC provides a simple, plug-and-play method to enhance the performance and adaptability of existing robotic control systems by leveraging and combining deployed policy assets, thus avoiding costly retraining. |
| Self-Improvement in Multimodal Large Language Models: A Survey (Read more on [arXiv](https://arxiv.org/abs/2510.02665) or [HuggingFace](https://huggingface.co/papers/2510.02665))| Yapeng Tian, Harsh Singh, Tianyu Yang, Kai Wang, Shijian Deng | This paper surveys and taxonomizes self-improvement methodologies for Multimodal Large Language Models (MLLMs). The objective is to provide the first comprehensive, structured overview of self-improvement techniques in MLLMs by categorizing the literature and identifying open challenges. The authors conducted a literature review, structuring the field into a three-stage pipeline: data collection (e.g., random sampling, guided generation), data organization (e.g., rule-based verification, model-based verification), and model optimization (e.g., Supervised Fine-Tuning, Direct Preference Optimization, Reinforcement Learning). The survey identifies that method-task matching is critical, with rule/verification-based RL driving the largest gains on verifiable tasks, while citing a surveyed work that achieved a 15.50% improvement in visuo-motor control tasks. The principal implication for AI practitioners is that this survey provides a taxonomy to select specific self-improvement pipelines tailored to their application, such as using verification-based RL for tasks with ground-truth checks and preference data to improve model helpfulness and mitigate hallucinations. |
| Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents (Read more on [arXiv](https://arxiv.org/abs/2509.26354) or [HuggingFace](https://huggingface.co/papers/2509.26354))| Boyi Wei, Chen Qian, Qihan Ren, Shuai Shao, JY-Young | This paper introduces and empirically investigates "misevolution," the phenomenon where self-evolving LLM agents' autonomous improvement processes lead to unintended and harmful outcomes. The primary objective is to systematically assess whether an agent's self-evolution across four key pathways—model, memory, tool, and workflow—compromises its safety alignment or introduces new vulnerabilities. The methodology involves evaluating the safety performance of various self-evolving agent architectures on security benchmarks (e.g., RedCode-Gen, RiOSWorld) both before and after their evolution cycles. The findings reveal that misevolution is a pervasive risk, with one key result showing that tool-evolving agents built on top-tier LLMs failed to identify and reject malicious external tools nearly 84% of the time. The principal implication for AI practitioners is that deploying self-evolving agents requires new safety paradigms beyond static checks, such as continuous monitoring and automated safety verification for dynamically created components, as inherent safety alignment can degrade unpredictably during autonomous operation. |
| CoDA: Agentic Systems for Collaborative Data Visualization (Read more on [arXiv](https://arxiv.org/abs/2510.03194) or [HuggingFace](https://huggingface.co/papers/2510.03194))|  | This paper introduces CoDA, a collaborative multi-agent system that automates the generation of complex data visualizations from natural language by decomposing the task into specialized agent-driven stages. The primary objective is to address the failures of existing systems in handling complex multi-file datasets and iterative refinement by reframing visualization automation as a collaborative multi-agent problem. CoDA's methodology employs specialized LLM agents for metadata analysis, task planning, code generation, and self-reflection, utilizing a metadata-centric approach to bypass token limits and a quality-driven feedback loop for robust refinement. Extensive evaluations show that CoDA achieves substantial gains over baselines, outperforming competitors by up to 41.5% in overall score on visualization benchmarks. The principal implication for AI practitioners is that designing integrated, collaborative agentic workflows with specialized roles and feedback loops is a more effective paradigm for complex automation tasks than relying on monolithic or simple agent systems, enabling robust handling of real-world data and user requirements. |
| SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys? (Read more on [arXiv](https://arxiv.org/abs/2510.03120) or [HuggingFace](https://huggingface.co/papers/2510.03120))| Shuo Wang, Xin Tong, Xuanhe Zhou, Xuzhou Zhu, Zhaojun Sun | This paper introduces SurveyBench, a fine-grained, quiz-driven evaluation framework to rigorously assess the ability of LLM-based agents to write academic surveys that align with reader needs. The primary objective is to create a robust benchmark to systematically evaluate and quantify the deficiencies of LLM-generated academic surveys by comparing them against high-quality human-authored works. The methodology centers on a curated dataset of topics from recent arXiv papers, a multifaceted metric hierarchy for outline and content quality, and a dual-mode evaluation protocol featuring both content-based scoring and a novel quiz-based assessment to probe technical depth. Results demonstrate that LLM-generated surveys, while structurally coherent, are quantitatively inferior to human-written ones, scoring on average 21% lower in content-based evaluation and achieving a maximum score of only 3.19 out of 10 on topic-specific quizzes where human surveys served as the reference. The principal implication for AI practitioners is that current LLM-agent pipelines for automated content generation excel at surface-level fluency but lack the deep synthesis, critical reasoning, and technical detail required for high-quality academic writing, highlighting the need to develop more sophisticated knowledge integration capabilities. |
| REPAIR: Robust Editing via Progressive Adaptive Intervention and
  Reintegration (Read more on [arXiv](https://arxiv.org/abs/2510.01879) or [HuggingFace](https://huggingface.co/papers/2510.01879))|  | The paper introduces REPAIR, a lifelong editing framework for large language models that enables precise, low-cost updates by integrating closed-loop feedback, dynamic memory management, and distribution-aware optimization. The main objective is to develop a robust and scalable method for sequential model editing that corrects errors or integrates new facts without causing catastrophic forgetting, routing instability, or unintended side effects on non-target knowledge. REPAIR's methodology combines a dual-memory system with parametric editing, a closed-loop feedback mechanism for monitoring and pruning underperforming memory modules, distribution-aware optimization via inner-batch knowledge distillation, and loss-aware weighted merging of updates. Experiments demonstrate that REPAIR boosts editing accuracy by 10%-30% across multiple model families and significantly reduces knowledge forgetting, particularly in large-scale sequential editing scenarios. The principal implication for AI practitioners is a framework for developing more reliable and continually evolving LLMs, enabling low-cost updates to correct factual errors or add new knowledge to deployed models while preserving existing capabilities. |
| OrtSAE: Orthogonal Sparse Autoencoders Uncover Atomic Features (Read more on [arXiv](https://arxiv.org/abs/2509.22033) or [HuggingFace](https://huggingface.co/papers/2509.22033))| Elena Tutubalina, Oleg Rogov, Alexey Dontsov, Andrey Galichin, Anton Korznikov | This paper introduces Orthogonal Sparse Autoencoder (OrtSAE), a training method that enforces orthogonality between learned features to mitigate feature absorption and composition issues in standard sparse autoencoders (SAEs). The primary objective is to improve the atomicity and disentanglement of features learned by SAEs by directly addressing the failure modes where broad features are absorbed by specific ones or independent features merge into composite ones. The key methodology involves adding an orthogonality penalty to the SAE loss function, which penalizes high pairwise cosine similarity between decoder feature vectors, implemented with a chunk-wise strategy to reduce computational complexity from quadratic to linear. Primary results show that, at an L0 sparsity of 70, OrtSAE reduces feature absorption by 65% and composition by 15%, discovers 9% more distinct features, and improves spurious correlation removal performance by 6% compared to traditional SAEs. The principal implication for AI practitioners is that OrtSAE provides a computationally efficient method to train SAEs that produce more disentangled and interpretable feature dictionaries from LLMs, improving model analysis and intervention capabilities without significant architectural changes or overhead. |
| FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of
  Web Agents (Read more on [arXiv](https://arxiv.org/abs/2510.03204) or [HuggingFace](https://huggingface.co/papers/2510.03204))| Léo Boisvert, Xing Han Lù, Megh Thakkar, Sahar Omidi Shayegan, Imene Kerboua | FOCUSAGENT introduces a two-stage pipeline where a lightweight LLM-retriever prunes accessibility tree (AxTree) observations to reduce context size for a primary agent LLM, maintaining task performance while enhancing efficiency and security. The primary objective is to develop an observation pruning strategy for LLM-based web agents that reduces the token count of web page representations (AxTrees) to manage computational costs and security risks, while preserving the critical information necessary for high task-completion success rates. The core methodology is a two-stage pipeline where a lightweight retriever LLM first analyzes the full accessibility tree (AxTree) observation, guided by the task goal, to identify and extract line ranges corresponding to relevant UI elements. This pruned AxTree is then passed to a more powerful agent LLM, which performs Chain-of-Thought reasoning and predicts the subsequent action. On the WorkArena L1 benchmark, FOCUSAGENT reduced the observation token count by over 50% while achieving a 51.5% task success rate, which is comparable to the 53.0% success rate of a strong baseline agent operating on the full, unpruned observation. In security evaluations, a variant of the agent reduced the success rate of popup-based prompt injection attacks from 90.4% to 1.0%. AI practitioners can implement a cascaded LLM architecture, using a smaller, cost-effective model as an intelligent pre-processing filter to prune large contexts for a more powerful downstream model, thereby reducing API costs and latency while simultaneously hardening the agent against environmental security threats like prompt injection without requiring separate, complex defense layers. |
| Improving GUI Grounding with Explicit Position-to-Coordinate Mapping (Read more on [arXiv](https://arxiv.org/abs/2510.03230) or [HuggingFace](https://huggingface.co/papers/2510.03230))| Spandana Gella, Christopher Pal, Ahmed Masry, Tianyu Zhang, Suyuchen Wang | This paper introduces RULER tokens and Interleaved MROPE (I-MROPE) to improve GUI grounding by creating an explicit mapping from spatial positions to pixel coordinates. The objective is to address the unreliable coordinate prediction and poor resolution generalization of current Vision-Language Models (VLMs) which learn this mapping implicitly. The methodology uses RULER tokens as explicit coordinate markers that transform coordinate generation from an unstable regression problem into a robust reference-and-adjust mechanism, complemented by I-MROPE which balances spatial positional encodings. The primary result shows that finetuning Qwen2.5-VL with RULER tokens improves grounding accuracy on the high-resolution ScreenSpot-Pro benchmark from 34.6% to 37.2%. For AI practitioners, the principal implication is that architecturally incorporating explicit spatial guidance is a more effective method for achieving precise visual localization in GUI automation agents than relying solely on implicit learning from positional embeddings. |
| LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.01459) or [HuggingFace](https://huggingface.co/papers/2510.01459))|  | LSPO is a meta-reinforcement learning algorithm that improves LLM reasoning performance by dynamically filtering training data based on response length. The paper's objective is to improve the final model effectiveness of LLMs on reasoning tasks, rather than just training efficiency, by introducing a novel dynamic data sampling strategy for Reinforcement Learning with Verifiable Rewards (RLVR). The proposed method, Length-aware Sampling for Policy Optimization (LSPO), operates on top of existing RLVR algorithms by calculating the average response length for each prompt in a rollout batch and retaining only a fixed percentile of prompts that yield the shortest and longest responses for the training update. Experiments show LSPO consistently improves performance across multiple models and benchmarks; for instance, when applied to the GSPO algorithm on a Qwen3-4B model, it increased the average accuracy on three math benchmarks from 37.2% to 39.6%. AI practitioners can integrate LSPO as a data-filtering layer during RL fine-tuning to enhance the final reasoning accuracy of their models by selectively training on prompts that are either very easy (short responses) or very difficult (long responses) for the current model. |
| WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents (Read more on [arXiv](https://arxiv.org/abs/2510.01354) or [HuggingFace](https://huggingface.co/papers/2510.01354))| Neil Zhenqiang Gong, Yuqi Jia, Xilong Wang, Ruohan Xu, Yinuo Liu | This paper introduces WAInjectBench, a comprehensive benchmark for systematically evaluating prompt injection detection methods specifically for web agents. The research objective is to assess the effectiveness of existing text-based and image-based detectors against a fine-grained categorization of prompt injection attacks that manipulate web content. The methodology involves constructing a new dataset of malicious and benign text and image samples from six distinct attack types and evaluating 12 different detection methods (e.g., prompting-based, embedding-based, fine-tuning-based) across multiple scenarios. The primary result shows that while some detectors can identify attacks with explicit instructions or visible perturbations with moderate-to-high accuracy (e.g., GPT-4o-Prompt achieved a 0.93 True Positive Rate against VPI screenshots), they largely fail against attacks with imperceptible perturbations or implicit instructions (e.g., 0.00 TPR against WebInject). For AI practitioners, the principal implication is that current detection methods are insufficient against sophisticated, stealthy prompt injection attacks, necessitating the development of more robust defenses that do not rely on detecting explicit malicious content. |
| Free Lunch Alignment of Text-to-Image Diffusion Models without
  Preference Image Pairs (Read more on [arXiv](https://arxiv.org/abs/2509.25771) or [HuggingFace](https://huggingface.co/papers/2509.25771))|  | The paper introduces Text Preference Optimization (TPO), a framework for aligning text-to-image models using LLM-generated text preference pairs, eliminating the need for human-annotated image preference data. The research aims to determine if text-to-image alignment can be improved cost-effectively by optimizing over text conditions rather than image outputs. The core methodology involves using a Large Language Model (LLM) to create mismatched (negative) text prompts by perturbing original captions, then fine-tuning the diffusion model with adapted objectives (TDPO and TKTO) to prefer the original prompt for a given image. The proposed methods demonstrate superior performance over baselines, with the TDPO variant achieving an 83.25% win rate on the HPSv2 dataset as measured by PickScore, compared to 77.00% for the Diffusion-DPO baseline. For AI practitioners, this provides a "free lunch" technique to significantly improve model alignment and quality by repurposing existing image-caption datasets, thus bypassing the expensive and time-consuming process of collecting human preference feedback on images. |
| LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks
  for Multimodal Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.03232) or [HuggingFace](https://huggingface.co/papers/2510.03232))| Yu-Chiang Frank Wang, Yu-Yang Sheng, Min-Hung Chen, Ci-Siang Lin | This paper introduces LEAML, a two-stage framework for efficiently adapting Multimodal Large Language Models (MLLMs) to out-of-distribution (OOD) visual question answering (VQA) tasks using limited labeled data. The research objective is to develop a label-efficient method to fine-tune MLLMs for specialized domains like medical imaging where annotated data is scarce. The methodology involves a "Pseudo QA Generation" stage, where a QA Generator trained on few-shot examples creates synthetic question-answer pairs for unlabeled images, regularized via "Selective Neuron Distillation" from a larger model's captions, followed by an "OOD VQA Finetuning" stage using both original and pseudo-labeled data. On the Kvasir-VQA medical dataset, using only 1% of labeled data, LEAML achieved 76.7% average accuracy, significantly outperforming standard full fine-tuning (63.1%). For AI practitioners, this work provides a validated approach to adapt general-purpose MLLMs for specialized VQA tasks with minimal annotation budget by effectively leveraging unlabeled image corpora to generate synthetic training data. |
| SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the
  SpineMed-450k Corpus (Read more on [arXiv](https://arxiv.org/abs/2510.03160) or [HuggingFace](https://huggingface.co/papers/2510.03160))| Zhonghao Zhang, Xiang Zheng, Yang Zhang, Wenhui Dong, Ming Zhao | This research introduces SpineMed-450k, a large-scale, multimodal instruction corpus, and SpineBench, a benchmark for evaluating large vision-language models (LVLMs) on vertebral-level spine disorder analysis. The objective is to facilitate the development of AI systems with sophisticated, level-aware clinical reasoning by creating a traceable, clinically-grounded instruction dataset and a standardized evaluation framework for spine-specific tasks. The methodology consists of a clinician-in-the-loop pipeline that curates over 450,000 instruction instances from textbooks, clinical cases, and guidelines using a two-stage LLM generation method, followed by fine-tuning a 7B parameter LVLM and evaluating it on the SpineBench. The primary result is that the authors' fine-tuned model achieves an 87.44% average score on SpineBench, significantly outperforming other open-source models and revealing systematic weaknesses in the fine-grained, level-specific reasoning of existing generalist LVLMs. The principal implication for AI practitioners is that achieving clinically relevant performance in specialized, high-stakes domains requires the creation of domain-specific, high-quality instruction datasets for targeted fine-tuning, as the capabilities of general-purpose models are insufficient for complex, multimodal diagnostic reasoning. |
| How Confident are Video Models? Empowering Video Models to Express their
  Uncertainty (Read more on [arXiv](https://arxiv.org/abs/2510.02571) or [HuggingFace](https://huggingface.co/papers/2510.02571))| Anirudha Majumdar, Ola Shorinwa, Zhiting Mei | This paper introduces S-QUBED, a black-box framework for quantifying and decomposing uncertainty in generative video models, and establishes a metric for evaluating its calibration. The main objective is to develop a method for generative video models to express their predictive uncertainty, enabling a rigorous decomposition of this uncertainty into its aleatoric (due to prompt ambiguity) and epistemic (due to model knowledge gaps) components. The key methodology, S-QUBED, models the generation process with a latent variable; it quantifies aleatoric uncertainty as the entropy of a Von-Mises Fisher (VMF) distribution fitted to refined text prompts from an LLM, and epistemic uncertainty as the expected entropy of video outputs conditioned on those prompts, also modeled with VMF distributions. The primary result is that S-QUBED's total uncertainty estimates are shown to be calibrated, demonstrating a statistically significant negative correlation with video generation accuracy (CLIP score); on the Panda-70M dataset, this correlation yielded a Kendall's rank correlation p-value of 0.001. The principal implication for AI practitioners is that S-QUBED provides a model-agnostic tool to assess the confidence of generated videos, allowing engineers to identify and flag low-confidence or potentially inaccurate outputs without needing access to model internals or performing retraining. |
| TalkPlay-Tools: Conversational Music Recommendation with LLM Tool
  Calling (Read more on [arXiv](https://arxiv.org/abs/2510.01698) or [HuggingFace](https://huggingface.co/papers/2510.01698))| Juhan Nam, Keunwoo Choi, Seungheon Doh | This paper presents TalkPlay-Tools, a conversational music recommendation system that uses a Large Language Model (LLM) as an agent to orchestrate a pipeline of diverse retrieval and reranking tools. The main objective is to create a unified framework that can interpret multi-turn user intent to dynamically plan and execute a sequence of tools, including SQL, BM25, dense retrieval, and generative retrieval with Semantic IDs. The methodology centers on guiding a Qwen3-LM with a structured three-stage prompt (planning, retrieval, reranking) to call functions that query various databases based on user profiles, dialogue history, and the current query. The proposed tool-calling system achieves a Hit@1 of 0.022 in a zero-shot setting, outperforming a generative baseline with BM25 (0.018), and demonstrates high success rates for tools utilizing rich in-context information, such as User-to-Item (98.8%), but a low success rate for syntactically complex tools like SQL (24.7%). The principal implication for AI practitioners is that this LLM-based agentic architecture provides a viable method for integrating multiple, heterogeneous retrieval systems into a single, flexible conversational recommender, underscoring the critical need for rich in-context information to ensure reliable tool execution. |
| A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2510.01132) or [HuggingFace](https://huggingface.co/papers/2510.01132))|  | This paper systematically investigates and provides a practical recipe for training LLM agents using multi-turn reinforcement learning by analyzing the pillars of environment, reward, and policy. The main objective is to determine which design choices are most effective for multi-turn agentic RL, addressing the lack of systematic formulation in prior work. The methodology involves empirically evaluating agents on TextWorld, ALFWorld, and SWE-Gym, ablating factors like environment complexity, reward sparsity, SFT-to-RL ratios, and comparing biased (PPO, GRPO) versus unbiased (RLOO) algorithms. Key results show that combining SFT with RL is highly sample-efficient; an SFT prior from 60 demonstrations plus 400 RL episodes achieved 85% success, nearly matching the 88% from 5000 pure RL episodes. The principal implication for AI practitioners is that initializing agent policies with a small amount of demonstration data via SFT drastically reduces the need for expensive online RL episodes, and that biased algorithms like PPO are more robust for these complex, sequential decision-making tasks. |
| DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via
  Repetitive Pattern (Read more on [arXiv](https://arxiv.org/abs/2509.24975) or [HuggingFace](https://huggingface.co/papers/2509.24975))| Jia Li, Yitong Zhang, Yuetong Liu, wellbeing | DIFFTESTER is a framework that accelerates unit test generation for diffusion LLMs by identifying and jointly generating repetitive code patterns across multiple test cases. The main objective is to increase the inference speed of diffusion LLMs for automated unit test generation (UTG) without degrading the quality or coverage of the resulting tests. The key methodology involves parsing partially generated code into Abstract Syntax Trees (ASTs) at intermediate decoding steps, identifying common subtrees across a batch of test cases, and then unmasking all tokens corresponding to these shared patterns in a single operation. Primary results demonstrate significant acceleration, with throughput on the TestEval-C++ benchmark increasing by up to 2.45x using the DiffuCoder model while preserving maximum achievable test coverage. For AI practitioners, this task-specific approach offers a practical method to substantially reduce the latency and computational cost of generating large volumes of unit tests, making diffusion model-based testing more efficient for software development. |
| Align Your Tangent: Training Better Consistency Models via
  Manifold-Aligned Tangents (Read more on [arXiv](https://arxiv.org/abs/2510.00658) or [HuggingFace](https://huggingface.co/papers/2510.00658))| Jong Chul Ye, Byunghee Cha, Beomsu Kim | This paper introduces Align Your Tangent (AYT), a method that accelerates and stabilizes the training of Consistency Models (CMs) by using a self-supervised manifold feature distance (MFD) loss. The main objective is to address the slow convergence of CMs, which the authors identify as being caused by update directions (tangents) that oscillate parallel to the data manifold instead of pointing towards it. The key methodology involves training an auxiliary network to create a feature space that is sensitive to various off-manifold data perturbations (e.g., geometric, color, degradation); the distance in this learned feature space is then used as the CM's loss function, which forces tangents to align orthogonally to the manifold. On unconditional CIFAR10, AYT improves the 1-step generation FID from 3.60 to 2.61 and demonstrates robustness to training with batch sizes as small as 16, outperforming a baseline trained with a batch size of 128. For AI practitioners, AYT offers a self-supervised and interpretable loss function that can replace standard losses to train CMs more efficiently and robustly, reducing compute requirements without relying on human-curated perceptual datasets or complex training schedules. |
| NuRisk: A Visual Question Answering Dataset for Agent-Level Risk
  Assessment in Autonomous Driving (Read more on [arXiv](https://arxiv.org/abs/2509.25944) or [HuggingFace](https://huggingface.co/papers/2509.25944))|  | This paper introduces NuRisk, a comprehensive Visual Question Answering dataset for quantitative, agent-level risk assessment in autonomous driving, demonstrating that specialized fine-tuning is necessary to overcome the limitations of general pre-trained models. The core objective is to evaluate and improve the spatio-temporal reasoning capabilities of Vision Language Models (VLMs) for predicting how risks evolve over time. The methodology involves creating a 1.1M-sample dataset from nuScenes, Waymo, and CommonRoad with sequential Bird-Eye-View images and quantitative risk annotations, then using it to benchmark existing VLMs and fine-tune a 7B parameter agent. The primary result is that leading proprietary VLMs peak at 33% accuracy and completely fail at explicit spatio-temporal reasoning, whereas the fine-tuned NuRisk agent achieves 41% accuracy while demonstrating these crucial reasoning capabilities. The principal implication for AI practitioners is that deploying VLMs in safety-critical applications requires domain-specific fine-tuning on specialized, quantitative datasets, as prompt-based adaptation of general models is insufficient for reliable performance. |
| Triangle Splatting+: Differentiable Rendering with Opaque Triangles (Read more on [arXiv](https://arxiv.org/abs/2509.25122) or [HuggingFace](https://huggingface.co/papers/2509.25122))| Matheus Gadelha, Daniel Rebain, Sanghyun Son, Renaud Vandeghen, Jan Held | i) Triangle Splatting+ is a differentiable rendering framework that directly optimizes a semi-connected mesh of opaque triangles for high-quality novel view synthesis compatible with standard graphics engines. ii) The primary objective is to develop an end-to-end optimizable, mesh-based 3D scene representation that eliminates the need for post-processing steps like mesh extraction, making it directly usable in real-time applications. iii) The methodology introduces a shared-vertex triangle parameterization for connectivity and a tailored training strategy that anneals a global smoothness parameter while enforcing an increasing opacity floor to converge on a sharp, fully opaque mesh. iv) On the Mip-NeRF360 dataset, the method achieves a PSNR of 25.21, outperforming other state-of-the-art mesh-based approaches such as MiLo (24.09 PSNR) while using fewer vertices. v) The principal implication for AI practitioners is a method to generate high-fidelity 3D scene assets that are natively compatible with existing graphics pipelines, enabling direct integration into game engines and VR applications for physics simulation and interactive walkthroughs without conversion. |
| Scaling Policy Compliance Assessment in Language Models with Policy
  Reasoning Traces (Read more on [arXiv](https://arxiv.org/abs/2509.23291) or [HuggingFace](https://huggingface.co/papers/2509.23291))|  | The paper introduces Policy Reasoning Traces (PRTs), which are reasoning chains generated by a powerful pseudo-expert language model to improve policy compliance assessment in other LLMs. The research objective is to create a scalable method for generating expert-like reasoning demonstrations to enhance LLM performance on rule-based tasks without requiring expensive human-annotated rationales. The methodology involves prompting a frontier model like DEEPSEEK-R1 with a case, its verdict, and the relevant policy to generate a PRT, which is then used either as a few-shot in-context learning example or as data for supervised fine-tuning of a learner model. The primary results show that using PRTs as few-shot demonstrations on the HIPAA policy boosted the accuracy of open-weight models by 16-30 percentage points and established a new state-of-the-art accuracy of 81.0% on GDPR policy compliance. The principal implication for AI practitioners is that they can leverage a large model to generate synthetic reasoning data to significantly improve the performance and interpretability of smaller or more general models on specific, rule-based tasks like legal or safety compliance. |
