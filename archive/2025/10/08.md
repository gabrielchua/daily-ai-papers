

## Papers for 2025-10-08

| Title | Authors | Summary |
|-------|---------|---------|
| TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.06217) or [HuggingFace](https://huggingface.co/papers/2510.06217))|  | The paper presents TATTOO, a tool-grounded Process Reward Model (PRM) designed to provide reliable step-level supervision for large reasoning models (LRMs) in tabular reasoning. The primary research objective is to determine how to provide reliable step-level supervision for advanced LRMs to overcome performance bottlenecks in table-specific operations like sub-table retrieval and schema interaction. The methodology involves a dual-stage training paradigm: first, supervised fine-tuning on a curated dataset of over 60k instances with tool-integrated verification rationales, followed by reinforcement learning with a tool-grounded reward shaping scheme to align the model with table-based verification. Across five challenging tabular reasoning benchmarks, the 8B parameter TATTOO model improves downstream policy LRM performance by an average of 30.9% at inference, surpassing much larger baselines like the 72B Qwen-2.5-Math-PRM. For AI practitioners, the principal implication is that developing specialized, tool-augmented PRMs for structured domains is a critical strategy for enhancing reasoning fidelity, enabling more parameter-efficient models to achieve state-of-the-art performance. |
| Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and
  Synthesis for SLMs (Read more on [arXiv](https://arxiv.org/abs/2509.24107) or [HuggingFace](https://huggingface.co/papers/2509.24107))|  | This paper introduces Fathom-DeepResearch, a dual-model 4B parameter agentic system that addresses training instabilities in long-horizon, tool-augmented reasoning for small language models. The core methodology employs Reward Aware Policy Optimization (RAPO), a stabilized variant of GRPO using dataset pruning and replay buffers, alongside a steerable step-level reward function to train a search agent, which is paired with a synthesizer model trained on a synthetic corpus to generate citation-dense reports. The system achieves state-of-the-art results on multiple DeepSearch benchmarks, with the Fathom-Search-4B model scoring 90.0% on SimpleQA, outperforming both open and closed-source baselines. For AI practitioners, this work provides a validated training recipe and architecture for developing capable, tool-using SLM agents that avoid common RL instabilities like reward hacking and can perform complex multi-step web research. |
| Fast-dLLM v2: Efficient Block-Diffusion LLM (Read more on [arXiv](https://arxiv.org/abs/2509.26328) or [HuggingFace](https://huggingface.co/papers/2509.26328))|  | The paper introduces Fast-dLLM v2, a block diffusion language model that adapts pretrained autoregressive models to achieve efficient, parallel text generation. The research objective is to overcome the inherent sequential decoding inefficiency of autoregressive (AR) LLMs by developing a block diffusion model that can be fine-tuned from a pretrained AR model with minimal data, while maintaining or improving performance. The key methodology involves a novel training recipe that combines a block diffusion mechanism with a complementary attention mask to enable block-wise bidirectional context modeling, and a hierarchical caching mechanism with block-level and sub-block caches to accelerate parallel decoding during inference. Primary results show that the 7B parameter Fast-dLLM v2 achieves a 2.54Ã— higher throughput than the original Qwen2.5-7B-Instruct AR model on the GSM8K benchmark while offering comparable accuracy. The principal implication for AI practitioners is that this method provides a practical and data-efficient (~1B fine-tuning tokens) approach to convert existing high-performance AR models into significantly faster parallel decoders, making them more viable for deployment in latency-sensitive applications. |
| CoDA: Coding LM via Diffusion Adaptation (Read more on [arXiv](https://arxiv.org/abs/2510.03270) or [HuggingFace](https://huggingface.co/papers/2510.03270))|  | The paper introduces CoDA, a 1.7B-parameter diffusion language model for code generation, trained via a multi-stage adaptation process. The primary objective is to develop a compact, efficient diffusion coder that is competitive with larger autoregressive and diffusion models, particularly for tasks requiring bidirectional context and infilling. The methodology involves adapting the Qwen3-1.7B backbone through large-scale diffusion pre-training, code-centric mid-training using a progressive masking schedule, and subsequent instruction tuning. CoDA-1.7B-Instruct achieves a pass@1 score of 63.2% on the MBPP-Plus benchmark, surpassing the larger Dream-7B-Instruct model. For AI practitioners, this research demonstrates that smaller-scale diffusion models can be a viable alternative to heavyweight autoregressive systems for coding assistants, offering competitive performance and inherent infilling capabilities with lower latency. |
| Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.04081) or [HuggingFace](https://huggingface.co/papers/2510.04081))| Zhuoshi Pan, Xin Gao, Qizhi Pei, Honglin Lin, apeters | This paper presents Caco (Code-Assisted Chain-of-ThOught), a framework for automatically synthesizing large-scale, verifiable, and diverse instruction and Chain-of-Thought (CoT) reasoning data using a code-driven augmentation pipeline. The primary objective is to overcome the scalability and verifiability limitations of natural language CoT by creating a fully automated framework for generating instruction-CoT pairs grounded in executable code. The methodology involves a closed-loop process: fine-tuning a "CodeGen" model on a unified corpus of seed solutions, generating new code-based CoTs at scale, verifying them via execution and filtering, and then reverse-engineering these validated code traces into natural language instructions and CoTs. Models fine-tuned on the resulting 1.3M-sample Caco dataset demonstrate superior performance, with the Caco-trained Qwen2.5-Math-7B model achieving an average accuracy of 67.7% across six mathematical reasoning benchmarks, outperforming strong baselines. For AI practitioners, this work provides a scalable method for creating verifiably correct instruction-tuning data for complex reasoning without human intervention, enabling the development of more trustworthy and generalizable LLMs. |
| ASPO: Asymmetric Importance Sampling Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2510.06062) or [HuggingFace](https://huggingface.co/papers/2510.06062))| Xiu Li, Wenping Hu, Lei Lin, Jiakang Wang, RyanLiu112 | ASPO is a policy optimization algorithm that corrects a token-weight mismatch in Outcome-Supervised Reinforcement Learning (OSRL) by asymmetrically flipping the importance sampling ratios for positive-advantage tokens. The primary objective is to address a flaw in GRPO-based OSRL where the standard clipping mechanism disproportionately weights high-probability positive-advantage tokens, leading to training instability, entropy collapse, and premature convergence. The key methodology, Asymmetric Importance Sampling Policy Optimization (ASPO), involves inverting the importance sampling (IS) ratio for positive-advantage tokens to assign larger update weights to less confident tokens, and incorporating a soft dual-clipping mechanism to stabilize extreme updates. On mathematical reasoning benchmarks, ASPO achieved an average score of 59.3, outperforming the strong GRPO-based baseline DAPO, which scored 53.5, and improving on the base model by 12.5%. The principal implication for AI practitioners is that when using GRPO-style OSRL for LLM fine-tuning, standard IS ratios can function as a misaligned weighting scheme; applying ASPO's asymmetric weighting corrects this, leading to more stable training and enhanced model performance. |
| TensorBLEU: Vectorized GPU-based BLEU Score Implementation for
  Per-Sentence In-Training Evaluation (Read more on [arXiv](https://arxiv.org/abs/2510.05485) or [HuggingFace](https://huggingface.co/papers/2510.05485))|  | This paper introduces TensorBLEU, a fully vectorized, memory-efficient GPU implementation for per-sentence Token-ID BLEU calculation designed for in-training evaluation. The objective is to eliminate the computational bottleneck of CPU-based BLEU calculation within training loops, particularly for Reinforcement Learning (RL) fine-tuning where per-sentence reward signals are required. The key methodology is a memory-efficient n-gram counting mechanism that leverages `torch.unfold` for parallel n-gram extraction and `torch.unique` to create a compact, batch-specific n-gram dictionary, enabling parallel counting via a "batched bincount" technique. Primary results show that TensorBLEU achieves speedups of over 40x on an NVIDIA A100 GPU (for a batch size of 256 with 1024-token sequences) compared to the standard CPU-based NLTK implementation. The principal implication for AI practitioners is that this tool transforms the calculation of BLEU-based rewards from a major training bottleneck into a negligible overhead, making large-scale RL fine-tuning of language models with dense reward signals computationally feasible. |
| Mixing Mechanisms: How Language Models Retrieve Bound Entities
  In-Context (Read more on [arXiv](https://arxiv.org/abs/2510.06182) or [HuggingFace](https://huggingface.co/papers/2510.06182))|  | This paper demonstrates that language models use a dynamic mixture of positional, lexical, and reflexive mechanisms to retrieve bound entities in-context, moving beyond the previously held view of a purely positional approach. The main objective is to mechanistically investigate how language models bind and retrieve entities, especially in long contexts where the simple positional mechanism becomes unreliable. The study employs interchange interventions with specially designed pairs of original and counterfactual inputs to isolate the causal contributions of the three distinct mechanisms, using the results to train a quantitative causal model. The primary result is that this three-mechanism model predicts the language model's next-token distributions with 95% agreement (Jensen-Shannon Similarity), vastly outperforming a model based on the prevailing positional-only view (44% JSS). For AI practitioners, this research provides a mechanistic explanation for the "lost-in-the-middle" effect, implying that the reliability of entity retrieval in long-context applications like RAG is non-uniform and depends on the entity's position, which should inform prompt design and error analysis. |
| AInstein: Assessing the Feasibility of AI-Generated Approaches to
  Research Problems (Read more on [arXiv](https://arxiv.org/abs/2510.05432) or [HuggingFace](https://huggingface.co/papers/2510.05432))| Jose Dolz, Laurent Charlin, Marco Pedersoli, Gaurav Sahu, Shambhavi Mishra | The paper introduces AInstein, a framework to evaluate whether Large Language Models (LLMs) can autonomously solve novel AI research problems using only their pretrained parametric knowledge. The central objective is to determine if LLMs can generate valid, technical solutions to research challenges extracted from scientific abstracts without external aids like fine-tuning or retrieval augmentation. The methodology uses a "Generalizer" LLM to distill a research problem from an ICLR 2025 paper abstract and a "Solver" LLM to propose a solution, both refined through iterative critique loops, with evaluation performed on 1,214 papers using an LLM-as-a-judge. Results show that while perfect rediscovery of human solutions is rare (dropping from ~84% to ~19% for the top model under stricter criteria), LLMs frequently generate novel, valid alternatives, with the best-performing agent achieving a strict Success Rate of 74.05%. For AI practitioners, this implies that state-of-the-art LLMs can serve as creative problem-solving partners, capable of generating technically sound and original approaches to engineering challenges, extending their utility beyond information retrieval to genuine solution ideation. |
| Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning? (Read more on [arXiv](https://arxiv.org/abs/2510.06036) or [HuggingFace](https://huggingface.co/papers/2510.06036))|  | This paper mechanistically investigates the "refusal cliff," a safety failure where reasoning models' internal refusal intentions are suppressed just before output generation by specific attention heads. The central research objective is to identify the underlying mechanism that makes safety alignment vulnerable in large reasoning models. The methodology involves using a linear probe to quantify refusal scores from hidden states across token positions, followed by causal tracing and ablation of attention heads to isolate components negatively impacting refusal. The primary result demonstrates that ablating a sparse set of "Refusal Suppression Heads," comprising just 3% of identified heads, reduces attack success rates to below 10%; a proposed data selection method achieves comparable safety performance using only 1.7% of the original training data. For AI practitioners, this implies that model safety can be significantly and efficiently improved through targeted interventions, specifically by identifying and mitigating the effect of these suppressive heads or by using computationally cheap probes to select high-impact data for fine-tuning. |
| HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video (Read more on [arXiv](https://arxiv.org/abs/2510.05560) or [HuggingFace](https://huggingface.co/papers/2510.05560))| Katelyn Gao, Quentin Leboutet, Hao-Yu Hsu, Chih-Hao Lin, Hongchi Xia | HoloScene is a framework for reconstructing simulation-ready, interactive 3D worlds from a single video using an energy-based optimization of a scene-graph representation. The primary objective is to create a 3D digital twin that is geometrically complete, physically plausible, interactive, and photorealistically rendered, overcoming limitations of prior methods. The key methodology involves representing the scene as an interactive scene graph with nodes for object geometry (neural SDFs), appearance (Gaussian splats), and physical properties, which is then reconstructed via a three-stage hybrid optimization: gradient-based initialization, generative sampling with tree search for physical plausibility and shape completion, and a final refinement stage. HoloScene significantly improves physical stability, achieving a physics failure rate of 18.3% on the Replica dataset, compared to rates of over 90% for baseline methods like PhyRecon and DP-Recon. For AI practitioners, this work provides a method to automate the creation of high-fidelity, physically interactive virtual environments from video, which can accelerate the development of simulators for robotics, autonomous systems, and interactive applications like gaming and AR/VR. |
| Margin Adaptive DPO: Leveraging Reward Model for Granular Control in
  Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2510.05342) or [HuggingFace](https://huggingface.co/papers/2510.05342))| sirano1004 | This paper introduces Margin-Adaptive Direct Preference Optimization (MADPO), a method that uses a reward model to apply an instance-level adaptive weight to the DPO loss. The primary objective is to overcome the limitations of DPO's fixed temperature parameter by developing a stable, instance-level regularization approach that avoids overfitting on easy examples and under-learning from hard ones. The methodology involves a two-step process: first, training a reward model to estimate preference margins, and second, using these margins to re-weight the DPO loss for each sample, amplifying the learning signal for hard pairs and dampening it for easy pairs. On a sentiment generation task, MADPO achieved performance gains of up to +33.3% on High Quality data and +10.5% on Low Quality data over the next-best baseline. For AI practitioners, this method provides a principled and more robust approach to preference alignment, enabling granular control over the training objective by leveraging an external reward model, which is particularly effective for datasets with diverse or noisy preference signals. |
| Discrete Diffusion Models with MLLMs for Unified Medical Multimodal
  Generation (Read more on [arXiv](https://arxiv.org/abs/2510.06131) or [HuggingFace](https://huggingface.co/papers/2510.06131))|  | MeDiM is a novel discrete diffusion model that leverages a Multimodal Large Language Model (MLLM) to unify the generation of medical images, reports, and joint image-report pairs within a single framework. The research objective is to create a versatile generative foundation model for medicine that learns shared distributions across modalities, overcoming the limitations of modality-specific systems. The key methodology involves adapting a pre-trained autoregressive MLLM as the backbone for a discrete diffusion process by removing its causal attention mask to enable bidirectional context and injecting timestep embeddings via adaptive layer normalization (AdaLN). The model achieves state-of-the-art results, including a FrÃ©chet Inception Distance (FID) of 16.60 on MIMIC-CXR for image generation, and demonstrates that its synthetic image-report pairs can improve downstream task performance by up to +4.80% in METEOR score. For AI practitioners, the principal implication is that autoregressive MLLMs can be effectively repurposed for high-performance discrete diffusion models, enabling the development of unified systems that can generate consistent, multimodal data for augmenting training sets in specialized domains. |
| MixReasoning: Switching Modes to Think (Read more on [arXiv](https://arxiv.org/abs/2510.06052) or [HuggingFace](https://huggingface.co/papers/2510.06052))|  | MixReasoning is an inference-time framework that improves the efficiency of reasoning models by dynamically adjusting the depth of thought within a single response. The main objective is to reduce the computational cost and redundancy of long Chain-of-Thought (CoT) by adaptively allocating detailed reasoning only to pivotal, high-difficulty steps. The key methodology involves using a lightweight LoRA adapter to enable a "concise" mode and monitoring token-level uncertainty (entropy) to trigger a temporary switch to a detailed "thinking" mode for regenerating high-uncertainty segments. The primary result shows that on the GSM8K benchmark with the QwQ-32B-Preview model, MixReasoning reduced token usage by 47% (from 750.3 to 400.5) while simultaneously improving accuracy by 1.01%. The principal implication for AI practitioners is that this single-model technique can be deployed to significantly lower latency and inference costs for reasoning tasks, offering a controllable trade-off between efficiency and response verbosity without requiring complex architectural changes or retraining of the base model. |
| LightCache: Memory-Efficient, Training-Free Acceleration for Video
  Generation (Read more on [arXiv](https://arxiv.org/abs/2510.05367) or [HuggingFace](https://huggingface.co/papers/2510.05367))| Zheng Zhan, Yushu Wu, Kaiyuan Deng, Gen Li, Yang Xiao | LightCache is a training-free framework that accelerates video diffusion model inference while reducing peak memory usage through stage-specific optimizations. The paper's objective is to mitigate the substantial GPU memory increase caused by existing cache-based acceleration methods in video generation without requiring model retraining. The key methodology involves decomposing inference into encoding, denoising, and decoding stages and applying three targeted techniques: asynchronous swapping of cached features to CPU, spatial chunking of feature maps during denoising, and slicing latent tensors for sequential frame-by-frame VAE decoding. For the Stable-Video-Diffusion-Img2vid-XT model, LightCache achieves a 2.86x inference speedup while simultaneously reducing peak memory usage by 1.4 GB compared to the baseline. The principal implication for AI practitioners is that this method enables the deployment of large video generation models on hardware with constrained GPU memory, as it concurrently reduces memory footprint and latency, making it a highly efficient solution for production environments. |
| Demystifying deep search: a holistic evaluation with hint-free multi-hop
  questions and factorised metrics (Read more on [arXiv](https://arxiv.org/abs/2510.05137) or [HuggingFace](https://huggingface.co/papers/2510.05137))|  | This paper introduces WebDetective, a benchmark with hint-free multi-hop questions and a factorised evaluation framework to diagnose autonomous reasoning failures in web agents. The primary objective is to evaluate how well models can autonomously discover reasoning chains for deep search tasks, as opposed to simply executing paths hinted at in the question. The methodology involves a co-designed benchmark consisting of hint-free questions and a controlled Wikipedia sandbox to ensure full traceability, paired with a holistic evaluation framework that separates search sufficiency, knowledge utilisation, and refusal behaviour. The primary result from evaluating 25 models is a consistent and significant gap between information retrieval and synthesis; for instance, the o3-Pro model achieved a 78% Search Score but only a 20.86% Generation Score, leading to a 56.0% final Pass@1 rate. The principal implication for AI practitioners is that improving multi-hop agents requires focusing on architectures for robust information synthesis and calibrated refusal, as the current bottleneck is not evidence discovery but its composition into a correct answer when the reasoning path is not provided. |
| EgoNight: Towards Egocentric Vision Understanding at Night with a
  Challenging Benchmark (Read more on [arXiv](https://arxiv.org/abs/2510.06218) or [HuggingFace](https://huggingface.co/papers/2510.06218))| Tianwen Qian, Yang Miao, Runyi Yang, Yuqian Fu, dehezhang2 | This paper introduces EgoNight, the first comprehensive benchmark to evaluate egocentric vision understanding models specifically under nighttime conditions using day-night aligned videos. The primary objective is to systematically investigate and quantify the performance gap of Multimodal Large Language Models (MLLMs) when transitioning from well-lit daytime scenarios to challenging low-light environments. The methodology involves the creation of a new dataset with synthetic and real-world aligned day-night video pairs, and the construction of three benchmark tasks: egocentric Visual Question Answering (VQA), day-night correspondence retrieval, and depth estimation. Experiments reveal that state-of-the-art models struggle significantly, showing an average performance decline of 32.8% on paired VQA tasks for the synthetic dataset when moving from day to night conditions. The principal implication for AI practitioners is that current MLLMs are not robust to illumination changes, and systems intended for real-world egocentric applications must be explicitly designed and validated for reliable performance in low-light and nighttime settings. |
| BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language
  Models via Lens of Dynamic Interactions (Read more on [arXiv](https://arxiv.org/abs/2510.05318) or [HuggingFace](https://huggingface.co/papers/2510.05318))| Shipei Lin, Per Jacobsson, Jinyang Li, Xiaohan Xu, Nan Huo | BIRD-INTERACT introduces a comprehensive benchmark to evaluate large language models on dynamic, multi-turn Text-to-SQL tasks that mirror real-world database interactions. The main objective is to assess an LLM's ability to handle ambiguous queries, execution errors, and evolving user needs in a stateful, interactive environment, moving beyond static, single-turn evaluation. The methodology involves a benchmark suite with two settings: `c-Interact` (protocol-guided conversation) and `a-Interact` (autonomous agentic interaction), coupled with an environment featuring a knowledge base and a two-stage, function-driven user simulator to ensure robust and reproducible evaluation. Primary results demonstrate the benchmark's difficulty, with the flagship model GPT-5 achieving only an 8.67% task completion success rate in the `c-Interact` setting and 17.00% in the `a-Interact` setting on the full task suite. The principal implication for AI practitioners is that optimizing for single-turn SQL generation is insufficient; focus must shift to developing strategic interaction capabilities, such as effective clarification, dialogue state management, and error recovery, which are the current bottlenecks for deploying robust database assistants. |
| VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation (Read more on [arXiv](https://arxiv.org/abs/2510.05156) or [HuggingFace](https://huggingface.co/papers/2510.05156))|  | VeriGuard is a novel framework that provides formal, provable safety guarantees for LLM agents by ensuring their actions comply with predefined constraints. Its methodology employs a dual-stage architecture: an offline stage uses an iterative loop of LLM-based generation, automated testing, and formal verification to create a "correct-by-construction" policy, which is then used in an online stage for lightweight runtime action monitoring. The primary result on the Agent Security Bench (ASB) shows VeriGuard reduces the Attack Success Rate (ASR) to 0.0% across all evaluated attack types while maintaining high task utility. The principal implication for AI practitioners is the provision of a structured, verifiable framework to enforce complex safety policies, enabling the deployment of agents in high-stakes domains by moving beyond reactive, pattern-based guardrails to a provably-sound safety paradigm. |
| CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support
  Conversation (Read more on [arXiv](https://arxiv.org/abs/2510.05122) or [HuggingFace](https://huggingface.co/papers/2510.05122))|  | The CARE framework augments Emotional Support Conversation (ESC) models with explicit, multi-step cognitive reasoning chains refined via reinforcement learning. The primary objective is to enhance the reasoning capabilities and supportive quality of ESC models without relying on large-scale synthetic data generation. The methodology involves a two-stage process: first, supervised fine-tuning (SFT) on the original ESConv dataset enriched with distilled four-step reasoning chains (Context, Cognition, Emotion, Support Plan), followed by reinforcement learning (RL) using the GRPO algorithm to optimize reasoning consistency and accuracy. The final CARE model achieved state-of-the-art results, increasing support strategy accuracy to 30.29 from the 26.36 of the baseline ESConv model. The principal implication for AI practitioners is that high-quality, specialized datasets can be effectively leveraged to create structured reasoning data through model-based distillation, providing a more robust and interpretable alternative to large-scale synthetic data augmentation for building cognitively-aware dialogue systems. |
| CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical
  Contrastive Decoding (Read more on [arXiv](https://arxiv.org/abs/2509.23379) or [HuggingFace](https://huggingface.co/papers/2509.23379))|  | This paper introduces Clinical Contrastive Decoding (CCD), a training-free and retrieval-free inference framework to mitigate medical hallucinations in radiology Multimodal Large Language Models (MLLMs). The main objective is to reduce clinically unsupported descriptions generated by radiology MLLMs by addressing their over-sensitivity to clinical prompts, without modifying the base model. CCD's methodology involves a dual-stage contrastive mechanism that leverages an external, task-specific expert model to extract structured clinical labels and probability scores, which are then used to refine the MLLM's token-level logits during generation. On the MIMIC-CXR dataset, applying CCD to a state-of-the-art model resulted in a 17% improvement in RadGraph-F1, demonstrating enhanced clinical fidelity. The principal implication for AI practitioners is that lightweight, domain-specific expert models can be integrated at inference-time to guide large foundation models, providing a generalizable and training-free approach to improve domain-specific accuracy and reduce hallucinations. |
| No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2510.03978) or [HuggingFace](https://huggingface.co/papers/2510.03978))| Xiao Xiao Sun, Vishwesh Nath, Javier Gamazo Tejero, Alejandro Lozano, Min Woo Sun | This paper introduces BMC-LongCLIP, a biomedical vision-language model trained with an extended text context of up to 512 tokens to leverage long-format captions and reduce token waste. The main objective is to investigate the impact of pretraining VLMs with longer context windows on downstream biomedical retrieval and classification performance. The methodology involves pretraining a CLIP-based model with a ViT-L/14 vision encoder and a long-context BioClinical ModernBERT text encoder on the BIOMEDICA dataset, while also introducing a new long-caption dataset, BIOMEDICA-LongCAP. Extending the context length from 77 to 512 tokens reduced token waste from 55% to 2.2% and achieved up to a +30% absolute gain in Recall@1 on long-caption retrieval benchmarks. The principal implication for AI practitioners is that increasing the text encoder's context length during pretraining is a critical technique for improving VLM performance in domains with long, descriptive text by utilizing previously discarded supervisory signals. |
| Equilibrium Matching: Generative Modeling with Implicit Energy-Based
  Models (Read more on [arXiv](https://arxiv.org/abs/2510.02300) or [HuggingFace](https://huggingface.co/papers/2510.02300))|  | This paper introduces Equilibrium Matching (EqM), a generative modeling framework that learns a time-invariant equilibrium gradient of an implicit energy landscape for optimization-based sampling. The objective is to develop a generative model that avoids the non-equilibrium, time-conditional dynamics of diffusion/flow models to enable more flexible inference. EqM is trained to predict a target gradient `(Îµ - x)c(Î³)` that points from noise `Îµ` to data `x`, with a magnitude `c(Î³)` that decays to zero at the data manifold, allowing samples to be generated via gradient descent on the learned landscape. The method achieves a state-of-the-art FrÃ©chet Inception Distance (FID) of 1.90 on class-conditional ImageNet 256Ã—256 generation, outperforming comparable flow-based models. For AI practitioners, EqM provides a route to replace fixed-horizon integrators with optimization-based samplers, enabling the use of adaptive step sizes, adaptive compute, and advanced optimizers like Nesterov Accelerated Gradient for more controllable and potentially more efficient inference. |
| Human3R: Everyone Everywhere All at Once (Read more on [arXiv](https://arxiv.org/abs/2510.06219) or [HuggingFace](https://huggingface.co/papers/2510.06219))| Yuliang Xiu, Anpei Chen, Yuxuan Xue, Xingyu Chen, Yue Chen | Human3R is a unified, real-time, feed-forward framework that jointly reconstructs multi-person 3D human meshes, dense 3D scenes, and camera trajectories from a single monocular video stream. The objective is to develop a unified, one-stage, online model for 4D human-scene reconstruction that eliminates dependencies on pre-processing modules (e.g., SLAM, human detection) and iterative refinement, enabling real-time performance. The methodology involves fine-tuning the 4D reconstruction foundation model CUT3R using parameter-efficient visual prompt tuning (VPT); human head tokens are detected from image features, augmented with human priors from a pre-trained Multi-HMR ViT encoder, and projected into human prompts that are processed by the frozen CUT3R decoder to regress multi-person SMPL-X parameters in a single forward pass. Human3R achieves state-of-the-art or competitive performance across multiple 4D reconstruction tasks with a single model; on the EMDB-2 dataset for global human motion estimation, it achieves a 20% lower W-MPJPE and 60% lower Root Translation Error (RTE) compared to the prior online state-of-the-art method WHAM, while operating at 15 FPS. For AI practitioners, Human3R provides a lightweight, dependency-free, and real-time model that can replace complex multi-stage pipelines for 4D human-scene reconstruction, simplifying deployment and enabling online applications in AR/VR, autonomous navigation, and humanoid policy learning with a low memory footprint (8 GB). |
| Training Dynamics Impact Post-Training Quantization Robustness (Read more on [arXiv](https://arxiv.org/abs/2510.06213) or [HuggingFace](https://huggingface.co/papers/2510.06213))| Jonas Geiping, NiccolÃ² Ajroldi, Albert Catalan-Tatjer | This paper demonstrates that training dynamics, particularly learning rate decay, are a primary driver of post-training quantization (PTQ) error in large language models, challenging the prevailing notion that dataset scale is the dominant factor. The research objective is to investigate and identify interventions that modulate the relationship between training hyperparameters and PTQ robustness. The methodology combines analysis of quantization error across hundreds of checkpoints from six major open-source LLM families with controlled pretraining experiments (up to 100B tokens) to isolate the effects of learning rate schedules, weight decay, and weight averaging. The primary result is that quantization error spikes coincide with learning rate decay, largely independent of the training data scale; for example, controlled experiments show that LAtest Weight Averaging (LAWA) can match or even surpass the 3-bit PTQ performance of models trained with a full learning rate decay schedule. The principal implication for AI practitioners is that PTQ robustness should be an active criterion during hyperparameter tuning, as strategic interventions like favoring higher learning rates or employing weight averaging can produce models that are inherently more robust to low-bit quantization. |
| ShapeGen4D: Towards High Quality 4D Shape Generation from Videos (Read more on [arXiv](https://arxiv.org/abs/2510.06208) or [HuggingFace](https://huggingface.co/papers/2510.06208))| Sergey Tulyakov, Jiaxu Zou, Jianqi Chen, Ashkan Mirzaei, Jiraphon Yenphraphai | ShapeGen4D is a feedforward framework that directly generates high-quality, dynamic 3D mesh sequences from a single monocular video. The objective is to develop a native video-to-4D model that overcomes the instabilities of score distillation sampling and the error accumulation inherent in two-stage multi-view reconstruction pipelines. The methodology extends a pre-trained 3D diffusion transformer by incorporating spatiotemporal attention layers, creating temporally-aligned latents via warped query points, and enforcing temporal stability by sharing the same diffusion noise across all frames. The method demonstrates superior geometric accuracy over baselines on the Objaverse test set, achieving a 0.3276 Intersection over Union (IoU) score. For AI practitioners, this work provides a data-efficient strategy for 4D content generation by showing that fine-tuning large, pre-trained static 3D models with targeted temporal modifications is more effective than training from scratch or using complex optimization pipelines. |
| Deforming Videos to Masks: Flow Matching for Referring Video
  Segmentation (Read more on [arXiv](https://arxiv.org/abs/2510.06139) or [HuggingFace](https://huggingface.co/papers/2510.06139))| Chengzu Li, Sizhe Dang, Liuzhuozheng Li, Dengyang Jiang, Zanyi Wang | This paper presents FlowRVS, a framework that reformulates Referring Video Object Segmentation (RVOS) as a continuous, text-conditioned flow process that deforms a video's latent representation into a target mask. The primary objective is to overcome the information bottlenecks and temporal inconsistencies of traditional 'locate-then-segment' pipelines by creating a unified, end-to-end model. The key methodology involves modeling RVOS with an Ordinary Differential Equation (ODE) to learn a velocity field that transforms a video latent from a pretrained Text-to-Video model to a mask latent, stabilized by start-point focused adaptations including Boundary-Biased Sampling (BBS) and Direct Video Injection (DVI). FlowRVS achieves new state-of-the-art results, attaining a J&F score of 73.3 on the zero-shot Ref-DAVIS17 benchmark, surpassing the prior SOTA by 2.7 points. The principal implication for AI practitioners is that complex video understanding tasks can be effectively framed as continuous deformation processes, enabling the direct adaptation of powerful pretrained generative models for discriminative objectives by focusing learning on the flow's initial, high-certainty conditions. |
| Distributional Semantics Tracing: A Framework for Explaining
  Hallucinations in Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.06107) or [HuggingFace](https://huggingface.co/papers/2510.06107))| Jacobo Azcona, Kevin Allan, Somayajulu G Sripada, gagan3012 | This paper introduces Distributional Semantics Tracing (DST), a unified framework that mechanistically explains LLM hallucinations by tracing internal representational drift to a specific "commitment layer" where failures become irreversible. The main objective is to diagnose how, when, and why hallucinations occur by treating them as predictable failures arising from the Transformer architecture, specifically from a conflict between distinct computational pathways. The DST methodology integrates causal tracing, patching, and subsequence analysis to build layer-wise semantic networks and uses a novel metric, Distributional Semantics Strength (DSS), to quantify the coherence of the model's contextual reasoning. The primary result is the identification of a "Reasoning Shortcut Hijack" failure mode and a strong negative Pearson correlation of -0.863 between the contextual pathway's coherence (measured by DSS) and hallucination rates. For AI practitioners, this reframes hallucination mitigation from post-hoc correction to proactive diagnosis, providing a method to identify the specific commitment layer where a failure solidifies, thus creating a concrete target for architectural interventions and improving model reliability. |
| Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI
  Models for Scatterplot-Related Tasks (Read more on [arXiv](https://arxiv.org/abs/2510.06071) or [HuggingFace](https://huggingface.co/papers/2510.06071))| Pedro Bizarro, Rita Costa, Diogo Duarte, joaompalmeiro | This paper introduces a synthetic dataset and benchmark to evaluate proprietary AI models on visual scatterplot analysis tasks. The objective is to systematically assess model capabilities in cluster and outlier counting, detection, and identification, addressing a gap in existing benchmarks. The authors generated a dataset of over 18,000 annotated scatterplots with varied designs and evaluated 10 models from OpenAI and Google using zero-shot, one-shot, and few-shot prompting strategies across five distinct tasks. Results show that while few-shot prompting enables high performance on counting tasks (e.g., Gemini 2.5 Flash achieved over 90% accuracy in outlier counting), performance on localization tasks is poor, with Precision and Recall generally below 50%, except for Flash which reached 65.01% Recall for outlier identification. The principal implication for AI practitioners is to use few-shot prompting for scatterplot analysis and to apply current models primarily to counting tasks, as their performance on precise localization tasks is unreliable. |
| In-the-Flow Agentic System Optimization for Effective Planning and Tool
  Use (Read more on [arXiv](https://arxiv.org/abs/2510.05592) or [HuggingFace](https://huggingface.co/papers/2510.05592))| Jianwen Xie, Sheng Liu, Seungju Han, Haoxiang Zhang, Zhuofeng Li | This research introduces AGENTFLOW, a trainable agentic framework, and Flow-GRPO, an on-policy RL algorithm, to optimize long-horizon planning and tool use by training a planner module within a live multi-turn interaction loop. The main objective is to overcome the limitations of monolithic RL models and static agentic systems by developing a method for effective on-policy learning in a multi-module agentic system facing long-horizon, sparse-reward credit assignment challenges. The key methodology is AGENTFLOW, a four-module (planner, executor, verifier, generator) system with an evolving memory, which is trained using Flow-based Group Refined Policy Optimization (Flow-GRPO); this algorithm converts multi-turn optimization into single-turn updates by broadcasting a final trajectory-level reward to every step. The primary result shows that the 7B-parameter AGENTFLOW system significantly outperforms specialized baselines, achieving an average accuracy gain of 14.9% on search tasks over the top-performing baseline. The principal implication for AI practitioners is that this in-the-flow optimization approach provides a scalable and stable method to train modular agentic systems for complex, long-horizon tasks, enabling the development of more reliable and adaptive agents that learn directly from final outcomes without requiring complex intermediate reward shaping. |
| A Contextual Quality Reward Model for Reliable and Efficient Best-of-N
  Sampling (Read more on [arXiv](https://arxiv.org/abs/2510.04087) or [HuggingFace](https://huggingface.co/papers/2510.04087))| sirano1004 | This paper presents a reward model that learns contextual acceptability, not just relative preference, by incorporating an "outside option" into the training data. The main objective is to develop a reward model and an associated inference strategy that can distinguish "good enough" responses from merely "better" ones, thereby mitigating the failure mode of standard Best-of-N (BoN) sampling where the least bad of many poor options is selected. The key methodology is to train a reward model on preference data augmented with a "reject all" option, based on a discrete choice framework, and then use this model in a "best of mini-N in-loop" adaptive inference strategy with a calibrated early-exit condition. Experiments show that when configured as an alignment guardrail, this method reduces reliability failures by 70% compared to standard BoN; when configured as an inference accelerator, it improves average inference speed by over 22%. The principal implication for AI practitioners is a tunable framework to explicitly manage the trade-off between reliability and computational efficiency, allowing them to configure systems for either maximum safety or maximum speed based on application requirements. |
| DRIFT: Learning from Abundant User Dissatisfaction in Real-World
  Preference Learning (Read more on [arXiv](https://arxiv.org/abs/2510.02341) or [HuggingFace](https://huggingface.co/papers/2510.02341))| Zheli Liu, Zhaoxuan Tan, Junlin Wu, Bolian Li, AmberYifan | DRIFT is an iterative preference learning method that improves LLMs by using abundant, real-world user dissatisfaction signals as negatives and dynamically sampling positives from the current policy. The main objective is to develop a scalable preference learning framework that leverages the naturally abundant signal of user dissatisfaction from real-world LLM interactions, avoiding reliance on costly curated positive examples. The methodology, Dissatisfaction-Refined Iterative preFerence Training (DRIFT), constructs preference pairs by using authentic user-dissatisfied responses as rejected examples and iteratively sampling fresh chosen examples from the evolving model policy, then updating the model using the Direct Preference Optimization (DPO) loss. On the synthetic UltraFeedback dataset, a 14B model trained with DRIFT achieved a +7.61% increase in WildBench Task Score and a +12.29% absolute win rate increase on AlpacaEval2 over the base model, outperforming strong baselines like iterative DPO and SPIN. The principal implication for AI practitioners is that they can align deployed LLMs more effectively and scalably by directly using logs of user corrections and complaints as negative examples in a DPO framework, reducing dependence on expensive human annotation for positive preference data. |
| Revisiting Modeling and Evaluation Approaches in Speech Emotion
  Recognition: Considering Subjectivity of Annotators and Ambiguity of Emotions (Read more on [arXiv](https://arxiv.org/abs/2510.05934) or [HuggingFace](https://huggingface.co/papers/2510.05934))|  | This dissertation introduces three novel approaches for speech emotion recognition (SER) that challenge traditional label aggregation methods by incorporating annotator subjectivity, emotion ambiguity, and co-occurrence frequencies. The primary objective is to determine if SER model performance and evaluation can be improved by retaining all human-provided emotional ratings, including minority and non-consensus labels, rather than discarding them through conventional aggregation techniques like majority or plurality voting. The author introduces an "all-inclusive rule" (AR) that uses all annotated data to create distributional ground-truth labels, a rater-modeling approach that fuses representations from individual annotator models, and a penalization matrix integrated into the loss function to penalize the prediction of rare emotional co-occurrences. On the IEMOCAP dataset, the rater-modeling fusion achieved an unweighted average recall (UAR) of 61.48%, a 4.36% absolute improvement over a standard soft-label baseline, while the penalization matrix improved the macro F1-score on the MSP-PODCAST dataset by a relative 25.8% for distribution-label learning tasks. The principal implication for AI practitioners is that they should avoid discarding data that lacks label consensus; utilizing all available annotations to train models on distributional labels and evaluate them on complete, unfiltered test sets leads to more robust systems that better handle the ambiguity inherent in real-world emotional expression. |
| OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit
  Flows (Read more on [arXiv](https://arxiv.org/abs/2510.03506) or [HuggingFace](https://huggingface.co/papers/2510.03506))|  | OneFlow is a non-autoregressive multimodal framework that unifies variable-length text generation via Edit Flows and image synthesis via Flow Matching for concurrent and interleaved generation. The primary objective is to overcome the sequential and fixed-length limitations of existing multimodal models by enabling a flexible, non-causal generation process for both text and images. The methodology combines Edit Flows, a continuous-time Markov chain for discrete token insertion, with continuous Flow Matching for image latents within a single bidirectional Transformer, coordinated by an interleaved time schedule. The primary result shows that OneFlow scales more efficiently than autoregressive models, requiring up to 50% fewer training FLOPs to achieve performance parity on generation benchmarks. For AI practitioners, this model offers a computationally efficient alternative for building systems that can dynamically generate variable-length, interleaved text-and-image content without the constraints of sequential, autoregressive decoding. |
| HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate
  Hallucinations in Retrieval-Augmented Generation (Read more on [arXiv](https://arxiv.org/abs/2510.00880) or [HuggingFace](https://huggingface.co/papers/2510.00880))| Radu State, JÃ©rÃ´me FranÃ§ois, Ioana Buhnila, lrsbrgrn | HalluGuard is a 4B-parameter Small Reasoning Model designed to detect and justify hallucinations in Retrieval-Augmented Generation by classifying document-claim pairs as grounded or hallucinated. The main objective is to develop a computationally efficient and transparent model for mitigating RAG hallucinations, making it suitable for resource-constrained or on-premise enterprise environments where explainability is critical. The key methodology involves creating a synthetic dataset (HalluClaim) from the FineWeb corpus, generating preference pairs using models of different sizes (Qwen3-32B vs. Qwen3-0.6B), applying LLM-based consensus filtering, and fine-tuning a Qwen3-4B backbone with Odds Ratio Preference Optimization (ORPO). The primary result is achieving 84.0% balanced accuracy on the RAGTruth benchmark, matching the performance of larger specialized models like the 7B-parameter MiniCheck while using significantly fewer parameters. The principal implication for AI practitioners is that carefully curated synthetic preference data and advanced alignment techniques like ORPO can be used to distill the reasoning capabilities of large models into smaller, efficient models, enabling the deployment of reliable and auditable AI systems for critical enterprise tasks. |
