

## Papers for 2025-10-09

| Title | Authors | Summary |
|-------|---------|---------|
| Cache-to-Cache: Direct Semantic Communication Between Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2510.03215) or [HuggingFace](https://huggingface.co/papers/2510.03215))|  | This paper introduces Cache-to-Cache (C2C), a paradigm for direct semantic communication between LLMs by transferring and fusing their internal KV-Cache states instead of generating intermediate text. The primary objective is to overcome the information loss and latency inherent in text-based communication by enabling models to share richer, internal representations. The core methodology involves a neural network that projects a source model's KV-Cache and fuses it with a target model's cache, using a learnable gating mechanism to select which layers benefit from the fusion. C2C outperforms text communication by 3.0-5.0% in accuracy while delivering an average 2.0x speedup in latency. For AI practitioners, this work provides a method to build more performant and efficient multi-LLM systems by bypassing the token generation bottleneck and enabling direct, high-bandwidth semantic transfer between heterogeneous models. |
| Ming-UniVision: Joint Image Understanding and Generation with a Unified
  Continuous Tokenizer (Read more on [arXiv](https://arxiv.org/abs/2510.06590) or [HuggingFace](https://huggingface.co/papers/2510.06590))|  | Ming-UniVision is a unified autoregressive model that jointly performs image understanding and generation using MingTok, a novel three-stage continuous visual tokenizer, to eliminate quantization errors and reconcile competing representation demands. The research objective is to unify visual understanding and generation within a single autoregressive framework by developing a visual tokenizer that operates in a continuous latent space, thereby avoiding the architectural complexity of discrete or dual-representation approaches. The key methodology is the introduction of MingTok, a three-stage tokenizer with a low-level encoder for compact latents, a causal semantic decoder for high-dimensional semantic features, and a pixel decoder for reconstruction, all integrated into a large language model that treats vision-language tasks as next-token prediction. Primary results show the model achieves an overall score of 0.85 on the GenEval text-to-image benchmark, outperforming other models in spatial reasoning with a Position score of 0.92, and reduces input visual tokens for in-context editing by up to 66% compared to hybrid models. The principal implication for AI practitioners is that a single, shared continuous visual representation can effectively serve both discriminative and generative tasks, enabling simplified, stateful, and more computationally efficient multimodal systems that operate directly in the latent space for complex interactions. |
| Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal
  Generation and Understanding (Read more on [arXiv](https://arxiv.org/abs/2510.06308) or [HuggingFace](https://huggingface.co/papers/2510.06308))|  | Lumina-DiMOO is an open-source omni discrete diffusion large language model for multi-modal generation and understanding. The objective is to develop a foundational model for seamless multi-modal generation and understanding by utilizing a fully discrete diffusion modeling paradigm. It employs a unified discrete diffusion framework that processes multi-modal inputs and outputs via discrete tokens and a masked cross-entropy objective, incorporating a training-free Max Logit-based Cache (ML-Cache) for inference acceleration. Lumina-DiMOO achieves a 32x speed improvement in text-to-image generation compared to Lumina-mGPT 2.0 and sets new SOTA results with an 88% overall score on the GenEval benchmark. The open-sourced Lumina-DiMOO provides AI practitioners with a highly efficient and versatile foundation model for advancing research and applications in general-purpose multi-modal intelligence, including interactive image retouching. |
| SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.06917) or [HuggingFace](https://huggingface.co/papers/2510.06917))| Kevin Lin, Chung-Ching Lin, Linjie Li, Xiaofei Wang, Cheng-Han Chiang | SHANKS is an inference framework enabling spoken language models (SLMs) to perform internal reasoning concurrently with user speech input. The objective is to address high response latency in current SLMs/LLMs by allowing them to "think while listening," crucial for real-time speech-to-speech interaction. SHANKS streams user input speech in fixed-duration chunks, generating unspoken thinking tokens based on all previous speech and reasoning upon receiving each chunk to enable real-time decision-making like interruptions or tool calls. In experiments, SHANKS interrupted users 37.1% more accurately in a math problem-solving scenario and completed 56.9% of tool calls before the user's turn ended in a task-oriented dialogue. This enables AI practitioners to develop SLM applications with significantly reduced latency and improved real-time interactivity, particularly for scenarios requiring early intervention or proactive task completion. |
| RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training (Read more on [arXiv](https://arxiv.org/abs/2510.06710) or [HuggingFace](https://huggingface.co/papers/2510.06710))|  | RLinf-VLA is a unified and efficient framework designed for scalable Reinforcement Learning (RL) training of Vision-Language-Action (VLA) models, addressing the challenges of generalization and fragmented experimentation in embodied AI. Its main objective is to provide a comprehensive platform that integrates diverse VLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (PPO, GRPO), and various simulators (ManiSkill, LIBERO) with flexible GPU allocation. The framework utilizes a novel hybrid fine-grained pipelining allocation mode, achieving a 1.61x-1.88x speedup in training for GPU-parallelized simulators. A single unified model achieved 98.11% success across 130 LIBERO tasks and 97.66% across 25 ManiSkill tasks in simulation, with RL-trained policies exhibiting stronger zero-shot generalization on a real-world Franka robot compared to SFT. This provides AI practitioners with a robust and efficient open-source foundation to accelerate and standardize research and deployment in embodied intelligence. |
| MATRIX: Mask Track Alignment for Interaction-aware Video Generation (Read more on [arXiv](https://arxiv.org/abs/2510.07310) or [HuggingFace](https://huggingface.co/papers/2510.07310))| Hyunwook Choi, Jaeho Lee, Dahyun Chung, Siyoon Jin, Seongchan | MATRIX introduces a regularization framework to enhance interaction-aware video generation in Diffusion Transformers (DiTs). The main objective is to understand how video DiTs internally represent multi-instance and subject-object interactions and then improve generation quality. The methodology involves curating MATRIX-11K, a dataset with multi-instance mask tracks and interaction-aware captions, followed by a systematic analysis of semantic grounding and propagation within DiT attention layers. MATRIX applies Semantic Grounding Alignment (SGA) and Semantic Propagation Alignment (SPA) losses to interaction-dominant layers, finetuning with LoRA. Experimentally, MATRIX achieves an Interaction Fidelity (IF) of 0.593, outperforming baseline models. This enables AI practitioners to generate videos with significantly improved interaction fidelity, semantic alignment, and reduced drift and hallucination. |
| Vibe Checker: Aligning Code Evaluation with Human Preference (Read more on [arXiv](https://arxiv.org/abs/2510.07315) or [HuggingFace](https://huggingface.co/papers/2510.07315))|  | Vibe Checker introduces a novel testbed for evaluating large language models' code generation, integrating verifiable instruction following alongside functional correctness to better align with human preference. The core objective is to quantify models' adherence to non-functional coding instructions, hypothesizing this is a key, under-measured component of human preference in "vibe checking" code solutions. The methodology involves VeriCode, a taxonomy of 30 verifiable instructions with deterministic verifiers, used to augment standard benchmarks (BigVibeBench, LiveVibeBench) and evaluate 31 LLMs in single-turn and multi-turn settings. Results show that even strong models exhibit significant functional regression with added instructions, with average pass@1 dropping by 5.85% and 6.61% under five instructions on respective benchmarks; a composite score of functional correctness and instruction following consistently correlates best with human preference (e.g., optimal Pearson alpha=0.4 IF on BigVibeBench). This work implies that AI practitioners should prioritize integrating instruction following into both evaluation and training pipelines to improve LLM alignment with user preferences in code generation, especially for real-world programming tasks. |
| Multi-Agent Tool-Integrated Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2510.04678) or [HuggingFace](https://huggingface.co/papers/2510.04678))| Lidong Bing, Yuntao Chen, Xingxuan Li, Zhanfeng Mo | Multi-Agent Tool-Integrated Policy Optimization (MATPO) is introduced to train multi-agent LLM frameworks for knowledge-intensive tasks within a single model instance. The core objective is to enable effective multi-agent RL training, handle reward assignment for worker-agents, and support distinct planner and worker roles using one LLM. MATPO is derived from a principled credit assignment mechanism across planner and worker rollouts, using role-specific prompts and reinforcement learning built on single-agent multi-turn RL. Experiments on GAIA-text, WebWalkerQA, and FRAMES demonstrate MATPO consistently outperforms single-agent baselines by an average of 18.38% relative improvement in performance. This highlights the effectiveness of unifying multiple agent roles within a single LLM for stable and efficient multi-agent RL training, providing practical insights for AI practitioners. |
| OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot (Read more on [arXiv](https://arxiv.org/abs/2510.06751) or [HuggingFace](https://huggingface.co/papers/2510.06751))|  | OBS-Diff introduces a novel one-shot, training-free pruning framework for large-scale text-to-image diffusion models. The main objective is to establish a general and training-free pruning framework for diffusion models supporting diverse architectures and multiple pruning granularities in a single pass. The methodology revitalizes the Optimal Brain Surgeon (OBS) framework, adapting it with a Timestep-Aware Hessian Construction that uses a logarithmic weighting scheme and a computationally efficient group-wise sequential pruning strategy via "Module Packages". OBS-Diff achieves state-of-the-art one-shot pruning, evidenced by a 0.6468 ImageReward on SD 3-medium at 50% unstructured sparsity (outperforming Magnitude's -0.1076) and providing up to 1.31x inference speedup for structured pruning. This enables AI practitioners to deploy large diffusion models with substantially reduced computational and memory costs, enhancing efficiency and accessibility without requiring retraining or fine-tuning. |
| Revisiting Long-context Modeling from Context Denoising Perspective (Read more on [arXiv](https://arxiv.org/abs/2510.05862) or [HuggingFace](https://huggingface.co/papers/2510.05862))|  | This paper introduces Context Denoising Training (CDT) to enhance long-context models by identifying and suppressing contextual noise using Integrated Gradients, thereby improving attention on critical tokens. The primary objective is to address the performance degradation of Long-Context Models (LCMs) caused by irrelevant contextual noise, by developing a method to detect and mitigate this noise to improve model predictions. The proposed Context Denoising Training (CDT) involves two steps: first, Critical Token Detection using an Integrated Gradient (IG) score approximated by L2-normalized embedding gradients to identify noisy tokens; second, Emphasizing Training, which suppresses the influence of these detected noisy tokens by subtracting their corresponding gradients from input embeddings. Experiments demonstrate CDT's superiority; notably, a Llama3.1-8B-Instruct model trained with CDT achieved 50.92 points on real-world tasks (LongBench-E), closely matching GPT-40's 51.00 points. This indicates that AI practitioners can significantly enhance the robustness and long-context understanding of LLMs, particularly in noisy or very long input scenarios, by applying this efficient gradient-based denoising training strategy. |
| Artificial Hippocampus Networks for Efficient Long-Context Modeling (Read more on [arXiv](https://arxiv.org/abs/2510.07318) or [HuggingFace](https://huggingface.co/papers/2510.07318))|  | Artificial Hippocampus Networks (AHNs) enhance Transformer efficiency for long-context modeling by integrating fixed-size compressed memory. The main objective is to resolve the fundamental trade-off in long-sequence modeling between efficient fixed-size memory (RNN-like) and lossless growing memory (attention-based Transformers). AHNs achieve this by maintaining a sliding window for lossless short-term memory and using a learnable RNN-like module (e.g., Mamba2, DeltaNet, GatedDeltaNet) to recurrently compress out-of-window information into a fixed-size long-term memory, trained via self-distillation from pre-trained LLMs. For instance, augmenting Qwen2.5-3B-Instruct with AHNs (+0.4% parameters) reduced FLOPs by 40.5% and memory cache by 74.0% on the LV-Eval 128k sequence length benchmark, while improving the average score from 4.41 to 5.88. The principal implication for AI practitioners is that AHNs provide a method to significantly reduce the computational and memory requirements of Transformer models, enabling more efficient processing of extremely long sequences without substantial performance degradation. |
| Why Low-Precision Transformer Training Fails: An Analysis on Flash
  Attention (Read more on [arXiv](https://arxiv.org/abs/2510.04212) or [HuggingFace](https://huggingface.co/papers/2510.04212))|  | This paper provides a mechanistic explanation for catastrophic loss explosions during low-precision (BF16) transformer training using Flash Attention. The primary objective is to identify the root cause of a long-standing, reproducible training failure characterized by a sudden loss spike. The authors use a targeted analysis on a GPT-2 model, systematically comparing low-precision (BF16) and high-precision (FP32) computations to isolate the source of numerical error. The key result is that the failure stems from biased rounding errors in BF16 addition during the `PV` computation, which occurs specifically when multiple attention probabilities `P` become exactly 1, leading to a systematic negative bias in the output `O` and a corrupted, accumulating gradient error. For AI practitioners, this implies that the instability is a deterministic numerical artifact that can be mitigated by a minimal modification to the safe softmax implementation to prevent attention probabilities from becoming exactly 1, thereby stabilizing the training process. |
| Native Hybrid Attention for Efficient Sequence Modeling (Read more on [arXiv](https://arxiv.org/abs/2510.07019) or [HuggingFace](https://huggingface.co/papers/2510.07019))| Yu Cheng, Weigao Sun, Tao Zhang, Jiaxi Hu, Jusen Du | Native Hybrid Attention (NHA) is a novel architecture unifying linear and full attention for efficient and accurate sequence modeling. The primary objective is to develop a hybrid attention mechanism that overcomes the quadratic complexity of Transformers while maintaining recall accuracy. NHA integrates intra-layer hybridization by compressing long-term context via a linear RNN into KV slots and concatenating it with short-term sliding window tokens, then applying a single, unified softmax attention. Experimental results demonstrate NHA consistently outperforms Transformers and other hybrid baselines on recall-intensive and commonsense reasoning tasks, with NHA-Llama3-8B achieving superior recall accuracy with only 4 full attention layers, compared to other hybrids requiring more layers for lower accuracy. AI practitioners can leverage NHA to structurally hybridize existing pretrained Transformer LLMs, achieving competitive performance with significant efficiency gains and improved inference speed through brief finetuning. |
| When Benchmarks Age: Temporal Misalignment through Large Language Model
  Factuality Evaluation (Read more on [arXiv](https://arxiv.org/abs/2510.07238) or [HuggingFace](https://huggingface.co/papers/2510.07238))|  | This paper investigates the impact of benchmark aging on large language model factuality evaluation. It quantifies how widely used static benchmarks contain outdated factual answers and how this aging affects the evaluation of modern LLMs. The authors developed a fact retrieval pipeline for current real-world facts and introduced metrics like Dataset Drift Score, Evaluation Misleading Rate, and Temporal Alignment Gap. Experiments show that up to 63.78% of time-sensitive samples in older benchmarks are outdated, leading to an Evaluation Misleading Rate exceeding 10% for modern LLMs. AI practitioners should account for temporal misalignment, as relying on aging benchmarks results in unreliable factuality assessments and can unfairly penalize models for up-to-date responses. |
| Are We Using the Right Benchmark: An Evaluation Framework for Visual
  Token Compression Methods (Read more on [arXiv](https://arxiv.org/abs/2510.07143) or [HuggingFace](https://huggingface.co/papers/2510.07143))| Yiyu Wang, Xu Zheng, Zichen Wen, Wensong Wang, Chenfei Liao | This paper introduces VTC-Bench, an evaluation framework designed to address task mismatch and noise in existing MLLM benchmarks for visual token compression methods. The research investigates why simple image downsampling consistently outperforms advanced visual token compression methods on current MLLM benchmarks. The proposed VTC-Bench framework filters existing benchmark samples by using downsampling as a discriminator to categorize them into "simple" and "difficult" groups, then evaluates compression methods primarily on the "difficult" samples. Empirical results reveal that simple downsampling achieves a 91.0% Average Decline Ratio (ADR) on Qwen2-VL-7B at 75% compression across eight benchmarks, while DART achieves 40.2% on OCRBench for "difficult" samples where downsampling performs at 0% accuracy. AI practitioners should adopt specialized evaluation frameworks like VTC-Bench to denoise existing benchmarks and ensure fair assessment of visual token compression methods, enabling more accurate and relevant R&D. |
| StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact
  State Representation (Read more on [arXiv](https://arxiv.org/abs/2510.05057) or [HuggingFace](https://huggingface.co/papers/2510.05057))|  | StaMo presents an unsupervised framework for learning generalizable robot motion from compact state representations derived from static images. The core objective is to develop expressive yet compact state representations, investigating if robot motion can naturally emerge as the difference between state encodings from individual frames rather than complex temporal video modeling. StaMo leverages a Diffusion Autoencoder, with a DINOv2 encoder and Diffusion Transformer (DiT) decoder, to compress visual observations into two 1024-dimensional tokens, where latent motion is defined by the vector difference between these tokens for world modeling and policy co-training. This approach significantly improves performance by 14.3% on LIBERO and yields a 30% increase in real-world task success rates, outperforming prior methods by 10.4% in co-training. For AI practitioners, StaMo offers a scalable pathway for efficient world models and generalizable robot skills by implicitly capturing dynamics from static images, reducing reliance on computationally intensive video-based motion learning. |
| Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in
  MLLMs (Read more on [arXiv](https://arxiv.org/abs/2510.01954) or [HuggingFace](https://huggingface.co/papers/2510.01954))| Jingyi Liao, Nanqing Liu, Shijie Li, Haojie Zhang, Yongyi Su | Patch-as-Decodable Token (PaDT) unifies multimodal large language models (MLLMs) to directly generate diverse textual and visual outputs. The research addresses limitations of existing MLLMs that rely on indirect textual representations for vision tasks, aiming to enable direct generation of both textual and diverse visual outputs for dense prediction. PaDT introduces Visual Reference Tokens (VRTs) derived from visual patch embeddings, seamlessly interleaved with LLM output tokens using a Dynamic Embedding Module. A lightweight PaDT Decoder then transforms LLM outputs into structured visual predictions, optimized with a robust per-token cross-entropy loss and random VRT sampling. Notably, PaDT's 3B model surpasses prior state-of-the-art by 19.0 mAP on COCO detection and achieves an average accuracy of 93.6 on referring expression comprehension. AI practitioners can apply PaDT to develop MLLMs capable of direct, semantically aligned visual and textual generation, improving precision and robustness for a wide range of vision-language tasks beyond traditional text-based coordinate serialization. |
| WristWorld: Generating Wrist-Views via 4D World Models for Robotic
  Manipulation (Read more on [arXiv](https://arxiv.org/abs/2510.07313) or [HuggingFace](https://huggingface.co/papers/2510.07313))|  | WristWorld is a 4D world model for synthesizing geometrically and temporally consistent wrist-view videos from anchor views for robotic manipulation. The primary objective is to enrich existing third-person datasets with automatically generated, geometrically consistent wrist-view sequences to enhance both perception and control for robotic tasks. The framework operates in two stages: a reconstruction stage extending VGGT with a wrist head and Spatial Projection Consistency (SPC) Loss to estimate wrist-view poses and 4D point clouds, followed by a generation stage using a diffusion-based video generator conditioned on these projections and CLIP-encoded anchor-view features. Experiments on Droid, Calvin, and Franka Panda demonstrate state-of-the-art video generation quality, with WristWorld closing 42.4% of the anchor-wrist view performance gap and increasing average task completion length on Calvin by 3.81%. WristWorld serves as a plug-and-play add-on, enabling existing single-view world models to gain multi-view capabilities and expanding training data without requiring new wrist-view data collection, thereby improving downstream VLA model performance. |
| TTRV: Test-Time Reinforcement Learning for Vision Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.06783) or [HuggingFace](https://huggingface.co/papers/2510.06783))| Serena Yeung-Levy, Paul Gavrikov, Wei Lin, Shyam Marjit, Akshit Singh | TTRV is a novel test-time reinforcement learning framework that adapts Vision-Language Models (VLMs) at inference using self-supervised reward signals from unlabeled test data. Its primary objective is to enable VLMs to self-improve on-the-fly without requiring labeled datasets, addressing the limitations of static pretrained models. The methodology extends Group Relative Policy Optimization (GRPO) by incorporating frequency-based rewards for output consistency and diversity control rewards from the negative Shannon entropy of empirical response distributions. TTRV achieves substantial performance improvements, notably boosting Intern-VL-8B on image recognition by an average of 2.3% over GPT-40 across 8 benchmarks. This framework provides AI practitioners with a robust paradigm for deploying VLMs capable of continuous, unsupervised adaptation and self-improvement in dynamic, real-world scenarios. |
| MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline (Read more on [arXiv](https://arxiv.org/abs/2510.07307) or [HuggingFace](https://huggingface.co/papers/2510.07307))|  | MLE-Smith is a fully automated multi-agent pipeline designed to scale Machine Learning Engineering (MLE) task generation from raw datasets. The paper addresses the scalability and diversity limitations of existing manually curated MLE benchmarks by automating task generation and ensuring verifiable quality. It utilizes a multi-agent generation workflow (Brainstormer, Designer, Refactor) for structured task design, coupled with a hybrid verification mechanism comprising deterministic assertions, LLM-based reviews, and execution-based validation. MLE-Smith generated 606 verified tasks from 224 datasets at an average cost of $0.78 per task, with LLM performance on these tasks showing a strong linear correlation (Pearson r = 0.982) and excellent inter-set reliability (Cronbach's α = 0.993) compared to human-designed benchmarks. This framework enables AI practitioners to efficiently generate realistic, challenging, and discriminative MLE tasks for large-scale evaluation and training of next-generation MLE agents. |
| The African Languages Lab: A Collaborative Approach to Advancing
  Low-Resource African NLP (Read more on [arXiv](https://arxiv.org/abs/2510.05644) or [HuggingFace](https://huggingface.co/papers/2510.05644))|  | The African Languages Lab (All Lab) established a comprehensive framework for advancing low-resource African NLP through large-scale multi-modal data collection and fine-tuning of a multilingual language model. The initiative's objective is to address the critical technological gap for African languages, which are severely underserved in modern NLP, through systematic data collection, model development, and capacity building. The methodology involved building a mobile-first, community-driven "All Voices" platform for quality-controlled multi-modal data collection, rigorous two-tier data processing, statistical validation, and fine-tuning of the Llama-3.2-1B model on the collected dataset. The project yielded the largest validated African multi-modal dataset, comprising 19 billion tokens of monolingual text and 12,628 hours of aligned speech data across 40 languages. Fine-tuning demonstrated substantial performance improvements, with average gains of +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points across 31 evaluated languages. AI practitioners can leverage this new, large-scale, quality-controlled multi-modal dataset and the demonstrated fine-tuning approach to significantly enhance NLP capabilities for previously underserved African languages, enabling functional translation where none existed before. |
| Revisiting the Uniform Information Density Hypothesis in LLM Reasoning
  Traces (Read more on [arXiv](https://arxiv.org/abs/2510.06953) or [HuggingFace](https://huggingface.co/papers/2510.06953))| Jaehyung Kim, Guijin Son, Minju Gwak | This paper revisits the Uniform Information Density (UID) hypothesis to analyze information flow in LLM reasoning traces, linking step-level information density uniformity to reasoning quality. The research investigates whether step-level uniformity in LLM-generated reasoning traces reflects reasoning quality, particularly on complex mathematical benchmarks. The authors propose an entropy-based stepwise information density metric ($ID_i$) and introduce complementary local and global uniformity scores, computed as the variance of normalized $ID_i$ and step-to-step spikes/falls, respectively, evaluated across LLM reasoning traces. Experiments show that UID-based trace selection consistently improves reasoning accuracy; for instance, selecting traces with more uniform local information density yielded up to 32% relative accuracy gains over baselines on AIME2025 for Deepseek-R1. These findings establish information density uniformity as a robust diagnostic and selection criterion for AI practitioners, guiding the development of more reliable and accurate LLM reasoning systems. |
| Online Generic Event Boundary Detection (Read more on [arXiv](https://arxiv.org/abs/2510.06855) or [HuggingFace](https://huggingface.co/papers/2510.06855))| Jonghyun Choi, Jeany Son, Seunggyun Lim, Hyungrok Jung, carpedkm | This paper introduces Online Generic Event Boundary Detection (On-GEBD) to detect taxonomy-free event boundaries in streaming videos in real-time, mirroring human perception. The proposed ESTimator framework, inspired by Event Segmentation Theory, comprises a Consistent Event Anticipator (CEA) using a transformer decoder and an Online Boundary Discriminator (OBD) that employs statistical testing on a queue of past prediction errors for dynamic thresholding. ESTimator demonstrates superior performance, achieving an Avg. F1 score of 0.748 on Kinetics-GEBD, outperforming adapted online video understanding baselines (e.g., MiniROAD-BC at 0.681). Furthermore, it achieves comparable or superior results to most offline GEBD methods despite its online constraint. This work provides AI practitioners with a robust, real-time solution for generalizable video event segmentation, critical for applications requiring immediate analysis of continuous visual data. |
| The Markovian Thinker (Read more on [arXiv](https://arxiv.org/abs/2510.06557) or [HuggingFace](https://huggingface.co/papers/2510.06557))|  | The Markovian Thinker introduces a paradigm for LLMs to achieve long-chain-of-thought reasoning with linear compute and constant memory. Its main objective is to decouple thinking length from context size, addressing the quadratic compute growth of standard RL environments for reasoning LLMs. The key methodology is Delethink, an RL environment where policies reason in fixed-size chunks, with the environment resetting context at boundaries and reinitializing prompts using a short textual carryover, forcing the policy to learn to maintain a bounded Markovian state. Primary results show an R1-Distill 1.5B model, trained with Delethink, can think up to 24K tokens, matching or surpassing LongCoT-RL with the same budget, and training for 94K average thinking length costs 7 H100-months with Delethink compared to 27 for LongCoT-RL. This implies AI practitioners can develop efficient and scalable reasoning LLMs capable of very long reasoning without quadratic overhead by redesigning the RL environment to enforce constant-size states. |
| Bridging Text and Video Generation: A Survey (Read more on [arXiv](https://arxiv.org/abs/2510.04999) or [HuggingFace](https://huggingface.co/papers/2510.04999))| G. Maragatham, Priyansh Bhandari, nnilayy | This paper surveys the evolution of text-to-video (T2V) generative models, their architectures, training, and evaluation methods. The primary objective is to provide a comprehensive, unified overview of T2V models, detailing their development from early GANs and VAEs to current diffusion-based architectures, and explaining internal mechanisms, limitations, and architectural shifts. The methodology involves a systematic analysis of T2V model architectures, datasets (e.g., WebVid-10M, LAION-5B), and training configurations, alongside a review of evaluation metrics and benchmarks; for example, VideoFusion [12] achieved an Inception Score (IS) of 71.67 on the UCF-101 benchmark. The principal implication for AI practitioners is the need to overcome challenges such as limited data availability, high computational costs, and difficulties in modeling long-range temporal consistency by exploring novel architectures, synthetic data generation, and enhanced temporal modeling strategies, leveraging the detailed training parameters provided. |
| AlphaApollo: Orchestrating Foundation Models and Professional Tools into
  a Self-Evolving System for Deep Agentic Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.06261) or [HuggingFace](https://huggingface.co/papers/2510.06261))| Zongze Li, Xuan Li, Xiao Feng, Chentao Cao, Zhanke Zhou | AlphaApollo is a self-evolving agentic reasoning system designed to overcome limited model-intrinsic capacity and unreliable test-time iteration in foundation models. Its objective is to enable deliberate, verifiable reasoning by orchestrating multiple foundation models with professional tools. The system's methodology involves coupling computation and retrieval tools, along with a multi-round, multi-model solution evolution process via a shared state map and a rollout framework. Empirically, AlphaApollo achieved substantial performance gains, notably increasing Average@32 by 16.67% and Pass@32 by 23.34% (from 23.33% to 46.67%) on AIME 2025 for Llama-3.3-70B-Instruct. For AI practitioners, AlphaApollo demonstrates that orchestrating FMs with professional tools and iterative refinement significantly lifts the capability ceiling of FMs, enhancing both average performance and problem-solving abilities. |
| G^2RPO: Granular GRPO for Precise Reward in Flow Models (Read more on [arXiv](https://arxiv.org/abs/2510.01982) or [HuggingFace](https://huggingface.co/papers/2510.01982))|  | G2RPO introduces a novel online reinforcement learning framework for flow models, designed for precise and comprehensive reward assessments. It addresses sparse reward and incomplete evaluation in existing GRPO methods by localizing stochasticity and integrating multi-granularity advantages. The methodology relies on Singular Stochastic Sampling to confine SDE perturbations to single steps and Multi-Granularity Advantage Integration to fuse advantages from images denoised at various granularities. When jointly trained with HPS-v2.1 and CLIP, G2RPO achieved an HPS-v2.1 score of 0.376 and a CLIP Score of 0.406, outperforming baselines across in-domain and out-of-domain metrics. This framework offers AI practitioners a more robust and efficient approach for aligning generative models with human preferences through enhanced reward signals, crucial for stable and high-quality policy optimization. |
| U-Bench: A Comprehensive Understanding of U-Net through 100-Variant
  Benchmarking (Read more on [arXiv](https://arxiv.org/abs/2510.07041) or [HuggingFace](https://huggingface.co/papers/2510.07041))| Heqin Zhu, Zikang Xu, Wenxin Ma, Chengqi Dong, Fenghe Tang | U-Bench is a large-scale, statistically rigorous benchmark evaluating 100 U-Net variants across 28 datasets and 10 modalities, introducing U-Score to balance performance and efficiency. The main objective is to provide a fair and comprehensive comparison of U-Net variants in medical image segmentation, addressing gaps in prior evaluations regarding statistical robustness, zero-shot generalization, and computational efficiency. The methodology involves evaluating 100 U-Net variants on diverse 2D medical image segmentation datasets, calculating statistical significance, and assessing zero-shot generalization, while introducing U-Score which combines IoU, parameters, FLOPs, and FPS. Primary results show marginal in-domain IoU gains (average 1%-2%) but more pronounced zero-shot improvements (over 3% on average in 80% of modalities), with U-Score improvements averaging 33%. For AI practitioners, U-Bench provides open-source resources and a model advisor agent to guide model selection based on dataset characteristics and resource constraints, highlighting the critical role of efficiency for real-world deployment. |
| Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era
  of Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.07037) or [HuggingFace](https://huggingface.co/papers/2510.07037))|  | This paper surveys code-switched (CSW) NLP in the era of Large Language Models. The objective is to comprehensively analyze how LLMs have reshaped CSW modeling, identify key advancements, and highlight persistent challenges. The authors conducted a comprehensive literature review of 308 studies, categorizing them into five research areas across 12 NLP tasks, 30+ datasets, and 80+ languages. While LLMs have shown progress, multilingual NLU models can suffer up to a 15% drop in semantic accuracy, and ASR systems exhibit 30-50% higher word error rates on CSW data, although instruction tuning with models like COMMIT has achieved up to 32x gains in exact match for Hinglish QA. AI practitioners should prioritize developing inclusive datasets, fair evaluation metrics, and linguistically grounded models to achieve robust, truly multilingual AI systems. |
| NorMuon: Making Muon more efficient and scalable (Read more on [arXiv](https://arxiv.org/abs/2510.05491) or [HuggingFace](https://huggingface.co/papers/2510.05491))| Tuo Zhao, Weizhu Chen, Chen Liang, Liming Liu, Zichong Li | NorMuon is an optimizer that combines Muon's orthogonalization with neuron-wise adaptive learning rates for efficient and scalable large language model (LLM) training. The main objective was to determine if orthogonalization and adaptive learning rates could be synergistically combined to yield complementary benefits, addressing the high variance in per-neuron update norms observed in Muon. NorMuon's methodology involves augmenting Muon's orthogonalization with neuron-level adaptive learning rates, computed from accumulated second-order momentum statistics, applied as row-wise normalization after orthogonalization, and developed with an efficient distributed implementation under FSDP2. Primary results show NorMuon achieving 21.74% better training efficiency than Adam and an 11.31% improvement over Muon on a 1.1B pretraining setting, while maintaining comparable memory efficiency to Muon. This implies for AI practitioners that orthogonalization and blockwise adaptive learning rates are complementary rather than competing methods, offering superior training dynamics and efficiency for large-scale LLM pretraining. |
| D^3QE: Learning Discrete Distribution Discrepancy-aware
  Quantization Error for Autoregressive-Generated Image Detection (Read more on [arXiv](https://arxiv.org/abs/2510.05891) or [HuggingFace](https://huggingface.co/papers/2510.05891))| Yueqi Duan, Wenzhao Zheng, Yu Zheng, Bingyao Yu, Yanran Zhang | D³QE proposes a novel framework for detecting autoregressive (AR)-generated images by analyzing discrete distribution discrepancies. The primary objective is to exploit distinctive codebook utilization patterns and frequency distribution biases between real and AR-generated images. D³QE leverages a VQVAE encoder for quantization error features, a Discrete Distribution Discrepancy-Aware Transformer (D³AT) that integrates dynamic codebook frequency statistics into its attention mechanism, and fuses these with CLIP semantic features. The method achieved a superior average accuracy of 82.11% and average precision of 92.07% on the ARForensics dataset, outperforming baselines and demonstrating strong generalization across GANs and diffusion models. This provides AI practitioners with a robust and generalizable tool for synthetic image detection, addressing new challenges posed by advanced autoregressive generative models. |
| DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for
  Autonomous Travel Planning Agents (Read more on [arXiv](https://arxiv.org/abs/2509.21842) or [HuggingFace](https://huggingface.co/papers/2509.21842))|  | DeepTravel is an end-to-end agentic reinforcement learning framework for autonomous travel planning agents. The primary objective is to build autonomous travel planning agents capable of autonomously planning, executing tools, and reflecting on responses in multi-step reasoning. Key methodologies include a Robust SandBox for simulated real-world tool interactions, a Hierarchical Reward Modeling system with trajectory- and turn-level verifiers, and a Reply-Augmented Reinforcement Learning method utilizing SFT cold-start and experience replay. DeepTravel-32B achieved a 69.34% final pass rate on offline (without constraint) hard tasks, significantly outperforming DeepSeek-R1 (26.00%) and OpenAI-03 (21.19%). This framework enables small-size LLMs to achieve state-of-the-art performance in travel planning, providing AI practitioners a more efficient and accessible paradigm for developing autonomous agents for complex, real-world tasks. |
| Heptapod: Language Modeling on Visual Signals (Read more on [arXiv](https://arxiv.org/abs/2510.06673) or [HuggingFace](https://huggingface.co/papers/2510.06673))|  | Heptapod is an image autoregressive model that applies language modeling principles to visual signals using a novel next 2D distribution prediction objective. Its main objective is to overcome challenges in transferring 1D language modeling to the 2D visual domain by eschewing reliance on CFG and semantic tokenizers. Heptapod employs a causal Transformer with a reconstruction-focused visual tokenizer, learning to predict the distribution over the entire 2D spatial grid at each timestep, thereby unifying autoregressive modeling and masked autoencoding. On the ImageNet generation benchmark, Heptapod-H achieves an FID of 2.70, significantly outperforming previous causal autoregressive models like LlamaGen-3B (FID 9.38) with fewer parameters. This work demonstrates that visual semantics can intrinsically emerge from a well-posed generative objective, providing AI practitioners a principled framework for integrating visual generative training into multimodal LLMs without external semantic engineering. |
