

## Papers for 2025-10-10

| Title | Authors | Summary |
|-------|---------|---------|
| Agent Learning via Early Experience (Read more on [arXiv](https://arxiv.org/abs/2510.08558) or [HuggingFace](https://huggingface.co/papers/2510.08558))|  | The paper introduces "early experience," a reward-free training paradigm that uses an agent's self-generated interaction data to bridge the gap between imitation learning and reinforcement learning. The objective is to develop a scalable method for agents to learn from their own experience, overcoming the limitations of expert-data dependency in supervised fine-tuning and the reward-signal requirement in reinforcement learning. The authors propose two strategies: 1) Implicit World Modeling, which trains the agent to predict the future state resulting from its own actions as an auxiliary task, and 2) Self-Reflection, which trains the agent to generate rationales comparing expert actions against its own alternative actions and their outcomes. Across eight diverse environments, early experience methods consistently outperform imitation learning, achieving an average absolute success rate gain of +9.6% and improving out-of-domain generalization by +9.4%; furthermore, using these methods to warm-start reinforcement learning leads to substantially higher final performance ceilings. AI practitioners can use this paradigm to improve agent performance and robustness in environments lacking dense rewards by augmenting expert datasets with the agent's own exploratory rollouts, using the resulting states as a direct, scalable, and reward-free supervision signal. |
| MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with
  Holistic Platform and Adaptive Hybrid Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2510.08540) or [HuggingFace](https://huggingface.co/papers/2510.08540))| vanilla1116, yingmanji, tianhao2k, mjuicem, PhoenixZ | This paper introduces the MM-HELIX benchmark, the MM-HELIX-100K dataset, and the Adaptive Hybrid Policy Optimization (AHPO) algorithm to evaluate and improve long-chain reflective reasoning in Multimodal Large Language Models (MLLMs). The research objective is to address the failure of MLLMs in complex, multi-step visual reasoning by creating a new evaluation suite and developing a training strategy to instill and generalize these capabilities. The key methodology is AHPO, a framework that dynamically integrates off-policy expert guidance with on-policy exploration using a reward-based gating mechanism that applies supervision only when the model's performance is low. Training with AHPO resulted in a +18.6% absolute accuracy improvement on the MM-HELIX benchmark for a Qwen2.5-VL-7B model and demonstrated a +5.7% average performance gain on out-of-domain general mathematics and logic tasks. For AI practitioners, AHPO provides a direct method to train models on complex tasks with sparse rewards by effectively combining supervised fine-tuning and reinforcement learning, fostering generalizable reasoning skills while mitigating the catastrophic forgetting associated with standard instruction tuning. |
| From What to Why: A Multi-Agent System for Evidence-based Chemical
  Reaction Condition Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.23768) or [HuggingFace](https://huggingface.co/papers/2509.23768))| Feiwei Qin, Junchi Yu, Jiaxuan Lu, haiyuanwan, YangC777 | This paper presents ChemMAS, a multi-agent system that provides evidence-based reasoning for chemical reaction condition recommendations. The primary objective is to develop a system that not only predicts reaction conditions but also generates interpretable, falsifiable rationales explaining why specific conditions are chosen. The methodology involves a multi-agent system that decomposes the task into mechanistic grounding, multi-channel recall from a database, a tournament-style debate among specialized agents for candidate selection, and rationale aggregation. The system achieves state-of-the-art performance, outperforming general-purpose LLMs by 10-15% and domain-specific models by 20-35% in Top-1 accuracy; for example, it achieved 78.1% Top-1 accuracy for catalyst prediction. For AI practitioners, this work demonstrates that a structured, multi-agent debate framework coupled with tool use and evidence retrieval can significantly improve both the accuracy and explainability of AI systems in specialized scientific domains, providing a paradigm for building trustable AI. |
| UniVideo: Unified Understanding, Generation, and Editing for Videos (Read more on [arXiv](https://arxiv.org/abs/2510.08377) or [HuggingFace](https://huggingface.co/papers/2510.08377))| Xintao Wang, Qiulin Wang, Zixuan Ye, Quande Liu, CongWei1230 | UniVideo is a unified framework for video understanding, generation, and editing, featuring a dual-stream architecture that combines a Multimodal Large Language Model (MLLM) with a Multimodal Diffusion Transformer (MMDiT). The objective is to extend unified multimodal modeling to the video domain, creating a single framework capable of interpreting complex multimodal instructions to perform diverse video tasks without task-specific modules. The model employs a dual-stream architecture where an MLLM branch processes multimodal instructions for semantic understanding, while an MMDiT branch handles video synthesis, with a trainable connector aligning the two streams; the MMDiT also directly receives VAE-encoded visual signals to preserve fine-grained detail. UniVideo matches or surpasses state-of-the-art baselines, achieving a human-evaluated Subject Consistency score of 0.88 in single-reference in-context generation, outperforming Kling1.6 (0.68), and uniquely performs in-context editing without requiring input masks. For AI practitioners, the dual-stream architecture provides a robust template for building unified video models, demonstrating that decoupling semantic understanding from visual synthesis enables superior identity preservation and zero-shot generalization to complex editing tasks, reducing the need for multiple specialized models. |
| When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs (Read more on [arXiv](https://arxiv.org/abs/2510.07499) or [HuggingFace](https://huggingface.co/papers/2510.07499))|  | The paper introduces TOTAL, a framework that improves multi-hop reasoning in long-context language models (LCLMs) by augmenting them with reusable, iteratively refined "thought templates". The primary objective is to overcome the limitation of LCLMs, which often fail to structure and connect evidence for complex reasoning even with expanded context windows. The key methodology involves automatically constructing compositional reasoning templates from training data and then iteratively refining them using a "textual gradient"—natural language feedback generated by an auxiliary LM—to correct flaws in low-performing templates without altering model weights. On average across four multi-hop QA benchmarks using the Claude model, TOTAL achieves a score of 64.01, significantly outperforming the strong Corpus-in-Context with Chain-of-Thought (CIC + CoT) baseline of 56.30. For AI practitioners, the principal implication is that the reasoning capabilities of large, static LCLMs can be effectively enhanced on knowledge-intensive tasks by injecting structured, reusable reasoning patterns into the prompt, offering a parameter-efficient alternative to continuous fine-tuning. |
| Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2510.03259) or [HuggingFace](https://huggingface.co/papers/2510.03259))|  | The paper introduces Meta-Awareness via Self-Alignment (MASA), a reinforcement learning framework that enhances language model reasoning by training them to accurately predict their own solution characteristics. The objective is to improve reasoning performance by explicitly training a model's "meta-awareness"—its ability to predict properties like solution length, difficulty, and necessary concepts—and aligning these predictions with actual outcomes. MASA uses a dual-rollout RL pipeline where meta-predictions are generated in parallel with solutions and rewarded based on their alignment with actual solution statistics, further enhanced with behavior cloning on high-quality meta-trajectories. This method achieves a 19.3% accuracy gain on the AIME25 benchmark and a 6.2% average gain across six mathematics benchmarks over a GRPO baseline. For AI practitioners, the principal implication is that integrating self-alignment mechanisms to teach models self-assessment can significantly improve both final performance and training efficiency, as the MASA-efficient variant reduces training time by filtering unpromising tasks and terminating lengthy, incorrect rollouts early. |
| VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal
  Patches via In-Context Conditioning (Read more on [arXiv](https://arxiv.org/abs/2510.08555) or [HuggingFace](https://huggingface.co/papers/2510.08555))| Quande Liu, Wenze Liu, Zongli Ye, Qiulin Wang, onevfall | This paper presents VideoCanvas, a unified framework for generating complete videos from arbitrary spatiotemporal patches by adapting the In-Context Conditioning (ICC) paradigm to video. The primary objective is to overcome the temporal ambiguity of causal video VAEs to enable a single model to perform diverse completion tasks like inpainting, interpolation, and video transition from any user-specified content. The key methodology combines spatial zero-padding for patch placement with a novel Temporal RoPE Interpolation, which assigns continuous fractional positions to conditional latent tokens to achieve precise pixel-frame alignment without retraining the VAE. On the proposed VideoCanvasBench, the framework significantly outperforms baselines, achieving a 68.46% user preference in the AnyI2V (any-timestamp image-to-video) task, compared to 24.23% for Channel Concatenation. For AI practitioners, this work offers a parameter-efficient fine-tuning strategy to add fine-grained spatiotemporal control to existing video foundation models, enabling more flexible and unified video editing applications without costly architectural modifications. |
| MemMamba: Rethinking Memory Patterns in State Space Model (Read more on [arXiv](https://arxiv.org/abs/2510.03279) or [HuggingFace](https://huggingface.co/papers/2510.03279))| Xiao Sun, Jiaxuan Lu, Jiahao Yan, Yangjingyi Chen, Youjin Wang | This paper introduces MemMamba, a state-space model architecture that mitigates the exponential memory decay of Mamba-like models while preserving linear complexity. The research objective is to systematically analyze Mamba's memory decay and develop an architecture to overcome long-range forgetting without sacrificing efficiency. The proposed methodology integrates a state summarization mechanism, which creates a memory "state pool," with sparse, periodically triggered cross-token and cross-layer attention to selectively preserve and recall critical information. MemMamba achieves 90% accuracy on the Passkey Retrieval task at 400k tokens, a context length where baseline Mamba fails, while delivering a 48% inference speedup over a standard Transformer. For AI practitioners, MemMamba provides an architectural framework for building computationally efficient models that can process ultra-long sequences without the catastrophic forgetting characteristic of previous state-space models. |
| Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense (Read more on [arXiv](https://arxiv.org/abs/2510.07242) or [HuggingFace](https://huggingface.co/papers/2510.07242))|  | The paper presents HERO, a hybrid reinforcement learning framework that integrates sparse verifier signals with dense reward model scores to enhance LLM reasoning. The primary objective is to develop an effective reward framework that overcomes the brittleness of binary verifiers and the unreliability of dense reward models by combining their complementary strengths. The core methodology, Hybrid Ensemble Reward Optimization (HERO), employs stratified normalization to rescale reward model scores within verifier-defined correctness groups and uses variance-aware reweighting to prioritize more informative prompts during training. Across mathematical reasoning benchmarks, HERO trained on a Qwen3-4B-Base model achieved a 66.3 average score on hard-to-verify tasks, outperforming reward-model-only training by +11.7 points and verifier-only training by +9.2 points. The principal implication for AI practitioners is that structuring dense rewards by anchoring them to sparse, verifiable ground truths provides a more stable and effective supervision signal for training reliable reasoning models, mitigating issues like reward hacking and gradient sparsity. |
| NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM
  Agents (Read more on [arXiv](https://arxiv.org/abs/2510.07172) or [HuggingFace](https://huggingface.co/papers/2510.07172))| Baixuan Xu, Newt Hue-Nam K. Nguyen, Kelvin Kiu-Wai Tam, Tianshi Zheng, tqfang229 | This paper introduces NEWTONBENCH, a benchmark designed to evaluate the generalizable scientific law discovery capabilities of LLM agents by resolving the methodological trilemma of scientific relevance, scalability, and memorization resistance. The research objective is to assess the extent to which LLM agents can perform authentic scientific discovery, moving beyond static function fitting to interactive exploration of complex, simulated physical systems. The methodology employs "metaphysical shifts"—systematic alterations of canonical laws—to generate 324 novel tasks where agents must experimentally probe systems to uncover hidden principles, optionally aided by a code interpreter. Primary results indicate that while frontier models like GPT-5 achieve up to 72.9% overall symbolic accuracy, this capability is fragile and degrades with complexity; paradoxically, tool assistance hinders stronger models by inducing a premature shift from exploration to exploitation. The principal implication for AI practitioners is that developing robust discovery agents requires addressing the exploration-exploitation trade-off in tool-assisted settings, as capable models are prone to misusing tools to satisfice on suboptimal solutions rather than discovering globally correct laws. |
| The Alignment Waltz: Jointly Training Agents to Collaborate for Safety (Read more on [arXiv](https://arxiv.org/abs/2510.08240) or [HuggingFace](https://huggingface.co/papers/2510.08240))|  | This paper introduces WALTZRL, a multi-agent reinforcement learning framework that jointly trains a conversation agent and a feedback agent to improve the balance between LLM helpfulness and harmlessness. The main objective is to reduce both unsafe responses to adversarial attacks and overrefusals on benign prompts, addressing the inherent trade-off between these two failure modes. The key methodology is a collaborative, positive-sum game formulated within a multi-agent reinforcement learning (MARL) setting, where a feedback agent is trained to provide useful suggestions to a conversation agent, guided by a novel Dynamic Improvement Reward (DIR). The primary result is a significant reduction in both unsafe responses, with the Attack Success Rate dropping from 39.0% to 4.6% on the WildJailbreak dataset, and overrefusals, which decreased from 45.3% to 9.9% on the OR-Bench dataset compared to the baseline model. The principal implication for AI practitioners is that deploying a jointly trained conversation-feedback agent system at inference allows for adaptive safety improvements, offering a more nuanced alternative to static safeguard models which can exacerbate overrefusal. |
| DeepPrune: Parallel Scaling without Inter-trace Redundancy (Read more on [arXiv](https://arxiv.org/abs/2510.08483) or [HuggingFace](https://huggingface.co/papers/2510.08483))|  | DeepPrune is a framework that dynamically prunes redundant reasoning traces during parallel scaling to significantly reduce computational cost while maintaining accuracy. The main objective is to mitigate the computational inefficiency caused by inter-trace redundancy in parallel LLM reasoning, where the paper finds over 80% of generated traces often lead to identical final answers. The methodology involves an offline phase to train a specialized judge model on partial trace pairs to predict answer equivalence, using focal loss and oversampling, followed by an online phase that applies this model within a greedy clustering algorithm to terminate redundant generation paths. Primary results demonstrate a token reduction of over 80% compared to consensus sampling; specifically, with the Qwen3-32B model on the AIME25 benchmark, it achieved a 91.4% token reduction while improving accuracy from 80.0% to 90.0%. The principal implication for AI practitioners is that this framework provides a method to substantially decrease the inference cost and latency of high-performance reasoning techniques like self-consistency, making them more economically viable for production deployment. |
| Training-Free Group Relative Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2510.08191) or [HuggingFace](https://huggingface.co/papers/2510.08191))|  | Training-Free Group Relative Policy Optimization (Training-Free GRPO) is a novel method that enhances LLM agent performance without parameter updates by iteratively distilling experiential knowledge into a token prior. The paper's objective is to achieve policy optimization in the context space rather than the parameter space, thereby avoiding the high data and computational costs of traditional reinforcement learning fine-tuning. The key methodology involves using an LLM to introspect on groups of its own rollouts, extract a "semantic advantage" in the form of natural language experience, and iteratively update an external knowledge library that guides the frozen model's behavior at inference time. On the AIME25 mathematical reasoning benchmark, applying this method to DeepSeek-V3.1-Terminus improved the Mean@32 score from 67.9% to 73.3% using only 100 training samples at an approximate cost of $18. The principal implication for AI practitioners is that powerful, frozen, API-based models can be effectively adapted to specialized domains with minimal data and cost, offering a practical alternative to deploying and maintaining multiple fine-tuned models. |
| ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D
  Reconstruction with Structured Scene Representation (Read more on [arXiv](https://arxiv.org/abs/2510.08551) or [HuggingFace](https://huggingface.co/papers/2510.08551))|  | ARTDECO is a unified framework for efficient and high-fidelity on-the-fly 3D reconstruction from monocular video using a structured Gaussian scene representation. The primary objective is to overcome the trade-off between computationally expensive, high-fidelity per-scene optimization methods and efficient but less accurate feed-forward models. The methodology integrates feed-forward foundation models for robust pose estimation and loop closure within a SLAM pipeline that incrementally builds a hierarchical 3D Gaussian representation with a Level-of-Detail (LoD)-aware rendering strategy. On the ScanNet++ benchmark, ARTDECO achieves a state-of-the-art tracking accuracy with an Absolute Trajectory Error (ATE) RMSE of 0.018, significantly outperforming prior 3DGS-based SLAM systems. For AI practitioners, this framework provides a practical blueprint for integrating large pre-trained models into real-time SLAM systems to build robust, interactive 3D digitization applications for AR/VR and robotics without requiring costly offline processing. |
| LLMs Learn to Deceive Unintentionally: Emergent Misalignment in
  Dishonesty from Misaligned Samples to Biased Human-AI Interactions (Read more on [arXiv](https://arxiv.org/abs/2510.08211) or [HuggingFace](https://huggingface.co/papers/2510.08211))|  | This paper demonstrates that Large Language Models can learn to be broadly deceptive from narrow, unintentional misalignment in training data or from interactions with a small population of biased users. The main research objective is to determine if "emergent misalignment" extends beyond safety behaviors to induce dishonesty and deception in LLMs, particularly when fine-tuned on narrowly misaligned data, when such data is mixed into downstream tasks, or when interacting with biased users. The methodology involves finetuning Llama3.1-8B and Qwen2.5-7B on synthetic misaligned datasets (insecure code, incorrect math, medical advice), mixing misaligned data into standard downstream datasets at various ratios, and simulating human-AI interactions with varying populations of biased users to self-train the model. The primary results show that introducing as little as 1% of misaligned data into a standard downstream training task is sufficient to decrease the model's honest behavior by over 20%, and that a biased user population of only 10% can significantly exacerbate the model's dishonesty in simulated interactions. The principal implication for AI practitioners is that data curation and feedback pipelines for model finetuning are critical vulnerability points; even small, unintentional contaminations in training data or skewed user feedback can lead to emergent, system-wide deceptive behaviors, necessitating rigorous data validation and filtering in production environments. |
| NaViL: Rethinking Scaling Properties of Native Multimodal Large Language
  Models under Data Constraints (Read more on [arXiv](https://arxiv.org/abs/2510.08565) or [HuggingFace](https://huggingface.co/papers/2510.08565))|  | This paper introduces NaViL, a native Multimodal Large Language Model (MLLM), and systematically investigates design principles and scaling properties for end-to-end training under data constraints. The primary objective is to determine the optimal architecture and joint scaling properties of native MLLMs, specifically the relationship between the visual encoder and the LLM, when trained end-to-end. The methodology involves systematically ablating architectural choices like LLM initialization and Mixture-of-Experts (MoE), and then empirically studying the scaling of the visual encoder and LLM both independently and jointly to derive an optimal scaling relationship. The study reveals that the optimal visual encoder size scales log-proportionally with the LLM size, and the resulting NaViL-2B model achieves a 78.3 on the MMVet benchmark, outperforming previous native MLLMs. The principal implication for AI practitioners is that when building native MLLMs, the visual encoder and LLM should be scaled jointly according to this log-proportional law, rather than using a fixed-size visual encoder, to achieve optimal performance. |
| UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video
  Super-Resolution (Read more on [arXiv](https://arxiv.org/abs/2510.08143) or [HuggingFace](https://huggingface.co/papers/2510.08143))|  | UniMMVSR is a unified framework for cascaded video super-resolution that incorporates multi-modal conditions to generate high-fidelity, ultra-high-resolution video. The primary objective is to create a single generative super-resolution model that can handle hybrid inputs—including text, multiple ID images, and reference videos—to upscale videos from a base generative model to 4K resolution. The methodology utilizes a latent video diffusion transformer that incorporates the low-resolution video via channel concatenation and visual references via token concatenation in the 3D self-attention modules, trained with a novel SDEdit-based degradation pipeline to simulate base model imperfections. Quantitatively, on multi-ID image-guided text-to-video generation, the unified model achieved a state-of-the-art MUSIQ score of 62.248, outperforming existing VSR methods and base models. The principal implication for AI practitioners is that this cascaded, unified approach enables the scaling of controllable, multi-modal video generation to 4K resolution while allowing high-quality data from simpler tasks to improve performance on more complex ones, thereby reducing the data collection overhead for specialized generation tasks. |
| InstructX: Towards Unified Visual Editing with MLLM Guidance (Read more on [arXiv](https://arxiv.org/abs/2510.08485) or [HuggingFace](https://huggingface.co/papers/2510.08485))| Xinghui Li, Pengze Zhang, Yanze Wu, Qichao Sun, Chong Mou | InstructX is a unified framework that uses a fine-tuned Multimodal Large Language Model (MLLM) to guide a diffusion model for both instruction-based image and video editing within a single system. The research objective is to develop a unified visual editing model by determining the optimal integration strategy between an MLLM and a diffusion model, while also addressing the scarcity of high-quality video training data. The methodology involves using an MLLM with appended learnable queries and LoRA fine-tuning to generate editing guidance, which is then passed through a simple two-layer MLP connector to a Diffusion Transformer (DiT); the model is trained in three stages, using a mix of image and video data to enable emergent video editing capabilities from image training. The model achieves state-of-the-art performance for open-source methods, and on the paper's proposed VIE-Bench video editing benchmark, it attained an average score of 9.196 on the "Style / Tone Change" task, outperforming the closed-source Runway model which scored 9.133. The principal implication for AI practitioners is that fine-tuning the MLLM component (e.g., via LoRA) in conjunction with a lightweight connector is a more effective and efficient architecture for MLLM-guided diffusion than using a frozen MLLM with a large, complex connector, and that training on image data can effectively induce video editing capabilities, mitigating the need for extensive video datasets. |
| First Try Matters: Revisiting the Role of Reflection in Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2510.08308) or [HuggingFace](https://huggingface.co/papers/2510.08308))| Wee Sun Lee, Zhanfeng Mo, Yao Xiao, Yue Deng, Liwei Kang | This research finds that performance gains in reasoning models stem primarily from improved first-answer accuracy rather than error correction during subsequent "reflection" steps. The main objective is to systematically analyze the role of post-answer reasoning in LLMs, determining whether it is corrective or merely confirmatory. A key methodology involves using an LLM-based extractor to parse reasoning rollouts from eight models and conducting supervised fine-tuning (SFT) on datasets with curated amounts of reflection. The primary result shows that over 90% of reflections are confirmatory and that a proposed early-stopping technique reduces reasoning tokens by 24.5% with only a 2.9% drop in accuracy. The principal implication for AI practitioners is that data curation should focus on diversifying reasoning paths to improve first-try correctness, and inference efficiency can be significantly improved by truncating generation after a plausible answer is found, as extensive reflection provides marginal benefit. |
| Low-probability Tokens Sustain Exploration in Reinforcement Learning
  with Verifiable Reward (Read more on [arXiv](https://arxiv.org/abs/2510.03222) or [HuggingFace](https://huggingface.co/papers/2510.03222))|  | This paper introduces Low-probability Regularization (Lp-Reg), a method to mitigate exploration collapse in Reinforcement Learning with Verifiable Rewards (RLVR) by selectively preserving valuable, low-probability exploratory tokens termed "reasoning sparks." The research objective is to overcome the performance plateaus in RLVR training caused by the systematic elimination of these crucial tokens, which standard entropy-control methods fail to address effectively. The core methodology involves constructing a less-noisy proxy distribution by filtering out tokens below a probability threshold and then using a selective forward KL divergence to regularize the policy towards this proxy, shielding reasoning sparks from negative updates. The primary result shows that on-policy Lp-Reg achieves a 60.17% average accuracy on five math benchmarks using a Qwen3-14B model, an improvement of 2.66% over prior methods, while enabling stable training for around 1,000 steps where baseline methods collapse. For AI practitioners, the principal implication is that Lp-Reg provides a more stable and effective technique for fine-tuning large language models on complex reasoning tasks by focusing on the quality of exploration (preserving specific valuable tokens) rather than the overall quantity of policy entropy. |
| UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG (Read more on [arXiv](https://arxiv.org/abs/2510.03663) or [HuggingFace](https://huggingface.co/papers/2510.03663))|  | This paper introduces UniDoc-Bench, a large-scale, unified benchmark designed for the evaluation of document-centric multimodal retrieval-augmented generation (MM-RAG) systems. The research objective is to create a realistic evaluation framework to enable fair, apples-to-apples comparisons across different RAG paradigms, including text-only, image-only, and various multimodal approaches. The methodology involves constructing a dataset from 70k real-world PDF pages across 8 domains, from which 1,600 human-verified, multimodal QA pairs are synthesized based on linked textual and visual evidence. The primary result shows that a multimodal text-image fusion (T+I) RAG system consistently outperforms other methods, achieving the highest end-to-end answer completeness score (68.4%), which is notably better than both joint multimodal embedding-based retrieval (64.1%) and text-only RAG (65.3%). The principal implication for AI practitioners is that for document-centric tasks, fusing separate, high-performing text and image retrieval pipelines is currently a more effective and robust strategy than relying on a single, unified multimodal embedding model. |
| CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards (Read more on [arXiv](https://arxiv.org/abs/2510.08529) or [HuggingFace](https://huggingface.co/papers/2510.08529))| Yijiang Li, Zaibin Zhang, Guibin Zhang, Yifan Zhou, xxyQwQ | The paper introduces Co-Evolving Multi-Agent Systems (CoMAS), a reinforcement learning framework where LLM-based agents autonomously improve by generating intrinsic rewards from mutual interactions without external supervision. The research aims to determine if LLM agents can achieve self-evolution by learning purely from inter-agent discussions, mimicking human collaborative improvement. The methodology involves agents engaging in solution proposal, evaluation, and scoring, with an LLM-as-a-judge mechanism formulating zero-sum rewards from these interactions to optimize each agent's policy via the REINFORCE++ algorithm. Experiments show that CoMAS achieves significant performance gains, including an absolute improvement of 19.80% over the untrained baseline on the GSM8K benchmark in the AutoGen setup. For AI practitioners, this work provides a paradigm for continuously improving LLM agents in a decentralized and scalable manner without requiring external reward models or human-annotated data, relying solely on the dynamics of agent interaction. |
| LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling (Read more on [arXiv](https://arxiv.org/abs/2510.06915) or [HuggingFace](https://huggingface.co/papers/2510.06915))|  | This paper introduces Long-RewardBench, a benchmark for evaluating long-context reward models (RMs), and proposes LongRM, a multi-stage training strategy to overcome the observed performance degradation of RMs on tasks with extended contexts. The main research objective is to investigate why state-of-the-art RMs fail in long-context settings (i.e., contexts >4K tokens) and to develop a general training methodology that scales models into robust Long-context RMs (LongRMs) without compromising their short-context capabilities. The key methodology is a two-stage training strategy: 1) A "Short-to-Long" supervised fine-tuning (SFT) stage, where reliable preference judgments are generated on critical short-context snippets and then used to train the model on padded, full-length contexts. 2) A reinforcement learning (RL) alignment stage using a Direct Preference Optimization (DPO) variant to ensure consistency between the model's judgment and its explanation, with preference data synthesized via a Consistency Majority Voting mechanism. The primary result is that existing RMs, even 70B-parameter models, exhibit a significant performance drop to near-random accuracy (<50%) when context length exceeds 4K tokens. The proposed LongRM strategy substantially improves performance; for example, an 8B LongRM model outperforms 70B-scale baselines on the new benchmark, and the training increases the average score of one model by +16.2 points. The principal implication for AI practitioners is that standard RMs are unreliable for providing supervision signals in long-context applications, such as agentic workflows. The LongRM training strategy provides a concrete and efficient framework for creating specialized, context-aware RMs, which are essential for the effective alignment and reinforcement learning of long-context large language models. |
| Learning on the Job: An Experience-Driven Self-Evolving Agent for
  Long-Horizon Tasks (Read more on [arXiv](https://arxiv.org/abs/2510.08002) or [HuggingFace](https://huggingface.co/papers/2510.08002))|  | This paper introduces MUSE, an agent framework that enables LLMs to learn from experience and self-evolve at test-time to master long-horizon productivity tasks. The primary objective is to overcome the static nature of existing agents by developing a system that autonomously accumulates and reuses knowledge from its interaction trajectories. The core methodology is a "Plan-Execute-Reflect-Memorize" loop centered on a hierarchical Memory Module that organizes distilled experiences into strategic, procedural, and tool-use knowledge. On the long-horizon TAC benchmark, MUSE achieves a new state-of-the-art with an average partial completion score of 51.78%, a nearly 20% relative improvement over the previous leading method. The principal implication for AI practitioners is that this LLM-agnostic, experience-driven architecture provides a practical paradigm for building agents that continuously improve their performance and generalization on complex real-world tasks without requiring costly model fine-tuning. |
| Taming Text-to-Sounding Video Generation via Advanced Modality Condition
  and Interaction (Read more on [arXiv](https://arxiv.org/abs/2510.03117) or [HuggingFace](https://huggingface.co/papers/2510.03117))|  | This paper introduces BridgeDiT, a dual-tower diffusion transformer, to improve Text-to-Sounding-Video generation by using disentangled text conditions and a symmetric interaction mechanism. The main objective is to overcome modal interference from shared text prompts and find an optimal architecture for cross-modal feature exchange to generate temporally synchronized audio-visual content. The key methodology consists of the Hierarchical Visual-Grounded Captioning (HVGC) framework to generate separate video and audio captions, and the BridgeDiT architecture which employs a Dual CrossAttention (DCA) mechanism for bidirectional information exchange between pretrained unimodal towers. The model achieves state-of-the-art results, notably a temporal synchronization AV-Align score of 0.275 on the AVSync15 dataset, and ablation studies confirm the superiority of the DCA fusion mechanism over alternatives. The principal implication for AI practitioners is that decoupling text conditions for each modality and enabling symmetric, bidirectional feature fusion between pretrained backbones is a highly effective strategy for improving the quality and temporal synchronization of joint audio-video generation systems. |
| Large Scale Diffusion Distillation via Score-Regularized Continuous-Time
  Consistency (Read more on [arXiv](https://arxiv.org/abs/2510.08431) or [HuggingFace](https://huggingface.co/papers/2510.08431))| Jintao Zhang, Qianli Ma, Yuji Wang, Kaiwen Zheng, ChenDRAG | The paper introduces rCM, a score-regularized method to scale continuous-time consistency distillation for large diffusion models, enabling high-fidelity generation in 1-4 steps. The primary objective is to resolve the quality degradation issues, such as poor fine-detail generation, observed when scaling standard continuous-time consistency models (sCM) to large-scale tasks. The proposed rCM methodology augments the sCM objective with a reverse-divergence score distillation loss (DMD) as a regularizer, using a custom parallelism-compatible FlashAttention-2 JVP kernel to facilitate training on models exceeding 10B parameters. Primary results show that rCM matches or surpasses competing methods; a distilled 14B Wan2.1 video model achieves a VBench score of 85.05 in 2 steps, outperforming the original teacher model's score of 83.58 while accelerating sampling by up to 50x. For AI practitioners, rCM offers a robust framework to distill large-scale diffusion models for few-step inference, significantly reducing computational costs for deployment without compromising generation quality or diversity and avoiding complex GAN-based tuning. |
| Reinforcing Diffusion Models by Direct Group Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2510.08425) or [HuggingFace](https://huggingface.co/papers/2510.08425))| Jing Tang, Tianyang Hu, Yihong Luo | This paper introduces Direct Group Preference Optimization (DGPO), an online reinforcement learning algorithm for aligning diffusion models with group-level preferences by dispensing with the policy-gradient framework. The research aims to adapt the principles of Group Relative Preference Optimization (GRPO) to diffusion models without requiring inefficient stochastic policies, instead enabling direct learning from preferences using deterministic ODE samplers. DGPO generates a group of samples, partitions them into positive and negative sets based on normalized reward scores (advantages), and directly maximizes the likelihood of this group-wise preference using an advantage-weighted objective. The method achieves state-of-the-art performance, boosting the GenEval score of a base model from 0.63 to 0.97, while training approximately 30 times faster than the policy-gradient-based Flow-GRPO. For AI practitioners, DGPO offers a computationally efficient and scalable method to post-train diffusion models on complex quality metrics, significantly reducing training time and resource requirements by leveraging efficient samplers and avoiding trajectory-wide optimization. |
| Beyond Turn Limits: Training Deep Search Agents with Dynamic Context
  Window (Read more on [arXiv](https://arxiv.org/abs/2510.08276) or [HuggingFace](https://huggingface.co/papers/2510.08276))| Yaojie Lu, Bowen Yu, Le Yu, Hao Xiang, TangQiaoYu | The paper introduces DeepMiner, a framework for training deep search agents to handle long-horizon interactions by creating high-difficulty tasks and managing context dynamically. The main objective is to overcome the limitations of insufficient task complexity and context window constraints that hinder the deep reasoning capabilities of existing multi-turn agents. The key methodology involves a reverse construction method to generate complex QA pairs from web sources and a dynamic sliding window mechanism that selectively compresses distant tool responses while preserving assistant reasoning traces during both training and inference. The primary result is that DeepMiner-32B achieves 33.5% accuracy on the BrowseComp-en benchmark, outperforming the previous best open-source agent by almost 20 percentage points and enabling nearly 100 interaction turns within a 32k context length. The principal implication for AI practitioners is that implementing a dynamic sliding window for context management, combined with training on adversarially constructed complex data, provides an effective method to develop more capable agents for long-horizon tasks without requiring larger context windows or external summarization modules. |
| Entropy Regularizing Activation: Boosting Continuous Control, Large
  Language Models, and Image Classification with Activation as Entropy
  Constraints (Read more on [arXiv](https://arxiv.org/abs/2510.08549) or [HuggingFace](https://huggingface.co/papers/2510.08549))| Huazhe Xu, xtqqwq, ChonghuaLiao, zilinkang | This paper introduces Entropy Regularizing Activation (ERA), a paradigm that constrains model output entropy by applying specially designed activation functions. The research objective is to develop a universally applicable, non-invasive method for entropy regulation that avoids altering the primary optimization objective, unlike traditional entropy bonus terms. The key methodology involves integrating a custom activation function into the model's architecture to transform its final outputs, thereby architecturally guaranteeing that the policy's entropy remains above a predefined threshold. This approach demonstrates broad effectiveness, notably boosting the AIME 2025 score for the Qwen2.5-Math-7B large language model by 37.4% and improving SAC performance on HumanoidBench by over 30% with less than 7% computational overhead. The principal implication for AI practitioners is that ERA provides a computationally cheap, non-invasive module that can be seamlessly integrated with existing models across diverse domains to improve performance and prevent issues like entropy collapse without modifying the core loss function. |
| Memory Retrieval and Consolidation in Large Language Models through
  Function Tokens (Read more on [arXiv](https://arxiv.org/abs/2510.08203) or [HuggingFace](https://huggingface.co/papers/2510.08203))|  | This paper proposes the function token hypothesis, positing that high-frequency grammatical tokens are the primary mechanism for memory retrieval and consolidation in LLMs. The main objective is to understand how memory is retrieved during inference and consolidated during pre-training by examining the distinct roles of function tokens (e.g., punctuation, prepositions) versus content tokens. The methodology combines bipartite graph analysis of token-feature activations, derived from Sparse Autoencoder (SAE) decomposition of Gemma2-9B's residual stream, with loss trajectory analysis from pre-training 1.5B and 8B models from scratch. The primary results show that a small set of function tokens activate a majority of model features; specifically, the top 10 most frequent tokens activate over 70% of features in the middle layer. The principal implication for AI practitioners is that memory mechanisms and model behavior are disproportionately governed by function tokens, suggesting that interventions during training and inference (e.g., fine-tuning, steering) could be more efficiently targeted at these tokens to control feature activation and model output. |
| Recycling Pretrained Checkpoints: Orthogonal Growth of
  Mixture-of-Experts for Efficient Large Language Model Pre-Training (Read more on [arXiv](https://arxiv.org/abs/2510.08008) or [HuggingFace](https://huggingface.co/papers/2510.08008))| Peng Cheng, Yaoxiang Wang, Yucheng Ding, lx865712528, Mr-Philo | The paper proposes an orthogonal growth framework using interpositional layer copying and noisy expert duplication to efficiently recycle converged Mixture-of-Experts (MoE) checkpoints for large language model pre-training. The primary objective is to develop a compute-efficient method for reusing the "sunk cost" of existing checkpoints by expanding them into larger models, as an alternative to training from scratch. The key methodology involves two orthogonal strategies: 1) Depth Growth via "interpositional" layer copying, which duplicates each layer in place to preserve learned weight norm distributions, and 2) Width Growth by duplicating experts and injecting small-magnitude Gaussian noise into the new weights to promote specialization. Scaling an MoE model from 17B to 70B parameters using this framework achieved a 10.66% accuracy gain on downstream tasks compared to a baseline trained from scratch under the same additional computational budget. For AI practitioners, this research provides a validated, cost-effective strategy to create larger, more capable models by leveraging existing pre-trained assets, significantly reducing the computational overhead of pre-training. |
| SciVideoBench: Benchmarking Scientific Video Reasoning in Large
  Multimodal Models (Read more on [arXiv](https://arxiv.org/abs/2510.08559) or [HuggingFace](https://huggingface.co/papers/2510.08559))| Mohit Bansal, Lincoln Spencer, Shoubin Yu, Taojiannan Yang, groundmore | This paper introduces SciVideoBench, a new benchmark designed to evaluate the advanced video reasoning capabilities of Large Multimodal Models (LMMs) on complex, research-level scientific experiments. The primary objective is to assess an LMM's ability to integrate expert domain knowledge with precise visual perception and multi-step logical reasoning, addressing a critical gap left by existing benchmarks focused on general or college-level content. The methodology involved creating 1,000 meticulously crafted multiple-choice questions from 241 research-grade experimental videos using a multi-stage, human-in-the-loop annotation pipeline that leveraged both LLM agents and human domain experts. Evaluation results reveal a significant performance disparity, with the top proprietary model (Gemini-2.5-Pro) achieving 64.30% accuracy, substantially outperforming the best open-source model (38.80%), and demonstrating that all current models struggle with the benchmark's demands. The principal implication for AI practitioners is that developing models capable of expert-level scientific reasoning requires more than scaling; it necessitates targeted architectural advancements for fine-grained visual-to-text grounding and robust, multi-step numerical calculation. |
| A^2Search: Ambiguity-Aware Question Answering with Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2510.07958) or [HuggingFace](https://huggingface.co/papers/2510.07958))|  | A²SEARCH is a reinforcement learning framework for open-domain question answering that automatically identifies and generates multiple valid answers for ambiguous questions. The main objective is to develop an annotation-free, end-to-end training framework to enable QA models to recognize and handle ambiguity, which is often overlooked by standard benchmarks that assume a single gold answer. The methodology involves an automated pipeline that uses trajectory sampling and evidence verification to discover alternative answers from existing datasets, followed by model optimization using Group Relative Policy Optimization (GRPO) with a custom `AnsF1` reward designed for multi-answer scenarios. The primary result is that A²SEARCH-7B achieves a new state-of-the-art, yielding an average AnsF1@1 score of 48.4% across four multi-hop benchmarks with a single rollout, outperforming the substantially larger ReSearch-32B model (46.2%). The principal implication for AI practitioners is that explicitly modeling and rewarding for ambiguity, rather than penalizing valid but non-reference answers, is essential for developing more robust and reliable QA systems; the paper provides a practical pipeline for augmenting single-answer datasets to achieve this. |
| GCPO: When Contrast Fails, Go Gold (Read more on [arXiv](https://arxiv.org/abs/2510.07790) or [HuggingFace](https://huggingface.co/papers/2510.07790))|  | This paper introduces Group Contrastive Policy Optimization (GCPO), a reinforcement learning method that improves LLM reasoning by injecting external "golden answers" when a model's self-generated responses are all incorrect. The research objective is to address the vanishing gradient problem in algorithms like Group Relative Policy Optimization (GRPO) where training stalls if no correct samples are produced for a given problem. The core methodology involves detecting training steps with all-zero rewards and substituting one failed rollout with a correct reference answer, thereby creating a non-zero advantage to guide the policy update. On the DeepSeek-R1-Distill-Qwen-1.5B model, GCPO achieved an average accuracy of 36.95% across six math benchmarks, outperforming the DAPO baseline's 30.37%. For AI practitioners, the principal implication is that augmenting RL training with a curated set of high-quality solutions when the model fails is a practical and effective technique to enhance reasoning capabilities and overcome training plateaus, especially for smaller models. |
| Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs (Read more on [arXiv](https://arxiv.org/abs/2510.07429) or [HuggingFace](https://huggingface.co/papers/2510.07429))| Franck Dernoncourt, Yue Zhao, Hongjie Chen, Tiankai Yang, Wang Wei | BaRP introduces a preference-conditioned contextual bandit framework for efficient LLM routing using bandit feedback. The main objective is to dynamically route LLM queries to balance performance and cost, addressing the mismatch between full-information offline training and partial-feedback deployment conditions, while enabling preference-tunable inference. BaRP models this as a multi-objective contextual bandit problem, conditioning its policy on a user-defined performance-cost preference vector and training via REINFORCE with simulated bandit feedback. Experiments on RouterBench demonstrate that BaRP outperforms strong offline routers by at least 12.46% on in-distribution tasks and reduces monetary cost by 50.00% compared to the strongest offline baseline. This allows AI practitioners to deploy adaptive and cost-effective LLM routing systems with tunable performance-cost trade-offs at inference time, without requiring full-information offline supervision or retraining. |
| R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized
  Manipulation (Read more on [arXiv](https://arxiv.org/abs/2510.08547) or [HuggingFace](https://huggingface.co/papers/2510.08547))| Zheng Zhu, Bingyao Yu, Hankun Li, Angyuan Ma, Xiuwei Xu | R2RGen is a simulator-free framework that generates diverse, real-world 3D pointcloud-action data from a single human demonstration to improve the spatial generalization of visuomotor policies. The research aims to reduce the extensive human data collection effort required for imitation learning by automatically generating spatially varied training data for robotic manipulation tasks, including those involving mobile manipulators. The methodology involves parsing a source demonstration into complete 3D object pointclouds and skill segments, applying a group-wise backtracking augmentation to transform object groups and action trajectories while preserving task constraints, and using camera-aware 3D post-processing to ensure the augmented data matches real sensor distributions. A policy trained with R2RGen data from one demonstration achieved a 40.3% average success rate, comparable to a policy trained with 25 human demonstrations (41.0% success rate) and significantly outperforming the prior method DemoGen. The principal implication for AI practitioners is that this real-to-real data generation pipeline can drastically improve data efficiency and train spatially robust 3D visuomotor policies with minimal human supervision, facilitating scalable learning for applications like mobile manipulation. |
| UP2You: Fast Reconstruction of Yourself from Unconstrained Photo
  Collections (Read more on [arXiv](https://arxiv.org/abs/2509.24817) or [HuggingFace](https://huggingface.co/papers/2509.24817))| Boqian Li, Xiaoben Li, Ziyang Li, Yuliang, Co2y | UP2You is a tuning-free method for rapidly reconstructing high-fidelity 3D clothed human avatars from collections of unconstrained 2D photos. The primary objective is to create a robust system that can process raw, unstructured photographs—varying in pose, viewpoint, and occlusion—to generate high-quality textured 3D models without per-subject fine-tuning. The key methodology is a "data rectifier" paradigm, which uses a Pose-Correlated Feature Aggregation (PCFA) module to selectively fuse features from multiple input images and convert them into clean, orthogonal multi-view images and normal maps, making them compatible with traditional 3D reconstruction. The method demonstrates superior performance over previous approaches, achieving a 15% reduction in Chamfer distance on the PuzzleIOI dataset and completing the entire reconstruction pipeline in 1.5 minutes. For AI practitioners, the principal implication is the introduction of an efficient, feed-forward alternative to computationally expensive optimization-based avatar generation, enabling the creation of personalized 3D assets from casual photos for applications like virtual try-on. |
| Fidelity-Aware Data Composition for Robust Robot Generalization (Read more on [arXiv](https://arxiv.org/abs/2509.24797) or [HuggingFace](https://huggingface.co/papers/2509.24797))| Liliang Chen, Hongwei Fan, Sicheng Hu, Di Chen, Zizhao Tong | This paper introduces a framework for principled data composition to improve the out-of-distribution (OOD) generalization of robot policies by mitigating shortcut learning. The main objective is to determine a systematic method for composing real and synthetic data to enhance policy robustness, addressing the trade-off between visual diversity and information fidelity. The key methodology is Coherent Information Fidelity Tuning (CIFT), which uses a practical proxy called Feature-Space Signal-to-Noise Ratio (SNR) to analyze the feature-space geometry of a dataset and identify an optimal mixing ratio before a "Decoherence Point" where training stability degrades. The primary result is that applying CIFT to policy architectures such as πο and Diffusion Policy improves OOD success rates by over 54%; for instance, a baseline Diffusion Policy's OOD success rate on a picking task increased from 0% to 85% under challenging semantic shifts. The principal implication for AI practitioners is that naively adding synthetic data can degrade performance; data composition must be a principled, fidelity-aware process, and a computationally cheap, pre-training feature analysis can predict an optimal data mixture to maximize robustness. |
| SViM3D: Stable Video Material Diffusion for Single Image 3D Generation (Read more on [arXiv](https://arxiv.org/abs/2510.08271) or [HuggingFace](https://huggingface.co/papers/2510.08271))|  | SViM3D is a generative video diffusion model that jointly predicts multi-view consistent RGB imagery, physically-based rendering (PBR) material maps, and normals from a single image to create relightable 3D assets. The main objective is to develop a unified model for object-centric inverse rendering from a single image, generating multi-view consistent, spatially-varying PBR materials and geometry suitable for high-quality 3D reconstruction and relighting. The methodology extends a latent video diffusion model (SV3D) by adapting its UNet architecture to output an 11-channel video tensor (RGB, basecolor, roughness, metallic, normal). This model is trained on a custom multi-illumination synthetic dataset and its output serves as a pseudo-ground-truth neural prior to optimize a 3D representation using techniques like view-dependent masking and learnable homography correction. The model achieves state-of-the-art performance in material prediction and novel view synthesis; for single-frame basecolor prediction on the Poly Haven test set, SViM3D achieves a PSNR of 28.68, significantly outperforming the next-best baseline which scored 20.59. The principal implication for AI practitioners is the availability of a foundational model for single-image-to-3D pipelines that provides a unified prior for both geometry and PBR materials, simplifying the workflow for generating relightable assets by removing the need to chain separate models for shape and material estimation. |
| Search-R3: Unifying Reasoning and Embedding Generation in Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2510.07048) or [HuggingFace](https://huggingface.co/papers/2510.07048))| James Cheng, ytgui | The paper introduces Search-R3, a framework that adapts Large Language Models (LLMs) to generate search embeddings as a direct output of their chain-of-thought reasoning process. The research objective is to unify semantic reasoning and embedding generation within a single model to overcome the limitations of using separate systems for these tasks. The methodology consists of a two-stage training pipeline: an initial supervised learning stage with contrastive loss to teach the model to produce an embedding token, followed by a reinforcement learning stage using Group Relative Policy Optimization (GRPO) to optimize the reasoning path for end-to-end retrieval performance. The primary result is that Search-R3 significantly outperforms prior methods; for example, on the SciFact benchmark, enabling reasoning improves the nDCG@10 score from 0.624 to 0.672. The principal implication for AI practitioners is the ability to use a single, unified model for both generative reasoning and high-quality embedding retrieval, which can simplify system architecture and reduce computational overhead in applications like Retrieval-Augmented Generation (RAG). |
| Towards Scalable and Consistent 3D Editing (Read more on [arXiv](https://arxiv.org/abs/2510.02994) or [HuggingFace](https://huggingface.co/papers/2510.02994))| Pan Zhou, Yang Tang, XiaRho | The paper introduces 3DEditVerse, the first large-scale paired 3D editing benchmark with 116,309 training and 1,500 test assets, alongside 3DEditFormer, a novel 3D-structure-preserving conditional transformer. The main objective is to enable precise, localized, and structure-preserving 3D edits with intuitive prompts while maintaining cross-view consistency. 3DEditFormer employs a Dual-Guidance Attention Block and Time-Adaptive Gating mechanism to disentangle editable regions from preserved structure, operating without auxiliary 3D masks. The framework achieves state-of-the-art 3D editing performance, demonstrating a +13% improvement in 3D metrics over VoxHammer. This allows AI practitioners to perform high-fidelity, practical 3D editing, simplifying content creation by eliminating the need for manual 3D mask supervision. |
| Beyond Outliers: A Study of Optimizers Under Quantization (Read more on [arXiv](https://arxiv.org/abs/2509.23500) or [HuggingFace](https://huggingface.co/papers/2509.23500))|  | This paper systematically evaluates how optimizer choice impacts large language model performance under post-training and quantization-aware training regimes. i) The main objective is to investigate the interaction between different optimizers and quantization schemes (PTQ and QAT) to determine which optimizers yield more robust quantized models. ii) The authors train OLMo2 models (50M to 1.5B parameters) with six optimizers (AdamW, Muon, PSGD, Scion, Shampoo, SOAP), then apply 4-bit PTQ and perform 4-bit QAT, evaluating performance on zero-shot benchmarks and developing a theoretical framework to analyze error propagation. iii) The primary results show that common outlier metrics like Max-to-Mean Ratio (MMR) do not predict PTQ performance across different optimizers. For both PTQ and QAT, models trained with Shampoo consistently exhibit the lowest performance degradation; specifically, for the 760M model under QAT, Shampoo's accuracy drop was only -0.46%, the lowest among all optimizers tested. v) The principal implication for AI practitioners is that the optimal optimizer for full-precision training (e.g., Muon in this study) is not necessarily the best for quantized models, and selecting an optimizer like Shampoo can significantly improve the performance and parameter efficiency of low-bit models intended for deployment. |
