

## Papers for 2025-10-13

| Title | Authors | Summary |
|-------|---------|---------|
| Thinking with Camera: A Unified Multimodal Model for Camera-Centric
  Understanding and Generation (Read more on [arXiv](https://arxiv.org/abs/2510.08673) or [HuggingFace](https://huggingface.co/papers/2510.08673))| Linyi Jin, Zhonghua Wu, Size Wu, yikaiwang, KangLiao | The paper introduces Puffin, a unified multimodal model that jointly performs camera-centric scene understanding and controllable generation by interpreting camera parameters as a language. The objective is to unify these traditionally separate tasks by integrating a geometry-aligned vision encoder, an LLM, and a diffusion model, utilizing a "thinking with camera" mechanism that aligns visual cues with photographic terms for structured spatial reasoning. Trained on the new Puffin-4M dataset, the model demonstrates superior performance over specialized systems, achieving a median roll error of 0.41 degrees and a pitch error of 0.74 degrees on the Puffin-Und camera understanding benchmark. For AI practitioners, this provides a single framework for building spatially-aware applications in AR/VR and robotics, enabling both the interpretation of scene geometry and the generation of precisely controlled novel views without requiring separate models. |
| D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to
  Embodied AI (Read more on [arXiv](https://arxiv.org/abs/2510.05684) or [HuggingFace](https://huggingface.co/papers/2510.05684))| Haebin Seong, Suwhan Choi, Maangeek, shovelingpig, lastdefiance20 | The D2E framework uses large-scale desktop interaction data to pretrain vision-action models that successfully transfer to physical robotics tasks. The research aims to determine if sensorimotor primitives learned from abundant desktop data can serve as an effective pretraining substrate to overcome the data scarcity and high collection costs in embodied AI. The methodology consists of three parts: the OWA Toolkit for scalable, compressed desktop data collection; a Generalist Inverse Dynamics Model (Generalist-IDM) that uses timestamp-based next-event prediction to pseudo-label internet videos; and Vision-Action Pretraining (VAPT) to fine-tune the desktop-pretrained model on robotics tasks. The framework achieves a 96.6% success rate on the LIBERO manipulation benchmark and an 83.3% success rate on the CANVAS navigation benchmark, validating the transfer from digital interactions to physical embodied tasks. The principal implication for AI practitioners is that they can leverage vast, low-cost desktop gameplay data to pretrain foundation models, significantly improving performance on downstream robotics tasks and reducing the dependency on expensive, specialized physical trajectory data collection. |
| TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion
  Sampling (Read more on [arXiv](https://arxiv.org/abs/2510.04533) or [HuggingFace](https://huggingface.co/papers/2510.04533))| Seungryong Kim, Jee Eun Kim, Susung Hong, Donghoon Ahn, hyeoncho01 | Tangential Amplifying Guidance (TAG) is a novel inference-time method that reduces hallucinations in diffusion models by selectively amplifying the tangential component of the sampling update step. The objective is to develop a direct, computationally efficient guidance mechanism that improves sampling fidelity by steering trajectories towards higher-probability regions of the data manifold without modifying the model architecture. The methodology involves decomposing each update increment into components parallel and orthogonal (tangential) to the current latent vector and then scaling only the tangential component, which is shown to encode critical semantic information. Experimentally, applying TAG to a DDIM sampler on Stable Diffusion v1.5 for unconditional ImageNet generation reduces the FID score from 76.942 to 67.805 at a matched 50 NFEs. For AI practitioners, TAG provides a practical, architecture-agnostic, plug-and-play module to enhance the output quality and semantic consistency of pre-trained diffusion models with minimal additional computational cost. |
| Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for
  MLLMs (Read more on [arXiv](https://arxiv.org/abs/2510.09201) or [HuggingFace](https://huggingface.co/papers/2510.09201))|  | This paper introduces multimodal prompt optimization and proposes the Multimodal Prompt Optimizer (MPO), a framework for jointly optimizing textual and non-textual prompts for MLLMs. The primary objective is to automate the discovery of optimal multimodal prompt pairs to fully leverage the expressive capacity of MLLMs, which is unachievable with existing text-only methods. MPO uses an "alignment-preserving exploration" mechanism to jointly update prompts via a single semantic gradient and a "prior-inherited Bayesian UCB" strategy to efficiently select candidate prompts by using parent prompt performance as a warm-start prior. Across 10 diverse datasets spanning images, videos, and molecules, MPO achieves a 65.1% average score, outperforming the best text-only optimization methods, and its selection strategy reduces the evaluation budget by 42% compared to a prior-free baseline. The principal implication for AI practitioners is that optimizing only textual prompts is suboptimal for MLLMs; MPO provides an automated method to create effective non-textual prompts (e.g., reference images) in conjunction with text to significantly improve model performance. |
| AutoPR: Let's Automate Your Academic Promotion! (Read more on [arXiv](https://arxiv.org/abs/2510.09558) or [HuggingFace](https://huggingface.co/papers/2510.09558))| Yixin Yuan, Libo Qin, Mingda Yang, Zheng Yan, Qiguang Chen | This paper introduces AutoPR, a novel task for automatically transforming research papers into engaging promotional content, alongside a benchmark (PRBench) and a multi-agent framework (PRAgent). The primary objective is to automate scholarly promotion to increase visibility and citations while reducing manual effort. The key methodology is PRAgent, a three-stage multi-agent system that performs content extraction, collaborative synthesis of multimodal content, and platform-specific adaptation. In a real-world social media study, PRAgent substantially outperformed direct LLM baselines, achieving a 604% increase in total watch time and a 438% rise in likes. The principal implication for AI practitioners is that this work provides a validated framework and benchmark for creating automated systems that can effectively translate complex technical documents into high-engagement, platform-optimized public content. |
| R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth
  and Depth? (Read more on [arXiv](https://arxiv.org/abs/2510.08189) or [HuggingFace](https://huggingface.co/papers/2510.08189))|  | The paper introduces R-HORIZON, a method for evaluating and enhancing the long-horizon reasoning of Large Reasoning Models (LRMs) by composing single-step problems into interdependent, multi-step tasks. The primary research objective is to assess the capabilities and limitations of LRMs in scenarios requiring reasoning across multiple sequential and interdependent problems, a dimension inadequately covered by existing benchmarks. The key methodology involves "query composition," where single-horizon tasks are programmatically linked by making the answer of one problem a required variable for a subsequent problem, thereby creating a long-horizon reasoning benchmark and training data. The primary results show that even advanced LRMs exhibit significant performance degradation on these composed tasks, but training a model with R-HORIZON data via reinforcement learning with verified rewards (RLVR) substantially improves performance on both multi-horizon tasks and standard benchmarks, achieving a +7.5 point gain on AIME2024. The principal implication for AI practitioners is that the R-HORIZON framework provides a scalable and low-cost paradigm to generate more challenging, realistic training data and benchmarks, enabling the development and validation of models with robust capabilities for complex, multi-step problem-solving. |
| Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining
  Levels (Read more on [arXiv](https://arxiv.org/abs/2510.06499) or [HuggingFace](https://huggingface.co/papers/2510.06499))|  | This paper introduces Webscale-RL, an automated data pipeline that converts web-scale pretraining corpora into verifiable question-answer pairs to scale reinforcement learning for LLMs. The primary objective is to overcome the data scarcity and diversity bottleneck that limits the application of reinforcement learning (RL) in LLMs by creating a scalable method to generate massive, diverse, RL-ready datasets from existing pretraining corpora. The key methodology is a four-stage automated pipeline: (1) Data Filtering to remove low-quality documents, (2) Domain Classification and Persona Assignment to guide question style, (3) Verifiable QA Generation using an LLM to create question-answer pairs grounded in the source text, and (4) Quality Check and Leakage Control to ensure correctness and prevent trivial questions. The primary result is that RL training using the generated Webscale-RL dataset is significantly more data-efficient than continual pretraining, achieving comparable performance improvements with up to 100× fewer tokens, and outperforming the strongest data refinement baseline by 3.4 points on average across a suite of benchmarks. The principal implication for AI practitioners is that this pipeline provides a scalable and efficient pathway to enhance LLM reasoning capabilities by repurposing vast, existing pretraining corpora for RL, avoiding the high cost of new data collection and offering a more compute-efficient alternative to continual pretraining. |
| SpaceVista: All-Scale Visual Spatial Reasoning from mm to km (Read more on [arXiv](https://arxiv.org/abs/2510.09606) or [HuggingFace](https://huggingface.co/papers/2510.09606))| Kaituo Feng, Yi Ding, Dongming Wu, Shiqiang Lang, spw2000 | This research introduces SpaceVista, a comprehensive framework comprising a dataset (SpaceVista-1M), benchmark (SpaceVista-Bench), and model (SpaceVista-7B) to enable MLLM visual spatial reasoning across a six-order-of-magnitude scale range from millimeters to kilometers. The paper's main objective is to address the limitations of existing spatial reasoning systems, which are largely confined to indoor scenes, by creating an effective solution for all-scale scene understanding. The key methodology involves curating a large-scale video dataset with 1M QA pairs using an automated pipeline, and developing a model that integrates dense self-supervised visual features (DINOv3) with LoRA-like scale experts and a progressive training strategy to mitigate cross-scale knowledge conflicts. The proposed SpaceVista-7B model achieves state-of-the-art performance on the new all-scale SpaceVista-Bench with an overall accuracy of 36.7%, outperforming other open-source and proprietary models. The principal implication for AI practitioners is the provision of a public dataset and a scale-aware expert architecture that enables the development of more robust and generalizable spatial reasoning models for applications like robotics, autonomous driving, and remote sensing. |
| ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level
  Entropy Shaping (Read more on [arXiv](https://arxiv.org/abs/2510.08457) or [HuggingFace](https://huggingface.co/papers/2510.08457))| Wenbo Hu, Yimeng Ye, Yue Guo, JoeYing, csfufu | ARES is a two-stage training framework that enables multimodal models to adaptively allocate reasoning effort based on task difficulty using token-level window-entropy as an exploration signal. The main objective is to overcome the tendency of models to "overthink" simple problems and "under-explore" complex ones by dynamically adjusting reasoning depth. The key methodology involves an "Adaptive Cold-Start" fine-tuning stage to create an initial correlation between reasoning length and problem difficulty, followed by "Adaptive Entropy Policy Optimization" (AEPO), a reinforcement learning stage that uses high window-entropy tokens to trigger exploration and a hierarchical reward to control its extent. The primary result is that ARES-7B substantially outperforms other open-source models, achieving a 55.9% average accuracy across ten multimodal benchmarks, which is a +9.7% absolute improvement over the previous state-of-the-art. The principal implication for AI practitioners is that this framework provides a method to fine-tune models for enhanced computational efficiency and performance, reducing inference costs on simple tasks while improving accuracy on complex reasoning problems. |
| StreamingVLM: Real-Time Understanding for Infinite Video Streams (Read more on [arXiv](https://arxiv.org/abs/2510.09608) or [HuggingFace](https://huggingface.co/papers/2510.09608))| Kelly Peng, Liuning He, Guangxuan Xiao, Ruyi Xu, Yukang | StreamingVLM introduces a unified framework for vision-language models to process near-infinite video streams in real-time with stable performance and low latency. The research objective is to resolve the trade-off between computational cost, latency, and temporal coherence that plagues existing methods when processing continuous video. The core methodology involves a supervised fine-tuning (SFT) strategy on short, overlapped video chunks, which mimics a streaming-aware inference scheme that utilizes a compact KV cache with attention sinks, recent vision/text windows, and contiguous Rotary Position Embeddings (RoPE). On the introduced Inf-Streams-Eval benchmark, StreamingVLM achieves a 66.18% win rate against GPT-4O mini and maintains real-time performance at up to 8 FPS on a single NVIDIA H100. For AI practitioners, this work provides a practical and efficient method to align model training on finite video datasets with the requirements of infinite-stream inference, enabling the deployment of VLMs in real-world, latency-sensitive applications like autonomous agents and live assistants. |
| Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence
  Reweighting (Read more on [arXiv](https://arxiv.org/abs/2510.08696) or [HuggingFace](https://huggingface.co/papers/2510.08696))| Julia Kempe, Yaqi Duan, Anthony Hartshorn, Parag Jain, Yunzhen Feng | This paper introduces Likelihood Estimation with Negative Samples (LENS), a principled modification to Group Relative Policy Optimization (GRPO) that leverages incorrect generations by assigning them confidence-weighted negative rewards. The research objective is to find a way to learn from "negative groups"—generation batches where all samples are incorrect—which are normally discarded in GRPO, thus wasting compute. The key methodology involves deriving a new reward function from a Maximum Likelihood Estimation (MLE) objective, which penalizes incorrect answers more heavily when the model is more confident, and integrating this directly into the GRPO advantage calculation. On the MATH benchmark with Llama-3.1-8B-Instruct, LENS achieved a Pass@1 score of 56.63, outperforming the GRPO baseline of 54.09, with greater improvements observed on harder problems. For AI practitioners, LENS offers a plug-and-play modification to GRPO-based RLVR that improves training efficiency and final model performance on complex reasoning tasks by converting previously wasted samples into useful learning signals. |
| KORMo: Korean Open Reasoning Model for Everyone (Read more on [arXiv](https://arxiv.org/abs/2510.09426) or [HuggingFace](https://huggingface.co/papers/2510.09426))|  | This paper introduces KORMo-10B, a 10.8B-parameter fully open bilingual Korean-English language model trained predominantly on synthetic data. The primary objective is to investigate the feasibility of using a high proportion of synthetic data to construct a stable, high-performing, and fully open model (FOM) for a non-English language. The methodology involves training a decoder-only transformer from scratch using a curated bilingual corpus where 68.74% of the Korean portion is synthetic, following a transparent FOM approach that releases all training artifacts. The study demonstrates that curated, diverse synthetic data sustains long-horizon pretraining without model collapse, and the resulting model achieves performance comparable to open-weight baselines, such as an average score of 8.61 on instruction-following benchmarks. The principal implication for AI practitioners is that they can build effective and reproducible fully open LLMs for low-resource languages by leveraging large-scale, diverse synthetic data, providing a scalable alternative where high-quality native data is scarce. |
| Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out
  of Distribution Generalization (Read more on [arXiv](https://arxiv.org/abs/2510.06274) or [HuggingFace](https://huggingface.co/papers/2510.06274))| Mahdi Ghaznavai, Mohamadreza Fereydooni, Arash Marioriyad, Mohammad Mahdi Samiei Paqaleh, OstadTahmasb | This paper introduces Complexity Out-of-Distribution (Complexity OoD) generalization as a formal framework for defining and measuring the reasoning abilities of AI models. The main objective is to establish a clear metric for reasoning by evaluating a model's ability to solve problems whose minimal required solution complexity (either in structure or computational steps) exceeds that of all training examples. The methodology involves theoretically formalizing Complexity OoD using Kolmogorov complexity and then using operational proxies, such as the number of arithmetic operations in GSM8K, to re-evaluate model performance across stratified complexity levels. A primary result shows that model accuracy consistently degrades with increasing complexity; for example, on the GSM8K benchmark, models like GPT-4o drop from approximately 98% accuracy on 2-3 operation problems to below 85% on 8-operation problems. For AI practitioners, the principal implication is that aggregate performance metrics are insufficient for evaluating reasoning; they must adopt complexity-aware evaluation and develop architectures with inductive biases for adaptive computation and memory, as merely scaling data cannot overcome this generalization challenge. |
| Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open
  Vocabulary Occupancy Prediction (Read more on [arXiv](https://arxiv.org/abs/2510.04759) or [HuggingFace](https://huggingface.co/papers/2510.04759))| danxuhk, yanchi3dv | The paper introduces PG-Occ, a Progressive Gaussian Transformer framework for efficient open-vocabulary 3D occupancy prediction. The objective is to resolve the trade-off between sparse Gaussian representations, which struggle with fine-grained details, and dense representations, which incur high computational costs. The key methodology employs a progressive online densification strategy to iteratively enhance a sparse 3D Gaussian representation in a feed-forward manner, coupled with an anisotropy-aware sampling technique for more effective feature aggregation. PG-Occ achieves state-of-the-art performance on the Occ3D-nuScenes dataset, with a mean Intersection over Union (mIoU) of 15.15, marking a 14.3% relative improvement over the previous best method. For AI practitioners, this framework demonstrates an effective method to build detailed and queryable 3D scene models from only 2D supervision, improving the efficiency and accuracy of perception systems in autonomous driving. |
| StatEval: A Comprehensive Benchmark for Large Language Models in
  Statistics (Read more on [arXiv](https://arxiv.org/abs/2510.09517) or [HuggingFace](https://huggingface.co/papers/2510.09517))|  | This paper introduces StatEval, a comprehensive benchmark designed to evaluate the statistical reasoning capabilities of Large Language Models across foundational and research-level problems. The primary objective is to create a dedicated, large-scale benchmark to systematically assess LLMs on statistical tasks, a domain currently underexplored in evaluation efforts. The methodology involves a multi-agent LLM pipeline with human-in-the-loop verification to extract and curate 13,817 foundational and 2,374 research-level proof-based problems, coupled with a process-based scoring framework for fine-grained assessment. Experimental results reveal that even top-tier models like GPT5-mini achieve below 57.62% accuracy on research-level tasks, with open-source models performing significantly lower. The principal implication for AI practitioners is that current LLMs exhibit substantial weaknesses in rigorous statistical reasoning, indicating that they are not yet reliable for advanced theoretical analysis or formal proof generation in statistics without significant targeted improvements. |
| MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for
  Reasoning-Intensive Multimodal Retrieval (Read more on [arXiv](https://arxiv.org/abs/2510.09510) or [HuggingFace](https://huggingface.co/papers/2510.09510))| Tingyu Song, Yilun Zhao, Xiao Zhou, Siyue Zhang, ItzYog | The paper introduces MRMR, an expert-level, multidisciplinary benchmark with 1,502 queries designed for evaluating reasoning-intensive multimodal retrieval systems on interleaved image-text data. The objective is to create and evaluate a benchmark that tests multimodal retrieval systems on complex, expert-domain reasoning, moving beyond simple semantic matching to tasks requiring deeper logical inference. The methodology involves constructing three retrieval tasks—Knowledge, Theorem, and a novel Contradiction Retrieval task—by sourcing queries from the MMMU-Pro benchmark, collecting human-verified positive web documents, and using PIN-14M for negative samples, then evaluating 14 models across four retrieval paradigms. The primary finding is that text-only retrieval with LLM-generated captions (52.1 nDCG@10) outperforms native multimodal models; the best multimodal model, Ops-MM-Embedding, sees its performance drop from 67.4 nDCG@10 on knowledge tasks to 30.1 on Theorem tasks, highlighting a significant reasoning deficiency. For AI practitioners, this implies that current multimodal models are fundamentally limited in their reasoning capabilities for expert-level applications, indicating a critical need to develop models that can perform deeper logical inference over integrated visual and textual data instead of relying on text-based workarounds. |
| DISCO: Diversifying Sample Condensation for Efficient Model Evaluation (Read more on [arXiv](https://arxiv.org/abs/2510.07959) or [HuggingFace](https://huggingface.co/papers/2510.07959))|  | This paper introduces DISCO, a method for efficient model evaluation that selects a small data subset by maximizing inter-model disagreement to predict full benchmark performance. The objective is to reduce the prohibitive computational cost of evaluation; the methodology involves selecting samples with the highest Predictive Diversity Score (PDS) across a set of source models, then training a regression model on the target model's "signature" (concatenated raw outputs) on this subset to predict its final score. Empirically, DISCO reduces the MMLU evaluation set by 99.3% to just 100 samples while achieving a state-of-the-art Mean Absolute Error of 1.07 percentage points in accuracy prediction. The principal implication for AI practitioners is the ability to perform high-fidelity model evaluation with over 99% less compute, enabling rapid performance tracking by focusing on samples that maximally differentiate model capabilities rather than on data representativeness. |
| Dyna-Mind: Learning to Simulate from Experience for Better AI Agents (Read more on [arXiv](https://arxiv.org/abs/2510.09577) or [HuggingFace](https://huggingface.co/papers/2510.09577))| Qianhui Wu, Hao Cheng, Michel Galley, Baolin Peng, Xiao Yu | This paper introduces Dyna-Mind, a two-stage training framework that teaches AI agents to explicitly simulate future states from real experience to improve performance on long-horizon interactive tasks. The main objective is to enhance (V)LM agent performance in complex interactive environments by explicitly teaching them to integrate mental simulation, or "vicarious trial and error," into their reasoning process. The key methodology is a two-stage process: 1) Reasoning with Simulations (RESIM), an offline supervised fine-tuning stage using reasoning traces constructed from real-experience search trees, followed by 2) Dyna-GRPO, an online reinforcement learning stage that refines the agent's simulation ability using both outcome rewards and intermediate ground-truth states from the environment. Primary results demonstrate that on the ALFWorld benchmark, the full Dyna-Mind framework achieved an average task success rate of 90.8%, significantly outperforming the 74.1% from the RESIM stage alone and the 62.5% from a strong ReACT baseline using DeepSeek-R1. The principal implication for AI practitioners is that explicitly training agents to generate and reason over simulated future outcomes, grounded by real experience and refined with online feedback, is a highly effective strategy for building more capable agents for tasks requiring multi-step planning, suggesting a shift from simple imitation learning to teaching structured, model-based reasoning. |
| ReviewerToo: Should AI Join The Program Committee? A Look At The Future
  of Peer Review (Read more on [arXiv](https://arxiv.org/abs/2510.08867) or [HuggingFace](https://huggingface.co/papers/2510.08867))| Christopher Pal, Laurent Charlin, Hugo Larochelle, Gaurav Sahu | This paper introduces ReviewerToo, a modular framework that uses persona-based LLM agents to systematically evaluate and assist in the academic peer-review process. The main objective is to assess the viability of AI-assisted peer review by deploying specialized AI agents in a structured workflow and comparing their performance against human reviewers on real conference submissions. The methodology involves using the `gpt-oss-120b` model to instantiate various reviewer personas which analyze a curated dataset of 1,963 ICLR 2025 papers, with performance evaluated on classification accuracy and review quality via ELO ratings. The ensembled meta-reviewer agent achieved 81.8% accuracy for accept/reject decisions, closely approaching the 83.9% accuracy of the average human reviewer, and generated reviews that were rated as higher quality than the human average by an LLM judge. The principal implication for AI practitioners is that multi-agent, protocol-driven LLM systems can serve as effective complements in quality assurance pipelines by providing scalable and structured baseline assessments, but human expertise remains critical for final, nuanced judgments and mitigating AI biases like sycophancy. |
| BigCodeArena: Unveiling More Reliable Human Preferences in Code
  Generation via Execution (Read more on [arXiv](https://arxiv.org/abs/2510.08697) or [HuggingFace](https://huggingface.co/papers/2510.08697))| Juyong Jiang, Hange Liu, Xiaolong Jin, Terry Yue Zhuo, Benjamin-eecs | The paper introduces BIGCODEARENA, a human evaluation platform with an integrated execution environment to more reliably assess the quality of LLM-generated code. The primary objective is to demonstrate that human evaluation of code is more reliable when based on execution feedback rather than static source code, and to leverage collected preference data to build automated evaluation benchmarks. The methodology involves deploying an arena-style platform where users compare two anonymized LLM outputs by running the generated code in a sandbox, from which over 4.7K high-quality pairwise preference samples were collected to create the BIGCODEREWARD and AUTOCODEARENA benchmarks. Results show that execution feedback significantly improves evaluation accuracy; for instance, on the BIGCODEREWARD benchmark, the Qwen2.5-VL-72B Instruct model's accuracy in judging code preferences increased from 58.7% to 66.2% when provided with execution outputs. The principal implication for AI practitioners is that static code analysis is an unreliable proxy for quality, and evaluation frameworks for code generation models must incorporate execution-based testing to accurately measure functional correctness and user intent alignment. |
| Which Heads Matter for Reasoning? RL-Guided KV Cache Compression (Read more on [arXiv](https://arxiv.org/abs/2510.08525) or [HuggingFace](https://huggingface.co/papers/2510.08525))| Huan Wang, Xue Liu, Keda Tao, Li Jiang, Kurt232 | The paper introduces RLKV, a reinforcement learning framework that identifies critical "reasoning heads" in language models to enable significant KV cache compression for efficient inference. The main objective is to develop a compression method that preserves the complex, long-sequence reasoning capabilities of LLMs, which degrade under existing token-dropping or head-reallocation techniques. RLKV uses reinforcement learning with verifiable rewards to optimize learnable gating adapters for each attention head, controlling the mix between full and compressed cache access while an L1 penalty encourages sparsity to isolate essential heads. The method achieves a 20-50% reduction in KV cache usage with near-lossless performance, and in some cases improves it; for example, on Llama-3.1-8B-R1, it improved performance by 2% on the Math500 benchmark while using 50% less cache. For AI practitioners, this allows the deployment of reasoning-intensive LLMs with substantially lower GPU memory requirements, enabling larger inference batch sizes or use on memory-constrained hardware without sacrificing chain-of-thought reasoning capabilities. |
| Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic
  Speech Recognition (Read more on [arXiv](https://arxiv.org/abs/2510.08047) or [HuggingFace](https://huggingface.co/papers/2510.08047))| Shang-Tse Chen, Tzu-Quan Lin, Yu-Hsuan Li Liang, Yi-Cheng Lin, jacksukk | Pseudo2Real introduces a task arithmetic method to create a correction vector that mitigates systematic biases in pseudo-labeled data for unsupervised ASR domain adaptation. The objective is to correct recurring error patterns introduced by pseudo-labeling when no ground-truth transcriptions are available for the target domain. The methodology computes a correction vector by taking the parameter-space difference between a model fine-tuned on ground-truth labels and another on pseudo-labels within a source domain; this vector is then added to a target-domain model trained on pseudo-labels. On the AFRISPEECH-200 benchmark, this approach achieved up to a 35% relative Word Error Rate (WER) reduction with the Whisper TINY model across ten African accents compared to standard pseudo-label fine-tuning. For AI practitioners, this provides a technique to enhance ASR model robustness for new, unlabeled domains by learning a reusable bias correction vector from an existing labeled source, directly addressing the issue of error propagation from teacher models in self-training. |
| Parallel Test-Time Scaling for Latent Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2510.07745) or [HuggingFace](https://huggingface.co/papers/2510.07745))|  | This work introduces a framework for parallel test-time scaling in latent reasoning models by proposing novel stochastic sampling and aggregation techniques for continuous vector spaces. The paper's objective is to enable parallel test-time scaling (TTS) for latent reasoning models, addressing the challenges of sampling diverse trajectories in a continuous space and aggregating them without explicit probabilistic scores. The key methodology involves two uncertainty-inspired sampling strategies—Monte Carlo Dropout and Additive Gaussian Noise—to generate diverse latent thoughts, and a Latent Reward Model (LatentRM) trained with a step-wise contrastive objective to score and aggregate these latent trajectories. The primary results show that the proposed framework scales effectively; an ablation study on GSM-Test using Best-of-N (N=8) aggregation demonstrates that the LatentRM achieves 35.4% accuracy, outperforming a majority voting baseline (33.6%). The principal implication for AI practitioners is that the performance of efficient latent reasoning models can now be scaled with additional inference compute, providing a practical method to improve reasoning accuracy without retraining, a capability previously limited to token-based models. |
| Mind-Paced Speaking: A Dual-Brain Approach to Real-Time Reasoning in
  Spoken Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.09592) or [HuggingFace](https://huggingface.co/papers/2510.09592))| Zhang, Xiangyu, Jun Chen, Haoyang Zhang, Donghang Wu | This paper introduces Mind-Paced Speaking (MPS), a dual-brain framework for Spoken Language Models (SLMs) to enable concurrent reasoning and speech generation. The research aims to overcome the high latency of traditional Chain-of-Thought (CoT) reasoning in SLMs by developing a system that can "think while speaking." The key methodology is a dual-LLM architecture where a "Formulation Brain" continuously generates CoT segments that incrementally pace and guide a separate "Articulation Brain" responsible for speech generation, which is trained using "think-incomplete" supervised fine-tuning. The proposed method achieves 92.8% accuracy on the Spoken-MQA mathematical reasoning task and a score of 82.5 on the URO-Bench speech conversation task in a zero-latency configuration. For AI practitioners, this dual-brain, paced-generation architecture offers a practical design for implementing low-latency, reasoning-capable conversational agents by decoupling the thinking and speaking processes, making complex reasoning viable for real-time applications. |
| A Goal Without a Plan Is Just a Wish: Efficient and Effective Global
  Planner Training for Long-Horizon Agent Tasks (Read more on [arXiv](https://arxiv.org/abs/2510.05608) or [HuggingFace](https://huggingface.co/papers/2510.05608))| Fanchao Qi, Gang Chen, Kangyang Luo, Haozhe Zhao, Shuzheng Si | The paper presents EAGLET, an efficient and effective training method for a plug-and-play global planner to enhance LLM-based agents' performance on long-horizon tasks. The primary objective is to improve the planning abilities of LLM-based agents to mitigate planning hallucinations and brainless trial-and-error behavior without requiring human annotation or extra training data. The methodology involves a two-stage process: 1) a cold-start supervised fine-tuning (SFT) on plans synthesized by an advanced LLM and filtered using a novel homologous consensus filtering strategy, and 2) a rule-based reinforcement learning stage using a custom executor capability gain reward (ECGR) to further refine the planner. The primary result is that executor agents equipped with the EAGLET planner achieve state-of-the-art performance; for instance, the GiGPO agent's average reward across three long-horizon benchmarks increases from 82.8% to 87.6% while reducing training costs by 8x compared to RL-based baselines. The principal implication for AI practitioners is that decoupling global planning from execution via a separately trained, plug-and-play planner offers a modular, efficient, and automated method to significantly enhance the reliability and performance of existing LLM agents on complex tasks without altering the core executor model. |
| PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs (Read more on [arXiv](https://arxiv.org/abs/2510.09507) or [HuggingFace](https://huggingface.co/papers/2510.09507))| Xu Zheng, Lutao Jiang, Xingwang Lin, Kanghao Chen, Zixin Zhang | PhysToolBench introduces the first Visual Question Answering (VQA) benchmark to evaluate Multimodal Large Language Models' (MLLMs) understanding of physical tools. The main objective is to quantify the true depth of MLLM comprehension of physical tools. The methodology involves over 1,000 text-image pairs across three progressive difficulty levels: Tool Recognition, Tool Understanding, and Tool Creation, simulating a robotic workflow. Primary results show that 32 evaluated MLLMs exhibit significant deficiencies, with even top proprietary models scoring no higher than 63% overall accuracy, falling far short of human performance (over 87.85%). This implies that current MLLMs have a superficial understanding of tools, necessitating advancements in visual reasoning and larger MLLM backbones for robust embodied AI applications. |
| Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols (Read more on [arXiv](https://arxiv.org/abs/2510.09462) or [HuggingFace](https://huggingface.co/papers/2510.09462))| Maksym Andriushchenko, Caglar Gulcehre, Daniil Dzenhaliou, Mikhail Terekhov, kotekjedi | Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols explores how untrusted models can compromise AI control protocols through adaptive attacks on LLM monitors. The main objective is to demonstrate that knowledge of the protocol and monitor enables untrusted models to subvert safety measures via prompt injections. The methodology involved embedding LLM-generated prompt injections into untrusted model outputs, evaluated across diverse monitors and control protocols on APPS Backdooring and BashBench benchmarks. Key results include that prompt injections consistently evade monitors; for instance, on APPS, the GPT-4.1 monitor labeled all 375 backdoored GPT-5 samples as benign (score 0), and the Defer-to-Resample protocol amplified attacks. For AI practitioners, this implies that current LLM monitors are a significant blind spot, necessitating stronger red-teaming, worst-case analysis, and robust mitigation strategies in future AI control mechanisms. |
| GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare (Read more on [arXiv](https://arxiv.org/abs/2510.08872) or [HuggingFace](https://huggingface.co/papers/2510.08872))|  | GTAlign is a novel alignment framework that integrates game-theoretic decision-making into LLM reasoning and training to optimize for mutual user-LLM welfare. The research objective is to resolve the "prisoner's dilemma" in user-LLM interactions, where individually rational model choices (e.g., over-clarification) lead to socially suboptimal outcomes, by enabling the model to reason strategically for mutually beneficial results. The key methodology involves a Game-Theoretic Reasoning Chain, where the model explicitly constructs a payoff matrix to estimate user and LLM welfare for potential actions, combined with a Mutual Welfare Reward function used in RL training to reinforce cooperative behaviors. GTAlign improves mutual welfare by an average of 7.2% and answer quality by 4.9% across four in-distribution datasets compared to baseline methods. The principal implication for AI practitioners is a framework for building more transparent and adaptive LLM assistants, with an inference-time steering mechanism that allows for dynamic modification of model behavior (e.g., adapting to different pricing policies) by altering the payoff matrix without requiring retraining. |
| Understanding DeepResearch via Reports (Read more on [arXiv](https://arxiv.org/abs/2510.07861) or [HuggingFace](https://huggingface.co/papers/2510.07861))| Chengen Huang, Fengji Zhang, Yuxiang Zheng, Xinyao Niu, T1anyu | This paper introduces DEEPRESEARCH-REPORTEVAL, a framework for holistically evaluating AI research agents by systematically assessing their primary output—research reports—on quality, redundancy, and factuality. The objective is to overcome the limitations of existing benchmarks that test isolated skills rather than the integrated, end-to-end performance required for complex research tasks. The methodology employs an LLM-as-a-Judge with iteratively refined prompts to score reports, achieving strong concordance with human evaluators, demonstrated by a 61.11% exact match in ranking agreement. The primary result from evaluating four commercial systems is the identification of distinct design trade-offs between report conciseness, analytical depth, and factuality, with systems like Qwen excelling in quality while OpenAI led in evidence grounding. The principal implication for AI practitioners is the provision of a standardized, automated framework and benchmark for quantitatively measuring and comparing the end-to-end capabilities of complex agentic systems, directly informing design choices for building more effective AI research partners. |
| One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework (Read more on [arXiv](https://arxiv.org/abs/2510.02898) or [HuggingFace](https://huggingface.co/papers/2510.02898))| Giuseppe Amato, Nicola Messina, Fabio Carrara, Ruggero1912, lorebianchi98 | The paper introduces Patch-ioner, a unified zero-shot framework that generates captions for arbitrary image regions by treating individual visual patches as atomic captioning units. The primary objective is to develop a zero-shot captioning framework capable of describing arbitrary image regions—from single patches to entire images—without requiring any region-level supervision, thereby overcoming the limitations and data costs of traditional region-based and global image captioners. The proposed Patch-ioner framework adopts a patch-centric paradigm where a vision backbone with strong local feature capabilities (e.g., DINOv2) first extracts dense patch embeddings from an image. A parameter-free aggregation function then combines embeddings from a specified region into a single vector, which is then decoded into a caption by a text-only trained decoder using a latent projection mechanism to mitigate the vision-language modality gap. The framework demonstrates state-of-the-art performance on multiple zero-shot regional captioning tasks. On the zero-shot dense captioning task using the VG v1.2 dataset, the Talk2DINO-based Patch-ioner model achieves a CIDEr score of 31.9, significantly outperforming prior whole-image captioners adapted for the task, such as a crop-based DeCap which scored 24.6. For AI practitioners, this framework enables the development of flexible, multi-granularity captioning systems without expensive, region-annotated datasets. Its design allows for captioning multiple arbitrary regions from a single backbone forward pass, offering computational efficiency for interactive applications and detailed scene analysis. |
| TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion
  Control (Read more on [arXiv](https://arxiv.org/abs/2510.09561) or [HuggingFace](https://huggingface.co/papers/2510.09561))| Adityan Jothi, Christian Jacobsen, Ruben Ohana, Minkyoung Cho, cmhungsteve | TC-LoRA introduces a framework for adaptive diffusion control by dynamically generating conditional LoRA weights using a hypernetwork at each denoising step. The paper's objective is to develop a dynamic weight conditioning mechanism that adapts the model's computational strategy throughout the generation process, moving beyond static, activation-based control methods. The key methodology involves a hypernetwork that takes diffusion time, layer identity, and a user's spatial condition (e.g., depth map) as input to generate LoRA adapters on-the-fly, modifying the weights of a frozen diffusion model backbone. TC-LoRA demonstrates superior generative fidelity, reducing the Normalized Mean Squared Error (NMSE) by 11.7% on the TransferBench benchmark compared to a ControlNet-style baseline, while using significantly fewer trainable parameters (251M vs. 900M). For AI practitioners, the principal implication is a more parameter-efficient and effective method for achieving precise spatial control in generative models, crucial for applications like high-fidelity synthetic data generation. |
| Mitigating Overthinking through Reasoning Shaping (Read more on [arXiv](https://arxiv.org/abs/2510.09535) or [HuggingFace](https://huggingface.co/papers/2510.09535))| Wen Luo, Yejie Wang, Bofei Gao, Shaohang Wei, Feifan Song | This paper presents Group Relative Segment Penalization (GRSP), a method for regularizing the reasoning process of Large Reasoning Models (LRMs) to reduce computational overhead. The objective is to mitigate "overthinking" by balancing task accuracy and token efficiency within Reinforcement Learning with Verifiable Reward (RLVR) frameworks. The methodology involves segmenting a model's reasoning into steps, clustering these segments by length, and applying a group-relative penalty with length-aware descending weights to discourage an excessive number of short reasoning segments. On the Omni-MATH 500 benchmark, GRSP improved accuracy to 45.60% while reducing average response length to 4866 tokens, outperforming the baseline Reinforce method's 44.20% accuracy and 5131 token length. For AI practitioners, the principal implication is that supervising reasoning at the segment level, rather than the token level, offers a more stable and effective method for training computationally efficient models for complex tasks without degrading performance. |
| Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive
  Text-to-image Generation (Read more on [arXiv](https://arxiv.org/abs/2510.08994) or [HuggingFace](https://huggingface.co/papers/2510.08994))| Han Shi, Zhekai Chen, Xian Liu, Fuyun Wang, Yao Teng | This paper proposes Speculative Jacobi-Denoising Decoding (SJD2), a framework that integrates a denoising process into Jacobi iterations to accelerate parallel token generation for autoregressive text-to-image models. The objective is to reduce the significant inference latency caused by the sequential, token-by-token decoding process inherent in autoregressive models. The key methodology involves fine-tuning a pre-trained model for a "next-clean-token prediction" task, enabling it to accept noise-perturbed token embeddings and predict clean next tokens. During inference, token sequences are initialized with Gaussian noise and iteratively refined using a combination of denoising steps and Jacobi decoding, accepting multiple tokens in parallel based on a probabilistic criterion. The primary result is a significant reduction in model forward passes; on the Emu3 model, SJD2 achieved a 5.62x step compression ratio on the COCO2017 dataset, reducing average steps from 8193 to 1461 while maintaining visual quality. For AI practitioners, the principal implication is that this fine-tuning strategy and decoding algorithm can be applied to large pre-trained autoregressive models to achieve substantial inference acceleration (over 2x latency reduction), reducing computational costs for production deployment. |
| ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual
  Recall (Read more on [arXiv](https://arxiv.org/abs/2510.07896) or [HuggingFace](https://huggingface.co/papers/2510.07896))| Jiaqi Tang, Shengen Wu, Songning Lai, Yuxuan Fan, Jiayu Yang | The paper introduces Attribution-Controlled Knowledge Editing (ACE), a framework that improves multi-hop factual recall in LLMs by identifying and modifying critical query-value neuron pathways. The primary research objective is to develop a knowledge editing (KE) method that effectively updates facts involving intermediate implicit subjects within a multi-hop reasoning chain, a scenario where existing methods fail. The key methodology involves using neuron-level attribution scores to first locate critical "query neurons" in middle-to-shallow layers that orchestrate information flow and "value neurons" in deeper layers that store factual content, then applying targeted edits to both. The proposed ACE method outperforms the state-of-the-art PMET on the MQuAKE-3K benchmark, achieving a 37.46 percentage point increase in multi-hop accuracy on the Qwen3-8B model. The principal implication for AI practitioners is that effective KE for complex reasoning requires not just editing the factual knowledge stores (value neurons) but also modifying the upstream activation mechanisms (query neurons) that control how that knowledge is accessed and chained together. |
| Temporal Prompting Matters: Rethinking Referring Video Object
  Segmentation (Read more on [arXiv](https://arxiv.org/abs/2510.07319) or [HuggingFace](https://huggingface.co/papers/2510.07319))| Sifei Liu, Chien-Yi Wang, I-Jieh Liu, Ci-Siang Lin, cmhungsteve | This paper introduces Tenet, a framework for efficiently adapting image-based foundation segmentation models to Referring Video Object Segmentation (RVOS) by leveraging temporal prompts. The research investigates how to effectively and efficiently exploit foundation segmentation models for RVOS, focusing on referring and video factors while deferring segmentation to foundation models. Tenet's methodology involves generating reference proposals and candidate temporal tracks using off-the-shelf detectors (Grounding DINO) and trackers (OC-SORT), then employing a Transformer-based Prompt Preference Learning module to select the best visual prompt for foundation segmentation models (e.g., SAM). Experiments show that prompting SAM with ground-truth boxes achieves an 83.6% J&F score, which is 15.6% higher than the MUTR RVOS method, and Tenet yields 65.5% J&F on Ref-YouTube-VOS and 71.0% J&F on Ref-DAVIS17 with approximately 45M trainable parameters. This implies AI practitioners can achieve high-quality RVOS by efficiently integrating existing foundation models via prompt engineering and selection, reducing the computational cost and data requirements of traditional end-to-end training. |
| Instant4D: 4D Gaussian Splatting in Minutes (Read more on [arXiv](https://arxiv.org/abs/2510.01119) or [HuggingFace](https://huggingface.co/papers/2510.01119))| Li Lu, Haoxi Ran, Zhanpeng Luo | INSTANT4D is a system for rapid 4D reconstruction of dynamic scenes from uncalibrated monocular video using a streamlined Gaussian Splatting representation. The primary objective is to accelerate the reconstruction of dynamic 3D scenes from casual video by overcoming slow optimization and complex parameter estimation, enabling high-quality 4D view synthesis in minutes. The key methodology involves using a deep visual SLAM model for initial geometric recovery of camera poses and depth, followed by a grid pruning strategy to reduce point cloud redundancy, and finally optimizing a simplified, isotropic, motion-aware 4D Gaussian representation. On the DyCheck benchmark, the method achieves an average PSNR of 24.52 dB in 7.2 minutes of training time, outperforming the concurrent RoDyGS method by 7.15 dB. The principal implication for AI practitioners is the ability to rapidly generate high-quality, dynamic 4D assets for AR/VR and immersive content from uncalibrated, monocular video, drastically reducing the computational cost and time from hours to minutes. |
| Better Together: Leveraging Unpaired Multimodal Data for Stronger
  Unimodal Models (Read more on [arXiv](https://arxiv.org/abs/2510.08492) or [HuggingFace](https://huggingface.co/papers/2510.08492))|  | This paper introduces UNPAIRED MULTIMODAL LEARNER (UML), a training paradigm that leverages unpaired multimodal data to enhance unimodal representations. The main objective is to determine if auxiliary unpaired data can directly improve representation learning in a target modality without explicit (x, y) correspondences. UML's key methodology involves sharing model parameters across different modalities, enabling the model to extract synergies from cross-modal structure. Empirically, UML consistently improved downstream performance, with one specific finding showing a 54.4% relative improvement in 1-shot audio classification on ImageNet-ESC-19 when using unpaired image and text data. The principal implication for AI practitioners is the ability to improve unimodal models, especially in data-scarce domains, by leveraging abundant, readily available unpaired multimodal data, thereby reducing the dependency on costly paired datasets. |
