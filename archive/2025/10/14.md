

## Papers for 2025-10-14

| Title | Authors | Summary |
|-------|---------|---------|
| QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning
  for LLMs (Read more on [arXiv](https://arxiv.org/abs/2510.11696) or [HuggingFace](https://huggingface.co/papers/2510.11696))|  | QeRL is a quantization-enhanced reinforcement learning framework that accelerates LLM training and improves reasoning performance by leveraging quantization noise for exploration. The research objective is to mitigate the high memory usage and slow rollout speeds inherent in RL fine-tuning of LLMs. The key methodology integrates NVFP4 quantization with LoRA and introduces Adaptive Quantization Noise (AQN), a mechanism that dynamically injects scheduled noise into model parameters to enhance policy exploration. On the GSM8K benchmark, a 7B model trained with QeRL achieves 90.8% accuracy, surpassing 16-bit LoRA (88.1%) and delivering up to a 1.7x end-to-end training speedup. The principal implication for AI practitioners is that quantization can be utilized not merely for efficiency but as a performance-enhancing tool in RL, enabling faster training of more capable models with significantly lower computational resources. |
| Diffusion Transformers with Representation Autoencoders (Read more on [arXiv](https://arxiv.org/abs/2510.11690) or [HuggingFace](https://huggingface.co/papers/2510.11690))|  | This paper introduces Representation Autoencoders (RAEs), which replace traditional VAEs in Diffusion Transformers (DiTs) with a frozen pretrained representation encoder and a trained lightweight decoder. The objective is to determine if high-dimensional, semantically rich latent spaces from encoders like DINOv2 can overcome the architectural and representational limitations of VAEs to improve generative modeling. The core methodology involves training a DiT on these RAE latents, adapting the model by matching its width to the token dimension, using a dimension-dependent noise schedule, and introducing a new `DiTDH` architecture with a wide, shallow head for efficient scaling. The RAE-based `DiTDH-XL` model achieves a state-of-the-art FID of 1.51 on ImageNet 256x256 without guidance and 1.13 with guidance, while also converging significantly faster than VAE-based models. The principal implication for AI practitioners is that RAEs should be considered the new default for DiT training, as they provide a more efficient, scalable, and higher-performing alternative to the commonly used VAE. |
| OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni
  MLLMs (Read more on [arXiv](https://arxiv.org/abs/2510.10689) or [HuggingFace](https://huggingface.co/papers/2510.10689))|  | The paper introduces OmniVideoBench, a benchmark for evaluating the synergistic audio-visual reasoning of Multimodal Large Language Models (MLLMs). The main objective is to assess how MLLMs integrate complementary information from both audio and visual modalities over long temporal sequences, a capability underdeveloped in existing benchmarks. The methodology involves the creation of a dataset with 1,000 high-quality question-answer pairs derived from 628 diverse videos, where each question is annotated with explicit step-by-step reasoning chains specifying the modality and evidence used. The primary result is that current MLLMs perform poorly, with the top model, Gemini-2.5-Pro, achieving only 58.90% accuracy, highlighting a significant gap between model and human performance. For AI practitioners, the principal implication is that current MLLMs have critical weaknesses in long-context, cross-modal reasoning, and this benchmark provides a diagnostic tool to guide the development of more robust audio-visual understanding systems. |
| Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by
  Refining Belief States (Read more on [arXiv](https://arxiv.org/abs/2510.11052) or [HuggingFace](https://huggingface.co/papers/2510.11052))|  | Latent Refinement Decoding enhances diffusion-based language models by improving accuracy and inference speed through a two-stage decoding process that refines beliefs in a continuous latent space. The primary objective is to mitigate information loss from hard masking and premature token commitment inherent in standard parallel decoding methods for diffusion models. The key methodology is a two-phase framework: first, a "Latent Refinement" stage iteratively updates soft embeddings as entropy-weighted mixtures of predicted tokens and the mask embedding to establish global coherence; second, a "Predictive Feedback Loop" progressively finalizes confident tokens while feeding back soft embeddings for uncertain positions, using KL-divergence dynamics for adaptive phase transition and early stopping. Experiments show that LRD improves accuracy on coding tasks like HumanEval by +6.3 points and on reasoning tasks like GSM8K by +2.9 points, while achieving inference speedups of up to 10.6x. The principal implication for AI practitioners is that LRD provides a versatile, drop-in decoding method for diffusion LLMs that simultaneously boosts generation quality and reduces latency, offering a practical solution for deploying efficient and accurate parallel sequence generation systems. |
| RLFR: Extending Reinforcement Learning for LLMs with Flow Environment (Read more on [arXiv](https://arxiv.org/abs/2510.10201) or [HuggingFace](https://huggingface.co/papers/2510.10201))| Zheming Liang, Dongzhou Cheng, Ruilin Li, Naishan Zheng, JingHaoZ | RLFR introduces a novel framework for shaping Reinforcement Learning with Verifiable Rewards (RLVR) by deriving dense reward signals from the velocity deviations of an LLM's latent states within a dynamically constructed flow field. The primary objective is to move beyond coarse, binary outcome rewards in RLVR by exploring the LLM's expressive latent space as a more nuanced and stable source for auxiliary reward signals to guide policy exploration in complex reasoning tasks. The methodology involves using Flow Matching to learn a continuous velocity field from the latent states of high-quality off-policy expert data and on-policy rejection samples; the deviation of the current policy's latent states from this learned flow is quantified to serve as a token-level flow reward that shapes the advantage function. Experiments show RLFR consistently improves performance, achieving a 1.5% average score increase over the RLVR baseline on language reasoning benchmarks with the Qwen2.5-Math-7B model and outperforming entropy-based shaping methods on multimodal tasks. The principal implication for AI practitioners is that an LLM's latent space is a highly underexplored but potent substrate for reward engineering; using flow-based metrics on latent states provides a robust mechanism to generate dense, context-aware rewards, offering a more stable alternative to logit-based signals for fine-tuning reasoning abilities. |
| Spotlight on Token Perception for Multimodal Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2510.09285) or [HuggingFace](https://huggingface.co/papers/2510.09285))| Zefeng He, Yun Luo, Yafu Li, Xiaoye Qu, Siyuan Huang | This paper introduces Visually-Perceptive Policy Optimization (VPPO), a policy gradient algorithm for multimodal reinforcement learning that enhances reasoning by focusing updates on visually-grounded tokens and trajectories. The primary objective is to address the limitation of existing RLVR frameworks that apply uniform learning signals, by developing an optimization strategy that explicitly incorporates token-level visual perception into the learning process. The core methodology involves first quantifying a token's visual dependency using the KL divergence between model outputs on original versus perturbed images, and then using this metric to reweight trajectory advantages and create a sparse gradient mask that targets only perceptually pivotal tokens. On eight reasoning benchmarks, VPPO achieves a 19.2 absolute percentage point increase in average accuracy for the 7B model over its base model, outperforming other leading RL-tuned methods. For AI practitioners, this provides an effective optimization strategy to integrate into LVLM training pipelines to improve visual grounding and reasoning performance by ensuring learning signals prioritize visually-dependent components of the model's output. |
| AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration (Read more on [arXiv](https://arxiv.org/abs/2510.10395) or [HuggingFace](https://huggingface.co/papers/2510.10395))| Weihong Lin, Yue Ding, DogNeverSleep, hjy, XinlongChen | This paper introduces AVoCaDO, an audiovisual video captioner designed to generate descriptions with strong temporal alignment between visual and auditory events. The primary objective is to improve video captioning by holistically integrating and reasoning over both audio and visual modalities, addressing the limitations of vision-centric or decoupled approaches. The methodology involves a two-stage post-training pipeline applied to the Qwen2.5-Omni model: (1) Supervised Fine-Tuning (SFT) on a new 107K dataset of temporally-aligned audiovisual captions, and (2) Group Relative Policy Optimization (GRPO) using custom rewards for temporal coherence, dialogue accuracy, and length regularization. AVoCaDO achieves state-of-the-art results among open-source models, notably scoring 73.2 on the UGC-VideoCap benchmark, outperforming concurrent models like video-SALMONN-2 (67.2) and the commercial Gemini-2.5-Flash (73.0). For AI practitioners, this work demonstrates that combining a high-quality, temporally-aligned SFT dataset with targeted reinforcement learning is a potent strategy for enhancing multimodal models' ability to generate accurate and contextually-grounded video captions. |
| DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training (Read more on [arXiv](https://arxiv.org/abs/2510.11712) or [HuggingFace](https://huggingface.co/papers/2510.11712))| Lu Qi, Bo Du, Xiangtai Li, Dizhe Zhang, fenghora | The paper presents DiT360, a Diffusion Transformer-based framework that generates high-fidelity panoramic images by employing a hybrid training strategy on both panoramic and perspective data. The primary objective is to address the poor geometric fidelity and photorealism in panoramic image generation, which the authors attribute to the scarcity of high-quality, large-scale panoramic training data. The core methodology involves a hybrid paradigm with regularization at two levels: at the image level, it refines existing panoramic data polar regions and incorporates perspective images for photorealistic guidance; at the post-VAE token level, it applies hybrid supervision via circular padding for boundary continuity, a rotation-consistent yaw loss, and a distortion-aware cube loss. The proposed method achieves state-of-the-art results on text-to-panorama generation tasks, demonstrating superior performance with a Fréchet Inception Distance (FID) of 42.88 and a BRISQUE score of 10.25, surpassing prior methods. For AI practitioners, the principal implication is the effectiveness of a hybrid data strategy; by combining limited, lower-quality in-domain data with abundant, high-quality out-of-domain data through domain transformation and multi-level supervision, generative model performance can be significantly enhanced in data-scarce scenarios. |
| Demystifying Reinforcement Learning in Agentic Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.11701) or [HuggingFace](https://huggingface.co/papers/2510.11701))| Mengdi Wang, Shuicheng Yan, Jiaru Zou, Ling Yang, Zhaochen Yu | This research systematically investigates data curation, algorithmic design, and reasoning modes to establish practical recipes for optimizing agentic LLMs via reinforcement learning. The primary objective is to demystify the core principles of agentic RL through an empirical study that fine-tunes Qwen models with GRPO variants, comparing different data strategies (e.g., real vs. synthetic trajectories) and analyzing algorithmic impacts on training dynamics like policy entropy and tool-call frequency. The study finds that a "deliberative" reasoning mode with fewer, more accurate tool calls is superior and that optimized practices enable their 4B parameter model, DemyAgent-4B, to achieve 70.0% on the AIME2025 benchmark, surpassing a 32B parameter model. The principal implication for AI practitioners is that building high-performing agents relies more on curating high-quality, real end-to-end trajectory data and implementing simple, exploration-friendly RL techniques (e.g., "clip higher," overlong reward shaping) than on model scale alone. |
| Making Mathematical Reasoning Adaptive (Read more on [arXiv](https://arxiv.org/abs/2510.04617) or [HuggingFace](https://huggingface.co/papers/2510.04617))| Jiahuan Li, Yang Bai, Zhijun Wang, Xiang Geng, DreamW1ngs | This paper introduces AdaR, a framework for making large language model (LLM) mathematical reasoning adaptive by training them to rely on problem-solving logic rather than superficial features. The main objective is to address LLM failures in robustness and generalization, which the authors attribute to spurious reasoning, by enabling models to adapt to varying numerical values within a consistent logical structure. The key methodology involves synthesizing logically equivalent query-answer pairs through controllable perturbation of variable values, generating gold answers via code execution, and then training the model with Reinforcement Learning with Verifiable Rewards (RLVR) to penalize incorrect answers on this new data. Experimental results demonstrate that AdaR achieves substantial improvements, with the Qwen2.5-MATH-7B model showing an average gain of +8.50 points across in-domain and out-of-domain benchmarks with only 9K synthetic data. For AI practitioners, AdaR provides a highly data-efficient, automated method to generate high-quality training data that improves the fundamental reasoning, robustness, and generalization capabilities of LLMs. |
| Building a Foundational Guardrail for General Agentic Systems via
  Synthetic Data (Read more on [arXiv](https://arxiv.org/abs/2510.09781) or [HuggingFace](https://huggingface.co/papers/2510.09781))| Manish Nagireddy, Pengcheng Jing, Yujun Zhou, Yue Huang, hhua2 | This paper introduces a framework for pre-execution safety in LLM agents, featuring a synthetic data engine (AuraGen), a foundational guardrail model (Safiron), and an evaluation benchmark (Pre-Exec Bench). The objective is to address critical data, model, and evaluation gaps by creating a guardrail that can proactively detect, categorize, and explain risks in an agent's plan before any actions are executed. The methodology involves using AuraGen to synthesize diverse, labeled risky agent trajectories and training the Safiron model via a two-stage process of Supervised Fine-Tuning (SFT) followed by Group Relative Policy Optimization (GRPO). The proposed Safiron model significantly surpasses proprietary and open-weight baselines, achieving a classification accuracy of 0.949 and harmful detection precision of 0.973, compared to 0.606 and 0.822 for GPT-4o, respectively. For AI practitioners, this work provides a practical template demonstrating that a smaller, specialized guardian model trained on high-quality synthetic data is more effective for interpretable pre-execution safety than relying on general-purpose LLMs. |
| InternSVG: Towards Unified SVG Tasks with Multimodal Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2510.11341) or [HuggingFace](https://huggingface.co/papers/2510.11341))|  | The paper introduces the InternSVG family, a comprehensive data-benchmark-model suite for unified Scalable Vector Graphics (SVG) understanding, editing, and generation using Multimodal Large Language Models (MLLMs). The research objective is to address the challenges of fragmented datasets and limited model transferability by creating a single, generalist model for diverse SVG tasks. The methodology involves creating SAgoge, a large-scale (16M+ samples) multimodal SVG dataset; SArena, a standardized evaluation benchmark; and the InternSVG model, a unified MLLM featuring SVG-specific tokenization and a two-stage curriculum training strategy. The InternSVG model significantly outperforms existing methods, achieving an 8-point higher overall accuracy in understanding tasks on the SArena-Icon benchmark compared to the strongest proprietary baseline, Claude-Sonnet-4. For AI practitioners, this work provides a unified framework and a pretrained model that can replace multiple specialized tools, streamlining the development of applications that require automated generation, editing, or interpretation of complex vector graphics. |
| ACADREASON: Exploring the Limits of Reasoning Models with Academic
  Research Problems (Read more on [arXiv](https://arxiv.org/abs/2510.11652) or [HuggingFace](https://huggingface.co/papers/2510.11652))|  | This paper introduces ACADREASON, a new benchmark derived from 50 recent, high-level theoretical papers across five domains, designed to evaluate the limits of advanced reasoning in LLMs and agentic systems. The primary objective is to assess model capabilities on problems requiring both cutting-edge knowledge and deep, multi-step reasoning, addressing gaps in existing benchmarks. The methodology involves expert extraction of research questions, golden answers, and dynamic checklists, with performance assessed by an LLM-as-Judge using Pass Rate and Checklist Score metrics. The benchmark proves highly challenging; the top-performing base model, GPT-5, scored only a 16.0 Pass Rate, while the best agent framework, OAgents, achieved a significantly higher 34.0 Pass Rate. For AI practitioners, this work demonstrates that current LLMs are deficient in academic-level reasoning and highlights that agentic systems, which can perform autonomous information retrieval and leverage methodological hints, represent a more promising architecture for tackling complex, knowledge-intensive tasks. |
| FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark
  for Evaluating LLMs (Read more on [arXiv](https://arxiv.org/abs/2510.08886) or [HuggingFace](https://huggingface.co/papers/2510.08886))|  | The paper introduces FINAUDITING, the first taxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs on financial auditing tasks. The objective is to assess LLM capabilities in reasoning over structured, interdependent, and taxonomy-driven financial documents by checking for semantic, relational, and numerical inconsistencies. The methodology involves creating three subtasks (FinSM, FinRE, FinMR) from real US-GAAP XBRL filings and conducting zero-shot experiments on 13 state-of-the-art LLMs. The primary result is that current models perform inconsistently, with accuracy drops of up to 60-90% when reasoning over hierarchical multi-document structures. The principal implication for AI practitioners is that modern LLMs have systematic limitations in taxonomy-grounded financial reasoning, establishing the need to develop more trustworthy and structure-aware models for regulation-aligned systems. |
| GIR-Bench: Versatile Benchmark for Generating Images with Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.11026) or [HuggingFace](https://huggingface.co/papers/2510.11026))|  | The paper introduces GIR-Bench, a comprehensive benchmark designed to evaluate the reasoning and generation alignment of unified multimodal models across understanding, generation, and editing tasks. The primary objective is to systematically investigate whether these models can consistently apply knowledge and reasoning across both understanding and generation modalities, thereby quantifying the gap between these capabilities. GIR-Bench employs a methodology based on three distinct components—Understanding-Generation Consistency (UGC), reasoning-centric Text-to-Image (T2I), and reasoning-based Editing—which are evaluated using task-specific, fine-grained metrics like object detection and IoU to avoid the biases of the MLLM-as-a-Judge paradigm. Results demonstrate a significant and persistent gap: while top models achieve near-perfect understanding scores on the UGC task (e.g., Gemini-2.5-Flash at 0.997 accuracy), their generation performance for the same entities from implicit prompts is substantially lower (e.g., GPT-Image-1 at 0.689 overall). The principal implication for AI practitioners is that enhancing a model's understanding capabilities does not automatically translate to improved reasoning-based generation, highlighting that the critical bottleneck lies in the mechanism for transferring reasoned constraints into the generative process, which requires dedicated architectural and training focus. |
| AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning
  in 4D Scenes (Read more on [arXiv](https://arxiv.org/abs/2510.10670) or [HuggingFace](https://huggingface.co/papers/2510.10670))|  | AdaViewPlanner adapts pre-trained text-to-video (T2V) diffusion models to automatically generate cinematic camera trajectories for given 4D scenes based on text prompts. The main objective is to repurpose the implicit cinematographic knowledge of large-scale T2V models for automated, text-guided viewpoint planning in 4D environments. The methodology is a two-stage paradigm: first, an adaptive learning branch injects 4D motion into a T2V model to generate a video with an implicit camera path; second, a multi-modal diffusion branch explicitly extracts camera extrinsic parameters by denoising them, conditioned on the generated video and the original 4D motion. The proposed method significantly outperforms existing baselines, achieving a user preference rate of 61.90% on a standard testset, compared to 23.81% for the next best competitor. The principal implication for AI practitioners is that this framework provides a viable method for adapting foundational video generation models for specialized downstream tasks like virtual cinematography, enabling the use of powerful priors from large-scale pre-training instead of building bespoke models from scratch. |
| Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.11027) or [HuggingFace](https://huggingface.co/papers/2510.11027))|  | Vlaser is a foundational vision-language-action model developed to bridge the gap between upstream reasoning and downstream policy learning by identifying which pretraining data most effectively improves robot control. The research objective is to construct a VLM with strong embodied reasoning and systematically analyze how different data streams affect its transfer to low-level robotic manipulation tasks. The methodology involves fine-tuning an InternVL3 backbone on the novel Vlaser-6M dataset—covering embodied grounding, planning, and QA—and integrating a flow-matching-based action expert for control, with evaluations conducted in the SimplerEnv simulator. The primary result is that while the full Vlaser model excels on reasoning benchmarks, fine-tuning on in-domain QA data proves most effective for downstream control, improving the average success rate on WidowX tasks to 63.2% from a 55.8% baseline. The principal implication for AI practitioners is that enhancing robot control requires prioritizing in-domain data curated from the target robot's perspective over improving performance on general, out-of-domain reasoning benchmarks, due to a significant domain shift. |
| BrowserAgent: Building Web Agents with Human-Inspired Web Browsing
  Actions (Read more on [arXiv](https://arxiv.org/abs/2510.10666) or [HuggingFace](https://huggingface.co/papers/2510.10666))|  | The paper introduces BrowserAgent, a web agent that directly interacts with web pages using human-like browser actions, trained with a two-stage Supervised and Rejection Fine-Tuning methodology. The main objective is to develop a scalable and interactive web agent that operates on raw web content via atomic browser operations (e.g., click, scroll, type), eliminating the reliance on costly external text parsing and summarization tools. The agent is built on Playwright for direct browser automation and is trained in two stages: Supervised Fine-Tuning (SFT) on expert trajectories, followed by Rejection Fine-Tuning (RFT) where the model is refined on its own high-quality generated outputs, complemented by an explicit memory mechanism for long-horizon tasks. The primary result is that BrowserAgent-7B achieves approximately 20% improvement over the Search-R1 baseline on multi-hop question-answering tasks such as HotpotQA, 2Wiki, and Bamboogle. The principal implication for AI practitioners is the provision of a practical framework for building more capable web agents by learning directly from browser interactions, offering a reproducible SFT+RFT training pipeline and a scalable architecture to handle complex tasks in dynamic web environments. |
| Don't Just Fine-tune the Agent, Tune the Environment (Read more on [arXiv](https://arxiv.org/abs/2510.10197) or [HuggingFace](https://huggingface.co/papers/2510.10197))|  | This paper introduces ENVIRONMENT TUNING, a paradigm that addresses the challenge of training robust, multi-turn tool-using agents under extreme data scarcity by shifting from static trajectory fine-tuning to dynamic, environment-based exploration. The core methodology involves a four-stage structured curriculum, actionable environment augmentation that provides corrective feedback on failure, and fine-grained progress rewards to enable stable and efficient learning directly from problem instances. On the BFCL benchmark, using only 400 training samples, the proposed method boosts the `watt-tool-8B` model's average performance by 18.50% and nearly doubles the out-of-distribution score of the `ToolACE-2` model on ACEBench from 8.34% to 15.00%, significantly outperforming SFT baselines. For AI practitioners, this work demonstrates that engineering the training environment to provide structured, informative feedback is a more data-efficient and effective strategy for developing generalizable agents than curating large-scale datasets for supervised fine-tuning. |
| SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.09541) or [HuggingFace](https://huggingface.co/papers/2510.09541))|  | The paper introduces Sandwiched Policy Gradient (SPG), a reinforcement learning algorithm to align masked diffusion language models by using both an upper and lower bound of the intractable log-likelihood. The objective is to develop a less biased policy gradient method for diffusion language models (dLLMs) that can effectively learn from both positive and negative rewards, which is challenging due to the intractable log-likelihood. The methodology maximizes a tractable Evidence Lower Bound (ELBO) for positive-reward sequences while minimizing a tractable Evidence Upper Bound (EUBO), derived from the Rényi variational bound, for negative-reward sequences, and employs a block-wise masking strategy for stable estimation. SPG significantly outperforms baselines on reasoning tasks, improving accuracy over prior state-of-the-art RL methods for dLLMs by 27.0% on Sudoku and 3.6% on GSM8K. For AI practitioners, SPG offers a more robust and principled method for applying reinforcement learning to dLLMs, overcoming the limitations of ELBO-only approximations and enabling more effective alignment with complex, reward-driven tasks. |
| CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven
  Images (Read more on [arXiv](https://arxiv.org/abs/2510.11718) or [HuggingFace](https://huggingface.co/papers/2510.11718))|  | This paper introduces CodePlot-CoT, a paradigm where Vision Language Models (VLMs) solve complex math problems by generating executable code for plotting images as intermediate "visual thoughts." The core objective is to overcome the limitations of text-only reasoning chains in VLMs for problems requiring visual assistance, such as constructing auxiliary lines in geometry. The methodology involves a code-driven Chain-of-Thought (CoT) where the VLM alternates between natural language reasoning and generating executable Python plotting code, which is then rendered into an image and fed back into the model to inform subsequent steps; this process is enabled by a new large-scale dataset, Math-VR, and a specialized image-to-code converter, MatplotCode. Experimental results show that the CodePlot-CoT model achieves up to a 21% absolute increase in Answer Correctness over its baseline model on the Math-VR benchmark. For AI practitioners, the principal implication is that for tasks requiring precise, structured visual reasoning, fine-tuning models to generate executable code for visualizations is a more effective and controllable strategy than relying on direct, often imprecise, pixel-level image generation. |
| DocReward: A Document Reward Model for Structuring and Stylizing (Read more on [arXiv](https://arxiv.org/abs/2510.11391) or [HuggingFace](https://huggingface.co/papers/2510.11391))|  | The paper introduces DOCREWARD, a reward model that evaluates document professionalism based on visual structure and style, independent of textual content. The objective is to create a reward model that can guide agentic workflows to generate more professionally structured and stylized documents, a capability existing models lack. The methodology involves training a Qwen-2.5-VL model on DoCPAIR, a new dataset of 117K document pairs with identical text but differing professionalism, using a Bradley-Terry loss function on rendered document images. On a human-annotated test set, DOCREWARD achieves 89.2% human preference accuracy, outperforming GPT-5 by 19.4 percentage points. For AI practitioners, this provides a computable reward signal to automate the optimization of document layout and style in generation agents, moving beyond simple textual quality metrics to align with human aesthetic and structural preferences. |
| On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in
  Large Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.09008) or [HuggingFace](https://huggingface.co/papers/2510.09008))|  | This paper mitigates object hallucination in Large Vision-Language Models (LVLMs) by identifying and masking uncertain visual tokens within the vision encoder using adversarial perturbations. The research objective is to establish and exploit the correlation between the epistemic uncertainty of visual tokens and the occurrence of object hallucinations to develop a training-free mitigation strategy. The methodology involves using Projected Gradient Descent (PGD) to create adversarial perturbations that maximize feature deviation in early vision encoder layers, which serves as a proxy for epistemic uncertainty, and then masking these identified uncertain tokens during self-attention in the vision encoder's intermediate layers. The proposed method significantly reduces hallucinations, lowering the sentence-level CHAIRs score on LLaVA-1.5-7B from 47.4 to 29.2, and is shown to be compatible with other prior art. For AI practitioners, this presents a computationally efficient, inference-only technique to improve LVLM reliability by modifying only the vision encoder, offering a complementary approach to existing language-model-focused mitigation strategies. |
| High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic
  Manipulation Learning with Gaussian Splatting (Read more on [arXiv](https://arxiv.org/abs/2510.10637) or [HuggingFace](https://huggingface.co/papers/2510.10637))|  | This paper presents RoboSimGS, a framework for generating high-fidelity, physically interactive simulated data from real-world scenes to enable zero-shot Sim2Real transfer for robotic manipulation. The primary objective is to overcome the data collection bottleneck in robotics by automatically converting multi-view images into scalable, realistic simulation environments. The key methodology involves a hybrid scene representation combining 3D Gaussian Splatting for static backgrounds with mesh primitives for interactive objects, and uniquely uses a Multi-modal Large Language Model (MLLM) to automatically infer objects' physical properties and kinematic structures. The framework achieves successful zero-shot real-world deployment, and augmenting 50 real demonstrations with 50 synthetic ones improved success rates on tasks like "Upright Bottle" from 0.86 to 0.91. For AI practitioners, this research provides a scalable pipeline to generate vast amounts of high-fidelity synthetic data, reducing the need for expensive real-world data collection and significantly improving the performance and generalization of visuomotor policies. |
| Skill-Targeted Adaptive Training (Read more on [arXiv](https://arxiv.org/abs/2510.10023) or [HuggingFace](https://huggingface.co/papers/2510.10023))|  | The paper introduces STAT, a fine-tuning strategy where a teacher LLM diagnoses a student model's specific skill deficiencies and creates a targeted training curriculum by selecting or synthesizing relevant data to address those gaps. The main objective is to develop a fine-tuning method that overcomes the performance saturation observed when training language models with vanilla supervised fine-tuning (SFT) on domain-specific datasets. The key methodology is a three-stage pipeline: a reward model identifies difficult questions for a student model, a stronger teacher LLM analyzes the student's responses to generate a `Missing-Skill-Profile`, and this profile is used to either re-weight existing training data (STAT-Sel) or synthesize new, targeted examples (STAT-Syn). The primary results show that on the MATH benchmark, STAT-Sel improved the Llama-3.2-3B-Instruct model's accuracy by 7.5% (from 44.0% to 51.5%), while also enhancing out-of-distribution performance by an average of 4.6%. The principal implication for AI practitioners is that this metacognition-driven approach provides a more efficient method than standard SFT to overcome performance plateaus in complex reasoning domains by using a powerful teacher model to automate the diagnosis of weaknesses and the generation of a corrective curriculum for a smaller student model. |
| ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web
  Coding (Read more on [arXiv](https://arxiv.org/abs/2510.11498) or [HuggingFace](https://huggingface.co/papers/2510.11498))|  | ReLook is a vision-grounded reinforcement learning framework that employs a multimodal LLM (MLLM) as a critic within an agentic generate-diagnose-refine loop to improve front-end code generation. The research objective is to enhance the visual and interactive fidelity of LLM-generated web code by enabling an agent to perceive rendered outputs and iteratively refine them based on visual feedback. The key methodology involves an agentic RL framework where a policy LLM invokes an MLLM critic to score code based on rendered screenshots; training is stabilized by a "Forced Optimization" mechanism that only accepts strictly improving code revisions to prevent behavioral collapse. On the ArtifactsBench-Lite benchmark, the ReLook-enhanced Qwen2.5-7B model achieved a VisualScore of 27.88, significantly outperforming the 21.59 score of the base model. The principal implication for AI practitioners is that this framework provides a practical method for training agents on perceptual tasks by integrating a powerful MLLM critic into the training loop, which can then be decoupled at inference to enable a fast, critic-free self-edit cycle that retains most accuracy gains. |
| PEAR: Phase Entropy Aware Reward for Efficient Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.08026) or [HuggingFace](https://huggingface.co/papers/2510.08026))|  | PEAR (Phase Entropy Aware Reward) is a reward mechanism that leverages phase-dependent entropy to train Large Reasoning Models (LRMs) for more efficient reasoning trace generation. The main research objective is to control LRM response length to reduce inference cost and verbosity without sacrificing problem-solving accuracy. The key methodology involves integrating a reward function into the Group Relative Policy Optimization (GRPO) framework that penalizes high entropy during the exploratory "thinking phase" and permits moderate entropy in the "final answer phase." The primary result is a substantial reduction in response length, ranging from 37.8% to 59.4% across different models and benchmarks, with a corresponding accuracy drop of less than 1%. For AI practitioners, the principal implication is a method to fine-tune LRMs for lower inference cost and latency by intrinsically promoting shorter reasoning chains, eliminating the need for curated concise datasets or rigid length constraints. |
| Self-Improving LLM Agents at Test-Time (Read more on [arXiv](https://arxiv.org/abs/2510.07841) or [HuggingFace](https://huggingface.co/papers/2510.07841))| Gokhan Tur, Dilek Hakkani-Tür, Heng Ji, Cheng Qian, emrecanacikgoz | The paper presents Test-Time Self-Improvement (TT-SI), a framework for language models to improve their performance on-the-fly during inference. The main objective is to create a more effective and generalizable agentic LM by enabling it to dynamically adapt to challenging test instances, thereby avoiding the costs and inefficiencies of large-scale inductive fine-tuning. The methodology consists of a three-step process: (1) Self-Awareness, where an uncertainty estimator identifies difficult test samples; (2) Self-Data Augmentation, where the model generates a new, similar training instance from the uncertain sample; and (3) Self-Improvement, where a temporary, lightweight fine-tuning update (LoRA) is performed on this new instance. Empirical evaluations show that TT-SI improves performance by an average of +5.48% absolute accuracy across four agent benchmarks, and on the SealTool benchmark, it outperforms standard supervised fine-tuning while using 68 times fewer training samples. For AI practitioners, the principal implication is that model performance on difficult or out-of-distribution tasks can be significantly and efficiently improved at inference time with minimal data and compute, offering a practical alternative to costly, full-scale retraining cycles. |
| FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging
  with Diffusion Decoding (Read more on [arXiv](https://arxiv.org/abs/2510.10868) or [HuggingFace](https://huggingface.co/papers/2510.10868))|  | FastHMR is a framework that accelerates transformer-based 3D Human Mesh Recovery (HMR) by merging redundant layers and tokens and using a diffusion decoder to restore accuracy. The research aims to reduce the high computational cost of transformer-based HMR models for real-time applications without performance degradation. The methodology combines Error-Constrained Layer Merging (ECLM) to fuse layers with minimal impact on Mean Per Joint Position Error (MPJPE), Mask-guided Token Merging (Mask-ToMe) to prune background tokens, and a diffusion-based decoder that leverages a motion VAE's latent space to recover accuracy. The method achieves up to a 2.3x speed-up over its baseline, improving throughput on the HMR2.0 model to 150.0 fps while slightly reducing estimation error on the EMDB benchmark. For AI practitioners, this paper provides a practical post-hoc framework for accelerating inference in existing transformer models by applying aggressive compression and compensating for the accuracy loss with a specialized, prior-informed generative decoder. |
| The Personalization Trap: How User Memory Alters Emotional Reasoning in
  LLMs (Read more on [arXiv](https://arxiv.org/abs/2510.09905) or [HuggingFace](https://huggingface.co/papers/2510.09905))|  | This research demonstrates that incorporating user memory into Large Language Models (LLMs) systematically introduces social biases into their emotional reasoning. The objective was to quantify how user profiles alter LLM performance on emotional intelligence tests and identify emergent biases in emotional understanding and guidance. The study evaluated 15 LLMs on the Situational Test of Emotional Understanding (STEU) and an adapted Situational Test of Emotion Management (STEM) by injecting either explicit "advantaged" vs. "disadvantaged" or intersectional demographic personas via system prompts. The primary result is that user memory consistently degrades performance while favoring privileged profiles; for example, Claude 3.7 Sonnet's accuracy was 80.10% for advantaged profiles but dropped to 77.37% for disadvantaged profiles, with biases persisting across demographic axes like gender and religion. The principal implication for AI practitioners is that personalization mechanisms designed to enhance empathy can embed societal hierarchies, requiring new approaches to balance adaptive capabilities with equitable performance across diverse user populations. |
| Stable Video Infinity: Infinite-Length Video Generation with Error
  Recycling (Read more on [arXiv](https://arxiv.org/abs/2510.09212) or [HuggingFace](https://huggingface.co/papers/2510.09212))|  | Stable Video Infinity (SVI) enables infinite-length video generation by fine-tuning a Diffusion Transformer to actively correct its own compounding errors through a novel error-recycling mechanism. The research objective is to resolve the training-test hypothesis gap in autoregressive video models, where models trained on clean data degrade when conditioned on their own error-prone outputs during inference. The key methodology is Error-Recycling Fine-Tuning (ERFT), a closed-loop process where a model's self-generated predictive errors are collected, stored in a replay memory, and then injected back into clean training data to teach the model to predict an error-recycled velocity. SVI significantly outperforms existing methods on long-video tasks; in the 250-second ultra-long consistent video benchmark, SVI-Shot achieved 97.50% subject consistency, exhibiting only a 0.63% performance drop from shorter videos, while the FramePack baseline degraded by 13.71%. The principal implication for AI practitioners is that the error-recycling paradigm can be adapted to improve the stability and coherence of other autoregressive systems, such as LLMs, by fine-tuning them on their own generated outputs to mitigate compounding errors in long-sequence generation. |
| InfiniHuman: Infinite 3D Human Creation with Precise Control (Read more on [arXiv](https://arxiv.org/abs/2510.11650) or [HuggingFace](https://huggingface.co/papers/2510.11650))| Gerard Pons-Moll, Margaret Kostyrko, Xianghui Xie, Yuxuan Xue | This paper presents InfiniHuman, a framework that automatically generates a large-scale, richly annotated 3D human dataset by distilling foundation models, and then trains a generative model for high-fidelity avatar creation. The primary objective is to overcome the data acquisition bottleneck by programmatically generating a theoretically unbounded dataset of 3D humans with multi-modal annotations. The methodology consists of two stages: first, an automated data generation pipeline, InfiniHumanData, uses a cascade of vision-language and diffusion models to create 111K diverse identities with text, SMPL, and clothing annotations; second, a generative model, InfiniHumanGen, is trained on this data to synthesize 3D avatars conditioned on text, body shape, and clothing images. Extensive experiments show that the high-resolution model, Gen-HRes, achieves a 92.39% user preference for visual quality compared to state-of-the-art methods and generates avatars at least 8 times faster than comparable high-resolution baselines. For AI practitioners, this work provides a publicly available, large-scale synthetic dataset and a powerful generative pipeline that democratizes the creation of controllable, high-quality 3D human avatars for applications in VR, gaming, and simulation without requiring expensive scan data. |
| HUME: Measuring the Human-Model Performance Gap in Text Embedding Task (Read more on [arXiv](https://arxiv.org/abs/2510.10062) or [HuggingFace](https://huggingface.co/papers/2510.10062))|  | The paper introduces HUME, a framework for measuring human performance on text embedding tasks to contextualize model evaluation. The primary objective is to quantify the performance gap between humans and embedding models across diverse tasks from the MTEB benchmark to assess both model capabilities and benchmark quality. The methodology involves measuring human performance on 16 datasets spanning reranking, classification, clustering, and semantic textual similarity (STS), and comparing these scores against 13 embedding models using identical metrics. The primary result is that humans achieve an average performance of 77.6%, ranking 4th overall compared to the best model's 80.1%, with findings indicating that "superhuman" model performance often occurs on tasks with low inter-annotator agreement, such as emotion classification (κ = 0.39). For AI practitioners, the principal implication is that model performance on benchmarks must be interpreted in the context of human agreement; high scores on low-agreement tasks may indicate exploitation of labeling artifacts rather than genuine semantic understanding, making high-agreement tasks more reliable for model selection. |
| LLaMAX2: Your Translation-Enhanced Model also Performs Well in Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.09189) or [HuggingFace](https://huggingface.co/papers/2510.09189))| Lei Li, Shujian Huang, Jingyang Gong, Zixian Huang, Changjiang Gao | This paper introduces Qwen3-XPlus, a series of translation-enhanced models that maintain strong reasoning capabilities by applying a novel layer-selective tuning recipe to existing instruct models. The main research objective is to enhance an LLM's translation performance, especially in low-resource languages, without the typical catastrophic forgetting of its inherent reasoning skills. The key methodology is a two-stage tuning process that trains only the bottom four and top fifteen transformer layers on a small, curated parallel dataset of 0.8B tokens, keeping the middle layers frozen. This approach results in significant translation gains, achieving over a 40+ xComet point increase in low-resource languages like Swahili, while maintaining reasoning performance on par with the original Qwen3 instruct model across 15 benchmarks. The principal implication for AI practitioners is that targeted, parameter-efficient tuning of specific layers on small, high-quality datasets is an effective strategy to specialize instruct models for new tasks without needing to retrain from a base model or suffering a loss of general capabilities. |
| From Data to Rewards: a Bilevel Optimization Perspective on Maximum
  Likelihood Estimation (Read more on [arXiv](https://arxiv.org/abs/2510.07624) or [HuggingFace](https://huggingface.co/papers/2510.07624))| Giuseppe Paolo, Youssef Attia El Hili, Gabriel Singer, corentinlger, abenechehab | This paper reframes Maximum Likelihood Estimation (MLE) as a bilevel optimization problem to learn implicit rewards for training generative models with policy gradient methods. The objective is to determine if a reward function can be learned from unlabeled data to train models more effectively than with standard MLE. The methodology consists of a bilevel framework where an outer loop optimizes a reward function to maximize data likelihood, while an inner loop uses this reward in a policy gradient objective to train the model parameters, solved practically using implicit differentiation. On the Poker tabular classification dataset, the proposed heuristic method achieved an accuracy of 52.4%, outperforming the 48.6% accuracy of the NLL baseline. For AI practitioners, this work provides a principled way to leverage policy gradient optimization using only a high-quality dataset, offering a potential alternative to MLE that may yield performance improvements without needing an explicit reward function. |
| LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion
  Models via Likelihood Preference (Read more on [arXiv](https://arxiv.org/abs/2510.11512) or [HuggingFace](https://huggingface.co/papers/2510.11512))| Ivan Laptev, Lars Kunze, Francesco Pinto, Fabio Pizzati, Jianhao Yuan | This paper introduces LikePhys, a training-free evaluation method that quantifies intuitive physics understanding in video diffusion models by measuring their preference for assigning higher likelihood to physically valid videos over invalid ones. The primary objective is to develop a grounded metric for physical reasoning, which is achieved by using a benchmark of controlled valid/invalid video pairs and calculating a Plausibility Preference Error (PPE) based on the model's denoising loss as a likelihood surrogate. The study benchmarks twelve models and finds that recent DiT-based architectures like Hunyuan T2V (43.6% PPE) significantly outperform UNet-based models like AnimateDiff (60.8% PPE), with the PPE metric showing a strong Kendall's τ correlation of 0.44 with human preference. For AI practitioners, LikePhys provides a quantitative, zero-shot method to assess and select models for physical realism, with findings indicating that physics understanding improves with model and data scale and is largely insensitive to classifier-free guidance strength at inference time. |
| RePro: Training Language Models to Faithfully Recycle the Web for
  Pretraining (Read more on [arXiv](https://arxiv.org/abs/2510.10681) or [HuggingFace](https://huggingface.co/papers/2510.10681))|  | REPRO introduces a reinforcement learning method to train a small language model to faithfully rephrase web data for pretraining. The research aims to develop an efficient and controllable method to "recycle" web data to augment the supply of high-quality pretraining corpora, addressing data scarcity for LLMs. The methodology involves training a 4B parameter rephraser model using reinforcement learning (GRPO) with a custom reward function that balances data quality (DataMan score) and faithfulness (semantic, structural, and length preservation). Primary results show that pretraining models on REPRO-recycled data yields a 4.7%-14.0% relative accuracy gain on 22 downstream tasks compared to an organic-only baseline, and improves organic data efficiency by 2-3x. The principal implication for AI practitioners is that a relatively small, specially-trained model can be used to cost-effectively expand high-quality training datasets, providing a practical alternative to the prohibitively expensive prompting of large-scale models for data generation. |
| Graph Diffusion Transformers are In-Context Molecular Designers (Read more on [arXiv](https://arxiv.org/abs/2510.08744) or [HuggingFace](https://huggingface.co/papers/2510.08744))| Tengfei Luo, Michael Sun, Yihan Zhu, Jie Chen, Gang Liu | This paper presents DemoDiff, a 0.7B parameter Graph Diffusion Transformer that performs in-context molecular design guided by molecule-score demonstrations. The primary objective is to enable effective in-context learning for molecular design by using a small set of molecule-score examples to define a design task, overcoming the limitations of both large language models and data-intensive specialized methods. The methodology involves a demonstration-conditioned diffusion model (DemoDiff) and a novel Node Pair Encoding (NPE) tokenizer that creates efficient motif-level molecular representations, reducing node count by an average of 5.5x. Across 33 design tasks, DemoDiff achieves a superior average rank of 3.63, outperforming specialized molecular optimization methods (average ranks 5.25–10.20) and matching or surpassing language models 100-1000x its size. For AI practitioners, this work provides a framework for building foundation models in scientific domains by conditioning diffusion processes on structured, domain-specific examples instead of natural language, enabling few-shot adaptation without task-specific fine-tuning. |
| VER: Vision Expert Transformer for Robot Learning via Foundation
  Distillation and Dynamic Routing (Read more on [arXiv](https://arxiv.org/abs/2510.05213) or [HuggingFace](https://huggingface.co/papers/2510.05213))|  | The paper presents VER, a Vision Expert Transformer that distills knowledge from multiple vision foundation models (VFMs) into a modular expert library and uses dynamic routing for robot learning. The main objective is to overcome the limitations of single or statically-fused VFMs by enabling flexible, task-specific selection of visual representations for visuomotor policies. The methodology involves a two-stage process: first, pretraining a Mixture-of-Experts (MoE) vision backbone by distilling features from multiple teacher VFMs (DINOv2, ViT, CLIP); second, freezing the experts and fine-tuning only a lightweight router (<0.4% of parameters) that dynamically selects experts for specific downstream robot tasks. VER achieves state-of-the-art performance, attaining a 74.7% average success rate across 11 diverse manipulation benchmarks, outperforming prior methods. The principal implication for AI practitioners is that distilling heterogeneous foundation models into a specialized expert library coupled with a lightweight, fine-tunable router provides a parameter-efficient and scalable approach to adapt large pre-trained models for diverse and complex robotic tasks. |
| Are Large Reasoning Models Interruptible? (Read more on [arXiv](https://arxiv.org/abs/2510.11713) or [HuggingFace](https://huggingface.co/papers/2510.11713))| Narges Norouzi, Trevor Darrell, David M. Chan, Mihran Miroyan, tsunghanwu | This paper evaluates the robustness of Large Reasoning Models (LRMs) in dynamic, non-static environments, revealing significant performance degradation not captured by traditional benchmarks. The research investigates how LRMs perform under two realistic scenarios: time-constrained interruptions that limit the reasoning budget and update-driven interruptions that modify the problem mid-inference. The methodology involves creating a new evaluation suite for math and programming tasks where interruptions are injected at relative points in the model's reasoning trace to assess the quality of partial outputs and adaptation to new information. The primary result is that even state-of-the-art LRMs exhibit critical failures, with performance dropping by up to 60% when updates are introduced late in the reasoning process, and the paper identifies novel failure modes including "reasoning leakage," "panic," and "self-doubt." For AI practitioners, the principal implication is that LRM robustness in interactive applications cannot be inferred from static benchmark performance; interruptibility and dynamic adaptation must be treated as capabilities that require explicit evaluation and design, as they are not inherent properties of current models. |
| IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing
  Assessment (Read more on [arXiv](https://arxiv.org/abs/2510.11647) or [HuggingFace](https://huggingface.co/papers/2510.11647))| Zhucun Xue, Yuxiang Zeng, Teng Hu, Jiangning Zhang, Yinan Chen | The paper introduces IVEBench, a comprehensive benchmark suite with a diverse 600-video dataset and a multi-dimensional evaluation protocol designed for assessing instruction-guided video editing (IVE) models. The primary objective is to address the limitations of existing benchmarks by creating a systematic framework to evaluate IVE models across diverse video sources, a wide range of editing tasks (8 categories, 35 subcategories), and robust, human-aligned metrics. The methodology involves curating a high-quality video corpus, generating LLM-assisted and expert-refined editing prompts, and establishing a three-dimensional evaluation protocol encompassing Video Quality, Instruction Compliance, and Video Fidelity that integrates traditional metrics with MLLM-based assessments. Benchmarking of state-of-the-art models reveals that they achieve a maximum Instruction Compliance score of only 0.45, and the proposed metrics demonstrate high human alignment with Spearman's Rho correlations consistently above 0.89 for most quality and fidelity metrics. For AI practitioners, IVEBench provides a standardized tool to rigorously identify model deficiencies—particularly in executing complex editing instructions and maintaining fidelity—and guides future development toward more capable and reliable video editing systems. |
| The Hidden DNA of LLM-Generated JavaScript: Structural Patterns Enable
  High-Accuracy Authorship Attribution (Read more on [arXiv](https://arxiv.org/abs/2510.10493) or [HuggingFace](https://huggingface.co/papers/2510.10493))| Tamás Bisztray, Mohamed Amine Ferrag, Richard A. Dubniczky, Norbert Tihanyi, Neo111x | This research demonstrates that LLM-generated JavaScript contains unique structural fingerprints enabling high-accuracy, model-level authorship attribution. The study investigates which machine learning approaches can most robustly attribute JavaScript to its source LLM and what underlying structural signals these models exploit for differentiation. To do this, the authors created the LLM-NodeJS dataset (250,000 code samples from 20 LLMs) and developed CodeT5-JSA, a custom transformer architecture derived from CodeT5 by removing the decoder and modifying the classification head. The CodeT5-JSA model achieved 95.8% accuracy on five-class attribution and 88.5% on twenty-class tasks, with performance remaining high on mangled and minified code, indicating reliance on deep dataflow and structural patterns over superficial syntax. The principal implication for AI practitioners is that AI-generated code is not a monolithic category; individual models produce stylometrically distinct outputs, enabling provenance tracking essential for security, vulnerability analysis, and ensuring accountability in software development. |
| CoBia: Constructed Conversations Can Trigger Otherwise Concealed
  Societal Biases in LLMs (Read more on [arXiv](https://arxiv.org/abs/2510.09871) or [HuggingFace](https://huggingface.co/papers/2510.09871))| Jana Diesner, Amir Hossein Kargaran, Nafiseh Nikeghbal | CoBia introduces lightweight adversarial attacks using constructed conversations to reveal and stress-test concealed societal biases in Large Language Models (LLMs). The primary objective is to systematically analyze conditions under which LLMs exhibit harmful biased behavior in dialogues and evaluate their ability to recover. The methodology, comprising History-based Constructed Conversation (HCC) and Single-block Constructed Conversation (SCC) methods, creates fabricated dialogues with biased claims, followed by biased follow-up questions, and evaluates 11 LLMs using three automated judges (Bias Judge, Granite Judge, NLI Judge) on a CoBia dataset of 112 social groups. CoBia methods consistently outperformed baseline attacks, with models like llama3.3:70b showing a Unified Constructed Conversation (UCC) Bias Judge score of 85.54%, indicating significant bias amplification and failure to reject biased follow-ups in conversational settings. This necessitates that AI practitioners extend LLM safety mechanisms beyond isolated prompts to encompass entire dialogues and potentially restrict user control over conversation history to ensure robust safety in realistic conversational scenarios. |
| Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole
  Slide Image Diagnosis Behavior (Read more on [arXiv](https://arxiv.org/abs/2510.04587) or [HuggingFace](https://huggingface.co/papers/2510.04587))|  | This paper presents a framework to convert pathologists' raw viewing logs from whole-slide image (WSI) diagnosis into a structured, agent-ready dataset called Pathology-CoT. The primary objective is to address the data bottleneck for training pathology agents by scalably capturing and encoding experts' tacit diagnostic behaviors ("where to look" and "why"). The core methodology involves an "AI Session Recorder" that processes noisy viewer logs into discrete commands and regions of interest (ROIs), which are then paired with expert-verified, AI-drafted rationales to form the training data. The resulting agent, Pathologist-o3, achieved 84.5% precision and 100% recall on lymph node metastasis detection, significantly outperforming the OpenAI o3 model's 46.7% precision and 87.5% recall. The principal implication for AI practitioners is that for complex, interactive domains, performance is constrained by a lack of behavioral supervision; this data-centric approach of converting procedural "digital exhaust" into structured training data provides a powerful, model-agnostic method to build more capable, human-aligned agents. |
