

## Papers for 2025-10-27

| Title | Authors | Summary |
|-------|---------|---------|
| DeepAgent: A General Reasoning Agent with Scalable Toolsets (Read more on [arXiv](https://arxiv.org/abs/2510.21618) or [HuggingFace](https://huggingface.co/papers/2510.21618))| Jiajie Jin, Jiarui Jin, Xiaoxi Li, dongguanting, wxjiao | This paper introduces DeepAgent, an end-to-end reasoning agent that unifies autonomous thinking, dynamic tool discovery, and action execution into a single, continuous process for complex tasks. The objective is to create a general-purpose agent that overcomes the limitations of predefined workflows by enabling dynamic tool retrieval and robust long-horizon reasoning over scalable toolsets. Key methodologies include an autonomous memory folding mechanism to compress interaction history into a structured schema (episodic, working, tool memory) and a reinforcement learning strategy, ToolPO, which leverages an LLM-based tool simulator and fine-grained advantage attribution for stable training. DeepAgent significantly outperforms baseline methods, particularly in open-set scenarios; on the ToolBench benchmark with open-set tool retrieval, it achieved a 64.0% success rate, surpassing the strongest workflow-based baseline's 54.0%. For AI practitioners, this work provides a framework demonstrating that a unified reasoning architecture with dynamic tool discovery and explicit memory management is more effective for building robust, general-purpose agents than traditional, rigid workflow-based approaches. |
| Video-As-Prompt: Unified Semantic Control for Video Generation (Read more on [arXiv](https://arxiv.org/abs/2510.20888) or [HuggingFace](https://huggingface.co/papers/2510.20888))|  | This paper introduces Video-As-Prompt (VAP), a unified framework that uses a reference video as an in-context prompt to achieve generalizable semantic control over video generation. The objective is to develop a single model for diverse, non-pixel-aligned semantic control (e.g., style, motion) that avoids the artifacts and poor generalization of existing methods. VAP employs a plug-and-play Mixture-of-Transformers (MoT) architecture, where a trainable expert processes the video prompt to guide a frozen Video Diffusion Transformer (DiT) via full attention, combined with a temporally biased position embedding to prevent incorrect spatial mapping priors. The primary result is that VAP achieves a 38.7% user preference rate, rivaling leading condition-specific commercial models, and demonstrates strong zero-shot generalization to unseen semantic conditions. For AI practitioners, the principal implication is the ability to add complex semantic control to existing frozen video generation models without costly per-condition retraining or specialized architectures, enabling more scalable and flexible content creation. |
| From Denoising to Refining: A Corrective Framework for Vision-Language
  Diffusion Model (Read more on [arXiv](https://arxiv.org/abs/2510.19871) or [HuggingFace](https://huggingface.co/papers/2510.19871))|  | The paper introduces ReDiff, a corrective framework for vision-language diffusion models that reframes generation from passive denoising to active refining to mitigate error cascades in parallel decoding. The primary objective is to overcome catastrophic error propagation during parallel generation, which is caused by a train-inference discrepancy where models must generate from their own noisy outputs. The methodology involves a two-stage training process: first, a foundational revision stage to correct synthetic errors, followed by an online self-correction loop where the model learns to fix its own intrinsic mistakes by training on draft-correction pairs generated by an expert model. The framework achieves a CLAIR score of 76.74 on the CapMAS benchmark, an 11.2 point improvement over the LLaDA-V baseline, while demonstrating superior stability in few-step parallel generation. For AI practitioners, this work provides a training paradigm to develop more robust vision-language diffusion models capable of stable and efficient parallel generation, directly addressing a key limitation that hinders their real-world application. |
| Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image
  Generation (Read more on [arXiv](https://arxiv.org/abs/2510.21583) or [HuggingFace](https://huggingface.co/papers/2510.21583))|  | Chunk-GRPO is a novel chunk-level reinforcement learning approach for flow-matching-based text-to-image generation that optimizes groups of consecutive timesteps to improve image quality and preference alignment. The research objective is to resolve the inaccurate advantage attribution and neglect of temporal dynamics inherent in standard step-level Group Relative Policy Optimization (GRPO). The key methodology involves segmenting the generation trajectory into "chunks" based on the temporal dynamics of flow matching, identified by the relative L1 distance between latent states, and applying a chunk-level optimization objective. Primarily, Chunk-GRPO with weighted sampling achieves a superior HPSv3 preference score of 15.373, outperforming the step-level Dance-GRPO baseline's score of 15.080. The principal implication for AI practitioners is that aligning the granularity of RL optimization (i.e., chunks) with the intrinsic dynamics of an iterative generation process offers a more effective fine-tuning strategy than applying uniform, step-wise credit assignment. |
| Sparser Block-Sparse Attention via Token Permutation (Read more on [arXiv](https://arxiv.org/abs/2510.21270) or [HuggingFace](https://huggingface.co/papers/2510.21270))|  | Permuted Block-Sparse Attention (PBS-Attn) is a plug-and-play method that accelerates long-context LLM prefilling by reordering tokens to increase the block-level sparsity of the attention matrix. The objective is to improve computational efficiency by creating a more favorable block-sparse structure, which is achieved through a novel segmented permutation strategy that reorders keys within segments based on query-aware importance scores while preserving inter-segment causality. Experiments show that PBS-Attn achieves an end-to-end prefilling speedup of up to 2.75× over the FlashAttention baseline, while maintaining model accuracy that is nearly on par with full attention on benchmarks like LongBench and LongBenchv2. For AI practitioners, this method provides a practical, training-free optimization to significantly reduce the latency and computational cost of the compute-bound prefilling stage for long-context inference applications. |
| UI-Ins: Enhancing GUI Grounding with Multi-Perspective
  Instruction-as-Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.20286) or [HuggingFace](https://huggingface.co/papers/2510.20286))|  | This research introduces "Instruction-as-Reasoning," a novel SFT+RL framework that enhances GUI grounding by treating diverse instructions as dynamic reasoning pathways. The paper's primary objective is to overcome the limitations of poor instruction quality and diversity in existing datasets by developing a model that actively selects the most effective analytical perspective for a given UI task. The methodology involves a two-stage training process: first, Supervised Fine-Tuning (SFT) on a curated dataset of multi-perspective instructions to instill reasoning capabilities, followed by Reinforcement Learning (RL) with Group Relative Policy Optimization (GRPO) to optimize pathway selection. The resulting UI-Ins-32B model establishes a new state-of-the-art, achieving 87.3% accuracy on the UI-I2E-Bench, after finding that 23.3% of instructions in existing datasets were flawed. For AI practitioners, this work highlights the critical importance of instruction data quality and diversity, providing a concrete SFT+RL framework to build more robust GUI agents that can reason effectively and avoid policy collapse during training. |
| A Definition of AGI (Read more on [arXiv](https://arxiv.org/abs/2510.18212) or [HuggingFace](https://huggingface.co/papers/2510.18212))| Yarin Gal, Honglak Lee, Christian Szegedy, Dawn Song, Dan Hendrycks | This paper introduces a quantifiable framework to define Artificial General Intelligence (AGI), grounding it in the Cattell-Horn-Carroll theory of human cognition to evaluate AI systems across ten core cognitive domains. The objective is to operationalize a concrete definition of AGI—an AI matching the cognitive versatility and proficiency of a well-educated adult—to create a standardized measurement tool. The methodology adapts human psychometric batteries to assess AI on ten equally-weighted components, including reasoning, memory, and perception, resulting in a standardized "AGI Score." The framework's application reveals a "jagged" cognitive profile in current models; GPT-4 achieves a total AGI Score of 27%, showing strength in knowledge-based tasks but critical deficits in foundational areas, scoring 0% in Long-Term Memory Storage. For AI practitioners, the principal implication is the direct identification of specific system bottlenecks, demonstrating that fundamental capabilities like continual learning (Long-Term Memory Storage) are entirely absent and require direct architectural solutions rather than being addressed by compensatory strategies like Retrieval-Augmented Generation. |
| Reasoning with Sampling: Your Base Model is Smarter Than You Think (Read more on [arXiv](https://arxiv.org/abs/2510.14901) or [HuggingFace](https://huggingface.co/papers/2510.14901))|  | This paper presents a training-free, inference-time sampling algorithm to enhance the reasoning capabilities of base large language models. The central research question is whether comparable reasoning performance to that achieved by reinforcement learning (RL) can be elicited from base models using only advanced sampling techniques. The proposed "Power Sampling" method employs an iterative Markov chain Monte Carlo (MCMC) algorithm to sample from the base model's power distribution (p^α), which systematically upweights higher-likelihood token sequences. The algorithm achieves performance on par with, and often superior to, the RL-posttrained GRPO baseline; for example, on the Qwen2.5-Math-7B model, it improves HumanEval accuracy from 32.9% (base) to 57.3%, outperforming GRPO's 53.7%, while also maintaining superior generation diversity on pass@k metrics. The principal implication for AI practitioners is that significant reasoning improvements can be extracted from existing base models by dedicating more compute at inference time, potentially obviating the need for complex and costly RL posttraining. |
| RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via
  Hierarchical Model Merging (Read more on [arXiv](https://arxiv.org/abs/2510.20479) or [HuggingFace](https://huggingface.co/papers/2510.20479))|  | The paper introduces RECALL, a data-free framework that mitigates catastrophic forgetting in large language models by performing hierarchical, layer-wise parameter merging guided by the similarity of internal representations. The primary objective is to develop a method that can identify and preserve learned knowledge across multiple fine-tuned models in a data-free and task-agnostic manner, thereby alleviating catastrophic forgetting during continual learning. RECALL first extracts hidden state representations from a small set of "typical" samples, identified via clustering, for each model. It then computes layer-wise inter-model similarity using an RBF kernel on these representations and uses these scores as adaptive weights for a hierarchical parameter merge, applying different weights for each layer. In a single-model merging scenario with Llama-2-7B, RECALL achieved the best generalization to unseen tasks with an average score of 38.92, outperforming the next-best baseline by +7.86%. The principal implication for AI practitioners is the ability to fuse multiple specialist models into a single, more capable generalist model without requiring access to the original training datasets, which saves computational resources and navigates data privacy constraints. |
| Visual Diffusion Models are Geometric Solvers (Read more on [arXiv](https://arxiv.org/abs/2510.21697) or [HuggingFace](https://huggingface.co/papers/2510.21697))| Or Patashnik, Andrey Voynov, Omer Dahary, Shai Yehezkel, Nir Goren | Visual diffusion models are presented as effective geometric solvers that operate directly in pixel space. The primary objective is to demonstrate that these models can reason about and discover geometric structures by recasting hard geometric problems as image generation tasks. The key methodology involves training a standard visual diffusion model (U-Net backbone with self-attention) on pixel-space representations of problems like the Inscribed Square Problem, Steiner Tree Problem, and Maximum Area Polygonization Problem, with problem instances provided as conditional input. Primary results include achieving a squareness metric of 0.891 for the Inscribed Square Problem (vs. 0.924 GT), a 0.996 valid tree rate and 1.0008 mean length ratio for Steiner Trees (10-20 points), and a 0.953 polygon validity rate and 0.9887 mean area ratio for Maximum Area Polygons (7-12 points). This research implies for AI practitioners that visual diffusion models offer a general and practical framework for approximating notoriously hard geometric problems through visual representations, enabling a bridge between generative modeling and mathematical problem-solving without requiring specialized architectures. |
| WorldGrow: Generating Infinite 3D World (Read more on [arXiv](https://arxiv.org/abs/2510.21682) or [HuggingFace](https://huggingface.co/papers/2510.21682))| Jia Lu, Taoran Yi, Chen Yang, Sikuang Li, JieminFang | WorldGrow is a novel framework for generating infinite 3D worlds. The research addresses the challenge of synthesizing infinitely extendable, large, continuous 3D environments with coherent geometry and photorealistic appearance. Its methodology involves a hierarchical framework using a data curation pipeline for structured scene blocks, a 3D block inpainting mechanism for context-aware extension, and a coarse-to-fine generation strategy. WorldGrow achieved a FID_DINOv2 score of 313.54 for visual fidelity in generated blocks, significantly outperforming SynCity (655.60), and demonstrated robust stability in distant expansions. This enables AI practitioners to construct scalable, high-quality 3D content for large-scale virtual environments, crucial for embodied AI training and simulation. |
| RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via
  Data Alignment and Test-Time Scaling (Read more on [arXiv](https://arxiv.org/abs/2510.20206) or [HuggingFace](https://huggingface.co/papers/2510.20206))|  | RAPO++ is a cross-stage prompt optimization framework designed to enhance Text-to-Video (T2V) generation quality model-agnostically. The primary objective is to overcome limitations of short, unstructured, and misaligned user prompts that hinder the generative potential of diffusion-based T2V models. Its methodology involves three stages: Retrieval-Augmented Prompt Optimization (RAPO) for training-data-aligned refinement using relation graphs and LLMs; Sample-Specific Prompt Optimization (SSPO) for iterative test-time scaling with multi-source feedback (e.g., VLM verifiers and optical flow); and LLM fine-tuning to internalize optimization patterns from SSPO. RAPO++ achieved a total score of 82.65% on VBench with the LaVie model and improved Consistent Attribute Binding from 0.620 (Naive) to 0.742 on T2V-CompBench, demonstrating significant gains in semantic alignment, compositional reasoning, temporal stability, and physical plausibility. This framework provides AI practitioners with a model-agnostic, cost-efficient, and scalable solution to substantially improve T2V outputs without modifying the underlying generative backbone. |
| Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs (Read more on [arXiv](https://arxiv.org/abs/2510.13251) or [HuggingFace](https://huggingface.co/papers/2510.13251))| Bohyung Han, taekyung-k, byminji | This research uses mechanistic interpretability to map the internal information flow in Video Large Language Models (VideoLLMs), revealing a structured, multi-stage process for temporal reasoning. The primary objective is to investigate where and how VideoLLMs extract spatiotemporal information from video, integrate it with textual queries, and propagate it through different layers and modalities to generate answers for video question answering tasks. The study employs Attention Knockout to causally trace information flow by selectively disabling attention connections between token groups and Logit Lens to analyze the emergence of semantic concepts within video token representations across layers. The analysis reveals that temporal reasoning begins with cross-frame interactions in early-to-middle layers, followed by video-language integration on temporal keywords in middle layers, after which the model is ready to generate answers in middle-to-late layers; retaining only these effective pathways while suppressing 58% of attention edges in LLaVA-NeXT-7B-Video-FT maintained its original VideoQA performance. For AI practitioners, this provides a blueprint for model optimization, suggesting that VideoLLMs can be pruned to retain only these critical information pathways, enabling the development of more computationally efficient models for inference without a significant loss in temporal reasoning capability. |
| Model Merging with Functional Dual Anchors (Read more on [arXiv](https://arxiv.org/abs/2510.21223) or [HuggingFace](https://huggingface.co/papers/2510.21223))|  | This paper introduces Functional Dual Anchors (FDAs) for model merging, enabling knowledge integration in the input-representation space. The primary objective is to mitigate task-specific knowledge conflicts in model merging by shifting the focus from parameter-space adjustments to modeling the input-representation space. The methodology involves constructing Functional Dual Anchors (FDAs) as synthetic inputs whose induced gradients align with task vectors. FDAs are optimized via gradient matching in the input-representation space and used for subsequent parameter optimization, guided by a principled initialization scheme. FDAs significantly improve multi-task performance, with a pretrained model adapted by FDAs achieving 87.26 average accuracy on ViT-B/16 tasks, representing an almost 18% improvement compared to vanilla Task Arithmetic (73.94). AI practitioners can leverage FDAs to achieve more robust and flexible model merging by integrating knowledge through synthetic inputs in the representation space, offering a viable alternative or complement to existing parameter-centric methods for consolidating diverse domain knowledge. |
| Document Understanding, Measurement, and Manipulation Using Category
  Theory (Read more on [arXiv](https://arxiv.org/abs/2510.21553) or [HuggingFace](https://huggingface.co/papers/2510.21553))|  | This paper introduces a category theory-based framework for document understanding, measurement, and manipulation. The primary objective is to extract and utilize multimodal document structure to enable information-theoretic measures, summarization, exegesis, and self-supervised improvement of large pretrained models. The methodology involves representing documents as categories of orthogonalized question-answer (QA) pairs, derived from rhetorical structure (abstractive DAGs) using large pretrained models. Key results include the formal definition of a Jaccard-like metric for assertion similarity, exemplified by `d(A, B) = ½` for contradictory assertions, and the development of rate distortion analysis for summarization techniques. This framework offers AI practitioners a principled, mathematical approach to semantic analysis and manipulation, enabling advanced document processing and self-correction mechanisms for LLMs based on consistency constraints. |
| PhysWorld: From Real Videos to World Models of Deformable Objects via
  Physics-Aware Demonstration Synthesis (Read more on [arXiv](https://arxiv.org/abs/2510.21447) or [HuggingFace](https://huggingface.co/papers/2510.21447))| Hui Li, Yihan Zeng, Xiang Zhang, Yu Yang, cszhilu1998 | PhysWorld is a novel framework for learning accurate and fast world models of deformable objects from limited real-world videos through physics-aware demonstration synthesis. The primary objective is to address the data scarcity challenge in learning physics-consistent dynamics models for deformable objects, enabling both high accuracy and real-time inference. Its methodology involves constructing an MPM-based digital twin using VLM-assisted constitutive model selection and global-to-local physical property optimization from real videos. This digital twin then generates diverse 4D demonstrations via Various Motion Pattern Generation and Part-aware Physical Property Perturbation, which train a GNN-based world model subsequently fine-tuned with real videos. Experimentally, PhysWorld achieved competitive prediction performance and enabled inference speeds 47 times faster than the state-of-the-art PhysTwin (799 FPS vs 17 FPS). This work provides AI practitioners with a robust and efficient method for developing physics-consistent world models for robotics, VR, and AR, mitigating data requirements and facilitating real-time deployment. |
| Are Large Reasoning Models Good Translation Evaluators? Analysis and
  Performance Boost (Read more on [arXiv](https://arxiv.org/abs/2510.20780) or [HuggingFace](https://huggingface.co/papers/2510.20780))| Min Yang, Lidia S. Chao, Xinyi Yang, Zhihong Huang, rzzhan | This paper systematically analyzes Large Reasoning Models (LRMs) as Machine Translation (MT) evaluators, identifies inefficiencies, and proposes a calibration method to improve performance and efficiency. The research aimed to systematically understand LRM performance and failure modes in MT evaluation, and to develop an effective alignment strategy for LRMs as MT judges. The authors employed LRMs within the MQM framework, conducting meta-evaluation and analysis across various model sizes, and proposing ThinMQM, a method to calibrate LRM thinking by fine-tuning models on synthetic, human-like evaluation trajectories derived from WMT23 MQM data. Experiments on WMT24 Metrics benchmarks demonstrated that ThinMQM largely reduced thinking budgets by approximately 35x, while concurrently improving evaluation performance, notably achieving an 8.7 correlation point improvement for R1-Distill-Qwen-7B. These findings highlight that efficiently calibrated LRMs have significant potential to advance fine-grained automatic MT evaluation, emphasizing the critical need for controlled thinking and careful calibration for AI practitioners developing LRM-as-a-judge systems. |
| ARC-Encoder: learning compressed text representations for large language
  models (Read more on [arXiv](https://arxiv.org/abs/2510.20535) or [HuggingFace](https://huggingface.co/papers/2510.20535))|  | The paper introduces ARC-Encoder, a novel method for learning compressed text representations for Large Language Models (LLMs) that replaces raw text input, aiming to improve inference efficiency and context handling without modifying the decoder. The primary objective is to develop a plug-and-play encoder that compresses LLM contexts into continuous representations, reducing inference costs and extending context windows while preserving general abilities. The methodology involves an LLM transformer-based encoder with a pooling mechanism that averages consecutive queries in the last self-attention module for a fixed pooling factor (e.g., 4x or 8x), trained via alternating reconstruction and continuation tasks with a two-layer MLP projector and adaptable to multiple decoders via shared encoder and specialized projector layers. Results demonstrate that ARC-Encoder achieves state-of-the-art performance, nearly matching the open-book baseline with a 4x pooling factor (e.g., ARC4-Encoder for Llama3.1 8B achieves an average score of 48.0 compared to open-book's 47.4) while providing up to 1.8x gains in prefilling FLOPs and extending context processing to 8x the original window size. This implies that AI practitioners can leverage ARC-Encoder as an efficient and portable solution to compress LLM input contexts, enhancing inference speed and long-context capabilities for various applications without requiring architectural changes or fine-tuning of the LLM decoder itself. |
| Taming Modality Entanglement in Continual Audio-Visual Segmentation (Read more on [arXiv](https://arxiv.org/abs/2510.17234) or [HuggingFace](https://huggingface.co/papers/2510.17234))| Zhaojin Fu, Zili Wang, Tao Zhang, Qi Yang, hongyuyang23casia | This paper introduces a novel Collision-based Multi-modal Rehearsal (CMR) framework to mitigate modality entanglement in Continual Audio-Visual Segmentation (CAVS). The primary objective is to enable models to continuously segment new audio-visual classes while preserving knowledge of previously learned ones, specifically addressing multi-modal semantic drift and co-occurrence confusion in fine-grained CAVS. The methodology proposes the CMR framework, which includes a Multi-modal Sample Selection (MSS) strategy to identify high-modality-consistency samples for rehearsal by quantifying audio contribution, and a Collision-based Sample Rehearsal (CSR) mechanism that dynamically adjusts rehearsal frequency for classes prone to co-occurrence confusion. Comprehensive experiments on AVSBench-CI, AVSBench-CIS, and AVSBench-CIM datasets demonstrate that CMR significantly outperforms single-modal continual learning methods, achieving an 11.3 mIoU increase on the AVSBench-CIS 60-10 overlapped setting compared to other methods. This research provides AI practitioners with a robust framework for designing continual learning systems in multi-modal, fine-grained tasks like audio-visual segmentation, offering effective strategies to combat catastrophic forgetting and modality entanglement. |
| AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research
  Suite (Read more on [arXiv](https://arxiv.org/abs/2510.21652) or [HuggingFace](https://huggingface.co/papers/2510.21652))| Bhavana Dalvi, Dan Bareket, Nishant Balepur, Mike D'Arcy, Jonathan Bragg | AstaBench introduces a rigorous benchmark suite for evaluating AI agents in scientific research. Its objective is to address shortcomings of existing benchmarks by providing holistic, reproducible, cost-accounted evaluations with standardized interfaces and comprehensive baselines across 2400+ problems and multiple scientific domains. The methodology includes a production-grade scientific research environment with controlled search tools and an evaluation toolkit to account for confounders like tool access and inference cost. Experimental results show that while some agents achieve meaningful progress in literature understanding (e.g., Asta v0 at 53.0%), overall scores for the full range of science tasks remain low, with the best open-source agent achieving only 11.1%. This indicates that AI is still far from solving the challenge of scientific research assistance, requiring significant development in areas like coding, data analysis, and end-to-end discovery. |
| Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video (Read more on [arXiv](https://arxiv.org/abs/2510.21581) or [HuggingFace](https://huggingface.co/papers/2510.21581))|  | Foley Control is a lightweight approach for video-guided Foley synthesis by aligning frozen text-to-audio models with video. The main objective is to achieve competitive temporal and semantic alignment while preserving the practicality of frozen generative backbones and reducing data requirements. This is accomplished by connecting V-JEPA2 video embeddings to a frozen Stable Audio Open DiT text-to-audio (T2A) model via compact, trainable video cross-attention layers inserted after the existing text cross-attention, utilizing pooled video tokens and Rotary Position Embeddings (RoPE) for temporal grounding. Foley Control delivers competitive alignment, with single-pooled embeddings achieving a KL-PANNs metric of 3.111351 at 400k training steps, comparable to denser grid embeddings, and comparable MovieGenBench scores (e.g., DeSync 0.32) while training with nearly two orders of magnitude less paired data and compute than end-to-end multimodal systems. This framework offers AI practitioners a modular and data-efficient solution for video-to-audio generation, enabling easy swapping or upgrading of encoders and T2A backbones without costly end-to-end retraining. |
| Soft Instruction De-escalation Defense (Read more on [arXiv](https://arxiv.org/abs/2510.21057) or [HuggingFace](https://huggingface.co/papers/2510.21057))|  | Soft Instruction Control (SIC) is an iterative prompt sanitization defense designed for tool-augmented Large Language Model (LLM) agents against prompt injection attacks. The method's objective is to neutralize adversarial instructions by repeatedly inspecting incoming untrusted data, rewriting, masking, or removing malicious content, and re-evaluating until clean or a maximum iteration limit is reached. SIC employs an LLM-based rewriting mechanism with canary injection and multi-granularity detection (full text and chunks) to identify and de-escalate imperative instructions. Against a strong adaptive genetic algorithm adversary, SIC achieved an Attack Success Rate (ASR) of 15%, outperforming other detector-based defenses which had ASRs up to 49%. For AI practitioners, SIC offers a pragmatic, modular preprocessing layer that significantly raises the bar for prompt injection attacks by making them more difficult and expensive, without requiring modifications to the underlying LLM agent. |
| PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language
  Models in Physical Environments (Read more on [arXiv](https://arxiv.org/abs/2510.21111) or [HuggingFace](https://huggingface.co/papers/2510.21111))| Chaoyang Zhao, Manli Tao, Yi Peng, Xuantang Xiong, JettZhou | This paper introduces Active Visual Reasoning (AVR), a task requiring multimodal models to interact with physical environments to resolve information incompleteness for reasoning. The main objective is to extend visual reasoning from static, fully-observable settings to dynamic, partially-observable environments where agents must actively gather information. The methodology involves creating the CLEVR-AVR benchmark for evaluation and the AVR-152k dataset with Chain-of-Thought annotations modeling the task as a higher-order Markov Decision Process. A model, PhysVLM-AVR, is trained on this dataset to learn sequential information gathering and reasoning. The primary result is that while PhysVLM-AVR achieves 90.5% accuracy in identifying the need for interaction (Information Sufficiency Judgment Accuracy), its final answer accuracy is 39.7%, indicating that models can detect missing information but struggle to strategically act to acquire it. The principal implication for AI practitioners is that the AVR framework and dataset provide a concrete methodology for training agents to perform goal-directed, interactive information seeking, addressing a key limitation of current MLLMs in robotics and dynamic environments. |
| Stabilizing MoE Reinforcement Learning by Aligning Training and
  Inference Routers (Read more on [arXiv](https://arxiv.org/abs/2510.11370) or [HuggingFace](https://huggingface.co/papers/2510.11370))|  | This paper introduces Rollout Routing Replay (R3) to stabilize reinforcement learning for Mixture-of-Experts (MoE) models by aligning router behavior between training and inference. The objective is to mitigate RL training instability and collapse in MoE models, which is attributed to discrepancies in expert routing distributions between the inference rollout and training update phases. The key methodology involves recording the routing masks from the inference engine during sequence generation and replaying them during the training forward pass to enforce consistent expert selection. R3 reduces the training-inference policy KL divergence for the Qwen3-30B-A3B model from 1.535×10⁻³ to 7.5×10⁻⁴ and decreases the frequency of tokens with large probability discrepancies by an order of magnitude. For AI practitioners, R3 offers a practical method to stabilize RL on MoE architectures, preventing training collapse and improving final model performance by resolving a foundational inconsistency between training and inference frameworks. |
