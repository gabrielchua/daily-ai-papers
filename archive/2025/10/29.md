

## Papers for 2025-10-29

| Title | Authors | Summary |
|-------|---------|---------|
| InteractComp: Evaluating Search Agents With Ambiguous Queries (Read more on [arXiv](https://arxiv.org/abs/2510.24668) or [HuggingFace](https://huggingface.co/papers/2510.24668))| Fashen Ren, Jiayi Zhang, Yani Fan, Lijun Huang, Mingyi Deng | This paper introduces INTERACTCOMP, a benchmark for evaluating the capability of search agents to resolve ambiguous queries through interaction, revealing a critical failure in current models. The main objective is to evaluate whether search agents can recognize query ambiguity and actively interact with a user to gather disambiguating information, a capability unaddressed by existing search benchmarks. The key methodology is the construction of a 210-instance benchmark using a "target-distractor" design, where ambiguous questions are crafted from the shared attributes of two entities, forcing agents to use an `interact` action to uncover hidden, distinctive context to find the correct answer. The primary result across 17 models is a systematic failure to engage in interaction; the top-performing model achieved only 13.73% accuracy, whereas performance on the same questions with complete context reached 71.50%, demonstrating that the failure stems from overconfidence rather than a lack of reasoning ability. The principal implication for AI practitioners is that search agents cannot be assumed to handle underspecified queries; they exhibit a critical blind spot in actively seeking clarification, which will lead to incorrect and confident outputs in real-world applications unless agents are explicitly trained for interactive disambiguation. |
| Tongyi DeepResearch Technical Report (Read more on [arXiv](https://arxiv.org/abs/2510.24701) or [HuggingFace](https://huggingface.co/papers/2510.24701))|  | This paper presents Tongyi DeepResearch, an open-source agentic language model designed for complex, long-horizon information-seeking and research tasks. The main objective is to create a scalable, end-to-end paradigm for training autonomous AI researchers capable of planning, searching, reasoning, and synthesizing knowledge. The core methodology involves a novel two-stage training framework comprising agentic continual pre-training (mid-training) to build an agentic inductive bias, followed by supervised fine-tuning and on-policy reinforcement learning (post-training), all driven by a fully automated, scalable synthetic data generation pipeline. The resulting 30.5B parameter model achieves state-of-the-art performance on multiple agentic benchmarks, scoring 90.6 on FRAMES and 70.9 on GAIA, while activating only 3.3B parameters per token. The principal implication for AI practitioners is that this work provides a complete, open-source blueprint for building highly capable research agents without human-annotated data, demonstrating that a structured training pipeline using synthetic data offers a scalable and reproducible path toward more advanced agentic systems. |
| AgentFold: Long-Horizon Web Agents with Proactive Context Management (Read more on [arXiv](https://arxiv.org/abs/2510.24699) or [HuggingFace](https://huggingface.co/papers/2510.24699))|  | AgentFold is a novel web agent paradigm that introduces proactive, learned context management to enhance performance and scalability on long-horizon tasks. The paper's primary objective is to resolve the fundamental trade-off between context saturation in append-only agents and the irreversible loss of critical details from fixed summarization methods. Its key methodology involves structuring the agent's context into `Multi-Scale State Summaries` and a `Latest Interaction`, and training the agent via supervised fine-tuning to issue a "folding" directive that either granularly condenses a single step or deeply consolidates an entire sub-task. The resulting AgentFold-30B-A3B agent achieves 36.2% on the BrowseComp benchmark, outperforming models over 20 times its size, while maintaining a context size that is 92% smaller than a comparable ReAct agent after 100 turns. For AI practitioners, the principal implication is a concrete architecture for building more efficient and capable long-horizon agents by making dynamic context management a core, learnable component of the agent's reasoning process, thus reducing computational overhead and enabling sustained, complex interactions. |
| RoboOmni: Proactive Robot Manipulation in Omni-modal Context (Read more on [arXiv](https://arxiv.org/abs/2510.23763) or [HuggingFace](https://huggingface.co/papers/2510.23763))|  | The paper introduces RoboOmni, an end-to-end omni-modal framework for proactive robotic manipulation that infers user intent from speech, environmental sounds, and visual cues. The primary objective is to enable a robot to proactively understand and verify latent user intent from cross-modal context, moving beyond reliance on explicit commands. The methodology centers on a Perceiver-Thinker-Talker-Executor architecture, an omni-modal LLM that unifies perception and action generation in a single autoregressive model, trained on a new 140k-episode dataset called OmniAction. RoboOmni achieves an 85.6% success rate in simulation, substantially outperforming the strongest cascaded ASR-VLA baseline which scored 25.9%. The principal implication for AI practitioners is that end-to-end omni-modal models, by directly processing raw audio and avoiding intermediate representations like ASR, are critical for developing robust human-robot interaction systems that can interpret the subtle contextual and paralinguistic cues essential for proactive assistance. |
| Game-TARS: Pretrained Foundation Models for Scalable Generalist
  Multimodal Game Agents (Read more on [arXiv](https://arxiv.org/abs/2510.23691) or [HuggingFace](https://huggingface.co/papers/2510.23691))|  | Game-TARS is a generalist multimodal agent pretrained on over 500B tokens using a unified action space of native keyboard-mouse inputs to achieve broad generalization across diverse digital environments. The research objective is to develop a scalable foundation model for game agents by shifting from environment-specific APIs to this universal, low-level action representation. The methodology involves large-scale continual pre-training on game and agentic trajectories using a Sparse ReAct paradigm, a "Thinking Aloud" data collection protocol, and a decaying continual loss function to mitigate causal confusion from repetitive actions. Experiments show Game-TARS achieves approximately double the success rate of previous state-of-the-art models on open-world Minecraft tasks, reaching a 72.0% success rate on embodied tasks compared to the prior best of 42.1%. The principal implication for practitioners is that employing a simple, scalable, device-level action space is a viable path for building general-purpose computer-use agents with strong zero-shot generalization capabilities, bypassing the need for environment-specific engineering. |
| Uniform Discrete Diffusion with Metric Path for Video Generation (Read more on [arXiv](https://arxiv.org/abs/2510.24717) or [HuggingFace](https://huggingface.co/papers/2510.24717))|  | This paper introduces URSA, a discrete diffusion framework for scalable video generation that operates by iteratively refining discrete spatiotemporal tokens. The research objective is to close the performance gap between discrete and continuous video generation methods by mitigating error accumulation and improving long-context consistency. The methodology integrates a Linearized Metric Path derived from token embedding distances, a Resolution-dependent Timestep Shifting mechanism, and an asynchronous temporal scheduling strategy to unify tasks like text-to-video and interpolation in a single model. URSA demonstrates performance comparable to state-of-the-art continuous methods, achieving a text-to-video score of 82.4 on the VBench benchmark. For AI practitioners, this work provides a unified and scalable discrete alternative to continuous diffusion models for high-quality, multi-task video generation, offering a competitive and potentially more efficient architectural paradigm. |
| Repurposing Synthetic Data for Fine-grained Search Agent Supervision (Read more on [arXiv](https://arxiv.org/abs/2510.24694) or [HuggingFace](https://huggingface.co/papers/2510.24694))|  | This paper introduces Entity-aware Group Relative Policy Optimization (E-GRPO), a framework that repurposes ground-truth entities from synthetic data to create a dense reward signal for training search agents. The core objective is to solve the sparse reward problem in methods like GRPO by distinguishing informative "near-miss" failures from complete failures. E-GRPO's methodology formulates a dense reward function that assigns partial rewards to incorrect trajectories based on their normalized entity match rateâ€”the fraction of ground-truth entities identified within the agent's reasoning thoughts. The primary result is that E-GRPO consistently outperforms its baseline; for instance, a 7B model trained in a local environment achieved a 64.2 average Pass@1 score on QA benchmarks, a 2.8-point improvement over standard GRPO, while also reducing the number of tool calls. The principal implication for AI practitioners is that metadata discarded during synthetic data generation is a computationally cheap yet powerful source for creating fine-grained reward signals, enhancing the sample efficiency and performance of RL-based agent alignment by enabling learning from partially correct solutions. |
| OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents (Read more on [arXiv](https://arxiv.org/abs/2510.24563) or [HuggingFace](https://huggingface.co/papers/2510.24563))|  | This paper introduces OSWorld-MCP, a benchmark for evaluating multimodal agents on their ability to jointly perform GUI operations and invoke external tools via the Model Context Protocol (MCP). The main objective is to create a fair evaluation framework to assess an agent's decision-making in choosing between GUI interactions and MCP tool invocations for complex computer tasks. The methodology involves extending the OSWorld environment with a curated set of 158 high-quality MCP tools and introducing new metrics like Tool Invocation Rate (TIR). Primary results show that MCP tools significantly improve performance, increasing task success for OpenAI o3 from 8.3% to 20.4%, yet even top models have low tool invocation rates (max of 36.3%). For AI practitioners, this benchmark provides a standardized method to assess and develop agent tool-use capabilities, revealing that effective decision-making between GUI and tool-based actions is a critical and underdeveloped area for creating more robust automated agents. |
| WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling
  Info-Rich Seeking (Read more on [arXiv](https://arxiv.org/abs/2510.24697) or [HuggingFace](https://huggingface.co/papers/2510.24697))|  | WebLeaper is a framework for generating entity-rich information-seeking tasks from Wikipedia tables to train web agents that are both more effective and efficient. The main objective is to overcome the low search efficiency of LLM-based agents, which is attributed to the sparsity of target entities in conventional training tasks, by developing a framework to construct high-coverage tasks and generate efficient solution trajectories. The methodology involves modeling information-seeking as a tree-structured reasoning problem and synthesizing tasks in three variants (Basic, Union, Reverse-Union) to systematically increase complexity, followed by curating training trajectories based on Information-Seeking Rate (ISR) and Information-Seeking Efficiency (ISE) metrics, and finally training an agent via supervised fine-tuning and reinforcement learning with a hybrid reward system. In a comprehensive training setting, WebLeaper achieved a 73.2 accuracy score on the GAIA benchmark, outperforming strong open-source models like DeepSeek-V3.1 (63.1) and proprietary models such as Claude-4-Sonnet (68.3). The principal implication for AI practitioners is that training agents on entity-dense tasks, as enabled by the WebLeaper framework, directly improves both task success rates and operational efficiency (fewer actions), providing a concrete strategy to build more capable and cost-effective web-browsing agents. |
| Group Relative Attention Guidance for Image Editing (Read more on [arXiv](https://arxiv.org/abs/2510.24657) or [HuggingFace](https://huggingface.co/papers/2510.24657))|  | The paper introduces Group Relative Attention Guidance (GRAG), a lightweight method for achieving fine-grained control over editing strength in Diffusion-in-Transformer (DiT) models. The primary objective is to address the lack of effective control over editing intensity in existing methods by enabling continuous modulation between instruction following and image consistency. The key methodology involves identifying a shared bias vector in the Query and Key embeddings of the MM-Attention mechanism and then reweighting the deviation of each token from this group bias to precisely control the editing process. Integrating GRAG into the Qwen-Edit model improved the overall EditScore from 7.2576 to 7.3245 on the PIE dataset. For AI practitioners, GRAG provides a simple, four-line code modification that can be integrated into existing DiT-based editors to enhance controllability and editing quality without any model tuning. |
| STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D
  Intelligence (Read more on [arXiv](https://arxiv.org/abs/2510.24693) or [HuggingFace](https://huggingface.co/papers/2510.24693))|  | This paper introduces STAR-Bench, a benchmark to evaluate audio 4D intelligence, defined as a model's ability to perform deep reasoning over sound dynamics in time and 3D space. The research objective is to assess how well Large Audio-Language Models (LALMs) handle linguistically hard-to-describe auditory cues, a gap in existing text-centric audio benchmarks. The methodology involves a two-level benchmark with a Foundational Acoustic Perception task using synthesized audio to test six core attributes and a Holistic Spatio-Temporal Reasoning task using curated real-world audio to evaluate complex event ordering and 3D scene understanding. Evaluation of 19 models reveals substantial performance gaps, showing that relying on audio captions instead of raw audio causes accuracy to drop by 31.5% on temporal tasks and 35.2% on spatial tasks, unlike in prior benchmarks. For AI practitioners, the principal implication is that current models fundamentally struggle to integrate information from multiple audio inputs and lack genuine spatial awareness, highlighting the need to develop architectures that natively process multi-channel audio rather than averaging it to a mono signal. |
| Routing Matters in MoE: Scaling Diffusion Transformers with Explicit
  Routing Guidance (Read more on [arXiv](https://arxiv.org/abs/2510.24711) or [HuggingFace](https://huggingface.co/papers/2510.24711))|  | This paper introduces ProMoE, a Mixture-of-Experts (MoE) framework that improves the scaling of Diffusion Transformers (DiTs) through explicit routing guidance. The main objective is to address the poor expert specialization in vision MoEs, which stems from the spatial redundancy and functional heterogeneity of visual tokens compared to language tokens. The key methodology is a two-step router that first performs conditional routing to separate tokens by functional role (conditional vs. unconditional) and then uses prototypical routing with a novel routing contrastive loss to assign conditional tokens to experts based on semantic content. On the ImageNet 256x256 benchmark with Rectified Flow, the ProMoE-L model achieves a FrÃ©chet Inception Distance (FID) of 2.79, surpassing its dense DiT counterpart's FID of 3.56 while activating the same number of parameters. For AI practitioners, this work provides a validated method for effectively applying MoE to vision transformers by introducing explicit routing signals that account for the unique characteristics of visual data, enabling more efficient model scaling. |
| ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking (Read more on [arXiv](https://arxiv.org/abs/2510.24698) or [HuggingFace](https://huggingface.co/papers/2510.24698))|  | PARALLELMUSE is a two-stage paradigm that improves deep information-seeking agents' performance and efficiency through uncertainty-guided partial rollouts and compressed reasoning aggregation. The objective is to develop a parallel thinking framework for deep information-seeking agents that overcomes the inefficiency of redundant rollouts and the difficulty of integrating long-horizon reasoning trajectories within limited context windows. The methodology consists of two stages: 1) Functionality-Specified Partial Rollout, which identifies high-uncertainty steps in distinct functional regions (reasoning vs. exploration) to branch from, reusing context via KV caching; and 2) Compressed Reasoning Aggregation, which condenses multiple reasoning trajectories into structured reports to enable coherent, comprehensive answer synthesis. The method achieves up to a 62% performance improvement over the base agent model and reduces exploratory token consumption by 10-30% compared to conventional from-scratch parallel rollouts; trajectory compression further reduces aggregation context by up to 99%. The principal implication for AI practitioners is that PARALLELMUSE offers a practical test-time scaling technique to significantly enhance agent problem-solving capabilities without model retraining, while simultaneously improving computational and token efficiency over standard parallel reasoning methods. |
| AgentFrontier: Expanding the Capability Frontier of LLM Agents with
  ZPD-Guided Data Synthesis (Read more on [arXiv](https://arxiv.org/abs/2510.24695) or [HuggingFace](https://huggingface.co/papers/2510.24695))|  | This paper introduces the AgentFrontier Engine, a data synthesis framework guided by the Zone of Proximal Development (ZPD) to enhance LLM agent reasoning. The research objective is to develop a scalable method for automatically generating frontier-level training data that is challenging enough to require guided learning but is ultimately solvable. The methodology involves a three-stage pipeline: generating multi-source seed questions, iteratively escalating their complexity with a tool-augmented agent, and using an LKP-MKO (Less Knowledgeable Peer vs. More Knowledgeable Other) adversarial calibration to filter for tasks within the LLM's ZPD. The resulting AgentFrontier-30B-A3B model achieved state-of-the-art performance, scoring 28.6% on the text-only Humanity's Last Exam and 93.4% on their ZPD Exam-v1. For AI practitioners, this work provides a principled, automated framework for creating high-quality, complex reasoning data, offering a scalable path to train more capable agents without relying on prohibitive manual curation. |
| Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal
  Reasoning in MLLMs (Read more on [arXiv](https://arxiv.org/abs/2510.24514) or [HuggingFace](https://huggingface.co/papers/2510.24514))|  | The paper introduces Latent Sketchpad, a framework that enables Multimodal Large Language Models (MLLMs) to generate internal visual latents as a form of "visual thought" to improve complex multimodal reasoning. The research objective is to enhance MLLMs' capabilities in scenarios requiring visual planning and imagination by equipping them with an internal mechanism to generate and utilize visual representations interleaved with their native textual reasoning process. The methodology involves augmenting a frozen pretrained MLLM with two components: a Context-Aware Vision Head that autoregressively generates sequences of visual latents, and a separately pretrained Sketch Decoder that translates these latents into interpretable sketch images for visualization. On the custom MAZEPLANNING dataset, the framework improved performance; a fine-tuned Gemma3 model equipped with Latent Sketchpad increased its task Success Rate from 70.0% to 72.2%, and the generated visual traces themselves demonstrated a Visual Success Rate of 75.6%, surpassing the text-only baseline. The principal implication for AI practitioners is that this modular, plug-and-play approach allows for the enhancement of MLLMs' reasoning abilities for spatial planning tasks without requiring full model retraining, providing a direct method to incorporate interpretable "visual thinking" into existing architectures. |
| Critique-RL: Training Language Models for Critiquing through Two-Stage
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2510.24320) or [HuggingFace](https://huggingface.co/papers/2510.24320))|  | Critique-RL is a two-stage reinforcement learning framework that trains language models for critiquing by first optimizing for discriminability and then for helpfulness without stronger supervision. The primary objective is to develop effective critique models by resolving the optimization conflict between a critic's ability to accurately judge a response (discriminability) and its ability to provide useful feedback for refinement (helpfulness). The methodology involves a two-stage RL process: Stage I uses direct, rule-based rewards to explicitly train the critic's discriminability, and Stage II uses indirect rewards from actor refinement to improve helpfulness, while KL regularization preserves the discriminative ability learned in Stage I. The proposed method significantly outperforms baselines; for instance, a Qwen2.5-7B model trained with Critique-RL achieved 58.40% accuracy on the MATH dataset, improving upon the 51.84% of an SFT baseline, while concurrently boosting discriminability accuracy to 85.20%. For AI practitioners, this two-stage framework offers a robust method for creating specialized critique models for scalable oversight, enhancing the performance of actor models on complex reasoning tasks by ensuring the critic first learns to reliably identify errors before learning to provide constructive feedback. |
| VisCoder2: Building Multi-Language Visualization Coding Agents (Read more on [arXiv](https://arxiv.org/abs/2510.23642) or [HuggingFace](https://huggingface.co/papers/2510.23642))|  | This work introduces VisCoder2, a family of open-source models for generating visualization code, alongside a large-scale multi-language dataset and a comprehensive benchmark for training and evaluation. The primary objective is to develop and systematically evaluate multi-language visualization coding agents capable of iterative generation, execution, and self-debugging across diverse programming environments. The methodology involves constructing VisCode-Multi-679K, a supervised dataset of 679K executable code samples and correction dialogues across 12 languages, and using it to fine-tune the Qwen2.5-Coder model family. On the introduced VisPlotBench benchmark, the 32B VisCoder2 model with iterative self-debug achieves an 82.4% overall execution pass rate, matching the performance of the proprietary GPT-4.1 model. For AI practitioners, this provides a set of open-source models and resources capable of reliably generating executable visualization code in multiple languages, with a robust framework for implementing execution-based self-correction to handle complex symbolic or compiler-dependent languages. |
| ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining,
  Finetuning, and Decoding the Curse of Multilinguality (Read more on [arXiv](https://arxiv.org/abs/2510.22037) or [HuggingFace](https://huggingface.co/papers/2510.22037))|  | This work introduces the ADAPTIVE TRANSFER SCALING LAW (ATLAS) for multilingual pretraining, which outperforms existing scaling laws in out-of-sample generalization. The research aims to empirically investigate multilingual scaling dynamics, measure cross-lingual transfer, model the "curse of multilinguality," and determine the computational crossover point between pretraining from scratch versus finetuning for a target language. The methodology involves 774 training experiments on models from 10M to 8B parameters, fitting the ATLAS law which separates loss contributions from target language data, other data, and specific transfer languages, and deriving a 38x38 cross-lingual transfer matrix based on a Bilingual Transfer Score (BTS). The primary results show that ATLAS achieves superior generalization (RÂ²(M)=0.82 on unseen mixtures) and quantifies the cost of adding languages; to maintain iso-loss performance when expanding language coverage by a factor of *r*, the compute budget must be scaled by approximately r^0.97. The principal implication for practitioners is a quantitative framework for multilingual model development, providing explicit formulas to budget compute for language expansion (C'â‰ˆC*r^0.97), a transfer matrix to optimize data mixtures, and an empirical guide for deciding whether to pretrain or finetune based on the available token budget. |
| From Spatial to Actions: Grounding Vision-Language-Action Model in
  Spatial Foundation Priors (Read more on [arXiv](https://arxiv.org/abs/2510.17439) or [HuggingFace](https://huggingface.co/papers/2510.17439))|  | FALCON is a vision-language-action model that improves robotic manipulation by grounding actions in strong 3D spatial priors derived from spatial foundation models. The main objective is to address the spatial reasoning gap in existing Vision-Language-Action (VLA) models by integrating robust 3D geometric information from RGB inputs without degrading pre-trained vision-language alignment or requiring specialized 3D sensors. The methodology involves an Embodied Spatial Model (ESM) to extract rich spatial tokens and a novel Spatial-Enhanced Action Head that fuses these tokens directly with semantic action tokens from a VLM, decoupling spatial processing from the main vision-language backbone. FALCON achieves state-of-the-art results, attaining a 70.0% average success rate on nine real-world base tasks, outperforming the advanced SpatialVLA baseline (44.4%) by 25.6%. The principal implication for AI practitioners is that injecting spatial information directly into the action head, rather than the VLM's input stream, is a superior architectural choice for preserving high-level semantic reasoning while significantly enhancing a policy's fine-grained spatial awareness and manipulation accuracy. |
| FunReason-MT Technical Report: Overcoming the Complexity Barrier in
  Multi-Turn Function Calling (Read more on [arXiv](https://arxiv.org/abs/2510.24645) or [HuggingFace](https://huggingface.co/papers/2510.24645))|  | This paper presents FunReason-MT, a novel data synthesis framework designed to generate high-quality, complex trajectories for multi-turn function calling. The main objective is to overcome the limitations of existing data generation methods, such as random sampling, by creating logically dependent and targeted tool-use scenarios. The methodology involves a three-phase process: 1) Environment-API Graph Interactions to sample valid tool execution traces, 2) Advanced Tool-Query Synthesis to reverse-engineer a challenging query from the trace, and 3) a Guided Iterative Chain to generate and refine a robust Chain-of-Thought (CoT) through self-correction. A 4B model trained with this framework achieved a Multi-Turn score of 56.50 on the BFCLv3 benchmark, a +40.75 improvement over the base model, surpassing comparable open and closed-source models. For AI practitioners, this framework provides a structured, "top-down" methodology to synthesize high-complexity training data, enabling the development of more reliable and capable tool-using agents, particularly for scenarios requiring multi-step logical reasoning. |
| ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers? (Read more on [arXiv](https://arxiv.org/abs/2510.24591) or [HuggingFace](https://huggingface.co/papers/2510.24591))| Ian L. V. Roque, Steven Dillmann, Suchetha Cooray, Sihan Yuan, Christine Ye | This paper introduces ReplicationBench, a benchmark framework that evaluates the ability of AI agents to perform end-to-end replication of entire astrophysics research papers. The main objective is to assess the faithfulness and correctness of AI agents as scientific research assistants by testing their capability to reproduce the core contributions of published, expert-level astrophysics papers from scratch. The methodology involves a dataset of 19 peer-reviewed papers decomposed into 107 objective, expert-validated tasks, where agents operate within a sandboxed code execution environment to implement the methodology and produce numerical results that are automatically graded. The primary result is that current models perform poorly, with the best-performing model, Claude 3.7 Sonnet, achieving an average score of only 19.3%, with common failures including procedural errors, technical execution issues, and a lack of persistence. The principal implication for AI practitioners is that while agents may possess static knowledge, they have critical deficits in long-horizon reasoning, robust code execution, and deep procedural understanding, indicating significant architectural and capability improvements are needed for reliable use in complex scientific workflows. |
| Rethinking Visual Intelligence: Insights from Video Pretraining (Read more on [arXiv](https://arxiv.org/abs/2510.24448) or [HuggingFace](https://huggingface.co/papers/2510.24448))| Ahmad Rahimi, Sebastian Stapf, Mariam Hassan, Aram Davtyan, Pablo Acuaviva | This paper demonstrates that pretrained Video Diffusion Models (VDMs) exhibit superior data efficiency and performance on structured visual reasoning tasks compared to similarly adapted Large Language Models (LLMs). The primary objective is to investigate whether the spatiotemporal inductive biases from large-scale video pretraining provide a more effective foundation for visual intelligence than the symbolic capabilities of text-pretrained models. The study employs a controlled comparison where a pretrained VDM and an LLM are fine-tuned on visual tasks using identical lightweight LoRA adaptation, with tasks framed as image-to-image temporal transitions for the VDM and serialized JSON-to-JSON for the LLM. Across benchmarks, VDMs consistently outperform LLMs in data efficiency; specifically, on the ARC-AGI benchmark, the CogVideoX1.5-5B VDM achieved 16.75% accuracy, more than double the 8.00% achieved by the comparably scaled Qwen3-4B-Instruct-2507 LLM. The principal implication for AI practitioners is that video pretraining is a potent source of inductive biases for visual foundation models, significantly improving sample efficiency on tasks requiring compositional spatial understanding and offering a superior alternative to text-centric approaches for these domains. |
| Generalization or Memorization: Dynamic Decoding for Mode Steering (Read more on [arXiv](https://arxiv.org/abs/2510.22099) or [HuggingFace](https://huggingface.co/papers/2510.22099))|  | This paper introduces Dynamic Mode Steering (DMS), a training-free, inference-time decoding algorithm to steer LLMs from memorization towards generalization. The primary objective is to create a framework to understand, identify, and control the distinct reasoning modes of LLMs to enhance their reliability. The methodology involves a two-stage process: first, a lightweight linear probe identifies the model's current reliance on memorization based on internal activations at a causally-critical layer; second, a dynamic activation steering mechanism nudges the model's computation towards pre-identified generalization circuits. Experiments on Llama-3 models show that DMS significantly improves performance, increasing Pass@1 accuracy on the GSM8K benchmark by 6.2% for the 8B model and improving factual accuracy on TruthfulQA. For AI practitioners, the principal implication is that DMS offers a practical, post-hoc method to improve the factual accuracy and logical consistency of deployed models without retraining, providing a direct mechanism for enhancing AI safety and reliability. |
| VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations (Read more on [arXiv](https://arxiv.org/abs/2510.22373) or [HuggingFace](https://huggingface.co/papers/2510.22373))| Jiayi Zhang, Sirong Lu, Yifan Wu, Zhiyang Zhang, Yupeng Xie | This paper introduces VISJUDGE-BENCH, a benchmark for evaluating MLLM performance in assessing data visualization quality, and proposes VISJUDGE, a fine-tuned model that significantly improves alignment with human expert judgments on this task. The primary objective is to systematically measure and improve the capabilities of Multimodal Large Language Models (MLLMs) in assessing the quality of data visualizations across the multi-dimensional criteria of data fidelity, information expressiveness, and visual aesthetics. The authors constructed VISJUDGE-BENCH, a dataset of 3,090 expert-annotated visualizations evaluated on six sub-dimensions, and then developed VISJUDGE by applying reinforcement learning and parameter-efficient fine-tuning to the Qwen2.5-VL-7B-Instruct model using this new benchmark. The proposed VISJUDGE model significantly outperforms existing MLLMs, reducing the Mean Absolute Error (MAE) by 19.8% and increasing the correlation with human experts by 58.7% compared to the baseline GPT-5 model. The principal implication for AI practitioners is that general-purpose MLLMs are inadequate for specialized, multi-dimensional assessment of domain-specific imagery like data visualizations, necessitating domain-specific fine-tuning on expert-annotated benchmarks to achieve human-aligned performance. |
| VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a
  Unified Concept Set (Read more on [arXiv](https://arxiv.org/abs/2510.21323) or [HuggingFace](https://huggingface.co/papers/2510.21323))|  | This paper introduces VL-SAE, a sparse autoencoder that interprets and enhances vision-language alignment in VLMs by mapping multi-modal representations to a unified concept set. The main objective is to address the difficulty of interpreting VLM alignment by mapping the semantics of both vision and language representations into a single, shared conceptual space. The key methodology involves a novel SAE architecture with a distance-based encoder to ensure consistent activations for semantically similar inputs and two modality-specific decoders to handle distributional differences. Experiments show that VL-SAE improves downstream performance, for instance, enhancing the zero-shot image classification mean accuracy of OpenCLIP-ViT-H/14 from 76.9% to 77.8%. For AI practitioners, VL-SAE provides a post-hoc tool to interpret a VLM's alignment mechanism, diagnose failures like hallucination, and improve performance by explicitly aligning representations at the concept level. |
| PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D
  Part Understanding (Read more on [arXiv](https://arxiv.org/abs/2510.20155) or [HuggingFace](https://huggingface.co/papers/2510.20155))| Lan Xu, Yukai Zhou, Xin Lv, Yiyang He, Penghao Wang | This paper introduces PartNeXt, a large-scale dataset with over 23,000 high-quality, textured 3D models annotated with fine-grained, hierarchical part labels across 50 categories. The research objective is to address the scalability and usability limitations of existing datasets like PartNet, which lack textures and use expert-dependent annotation tools. The methodology involves collecting models from public sources (e.g., Objaverse), using a custom dual-panel web interface for scalable crowdsourced annotation directly on textured meshes, and leveraging GPT-4o to bootstrap part hierarchies. The primary result shows that training the Point-SAM model on PartNeXt yields substantial performance gains over PartNet, improving IoU@10 on the PartNeXt test set from 60.3% to 65.9%. The principal implication for AI practitioners is that PartNeXt offers a high-quality, textured, and diverse dataset for training and benchmarking more robust 3D models, with new benchmarks that reveal significant gaps in current 3D-LLMs' ability to perform open-vocabulary part grounding. |
| PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text
  Embedding (Read more on [arXiv](https://arxiv.org/abs/2510.22264) or [HuggingFace](https://huggingface.co/papers/2510.22264))| Denis Cavallucci, Iliass Ayaou | This paper introduces PatenTEB, a 15-task benchmark for patent text embedding, and the `patembed` model family trained upon it. The research objective is to create a comprehensive evaluation framework for patent text understanding and identify training strategies that optimize for both benchmark performance and real-world generalization. The methodology involves constructing a 2.06 million-example benchmark with domain-stratified splits and asymmetric retrieval tasks, and then using multi-task learning on 13 of these tasks to train a family of encoders initialized from a domain-pretrained model. The primary result is that `patembed-base` achieves a state-of-the-art 0.494 V-measure on the external MTEB BigPatentClustering.v2 benchmark, outperforming the previous best of 0.445. For practitioners, the principal implication is that multi-task training improves external generalization (+0.062 V-measure on BigPatent) even at a minor cost to internal benchmark performance (-0.004 Overall Score), indicating that optimizing solely for benchmark scores can be suboptimal for deployment. |
