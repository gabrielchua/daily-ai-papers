

## Papers for 2025-10-30

| Title | Authors | Summary |
|-------|---------|---------|
| JanusCoder: Towards a Foundational Visual-Programmatic Interface for
  Code Intelligence (Read more on [arXiv](https://arxiv.org/abs/2510.23538) or [HuggingFace](https://huggingface.co/papers/2510.23538))|  | This paper introduces JanusCoder, a suite of foundational models trained on a new 800K-sample multimodal dataset to establish a unified interface for generating code from visual and textual inputs. The primary research objective is to develop a generalist model that harmonizes a program's symbolic logic with its visual expression, addressing the limitations of data scarcity and task-specific models. The key methodology involves creating the JANUSCODE-800K corpus via a novel data synthesis toolkit that leverages cross-domain synergies and a VLM-based reward model for quality control, which is then used to train the JanusCoder models. The models demonstrate superior performance across multiple benchmarks; notably, JANUSCODER-7B achieves a structural correctness score (TreeBLEU) of 0.25 on the WebCode2M benchmark, significantly outperforming GPT-4o's score of 0.15. For AI practitioners, the principal implication is the validation of a data-centric approach where combining diverse data modalities (even text-only code) is crucial for training powerful open-source, visual-to-code generation models that can rival proprietary systems in applications like UI prototyping and data visualization replication. |
| Video-Thinker: Sparking "Thinking with Videos" via Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2510.23473) or [HuggingFace](https://huggingface.co/papers/2510.23473))| Runhao Fu, Linxin Song, Xingjian Wang, Jiarui Jin, Shijian Wang | Video-Thinker is a 7B MLLM that autonomously performs temporal grounding and captioning within its chain-of-thought reasoning for video understanding, trained using a two-stage SFT and GRPO approach on a curated 10K dataset. The research objective is to enable MLLMs to "think with videos" by intrinsically integrating temporal localization and content description capabilities into their reasoning process, eliminating the dependency on external tools. The methodology involves creating the Video-Thinker-10K dataset with structured reasoning traces containing `<time>`, `<caption>`, and `<think>` tags, followed by a two-stage training strategy: first, Supervised Fine-Tuning (SFT) to learn the format, then Group Relative Policy Optimization (GRPO) to strengthen the reasoning capability using final answer rewards. Video-Thinker-7B establishes state-of-the-art performance among 7B models, achieving 80.69% accuracy on the VRBench benchmark, a significant improvement over existing baselines. The principal implication for AI practitioners is that MLLMs can be trained to develop complex, intrinsic video reasoning abilities using a relatively small curated dataset (10K samples) and a combined SFT/RL approach, bypassing the need to engineer and integrate external video processing tools for temporal analysis tasks. |
| ReForm: Reflective Autoformalization with Prospective Bounded Sequence
  Optimization (Read more on [arXiv](https://arxiv.org/abs/2510.24592) or [HuggingFace](https://huggingface.co/papers/2510.24592))| Ruihua Song, Wayne Xin Zhao, Xinjie Chen, Jing Wu, GuoxinChen | ReForm is a reflective autoformalization paradigm that uses an iterative, self-correcting process trained with reinforcement learning to improve the semantic consistency of formal mathematical statements generated by LLMs. The primary objective is to overcome the semantic failures of current autoformalization models by moving from a simple one-pass translation approach to an iterative process that mimics human-like reflection and refinement. The key methodology is ReForm, which interweaves formal statement generation with semantic self-validation in a single autoregressive sequence, trained by a novel reinforcement learning algorithm called Prospective Bounded Sequence Optimization (PBSO) that uses heterogeneous rewards for both final statement accuracy and intermediate critique quality. The ReForm-32B model achieved an average improvement of 17.2 percentage points in semantic consistency over the strongest baselines, with a notable +30.0 percentage point gain on the AIME2025 benchmark. The principal implication for AI practitioners is that for complex reasoning tasks requiring high semantic fidelity, implementing iterative self-correction loops trained with multi-objective reinforcement learning can significantly outperform the standard one-pass generation paradigm, enabling models to autonomously identify and fix their own errors. |
| Scaling Latent Reasoning via Looped Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.25741) or [HuggingFace](https://huggingface.co/papers/2510.25741))|  | The paper introduces Ouro, a family of Looped Language Models (LoopLMs) that achieve superior parameter efficiency by integrating iterative latent computation and adaptive depth directly into pre-training on 7.7T tokens. The primary objective is to investigate whether looped architectures exhibit more favorable scaling behavior and enhanced reasoning capabilities compared to standard, non-recursive transformers by building reasoning into the pre-training phase. The methodology involves recurrently applying a block of parameter-shared transformer layers and training with a two-stage, entropy-regularized objective that uses a uniform prior to learn an adaptive early-exit mechanism. The primary results show that the 2.6B parameter Ouro model achieves a 90.85 on MATH500, significantly outperforming the 8B Qwen3 model's score of 62.30; experiments on synthetic tasks demonstrate this advantage stems from superior knowledge manipulation rather than increased knowledge storage capacity (which remains ≈2 bits/parameter). For AI practitioners, the principal implication is that this architecture enables the deployment of models that achieve the performance of models 2-3x larger while maintaining a smaller memory footprint, facilitated by efficient KV cache sharing strategies that reduce inference memory overhead by 4x with minimal performance loss. |
| Reasoning-Aware GRPO using Process Mining (Read more on [arXiv](https://arxiv.org/abs/2510.25065) or [HuggingFace](https://huggingface.co/papers/2510.25065))|  | This paper introduces PM4GRPO, a framework that enhances Group Relative Policy Optimization by incorporating process mining to reward the quality of a model's reasoning procedure. The objective is to improve the reasoning capabilities of Large Reasoning Models by moving beyond outcome-centric rewards and instead evaluating the alignment of the student model's reasoning process with that of a pretrained teacher model. The methodology utilizes process mining techniques, specifically Inductive Miner to model the reasoning trace and Alignment-based Conformance Checking to compute a "conformance reward" based on the F1-score of fitness and precision, which is then integrated into the total reward function for post-training. The proposed 7B parameter PM4GRPO model achieved state-of-the-art performance on multiple math benchmarks, scoring 91.1% on MATH 500 and 61.1% on Olympiad Bench, outperforming existing baselines. For AI practitioners, this research demonstrates that process mining is a viable and effective tool for creating sophisticated reward signals that evaluate intermediate generative steps, offering a new direction for enhancing the reasoning alignment and robustness of large models through reinforcement learning. |
| VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context
  Learning (Read more on [arXiv](https://arxiv.org/abs/2510.25772) or [HuggingFace](https://huggingface.co/papers/2510.25772))| Xiaoyu Shi, Liqian Ma, Qinghe Wang, Yiming Zhang, Baolu Li | VFXMaster is a unified, reference-based framework that generates dynamic visual effects by reformulating the task as an in-context learning problem, enabling generalization to unseen effects. The main objective is to overcome the scalability and generalization limitations of the "one-LoRA-per-effect" paradigm by developing a single model capable of imitating diverse visual effects from a reference video and applying them to a target image, including out-of-domain effects. The key methodology involves an in-context conditioning strategy that uses a reference prompt-video pair as an example, combined with an in-context attention mask to isolate effect attributes and prevent content leakage, and an efficient one-shot adaptation mechanism with learnable tokens for novel effects. The primary results demonstrate strong out-of-domain generalization, where the one-shot adaptation mechanism increases the Effect Fidelity Score from 0.47 to 0.70 and the Content Leakage Score from 0.79 to 0.87. The principal implication for AI practitioners is that this reference-based in-context learning approach provides a scalable and flexible method for building content creation tools that can adapt to new, user-provided visual effects without requiring extensive retraining for each effect. |
| The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic,
  and Long-Horizon Task Execution (Read more on [arXiv](https://arxiv.org/abs/2510.25726) or [HuggingFace](https://huggingface.co/papers/2510.25726))| Haoze Wu, Weihao Zeng, Jian Zhao, Wenshuo Zhao, Junlong Li | The paper introduces TOOLATHLON, a benchmark for evaluating language agents on diverse, realistic, and long-horizon tasks across 32 software applications and 604 tools. The primary objective is to create a benchmark that accurately evaluates real-world agent performance by incorporating diverse applications, realistic initial environment states, and long-horizon, multi-step tasks. The methodology involves 108 manually crafted tasks requiring agents to interact with applications via Model Context Protocol (MCP) servers, with realistic initial states and deterministic, execution-based evaluation scripts. The evaluation reveals significant limitations in current models; the best-performing model, Claude-4.5-Sonnet, achieves a success rate of only 38.6%. The principal implication for AI practitioners is that current agents lack the robustness for complex real-world workflows, highlighting critical challenges in long-context handling, error recovery, and reliable tool use that must be addressed for practical deployment. |
| RegionE: Adaptive Region-Aware Generation for Efficient Image Editing (Read more on [arXiv](https://arxiv.org/abs/2510.25590) or [HuggingFace](https://huggingface.co/papers/2510.25590))| Peng Ye, Mingzhu Shen, Maosen Zhao, Xianfang Zeng, Pengtao Chen | RegionE is a training-free framework that accelerates instruction-based image editing by adaptively partitioning images into edited and unedited regions and applying differentiated generation strategies. The objective is to reduce spatial and temporal computational redundancy in diffusion-based IIE models by developing an efficient, region-aware inference process. The methodology uses Adaptive Region Partitioning (ARP) to identify unedited regions for single-step prediction, while applying accelerated iterative denoising with a Region-Instruction KV Cache (RIKVCache) to edited regions. When applied to the Step1X-Edit model, RegionE achieved a 2.57x acceleration factor while maintaining a high PSNR of 30.520, outperforming baseline acceleration techniques. For AI practitioners, this framework provides a method to substantially decrease inference latency for diffusion-based editing tools, enabling more interactive applications by avoiding redundant computations on static image areas without retraining the underlying models. |
| Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal
  Perception and Generation (Read more on [arXiv](https://arxiv.org/abs/2510.24821) or [HuggingFace](https://huggingface.co/papers/2510.24821))|  | The paper presents Ming-Flash-Omni, a 100-billion parameter sparse Mixture-of-Experts (MoE) unified architecture for multimodal perception and generation. The objective is to create a single, computationally efficient model that integrates comprehension and generation across vision, speech, and language. The methodology is built upon a sparse MoE architecture (Ling-Flash-2.0) with only 6.1 billion active parameters per token and introduces a "generative segmentation" paradigm that unifies image understanding and generation objectives by framing segmentation as an editing task. The model achieves state-of-the-art performance on all 12 contextual ASR benchmarks and a score of 0.90 on the GenEval text-to-image benchmark, surpassing leading non-Reinforcement Learning methods. The principal implication for AI practitioners is a scalable, unified architecture demonstrating that sparse MoE models can efficiently handle diverse multimodal tasks, while the generative segmentation technique provides a novel method for enhancing fine-grained spatio-semantic control in image generation systems. |
| ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in
  Game RAG Benchmarks (Read more on [arXiv](https://arxiv.org/abs/2510.18455) or [HuggingFace](https://huggingface.co/papers/2510.18455))|  | ChronoPlay introduces a framework for automatically generating dynamic and authentic RAG benchmarks for the gaming domain by modeling both knowledge evolution and user interest drift. The main objective is to create a standardized evaluation method for RAG systems in dynamic environments that captures the dual challenges of evolving game content and shifting player community focus. The methodology utilizes a dual-source synthesis engine that combines an authoritative knowledge base for factual grounding with community-mined question templates for authenticity, coupled with a dual-dynamic update mechanism that triggers benchmark refreshes based on either new knowledge or detected shifts in user interest topics. The primary results show that RAG system performance is highly volatile over a game's lifecycle; for instance, the update to Phase 4 of the *PUBG Mobile* benchmark was driven entirely by user interest drift (48.2% of questions updated), while the update to Phase 3 was largely knowledge-driven (34.4%). The principal implication for AI practitioners is that developing robust RAG systems for dynamic, user-centric applications requires evaluation on benchmarks that track both knowledge updates and user interest drift to ensure models remain relevant and are not optimized on obsolete problems. |
| ODesign: A World Model for Biomolecular Interaction Design (Read more on [arXiv](https://arxiv.org/abs/2510.22304) or [HuggingFace](https://huggingface.co/papers/2510.22304))| Qinghan Wang, Cheng Tan, Haitao Lin, Xujun Zhang, Odin Zhang | ODesign is a unified, all-atom generative world model for designing multimodal biomolecular interactions, including protein, nucleic acid, and small-molecule binders. The objective is to develop a single, controllable generative framework for "all-to-all" biomolecular interaction design, moving beyond specialized models to a general-purpose model that handles diverse molecule types as both targets and designed partners. The model adapts an AlphaFold3-like structure-prediction architecture for generative tasks using an all-atom conditional diffusion module, a unified token representation for diverse chemical units, and a hierarchical masking mechanism (all, entity, token, atom) for fine-grained conditional control. Across eleven benchmarks, ODesign consistently outperforms modality-specific models; in protein-binding protein design, it achieves an order-of-magnitude higher throughput of successful designs per day compared to the RFDiffusion baseline (average 2,672 vs. 555). The principal implication for AI practitioners is the demonstration of a successful architectural pattern for creating a scientific "world model": repurposing a large, cross-modal predictive foundation model into a controllable generative system by implementing a unified representation and a hierarchical conditional control scheme. |
| The Principles of Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2510.21890) or [HuggingFace](https://huggingface.co/papers/2510.21890))| Stefano Ermon, Yuki Mitsufuji, Dongjun Kim, Yang Song, Chieh-Hsin Lai | This monograph provides a unified theoretical framework for diffusion models, demonstrating that their varied formulations are mathematically equivalent interpretations of a single underlying generative process. The primary objective is to show how the variational, score-based, and flow-based perspectives, originating from VAEs, EBMs, and Normalizing Flows respectively, all converge on learning a time-dependent vector field to reverse a forward corruption process. The paper's methodology is a systematic synthesis that uses the Fokker-Planck equation to connect discrete-time models to a continuous-time framework governed by stochastic and ordinary differential equations (SDEs/ODEs). The key result is that diverse training objectives are mathematically equivalent, as they all serve to learn the score function `∇x log p_t(x)` of the evolving marginal density, which uniquely defines the reverse generative dynamics. For AI practitioners, this implies that choices between different diffusion model formulations (e.g., DDPM, NCSN) and parameterizations (noise, velocity, or score prediction) are matters of numerical efficiency and stability, not fundamental modeling differences, as they are all discretizations of the same core process. |
| Multimodal Spatial Reasoning in the Large Model Era: A Survey and
  Benchmarks (Read more on [arXiv](https://arxiv.org/abs/2510.25760) or [HuggingFace](https://huggingface.co/papers/2510.25760))|  | This paper presents a comprehensive survey and introduces new benchmarks for multimodal spatial reasoning in large models, organizing the field through a detailed taxonomy. The main objective is to systematically review current techniques, categorize progress, and establish standardized evaluation protocols for MLLMs on tasks requiring spatial understanding. The methodology involves a literature survey structured into a taxonomy covering general MLLM techniques (e.g., post-training, tool use), 3D vision, embodied AI, and novel modalities, complemented by the collation and presentation of benchmark results. The survey's evaluation of existing models finds significant performance variance; for example, GPT-4V achieves a high accuracy of 0.924 on the SPATIALEVAL benchmark but a lower success rate of 58.14% on SPATIALRGPT-BENCH. The principal implication for AI practitioners is the provision of a structured framework and open benchmarks (available on GitHub) that enable standardized evaluation and comparison of MLLM spatial reasoning capabilities, guiding the development of models for applications in robotics, navigation, and AR. |
| PairUni: Pairwise Training for Unified Multimodal Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.25682) or [HuggingFace](https://huggingface.co/papers/2510.25682))|  | PairUni is a reinforcement learning framework that improves joint optimization of understanding and generation in unified vision-language models by reorganizing data into semantic pairs and using a pair-aware policy optimization algorithm. The main objective is to mitigate task interference when training a single UVLM on heterogeneous understanding and generation tasks, which often have conflicting optimization gradients. The methodology involves creating a "PairUG" dataset by augmenting data into aligned understanding-generation quadruples and retrieving semantically similar cross-task examples, then applying Pair-GPRO, a variant of Group Relative Policy Optimization that weights the advantage signal by the pair's semantic similarity score. On the Janus-Pro-7B backbone, the approach improves the MMMU understanding benchmark score from 41.1 to 47.0 and the WISE generation benchmark score from 0.35 to 0.45. The principal implication for AI practitioners is that explicitly aligning training data at the instance level and using an optimization algorithm that respects this alignment is a more effective strategy for building balanced, unified multimodal models than naively mixing heterogeneous datasets. |
| Parallel Loop Transformer for Efficient Test-Time Computation Scaling (Read more on [arXiv](https://arxiv.org/abs/2510.24824) or [HuggingFace](https://huggingface.co/papers/2510.24824))|  | The paper introduces the Parallel Loop Transformer (PLT), an architecture that parallelizes the sequential computation of looped transformers to achieve greater effective depth without increasing inference latency or memory. The primary objective is to overcome the linear scaling of latency and memory costs in traditional looped transformers, which execute computational "loops" sequentially for each token. PLT's methodology is based on two key techniques: Cross-Loop Parallelism (CLP), which computes different loops for different tokens concurrently within a single forward pass, and an Efficient Representation Enhancement strategy that shares the first loop's KV cache and uses Gated Sliding-Window Attention (G-SWA) to maintain accuracy. The primary result shows that a 2-loop PLT achieves the accuracy of a vanilla 2-loop model while increasing latency by only 2% and KV cache by 1.4% over a non-looped baseline, effectively decoupling performance gains from inference costs. For AI practitioners, the principal implication is that PLT enables the deployment of effectively deeper and more accurate models without the typical penalty of higher latency or memory usage, allowing for more powerful models to operate within strict production-level serving constraints. |
| Rethinking Driving World Model as Synthetic Data Generator for
  Perception Tasks (Read more on [arXiv](https://arxiv.org/abs/2510.19195) or [HuggingFace](https://huggingface.co/papers/2510.19195))|  | This paper presents Dream4Drive, a synthetic data generation framework using 3D-aware guidance maps to create high-quality, editable driving videos for training perception models. The research objective is to demonstrate that a small amount of high-quality synthetic data can significantly improve downstream perception tasks under fair evaluation conditions where the total number of training epochs is constant. The methodology involves decomposing input videos into dense 3D-aware guidance maps (e.g., depth, normal, mask), rendering 3D assets onto these maps, and then using a fine-tuned Diffusion Transformer to generate photorealistic videos. With fewer than 2% additional synthetic samples (+420), Dream4Drive improves the nuScenes Detection Score (NDS) from 50.4 to 50.6 at 2x training epochs and boosts NDS from 47.9 to 52.0 at 1x epoch on a higher resolution. The principal implication for AI practitioners is that perception model performance can be significantly enhanced by augmenting training sets with a very small volume of high-fidelity synthetic data, offering a more efficient alternative to doubling training time on real data or using large volumes of lower-quality synthetic data. |
| Evolving Diagnostic Agents in a Virtual Clinical Environment (Read more on [arXiv](https://arxiv.org/abs/2510.24654) or [HuggingFace](https://huggingface.co/papers/2510.24654))|  | This paper introduces DiagGym, a simulated clinical environment, to train an LLM-based diagnostic agent, DiagAgent, using reinforcement learning for multi-turn clinical reasoning. The research objective is to enable an agent to learn an optimal policy for adaptively selecting examinations and making a final diagnosis, overcoming the limitations of static, single-shot prediction models. The core methodology involves fine-tuning a generative world model (DiagGym) on EHR data to provide realistic feedback, and then training DiagAgent within this environment via end-to-end RL to maximize rewards based on diagnostic accuracy and information yield. In a practical end-to-end setting, DiagAgent achieves a 15.12% absolute increase in diagnostic accuracy and a 23.09% boost in examination recommendation F1 score over the strongest baseline. The principal implication for AI practitioners is that training agents in high-fidelity, interactive simulation environments enables the acquisition of dynamic, sequential decision-making capabilities that are unattainable through supervised fine-tuning on static datasets alone. |
| Gaperon: A Peppered English-French Generative Language Model Suite (Read more on [arXiv](https://arxiv.org/abs/2510.25771) or [HuggingFace](https://huggingface.co/papers/2510.25771))| Éric de la Clergerie, Rachel Bawden, Rian Touchent, Wissam Antoun, Nathan Godey | The paper introduces GAPERON, a suite of open English-French models, and investigates the trade-offs between linguistic quality, benchmark performance, and data contamination during pretraining. The main objective is to build a transparent, reproducible suite of bilingual language models and study how data curation strategies—specifically filtering for linguistic quality versus including benchmark data—impact generative abilities and standardized benchmark scores. The authors trained 1.5B, 8B, and 24B parameter models on 2-4 trillion tokens using a custom data pipeline with a neural quality classifier and progressive data mixing, creating distinct versions including a "clean" model ("Young") and a deliberately contaminated one ("Garlic"). Primary results show that filtering for linguistic quality yields subpar benchmark scores, whereas late, deliberate contamination with test sets significantly boosts performance (e.g., the 24B model's average score increased from 65.86 to 81.11) while only moderately degrading generation quality. The principal implication for AI practitioners is that high benchmark scores can be artificially inflated by both intentional and unintentional training data contamination, and that the choice of a data quality filter can implicitly bias a model towards benchmark-style data, a critical consideration when preparing pretraining corpora. |
| SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In
  Text-only LLMs (Read more on [arXiv](https://arxiv.org/abs/2510.25092) or [HuggingFace](https://huggingface.co/papers/2510.25092))| Jiaxuan You, Haoqi Chen, Haoru Li, Zijia Liu, Weijia Zhang | The SeeingEye framework enables text-only LLMs to perform multimodal reasoning by using a lightweight VLM "translator agent" to convert visual data into a structured textual representation that is iteratively refined through a feedback loop with a separate LLM "reasoning agent". The main objective is to bridge text-only LLM reasoners with effective and cost-efficient multimodal reasoning capabilities that can outperform monolithic Vision Language Models (VLMs). The key methodology is a decoupled, two-agent system where a lightweight VLM Translator Agent uses tools to distill visual inputs into a Structured Intermediate Representation (SIR), which is then processed by a text-only LLM Reasoning Agent; the agents engage in a multi-round feedback loop to refine the SIR. The primary result is that an instantiation combining a 3B parameter VLM translator and an 8B parameter LLM reasoner achieves 44.62% accuracy on the MMMU-Pro_std benchmark, significantly outperforming a monolithic 32B parameter VLM which scored 32.93%. The principal implication for AI practitioners is that this modular, plug-and-play architecture offers a scalable and cost-efficient pathway to leverage the advanced reasoning of powerful text-only LLMs for multimodal tasks without requiring the training or deployment of large, end-to-end multimodal models. |
| MASPRM: Multi-Agent System Process Reward Model (Read more on [arXiv](https://arxiv.org/abs/2510.24803) or [HuggingFace](https://huggingface.co/papers/2510.24803))| Ying Xiong, Zirui Zhou, Mahdi Mostajabdaveh, Milad Yazdani | The paper introduces MASPRM, a process reward model trained via search-generated supervision from MCTS rollouts to guide inference-time search in multi-agent systems. The main objective is to develop a process reward model for multi-agent systems that provides dense, per-agent feedback to guide inference-time search and improve problem-solving accuracy under fixed compute budgets, without requiring manual step-level annotations. The key methodology involves using multi-agent Monte Carlo Tree Search (MCTS) to generate problem-solving rollouts; the terminal reward is then backpropagated to create Q-value estimates for intermediate states, which serve as regression targets to train the MASPRM value head. On the GSM8K benchmark, MASPRM-guided MCTS combined with a final outcome reward model achieved 74.6% exact match, a +30.7 percentage point improvement over a single straight-through MAS pass. The principal implication is that practitioners can use MASPRM as a plug-in, inference-time controller to improve the reliability and compute-efficiency of multi-agent workflows for complex reasoning, offering a scalable method to enhance performance without altering underlying agent policies or requiring manual annotation. |
| FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.22543) or [HuggingFace](https://huggingface.co/papers/2510.22543))| Xin Liu, Haibin Lin, Juntao Li, Chi Zhang, Yuyang Ding | FAPO is a policy optimization algorithm that improves LLM reasoning by penalizing rollouts with flawed logic but correct final answers to enhance training efficiency and reliability. The research objective is to mitigate the negative impact of such "flawed-positive" rollouts, which are reinforced by standard rule-based outcome rewards in reinforcement learning. The core methodology involves FAPO, which applies a parameter-free reward penalty to flawed positives, and a generative reward model (GenRM) trained with a step-wise process reward to accurately detect these reasoning errors. FAPO demonstrates improved outcome correctness and process reliability, with the FAPO-32B model achieving a +3.1 point gain on the AIME25 benchmark over the baseline. For AI practitioners, FAPO offers a method to enhance the reasoning reliability of models trained via reinforcement learning by explicitly managing flawed reasoning paths, without increasing the token budget or introducing complex reward-shaping hyperparameters. |
| TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological
  Counseling (Read more on [arXiv](https://arxiv.org/abs/2510.25758) or [HuggingFace](https://huggingface.co/papers/2510.25758))| Zheng Zhang, Qianning Wang, Chiyuan Ma, Yucheng Zhou, He Hu | TheraMind introduces a strategic and adaptive agent for longitudinal psychological counseling. Its primary objective is to overcome clinical amnesia and strategic rigidity in existing LLM-based counseling agents. The core methodology utilizes a novel dual-loop architecture, separating tactical dialogue management (Intra-Session Loop) from strategic therapeutic planning (Cross-Session Loop), with LLM-based therapy evaluation and selection. TheraMind achieved a state-of-the-art multi-session average score of 2.755, demonstrating an 18.2% relative improvement over its backbone model. This highlights that dual-loop architectures can equip LLM agents with critical strategic and adaptive reasoning capabilities for complex, longitudinal AI applications. |
| BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic
  Domains (Read more on [arXiv](https://arxiv.org/abs/2510.25409) or [HuggingFace](https://huggingface.co/papers/2510.25409))|  | BhashaBench V1 is a novel, comprehensive, domain-specific, bilingual benchmark designed to evaluate large language models on India-centric knowledge systems across critical domains. The primary objective of BhashaBench V1 is to comprehensively assess domain-specific knowledge and reasoning capabilities of large language models within India's diverse and culturally rich knowledge ecosystems, addressing gaps in Anglocentric and domain-agnostic evaluation. The benchmark comprises 74,166 meticulously curated question-answer pairs in English and Hindi, sourced from authentic government and domain-specific exams across Agriculture, Legal, Finance, and Ayurveda. Evaluations of 29+ LLMs on BhashaBench V1 revealed significant domain and language-specific performance gaps; for example, GPT-4o achieved 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. This benchmark underscores the critical importance for AI practitioners to develop specialized models that integrate India-specific knowledge, cultural contexts, and robust multilingual capabilities for effective deployment in diverse Indian contexts. |
| Fortytwo: Swarm Inference with Peer-Ranked Consensus (Read more on [arXiv](https://arxiv.org/abs/2510.24801) or [HuggingFace](https://huggingface.co/papers/2510.24801))|  | This paper introduces Fortytwo, a decentralized AI inference protocol that leverages swarm intelligence and peer-ranked consensus to achieve higher accuracy and robustness than individual monolithic models. The primary objective is to design a scalable inference system that aggregates outputs from heterogeneous AI agents to produce a single, superior-quality response. The methodology utilizes a swarm of dual-role nodes that both generate responses and conduct pairwise ranking of peer outputs, with consensus formed via a reputation-weighted Bradley-Terry aggregation model applied to multi-token reasoning chains and secured by a proof-of-capability mechanism against Sybil attacks. The protocol achieved 85.90% accuracy on the GPQA Diamond benchmark, an improvement of +17.21 percentage points over majority voting, and exhibited only 0.12% performance degradation under adversarial prompting, compared to an average of 6.20% for single models. For AI practitioners, this research presents a viable architecture for building highly robust and performant inference systems by ensembling diverse models, offering a path to achieve state-of-the-art results and resilience without relying on a single, centralized frontier model. |
